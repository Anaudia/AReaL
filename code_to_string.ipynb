{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89a1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def gather_py_files_with_content(\n",
    "    root: str | Path,\n",
    "    max_chars: int = 100_000,\n",
    "    separator: str = \"\\n\\n\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Recursively finds all .py files under `root`, but if a file's content\n",
    "    is longer than `max_chars` characters it will be skipped. At the end,\n",
    "    prints a list of skipped files with their character counts.\n",
    "\n",
    "    Returns a single string where each included file is prefixed by its full path:\n",
    "        ==== /full/path/to/module.py ====\n",
    "        <file contents>\n",
    "    \"\"\"\n",
    "    root = Path(root).expanduser().resolve()\n",
    "    segments: list[str] = []\n",
    "    ignored: list[tuple[str, int]] = []\n",
    "\n",
    "    for py_file in root.rglob(\"*.py\"):\n",
    "        # read the file (with a UTF-8 -> Latin-1 fallback)\n",
    "        try:\n",
    "            text = py_file.read_text(encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            text = py_file.read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "\n",
    "        length = len(text)\n",
    "        if length > max_chars:\n",
    "            ignored.append((str(py_file), length))\n",
    "            continue\n",
    "\n",
    "        header = f\"==== {py_file} ====\"\n",
    "        segments.append(header)\n",
    "        segments.append(text)\n",
    "\n",
    "    # print out ignored files\n",
    "    if ignored:\n",
    "        print(\"Ignored files (exceeded character limit of \"\n",
    "              f\"{max_chars}):\")\n",
    "        for path, count in ignored:\n",
    "            print(f\"{path}: {count:,} characters\")\n",
    "\n",
    "    return separator.join(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc39896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== /home/roeseler/github/AReaL/realhf/version.py ====\n",
      "\n",
      "import subprocess\n",
      "from pathlib import Path\n",
      "\n",
      "__version__ = \"0.3.0-dev\"\n",
      "__branch__ = \"\"\n",
      "__commit__ = \"\"\n",
      "__is_dirty__ = False\n",
      "\n",
      "try:\n",
      "    __branch__ = (\n",
      "        subprocess.check_output(\n",
      "            [\"git\", \"branch\", \"--show-current\"],\n",
      "            stderr=subprocess.DEVNULL,\n",
      "            cwd=Path(__file__).parent,\n",
      "        )\n",
      "        .decode(\"utf-8\")\n",
      "        .strip()\n",
      "    )\n",
      "    __commit__ = (\n",
      "        subprocess.check_output(\n",
      "            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n",
      "            stderr=subprocess.DEVNULL,\n",
      "            cwd=Path(__file__).parent,\n",
      "        )\n",
      "        .decode(\"utf-8\")\n",
      "        .strip()\n",
      "    )\n",
      "    __is_dirty__ = False\n",
      "    try:\n",
      "        subprocess.check_call(\n",
      "            [\"git\", \"diff-index\", \"--quiet\", \"HEAD\", \"--\"],\n",
      "            stderr=subprocess.DEVNULL,\n",
      "            cwd=Path(__file__).parent,\n",
      "        )\n",
      "    except subprocess.CalledProcessError:\n",
      "        __is_dirty__ = True\n",
      "except (subprocess.CalledProcessError, FileNotFoundError):\n",
      "    pass\n",
      "\n",
      "\n",
      "def get_full_version() -> str:\n",
      "    version = __version__\n",
      "    if __commit__ != \"\":\n",
      "        version = f\"{__version__}-{__commit__}\"\n",
      "    if __is_dirty__:\n",
      "        version = f\"{version}-dirty\"\n",
      "    return version\n",
      "\n",
      "\n",
      "def get_full_version_with_dirty_description() -> str:\n",
      "    version = get_full_version()\n",
      "    if __is_dirty__:\n",
      "        version = (\n",
      "            f\"{version} ('-dirty' means there are uncommitted code changes in git)\"\n",
      "        )\n",
      "    return version\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/utils.py ====\n",
      "\n",
      "def download_from_huggingface(\n",
      "    repo_id: str, filename: str, revision: str = \"main\", repo_type: str = \"dataset\"\n",
      ") -> str:\n",
      "    \"\"\"\n",
      "    Download a file from a HuggingFace Hub repository.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        from huggingface_hub import hf_hub_download\n",
      "    except ImportError:\n",
      "        raise ImportError(\n",
      "            \"Please install huggingface_hub to use this function: pip install huggingface_hub\"\n",
      "        )\n",
      "\n",
      "    return hf_hub_download(\n",
      "        repo_id=repo_id,\n",
      "        filename=filename,\n",
      "        revision=revision,\n",
      "        repo_type=repo_type,\n",
      "    )\n",
      "\n",
      "\n",
      "def load_hf_or_local_file(path: str) -> str:\n",
      "    \"\"\"\n",
      "    Load a file from a HuggingFace Hub repository or a local file.\n",
      "    hf://<org>/<repo>/<filename>\n",
      "    hf://<org>/<repo>@<revision>/<filename>\n",
      "\n",
      "    e.g,\n",
      "    hf-dataset://inclusionAI/AReaL-RL-Data/data/boba_106k_0319.jsonl\n",
      "    =>\n",
      "    repo_type = dataset\n",
      "    repo_id = inclusionAI/AReaL-RL-Data\n",
      "    filename = data/boba_106k_0319.jsonl\n",
      "    revision = main\n",
      "    =>\n",
      "    /root/.cache/huggingface/hub/models--inclusionAI--AReaL-RL-Data/data/boba_106k_0319.jsonl\n",
      "    \"\"\"\n",
      "    if path.startswith(\"hf://\") or path.startswith(\"hf-dataset://\"):\n",
      "        repo_type = \"dataset\" if path.startswith(\"hf-dataset://\") else \"model\"\n",
      "        hf_path = path.strip().split(\"://\")[1]\n",
      "        hf_org, hf_repo, filename = hf_path.split(\"/\", 2)\n",
      "        repo_id = f\"{hf_org}/{hf_repo}\"\n",
      "        revision = \"main\"\n",
      "        if \"@\" in repo_id:\n",
      "            repo_id, revision = repo_id.split(\"@\", 1)\n",
      "        return download_from_huggingface(repo_id, filename, revision)\n",
      "    return path\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# Initialize preset config before all submodules.\n",
      "\n",
      "from .version import __version__\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/stats_tracker.py ====\n",
      "\n",
      "from collections import defaultdict\n",
      "from enum import Enum, auto\n",
      "from typing import Dict\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "\n",
      "class ReduceType(Enum):\n",
      "    AVG = auto()\n",
      "    SUM = auto()\n",
      "    MIN = auto()\n",
      "    MAX = auto()\n",
      "    SCALAR = auto()\n",
      "\n",
      "\n",
      "MOE_AUX_LOSSES = {}\n",
      "\n",
      "\n",
      "class DistributedStatsTracker:\n",
      "    def __init__(self, name: str = \"\"):\n",
      "        self.scope_stack = []\n",
      "        if name:\n",
      "            self.scope_stack.append(name.strip(\"/\"))\n",
      "        self.denominators = {}  # key -> denominator key\n",
      "        self.reduce_types = {}  # key -> ReduceType\n",
      "\n",
      "        self.stats = defaultdict(list)\n",
      "\n",
      "    def scope(self, name):\n",
      "        \"\"\"Context manager for hierarchical scoping\"\"\"\n",
      "        return self.Scope(self, name)\n",
      "\n",
      "    class Scope:\n",
      "        def __init__(self, tracker, name):\n",
      "            self.tracker = tracker\n",
      "            self.name = name.strip(\"/\")\n",
      "\n",
      "        def __enter__(self):\n",
      "            self.tracker.scope_stack.append(self.name)\n",
      "            return self\n",
      "\n",
      "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "            self.tracker.scope_stack.pop()\n",
      "\n",
      "    def _get_full_key(self, key):\n",
      "        \"\"\"Combine scope stack with current key\"\"\"\n",
      "        if not self.scope_stack:\n",
      "            return key\n",
      "        return \"/\".join(self.scope_stack + [key])\n",
      "\n",
      "    def denominator(self, **kwargs):\n",
      "        for key, value in kwargs.items():\n",
      "            if not isinstance(value, torch.Tensor) or value.dtype != torch.bool:\n",
      "                raise ValueError(\n",
      "                    f\"`{key}` must be a pytorch bool tensor: {value.dtype}\"\n",
      "                )\n",
      "            if value.numel() == 0:\n",
      "                raise ValueError(f\"`{key}` must be non-empty\")\n",
      "            full_key = self._get_full_key(key)\n",
      "            self._set_reduce_type(full_key, ReduceType.SUM)\n",
      "            self.stats[full_key].append(value.detach().clone())\n",
      "\n",
      "    def scalar(self, **kwargs):\n",
      "        for key, value in kwargs.items():\n",
      "            full_key = self._get_full_key(key)\n",
      "            self._set_reduce_type(full_key, ReduceType.SCALAR)\n",
      "            self.stats[full_key].append(float(value))\n",
      "\n",
      "    def stat(\n",
      "        self,\n",
      "        denominator: str,\n",
      "        reduce_type: ReduceType | None = None,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        \"\"\"Record multiple values from a dictionary\"\"\"\n",
      "        for key, value in kwargs.items():\n",
      "            if not isinstance(value, torch.Tensor) or value.dtype != torch.float:\n",
      "                raise ValueError(\n",
      "                    f\"`{key}` should be a pytorch float tensor: {value.dtype}\"\n",
      "                )\n",
      "            if value.numel() == 0:\n",
      "                raise ValueError(f\"`{key}` should be non-empty\")\n",
      "            if reduce_type == ReduceType.SCALAR:\n",
      "                raise ValueError(\"Cannot use the scalar reduce type for a tensor\")\n",
      "            full_key = self._get_full_key(key)\n",
      "\n",
      "            denorm = self._get_full_key(denominator)\n",
      "            if denorm not in self.stats or not self.stats[denorm]:\n",
      "                raise ValueError(f\"Denominator `{denorm}` does not exist\")\n",
      "            for x, y in zip(self.stats[denorm], self.stats[full_key] + [value]):\n",
      "                assert x.shape == y.shape, (x.shape, y.shape)\n",
      "            self.denominators[full_key] = denorm\n",
      "\n",
      "            if reduce_type is not None:\n",
      "                self._set_reduce_type(full_key, reduce_type)\n",
      "\n",
      "            self.stats[full_key].append(value.detach().clone())\n",
      "\n",
      "    def _set_reduce_type(self, key, reduce_type):\n",
      "        if not isinstance(reduce_type, ReduceType):\n",
      "            raise ValueError(\"reduce_type must be a ReduceType enum\")\n",
      "        self.reduce_types[key] = reduce_type\n",
      "\n",
      "    def export(self, key=None, reduce_group=None, reset=True) -> Dict[str, float]:\n",
      "        \"\"\"Get aggregated statistics\"\"\"\n",
      "        self._amend_moe_losses()\n",
      "        if reduce_group is None:\n",
      "            try:\n",
      "                from realhf.base.constants import data_parallel_group\n",
      "\n",
      "                reduce_group = data_parallel_group()\n",
      "            except:\n",
      "                pass\n",
      "        if key is not None:\n",
      "            full_key = self._get_full_key(key)\n",
      "            result = self._aggregate(full_key, reduce_group)\n",
      "            if reset:\n",
      "                if full_key in self.denominators:\n",
      "                    self.denominators.pop(full_key)\n",
      "                if full_key in self.reduce_types:\n",
      "                    self.denominators.pop(full_key)\n",
      "                self.stats.pop(full_key)\n",
      "            return result\n",
      "\n",
      "        results = {}\n",
      "        for key in list(self.stats.keys()):\n",
      "            results.update(self._aggregate(key, reduce_group))\n",
      "        if reset:\n",
      "            self.denominators = {}\n",
      "            self.reduce_types = {}\n",
      "            self.stats = defaultdict(list)\n",
      "        results = {\n",
      "            k: v.cpu().item() if torch.is_tensor(v) else v for k, v in results.items()\n",
      "        }\n",
      "        return results\n",
      "\n",
      "    def _amend_moe_losses(self):\n",
      "        from realhf.base.constants import is_last_pipe_stage, pipe_parallel_group\n",
      "\n",
      "        global MOE_AUX_LOSSES\n",
      "        mean_losses = {}\n",
      "        for k, loss in MOE_AUX_LOSSES.items():\n",
      "            dist.all_reduce(loss, group=pipe_parallel_group())\n",
      "            mean_losses[k] = float(loss.mean())  # average over layers\n",
      "        MOE_AUX_LOSSES.clear()\n",
      "        if mean_losses and is_last_pipe_stage():\n",
      "            self.scalar(**mean_losses)\n",
      "\n",
      "    def _aggregate(self, key, reduce_group):\n",
      "        if key not in self.stats or not self.stats[key]:\n",
      "            return {}\n",
      "\n",
      "        reduce_type = self.reduce_types.get(key, None)\n",
      "\n",
      "        result = {}\n",
      "        if reduce_type is None:\n",
      "            result[\"/\".join([key, \"avg\"])] = self._avg_of(key, reduce_group)\n",
      "            result[\"/\".join([key, \"min\"])] = self._min_of(key, reduce_group)\n",
      "            result[\"/\".join([key, \"max\"])] = self._max_of(key, reduce_group)\n",
      "        elif reduce_type == ReduceType.AVG:\n",
      "            result[key] = self._avg_of(key, reduce_group)\n",
      "        elif reduce_type == ReduceType.SUM:\n",
      "            result[key] = self._sum_of(key, reduce_group)\n",
      "        elif reduce_type == ReduceType.MIN:\n",
      "            result[key] = self._min_of(key, reduce_group)\n",
      "        elif reduce_type == ReduceType.MAX:\n",
      "            result[key] = self._max_of(key, reduce_group)\n",
      "        elif reduce_type == ReduceType.SCALAR:\n",
      "            result[key] = sum(self.stats[key]) / len(self.stats[key])\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown reduce type: {reduce_type}\")\n",
      "\n",
      "        keys_to_pop = [k for k, v in result.items() if v is None]\n",
      "        for k in keys_to_pop:\n",
      "            result.pop(k)\n",
      "        return result\n",
      "\n",
      "    def _sum_of(self, key, reduce_group):\n",
      "        values = self.stats[key]\n",
      "        if key not in self.denominators:\n",
      "            x = sum([x.sum() for x in values])\n",
      "            if reduce_group is not None:\n",
      "                dist.all_reduce(x, group=reduce_group)\n",
      "        else:\n",
      "            denominator = self.denominators[key]\n",
      "            if denominator not in self.stats:\n",
      "                raise ValueError(\n",
      "                    f\"Denominator `{denominator}` not set for key `{key}`.\"\n",
      "                )\n",
      "            xs = []\n",
      "            for v, d in zip(values, self.stats[denominator]):\n",
      "                xs.append(torch.where(d, v, 0.0).sum())\n",
      "            x = sum(xs)\n",
      "            if reduce_group is not None:\n",
      "                dist.all_reduce(x, group=reduce_group)\n",
      "        return float(x)\n",
      "\n",
      "    def _avg_of(self, key, reduce_group):\n",
      "        values = self.stats[key]\n",
      "        denominator = self.denominators[key]\n",
      "        if denominator not in self.stats:\n",
      "            raise ValueError(f\"Denominator `{denominator}` not set for key `{key}`.\")\n",
      "        xs = []\n",
      "        ds = []\n",
      "        for v, d in zip(values, self.stats[denominator]):\n",
      "            xs.append(torch.where(d, v, 0.0).sum())\n",
      "            ds.append(d.sum())\n",
      "        x = sum(xs)\n",
      "        d = sum(ds)\n",
      "        if reduce_group is not None:\n",
      "            dist.all_reduce(x, group=reduce_group)\n",
      "            dist.all_reduce(d, group=reduce_group)\n",
      "        if d == 0:\n",
      "            return None\n",
      "        return x / d\n",
      "\n",
      "    def _min_of(self, key, reduce_group):\n",
      "        values = self.stats[key]\n",
      "        denominator = self.denominators[key]\n",
      "        if denominator not in self.stats:\n",
      "            raise ValueError(f\"Denominator `{denominator}` not set for key `{key}`.\")\n",
      "        xs = []\n",
      "        for v, d in zip(values, self.stats[denominator]):\n",
      "            xs.append(torch.where(d, v, float(\"inf\")).min())\n",
      "        x = min(xs)\n",
      "        if reduce_group is not None:\n",
      "            dist.all_reduce(x, group=reduce_group, op=dist.ReduceOp.MIN)\n",
      "        if torch.isinf(x):\n",
      "            return None\n",
      "        return float(x)\n",
      "\n",
      "    def _max_of(self, key, reduce_group):\n",
      "        values = self.stats[key]\n",
      "        denominator = self.denominators[key]\n",
      "        if denominator not in self.stats:\n",
      "            raise ValueError(f\"Denominator `{denominator}` not set for key `{key}`.\")\n",
      "        xs = []\n",
      "        for v, d in zip(values, self.stats[denominator]):\n",
      "            xs.append(torch.where(d, v, -float(\"inf\")).max())\n",
      "        x = max(xs)\n",
      "        if reduce_group is not None:\n",
      "            dist.all_reduce(x, group=reduce_group, op=dist.ReduceOp.MAX)\n",
      "        if torch.isinf(x):\n",
      "            return None\n",
      "        return float(x)\n",
      "\n",
      "\n",
      "DEFAULT_TRACKER = DistributedStatsTracker()\n",
      "stat = DEFAULT_TRACKER.stat\n",
      "denominator = DEFAULT_TRACKER.denominator\n",
      "export = DEFAULT_TRACKER.export\n",
      "scope = DEFAULT_TRACKER.scope\n",
      "scalar = DEFAULT_TRACKER.scalar\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/security.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "\n",
      "def read_key(service, name=\"default\"):\n",
      "    with open(f\"/data/marl/keys/{service}/{name}\", \"r\") as f:\n",
      "        return f.read().strip()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/datapack.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import bisect\n",
      "import itertools\n",
      "from typing import Any, List, Tuple, Union\n",
      "\n",
      "import numba\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def flat2d(arr: List[List[Any]]) -> List[Any]:\n",
      "    return list(itertools.chain(*arr))\n",
      "\n",
      "\n",
      "@numba.njit\n",
      "def partition_balanced(nums: np.ndarray, k: int, min_size: int = 1):\n",
      "    \"\"\"Partition an array into k subarrays with a minimum absolute difference\n",
      "    of sums and minimum subarray size.\n",
      "\n",
      "    Dynamic programming solution.\n",
      "\n",
      "    Args:\n",
      "        nums (np.ndarray): The array to be partitioned.\n",
      "        k (int): Number of partitions.\n",
      "        min_size (int): Minimum size of each subarray.\n",
      "\n",
      "    Returns:\n",
      "        List[int]: Partition slicing point indices in a list including start and end points.\n",
      "                   Length equals to k + 1.\n",
      "    \"\"\"\n",
      "    n = len(nums)\n",
      "\n",
      "    dp = np.full((n + 1, k + 1), dtype=np.int64, fill_value=int(1e10))\n",
      "    maxval = np.full((n + 1, k + 1), dtype=np.int64, fill_value=-int(1e10))\n",
      "    minval = np.full((n + 1, k + 1), dtype=np.int64, fill_value=int(1e10))\n",
      "    prefix_sums = np.concatenate((np.zeros(1, dtype=np.int64), np.cumsum(nums)), axis=0)\n",
      "    split = np.zeros((n + 1, k + 1), dtype=np.int64)\n",
      "\n",
      "    for i in range(n + 1):\n",
      "        dp[i, 1] = 0\n",
      "        maxval[i, 1] = prefix_sums[i] - prefix_sums[0]\n",
      "        minval[i, 1] = prefix_sums[i] - prefix_sums[0]\n",
      "\n",
      "    for j in range(2, k + 1):\n",
      "        for i in range(j * min_size, n + 1):\n",
      "            for x in range(min_size, i - min_size + 1):\n",
      "                xx = prefix_sums[i] - prefix_sums[x]\n",
      "                min_diff = max(\n",
      "                    dp[x, j - 1], maxval[x, j - 1] - xx, xx - minval[x, j - 1]\n",
      "                )\n",
      "                dp[i, j] = min(dp[i, j], min_diff)\n",
      "\n",
      "                if dp[i, j] == min_diff:\n",
      "                    split[i][j] = x\n",
      "                    if dp[i, j] == maxval[x, j - 1] - xx:\n",
      "                        maxval[i, j] = maxval[x, j - 1]\n",
      "                        minval[i, j] = xx\n",
      "                    elif dp[i, j] == xx - minval[x, j - 1]:\n",
      "                        maxval[i, j] = xx\n",
      "                        minval[i, j] = minval[x, j - 1]\n",
      "                    else:\n",
      "                        maxval[i, j] = maxval[x, j - 1]\n",
      "                        minval[i, j] = minval[x, j - 1]\n",
      "    res = [n]\n",
      "    idx = n\n",
      "    for i in range(k, 0, -1):\n",
      "        idx = split[idx][i]\n",
      "        res.append(idx)\n",
      "    return res[::-1]\n",
      "\n",
      "\n",
      "def partition_balanced_tuples(\n",
      "    nums: np.ndarray, k: int, min_size: int = 1\n",
      ") -> List[Tuple[int, int]]:\n",
      "    lst = partition_balanced(nums, k, min_size)\n",
      "    return [(lst[i], lst[i + 1]) for i in range(k)]\n",
      "\n",
      "\n",
      "def min_abs_diff_partition(\n",
      "    arr: Union[np.ndarray, List], k: int, min_size: int = 1\n",
      ") -> List[Tuple[int, int]]:\n",
      "    err_hint = (\n",
      "        \" Errors should not be reported in this function. It is probably a bug in the dataset code\"\n",
      "        \" or too small batch size in pipeline parallel realhf.experiments.\"\n",
      "    )\n",
      "\n",
      "    if isinstance(arr, list):\n",
      "        arr = np.array(arr)\n",
      "    if len(arr.shape) > 1:\n",
      "        raise ValueError(f\"The array to be partitioned must be 1D. ({arr})\" + err_hint)\n",
      "    if len(arr) < k:\n",
      "        raise ValueError(\n",
      "            f\"The array to be partitioned must have length >= k. (array {arr}, k={k})\"\n",
      "            + err_hint\n",
      "        )\n",
      "    if len(arr) < k * min_size:\n",
      "        raise ValueError(\n",
      "            f\"Length of the array to be partitioned must be at least k * min_size ({k} * {min_size}), current length {len(arr)}.\"\n",
      "        )\n",
      "    partitions = partition_balanced_tuples(arr, k, min_size)\n",
      "    last_end = 0\n",
      "\n",
      "    err_type = None\n",
      "    err_msg = f\"Lengths to be partitioned: {arr}, k={k}, current partition result {partitions}.\"\n",
      "    for start, end in partitions:\n",
      "        if start != last_end:\n",
      "            err_type = \"not contiguous\"\n",
      "        if end <= start:\n",
      "            err_type = \"empty\"\n",
      "        if err_type:\n",
      "            raise ValueError(\n",
      "                f\"Partition {start}-{end} is {err_type}. \" + err_msg + err_hint\n",
      "            )\n",
      "        last_end = end\n",
      "    return partitions\n",
      "\n",
      "\n",
      "# @numba.njit\n",
      "def reorder_to_balanced_batches(\n",
      "    seqlens: np.ndarray,\n",
      "    n_seqs_per_batch: int,\n",
      ") -> Tuple[np.ndarray, int]:\n",
      "    max_bins = (len(seqlens) + n_seqs_per_batch - 1) // n_seqs_per_batch\n",
      "\n",
      "    bins = [[] for _ in range(max_bins)]\n",
      "    bin_sizes = np.zeros(max_bins, dtype=np.int32)\n",
      "    bin_seqlens = np.zeros(max_bins, dtype=np.int32)\n",
      "    for i in seqlens.argsort()[::-1]:\n",
      "        idx = np.where(\n",
      "            bin_sizes + 1 <= n_seqs_per_batch,\n",
      "            bin_seqlens,\n",
      "            np.iinfo(np.int32).max,\n",
      "        ).argmin()\n",
      "        bins[idx].append(i)\n",
      "        bin_sizes[idx] += 1\n",
      "        bin_seqlens[idx] += seqlens[i]\n",
      "\n",
      "    assert np.all(bin_sizes <= n_seqs_per_batch), (bin_sizes, n_seqs_per_batch)\n",
      "    max_diff = 0\n",
      "    for i in range(max_bins):\n",
      "        for j in range(i + 1, max_bins):\n",
      "            max_diff = max(max_diff, abs(bin_seqlens[i] - bin_seqlens[j]))\n",
      "\n",
      "    reordered_indices = []\n",
      "    for i in bin_seqlens.argsort()[::-1]:\n",
      "        reordered_indices.extend(bins[i])\n",
      "    return np.array(reordered_indices), max_diff\n",
      "\n",
      "\n",
      "# @numba.njit\n",
      "def _ffd_allocate(\n",
      "    values: np.ndarray, capacity: int, min_groups: int\n",
      ") -> List[List[int]]:\n",
      "    \"\"\"A greedy allocation algorithm that partitions a list of numbers\n",
      "    into k groups, where the summation of each group is less than capacity\n",
      "    and k >= min_groups. We want to minimize k and make partitions as balanced\n",
      "    as possible.\n",
      "\n",
      "    1. Sort the numbers in reverse order.\n",
      "    2. If the number of groups is less than `min_groups`, create a new group.\n",
      "    3. For a new number, find all groups with the capacity to hold the new number.\n",
      "       Put the new number into the group with the smallest size.\n",
      "    4. Otherwise, create a new group.\n",
      "    \"\"\"\n",
      "    value_indices = np.argsort(-values)\n",
      "    group_indices: List[List[int]] = []\n",
      "    group_values: List[Tuple[float, int]] = []\n",
      "    group_cnt = 0\n",
      "    for idx in value_indices:\n",
      "        if (\n",
      "            len(group_values) < min_groups\n",
      "            or group_values[0][0] + values[idx] > capacity\n",
      "        ):\n",
      "            bisect.insort(group_values, (float(values[idx]), group_cnt))\n",
      "            group_indices.append([idx])\n",
      "            group_cnt += 1\n",
      "        else:\n",
      "            i = bisect.bisect_right(group_values, (capacity - values[idx], len(values)))\n",
      "            candidates = [group_values[j][1] for j in range(i)]\n",
      "            lens = [len(group_indices[g]) for g in candidates]\n",
      "            j = np.argmin(lens)\n",
      "            v, group_idx = group_values.pop(j)\n",
      "            assert group_idx == candidates[j]\n",
      "            bisect.insort(group_values, (float(values[idx] + v), group_idx))\n",
      "            group_indices[group_idx].append(idx)\n",
      "    return group_indices\n",
      "\n",
      "\n",
      "def ffd_allocate(values: List[int], capacity: int, min_groups: int) -> List[List[int]]:\n",
      "    if any(v > capacity for v in values):\n",
      "        raise RuntimeError(f\"Values {values} is larger than capacity {capacity}\")\n",
      "    if len(values) < min_groups:\n",
      "        raise RuntimeError(\n",
      "            f\"Number of values {len(values)} is smaller than min_groups {min_groups}\"\n",
      "        )\n",
      "    return _ffd_allocate(np.array(values), capacity, min_groups)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import time\n",
      "\n",
      "    for i in range(100):\n",
      "        st = time.monotonic()\n",
      "        nums = np.random.randint(1024, 8192, size=(100,))\n",
      "        # k = np.random.randint(2, 20)\n",
      "        # min_size = np.random.randint(1, len(nums) // k)\n",
      "        # res = min_abs_diff_partition(nums, k, min_size)\n",
      "        # assert all(y - x >= min_size for x, y in res)\n",
      "        max_tokens_per_mb = 163840\n",
      "        min_n_groups = np.random.randint(1, 8)\n",
      "        groups = ffd_allocate(nums, max_tokens_per_mb, min_n_groups)\n",
      "        assert len(groups) >= min_n_groups\n",
      "        import itertools\n",
      "\n",
      "        indices = list(itertools.chain(*groups))\n",
      "        assert len(set(indices)) == len(indices)\n",
      "        group_percent = [\n",
      "            sum(nums[i] for i in group) / max_tokens_per_mb for group in groups\n",
      "        ]\n",
      "\n",
      "        print(\n",
      "            len(groups),\n",
      "            min_n_groups,\n",
      "            [sum(nums[i] for i in group) for group in groups],\n",
      "            max(group_percent),\n",
      "            min(group_percent),\n",
      "            np.mean(group_percent),\n",
      "            time.monotonic() - st,\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/monitor.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import contextlib\n",
      "import dataclasses\n",
      "import enum\n",
      "import json\n",
      "import os\n",
      "import pickle\n",
      "import re\n",
      "import time\n",
      "from collections import defaultdict\n",
      "from statistics import mean\n",
      "from typing import TYPE_CHECKING, Callable, Dict, List, Optional, Union\n",
      "\n",
      "import numpy as np\n",
      "import psutil\n",
      "import pynvml\n",
      "import torch\n",
      "import tqdm\n",
      "import tqdm.asyncio\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from realhf.api.core.config import ModelName\n",
      "\n",
      "logger = logging.getLogger(\"benchmark\")\n",
      "\n",
      "IF_MARK = False\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RolloutStat:\n",
      "    submitted: int = 0\n",
      "    accepted: int = 0\n",
      "    running: int = 0\n",
      "\n",
      "\n",
      "def mock_time_mark(name, identifier, t, step):\n",
      "    if IF_MARK:\n",
      "        logger.debug(f\"*{name}* #{identifier}#  ${t}$ ns step &{step}&\")\n",
      "\n",
      "\n",
      "def time_mark(name, identifier, step=0):\n",
      "    if IF_MARK:\n",
      "        logger.debug(\n",
      "            f\"*{name}* #{identifier}#  ${int(time.time_ns())}$ ns step &{step}&\"\n",
      "        )\n",
      "\n",
      "\n",
      "def parse_time_mark_in_line(line, name, step_range=None):\n",
      "    if f\"*{name}*\" in line:\n",
      "        identifer, t, step = (\n",
      "            line.split(\"#\")[1],\n",
      "            int(line.split(\"$\")[1]),\n",
      "            int(line.split(\"&\")[1]),\n",
      "        )\n",
      "        if step_range:\n",
      "            if step >= step_range[1] or step < step_range[0]:\n",
      "                return None\n",
      "        return identifer, t\n",
      "\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "def parse_time_mark_in_file(file, name, step_range=None):\n",
      "    time_points = defaultdict(list)\n",
      "    with open(file, \"r\") as f:\n",
      "        count = 0\n",
      "        res_count = 0\n",
      "        for line in f.readlines():\n",
      "            count += 1\n",
      "            res = parse_time_mark_in_line(line, name, step_range=step_range)\n",
      "            if res is not None:\n",
      "                res_count += 1\n",
      "                identifier, time_point = res\n",
      "                time_points[identifier].append(time_point)\n",
      "    return time_points\n",
      "\n",
      "\n",
      "def parse_time_mark_in_dir(dir, name, step_range=None):\n",
      "    time_points = {}\n",
      "    for file in os.listdir(dir):\n",
      "        file_path = os.path.join(dir, file)\n",
      "        tpsf = parse_time_mark_in_file(file_path, name, step_range=step_range)\n",
      "        for k, v in tpsf.items():\n",
      "            if k not in time_points:\n",
      "                time_points[k] = v\n",
      "            else:\n",
      "                time_points[k].extend(v)\n",
      "    return time_points\n",
      "\n",
      "\n",
      "MATPLOTLIB_COLORS = [\n",
      "    \"red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"orange\",\n",
      "    \"purple\",\n",
      "    \"pink\",\n",
      "    \"black\",\n",
      "    \"brown\",\n",
      "    \"gray\",\n",
      "    \"cyan\",\n",
      "    \"magenta\",\n",
      "    \"lime\",\n",
      "    \"olive\",\n",
      "    \"navy\",\n",
      "]\n",
      "\n",
      "\n",
      "def summary_time_points(\n",
      "    start_keys,\n",
      "    end_keys,\n",
      "    identifiers,\n",
      "    dir_name=None,\n",
      "    file_name=None,\n",
      "    start_time=None,\n",
      "    figsize=(12, 4),\n",
      "    end_time=None,\n",
      "    step_range=None,\n",
      "    save_fig_path=\"time_points.png\",\n",
      "    draw_boundary=False,\n",
      "):\n",
      "    \"\"\"Plot and summary time marks in logs.\"\"\"\n",
      "    import matplotlib.pyplot as plt\n",
      "\n",
      "    assert file_name or dir_name, \"dir or file name must be specified\"\n",
      "    all_time_points = {}\n",
      "    if file_name is None:\n",
      "        for k in start_keys:\n",
      "            all_time_points[k] = parse_time_mark_in_dir(\n",
      "                dir_name, k, step_range=step_range\n",
      "            )\n",
      "        for k in end_keys:\n",
      "            all_time_points[k] = parse_time_mark_in_dir(\n",
      "                dir_name, k, step_range=step_range\n",
      "            )\n",
      "    else:\n",
      "        for k in start_keys:\n",
      "            all_time_points[k] = parse_time_mark_in_file(\n",
      "                file_name, k, step_range=step_range\n",
      "            )\n",
      "        for k in end_keys:\n",
      "            all_time_points[k] = parse_time_mark_in_file(\n",
      "                file_name, k, step_range=step_range\n",
      "            )\n",
      "\n",
      "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
      "    ax.set_ylim(-1, len(identifiers))\n",
      "    ax.set_yticks(list(range(len(identifiers))))\n",
      "    ax.set_yticklabels(identifiers)\n",
      "\n",
      "    label_set = {sk: False for sk in start_keys}\n",
      "    infos = {}\n",
      "    min_time = None\n",
      "    max_time = None\n",
      "    for id_index, identifier in enumerate(identifiers):\n",
      "        time_sum = {}\n",
      "        time_list = {}\n",
      "        for start_key_idx, (start_key, end_key) in enumerate(zip(start_keys, end_keys)):\n",
      "            time_sum[start_key] = 0\n",
      "            time_list[start_key] = []\n",
      "            try:\n",
      "                start_time_points = np.array(all_time_points[start_key][identifier])\n",
      "                end_time_points = np.array(all_time_points[end_key][identifier])\n",
      "            except KeyError:\n",
      "                continue\n",
      "            assert len(start_time_points) == len(end_time_points)\n",
      "\n",
      "            if start_time is not None:\n",
      "                valid_indices_st = np.where(start_time_points > start_time)\n",
      "                valid_indices_et = np.where(start_time_points < end_time)\n",
      "                valid_indices = np.intersect1d(valid_indices_st, valid_indices_et)\n",
      "                start_time_points = start_time_points[valid_indices]\n",
      "                end_time_points = end_time_points[valid_indices]\n",
      "\n",
      "            # plot time point pairs\n",
      "            for stp, etp in zip(list(start_time_points), list(end_time_points)):\n",
      "                min_time = stp if min_time is None else min(min_time, stp)\n",
      "                max_time = etp if max_time is None else max(max_time, etp)\n",
      "                time_sum[start_key] += etp - stp\n",
      "                time_list[start_key].append(etp - stp)\n",
      "\n",
      "                if label_set[start_key] is False:\n",
      "                    label = start_key\n",
      "                    label_set[start_key] = True\n",
      "                else:\n",
      "                    label = None\n",
      "\n",
      "                ax.barh(\n",
      "                    y=id_index,\n",
      "                    width=etp - stp,\n",
      "                    left=stp,\n",
      "                    height=0.8,\n",
      "                    color=MATPLOTLIB_COLORS[start_key_idx],\n",
      "                    label=label,\n",
      "                )\n",
      "\n",
      "                if draw_boundary:\n",
      "                    ax.plot(\n",
      "                        [stp, stp],\n",
      "                        [id_index - 0.4, id_index + 0.4],\n",
      "                        color=\"black\",\n",
      "                        linestyle=\"-\",\n",
      "                        linewidth=0.5,\n",
      "                    )\n",
      "                    ax.plot(\n",
      "                        [etp, etp],\n",
      "                        [id_index - 0.4, id_index + 0.4],\n",
      "                        color=\"black\",\n",
      "                        linestyle=\"-\",\n",
      "                        linewidth=0.5,\n",
      "                    )\n",
      "\n",
      "        infos[identifier] = (time_sum, time_list)\n",
      "\n",
      "    ax.set_xlim(min_time, max_time)\n",
      "    total_width = max_time - min_time\n",
      "    xticks = np.arange(\n",
      "        min_time - total_width // 12, max_time - total_width // 12, 10 * 1e9\n",
      "    )\n",
      "    xtick_labels = [f\"{int((i//1e9)%1000)}\" for i in xticks]\n",
      "    ax.set_xticks(xticks)\n",
      "    ax.set_xticklabels(xtick_labels)\n",
      "\n",
      "    # summary time cost percent\n",
      "    for id_index, identifier in enumerate(identifiers):\n",
      "        print(\"=\" * 30)\n",
      "        print(f\"Identifier {identifier} time cost percent:\")\n",
      "        bubble_time = 100\n",
      "        time_sum, time_list = infos[identifier]\n",
      "        for k in time_sum:\n",
      "            time_perc = round(time_sum[k] / (max_time - min_time) * 100, 2)\n",
      "            # print time cost percent\n",
      "            avg_val = (\n",
      "                round(mean(time_list[k]) / 10e6, 2) if len(time_list[k]) > 0 else \"-\"\n",
      "            )\n",
      "            max_val = (\n",
      "                round(max(time_list[k]) / 10e6, 2) if len(time_list[k]) > 0 else \"-\"\n",
      "            )\n",
      "            min_val = (\n",
      "                round(min(time_list[k]) / 10e6, 2) if len(time_list[k]) > 0 else \"-\"\n",
      "            )\n",
      "\n",
      "            bubble_time -= time_perc\n",
      "            print(\n",
      "                f\"{k} -- {time_perc} %, \"\n",
      "                f\"avg, min, max = {avg_val}, {min_val}, {max_val} ms, \"\n",
      "                f\"sum, n = {round(time_sum[k]/10e6, 2)} ms, {len(time_list[k])}\"\n",
      "            )\n",
      "        print(f\"bubble time -- {round(bubble_time, 2)}%\")\n",
      "\n",
      "    plt.legend(loc=(1.01, 0.0))\n",
      "    plt.tight_layout()\n",
      "\n",
      "    plt.savefig(save_fig_path)\n",
      "\n",
      "\n",
      "def gpu_utilization_monitor(worker_idx: int, interval: float, ttl: float):\n",
      "    pynvml.nvmlInit()\n",
      "    gpu_idx = worker_idx % 8\n",
      "    tik = time.time()\n",
      "    while time.time() - tik < ttl:\n",
      "        handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_idx)\n",
      "        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
      "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
      "        total_memory = memory_info.total / (1024**2)  # Convert bytes to megabytes\n",
      "        used_memory = memory_info.used / (1024**2)\n",
      "        memory_usage_percentage = (used_memory / total_memory) * 100\n",
      "        logger.debug(\n",
      "            f\"Worker Index {worker_idx}, GPU {gpu_idx}: \"\n",
      "            f\"Compute Utilization - {utilization.gpu}%, \"\n",
      "            f\"Total Memory - {total_memory:.2f}MB, Used Memory - {used_memory:.2f}MB, \"\n",
      "            f\"Memory Usage - {memory_usage_percentage:.2f}%\"\n",
      "        )\n",
      "        time.sleep(interval)\n",
      "    pynvml.nvmlShutdown()\n",
      "\n",
      "\n",
      "# Helper function to calculate FLOPs using the Megatron-LM paper's formula\n",
      "def calculate_llama_train_flops(\n",
      "    checkpoint_activations_factor: int,\n",
      "    batch_size: int,\n",
      "    seqlens: List[int],\n",
      "    num_layers: int,\n",
      "    hidden_size: int,\n",
      "    intermediate_size: int,\n",
      "    vocab_size: int,\n",
      "):\n",
      "    return checkpoint_activations_factor * caculuate_llama_forward_flops(\n",
      "        batch_size,\n",
      "        seqlens,\n",
      "        num_layers,\n",
      "        hidden_size,\n",
      "        intermediate_size,\n",
      "        vocab_size,\n",
      "    )\n",
      "\n",
      "\n",
      "def caculuate_llama_forward_flops(\n",
      "    batch_size: int,\n",
      "    seqlens: List[int],\n",
      "    num_layers: int,\n",
      "    hidden_size: int,\n",
      "    intermediate_size: int,\n",
      "    vocab_size: int,\n",
      "):\n",
      "    assert len(seqlens) == batch_size\n",
      "    attn_flops = sum(x**2 for x in seqlens) * hidden_size\n",
      "    return (\n",
      "        2\n",
      "        * num_layers\n",
      "        * (\n",
      "            4 * sum(seqlens) * hidden_size**2\n",
      "            + 2 * attn_flops\n",
      "            + 3 * sum(seqlens) * hidden_size * intermediate_size\n",
      "        )\n",
      "        + 4 * sum(seqlens) * vocab_size * hidden_size\n",
      "    )\n",
      "\n",
      "\n",
      "def calculate_llama_gen_flops(\n",
      "    batch_size,\n",
      "    prompt_lens,\n",
      "    gen_len,\n",
      "    num_layers,\n",
      "    hidden_size,\n",
      "    intermediate_size,\n",
      "    vocab_size,\n",
      "):\n",
      "    flops = caculuate_llama_forward_flops(\n",
      "        batch_size,\n",
      "        prompt_lens,\n",
      "        num_layers,\n",
      "        hidden_size,\n",
      "        intermediate_size,\n",
      "        vocab_size,\n",
      "    )\n",
      "    for i in range(gen_len):\n",
      "        prefix_lens = [x + i for x in prompt_lens]\n",
      "        flops += (\n",
      "            2\n",
      "            * num_layers\n",
      "            * (\n",
      "                4 * batch_size * hidden_size**2\n",
      "                + 2 * (sum(prefix_lens) + batch_size) * hidden_size\n",
      "                + 3 * batch_size * hidden_size * intermediate_size\n",
      "            )\n",
      "            + 4 * batch_size * vocab_size * hidden_size\n",
      "        )\n",
      "    return flops\n",
      "\n",
      "\n",
      "#################### CUDA Kernel Time Marking Start ####################\n",
      "# Used to create timeline plots.\n",
      "\n",
      "\n",
      "class CUDATimeMarkType(enum.Enum):\n",
      "    forward = \"forward\"\n",
      "    backward = \"backward\"\n",
      "    optim_step = \"optim_step\"\n",
      "    comm = \"comm\"\n",
      "    misc = \"misc\"\n",
      "    mem_layout = \"memory_layout\"\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TimeMarkEntry:\n",
      "    name: str\n",
      "    model_name: \"ModelName\"\n",
      "    type_: CUDATimeMarkType\n",
      "    start_time: int\n",
      "    end_time: int\n",
      "\n",
      "\n",
      "TIME_MARK_DB = []\n",
      "\n",
      "\n",
      "def cuda_tmark(name: str, type_: CUDATimeMarkType):\n",
      "    if os.getenv(\"REAL_CUDA_TMARK\", None) == \"1\":\n",
      "\n",
      "        def wrapper(f: Callable):\n",
      "\n",
      "            def _wrapped_f(*args, **kwargs):\n",
      "                import torch\n",
      "\n",
      "                if constants.use_cuda():\n",
      "                    from realhf.base.constants import _model_name\n",
      "\n",
      "                    torch.cuda.synchronize()\n",
      "                    tik = time.time_ns()\n",
      "                    res = f(*args, **kwargs)\n",
      "                    torch.cuda.synchronize()\n",
      "                    tok = time.time_ns()\n",
      "                    global TIME_MARK_DB\n",
      "                    TIME_MARK_DB.append(\n",
      "                        TimeMarkEntry(name, _model_name, type_, tik, tok)\n",
      "                    )\n",
      "                else:\n",
      "                    res = f(*args, **kwargs)\n",
      "                return res\n",
      "\n",
      "            return _wrapped_f\n",
      "\n",
      "    else:\n",
      "\n",
      "        def wrapper(f):\n",
      "            return f\n",
      "\n",
      "    return wrapper\n",
      "\n",
      "\n",
      "@contextlib.contextmanager\n",
      "def cuda_tmarked(name: str, type_: CUDATimeMarkType):\n",
      "    if os.getenv(\"REAL_CUDA_TMARK\", None) == \"1\":\n",
      "        import torch\n",
      "\n",
      "        if constants.use_cuda():\n",
      "            from realhf.base.constants import _model_name\n",
      "\n",
      "            torch.cuda.synchronize()\n",
      "            tik = time.time_ns()\n",
      "    yield\n",
      "    if os.getenv(\"REAL_CUDA_TMARK\", None) == \"1\":\n",
      "        if constants.use_cuda():\n",
      "            torch.cuda.synchronize()\n",
      "            tok = time.time_ns()\n",
      "            global TIME_MARK_DB\n",
      "            TIME_MARK_DB.append(TimeMarkEntry(name, _model_name, type_, tik, tok))\n",
      "\n",
      "\n",
      "def fetch_latest_tmark():\n",
      "    global TIME_MARK_DB\n",
      "    return TIME_MARK_DB[-1]\n",
      "\n",
      "\n",
      "def dump_tmark_db(worker_idx):\n",
      "    if os.getenv(\"REAL_CUDA_TMARK\", None) != \"1\":\n",
      "        return\n",
      "    fn = os.path.join(\n",
      "        constants.LOG_ROOT,\n",
      "        constants.experiment_name(),\n",
      "        constants.trial_name(),\n",
      "        f\"time_marks{worker_idx}.pkl\",\n",
      "    )\n",
      "    global TIME_MARK_DB\n",
      "    with open(fn, \"wb\") as f:\n",
      "        pickle.dump(TIME_MARK_DB, f)\n",
      "    TIME_MARK_DB.clear()\n",
      "\n",
      "\n",
      "#################### CUDA Kernel Time Marking End ####################\n",
      "\n",
      "#################### CUDA Kernel Time Statistics Start ####################\n",
      "# Categorizing CUDA kernels into computation, communication, memory IO, and MISC/IDLE,\n",
      "# used to plot the percentage of time spent on each category and show how much we can\n",
      "# improve over vanilla parallel strategies.\n",
      "\n",
      "COMPUTE_KERNEL_KEYS = [\n",
      "    \"elementwise_kernel\",\n",
      "    \"gemm\",\n",
      "    \"aten::\",\n",
      "    \"at::native::\",\n",
      "    \"flash\",\n",
      "    \"backward_kernel\",\n",
      "    \"reduce_kernel\",\n",
      "    \"multi_tensor_apply\",\n",
      "    \"gae_kernel\",\n",
      "    \"gemvx::kernel\",\n",
      "    \"cublas\",\n",
      "    \"cudnn\",\n",
      "    \"cutlass\",\n",
      "]\n",
      "\n",
      "P2P_COMM_KERNEL_KEYS = [\n",
      "    \"ncclDevKernel_SendRecv\",\n",
      "]\n",
      "\n",
      "COLL_COMM_KERNEL_KEYS = [\n",
      "    \"ncclDevKernel_AllReduce\",\n",
      "    \"ncclDevKernel_ReduceScatter\",\n",
      "    \"ncclDevKernel_AllGather\",\n",
      "]\n",
      "\n",
      "MEM_KERNEL_KEYS = [\n",
      "    \"Memcpy\",\n",
      "    \"cleanup\",\n",
      "    \"Memset\",\n",
      "]\n",
      "\n",
      "MISC_KERNEL_KEYS = [\n",
      "    \"at_cuda_detail\",\n",
      "    \"CudaCodeGen\",\n",
      "]\n",
      "\n",
      "\n",
      "class CUDAKernelTimeCategory(enum.Enum):\n",
      "    COMPUTE = \"compute\"\n",
      "    P2P_COMM = \"p2p_comm\"\n",
      "    COLL_COMM = \"coll_comm\"\n",
      "    MEM = \"memoryIO\"\n",
      "    IDLE = \"idle\"\n",
      "    MISC = \"misc\"\n",
      "\n",
      "    @classmethod\n",
      "    def from_name(cls, name):\n",
      "        # Order may matter. MEM & COMM keys are easier to find out.\n",
      "        if any(k in name for k in MEM_KERNEL_KEYS):\n",
      "            return cls.MEM\n",
      "        if any(k in name for k in P2P_COMM_KERNEL_KEYS):\n",
      "            return cls.P2P_COMM\n",
      "        if any(k in name for k in COLL_COMM_KERNEL_KEYS):\n",
      "            return cls.COLL_COMM\n",
      "        if any(k in name for k in MISC_KERNEL_KEYS):\n",
      "            return cls.MISC\n",
      "        if any(k in name for k in COMPUTE_KERNEL_KEYS):\n",
      "            return cls.COMPUTE\n",
      "        raise NotImplementedError(f\"Unknown kernel type. Name is `{name}`\")\n",
      "\n",
      "\n",
      "class CUDAKernelTimeStat:  # in us\n",
      "\n",
      "    def __init__(self, world_size, **kwargs):\n",
      "        self.world_size = world_size\n",
      "        for k in CUDAKernelTimeCategory:\n",
      "            setattr(self, k.value, kwargs.get(k.value, 0))\n",
      "\n",
      "    @property\n",
      "    def total(self):\n",
      "        return sum([getattr(self, k.value) for k in CUDAKernelTimeCategory])\n",
      "\n",
      "    def percentage(self) -> Dict:\n",
      "        return {\n",
      "            k.value: getattr(self, k.value) / self.total for k in CUDAKernelTimeCategory\n",
      "        }\n",
      "\n",
      "    def __add__(self, other):\n",
      "        return CUDAKernelTimeStat(\n",
      "            world_size=self.world_size + other.world_size,\n",
      "            **{\n",
      "                k.value: getattr(self, k.value) + getattr(other, k.value)\n",
      "                for k in CUDAKernelTimeCategory\n",
      "            },\n",
      "        )\n",
      "\n",
      "    def __truediv__(self, x):\n",
      "        assert self.world_size % x == 0\n",
      "        return CUDAKernelTimeStat(\n",
      "            world_size=self.world_size // x,\n",
      "            **{k.value: getattr(self, k.value) / x for k in CUDAKernelTimeCategory},\n",
      "        )\n",
      "\n",
      "    def gpu_average(self):\n",
      "        return self / self.world_size\n",
      "\n",
      "    def __repr__(self):\n",
      "        import tabulate\n",
      "\n",
      "        headers = [\n",
      "            \"\",\n",
      "            \"Total\",\n",
      "            \"Computation\",\n",
      "            \"P2P Comm\",\n",
      "            \"Collective Comm\",\n",
      "            \"Memory IO\",\n",
      "            \"Idle\",\n",
      "            \"Misc\",\n",
      "        ]\n",
      "        line1 = [\n",
      "            \"Time (s)\",\n",
      "            self.total / 1e6,\n",
      "            *[getattr(self, k.value) / 1e6 for k in CUDAKernelTimeCategory],\n",
      "        ]\n",
      "        line1 = [f\"{x:.2f}\" if isinstance(x, float) else x for x in line1]\n",
      "        line2 = [\n",
      "            \"Percentage (%)\",\n",
      "            \"-\",\n",
      "            *[f\"{self.percentage()[k.value]:.2%}\" for k in CUDAKernelTimeCategory],\n",
      "        ]\n",
      "        tab_str = tabulate.tabulate(\n",
      "            [headers, line1, line2],\n",
      "            headers=\"firstrow\",\n",
      "            tablefmt=\"fancy_grid\",\n",
      "            stralign=\"center\",\n",
      "        )\n",
      "        return (\n",
      "            f\" Number of GPUs: {self.world_size} \".center(\n",
      "                len(tab_str.split(\"\\n\")[0]), \"=\"\n",
      "            )\n",
      "            + \"\\n\"\n",
      "            + tab_str\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class KernelEventEntry:\n",
      "    ts: int\n",
      "    tid: int\n",
      "    dur: int\n",
      "    category: CUDAKernelTimeCategory\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class KernelEventBoundary:\n",
      "    ts: int\n",
      "    is_start: bool\n",
      "    category: CUDAKernelTimeCategory\n",
      "\n",
      "\n",
      "def kernelStatFromEvents(\n",
      "    entries: List[KernelEventEntry],\n",
      "    global_start_ts,\n",
      "    global_end_ts,\n",
      "):\n",
      "    events: List[KernelEventBoundary] = []\n",
      "    for entry in entries:\n",
      "        events.append(KernelEventBoundary(entry.ts, True, entry.category))\n",
      "        events.append(KernelEventBoundary(entry.ts + entry.dur, False, entry.category))\n",
      "    # A trick to count for idle time waiting other processes\n",
      "    events.append(\n",
      "        KernelEventBoundary(global_start_ts, True, CUDAKernelTimeCategory.IDLE)\n",
      "    )\n",
      "    events.append(\n",
      "        KernelEventBoundary(global_end_ts, False, CUDAKernelTimeCategory.IDLE)\n",
      "    )\n",
      "\n",
      "    events.sort(key=lambda x: x.ts)\n",
      "\n",
      "    times = {k: 0 for k in CUDAKernelTimeCategory}\n",
      "    active = {k: 0 for k in CUDAKernelTimeCategory}\n",
      "\n",
      "    current_time = events[0].ts\n",
      "\n",
      "    for i in range(len(events)):\n",
      "        next_time = events[i].ts\n",
      "\n",
      "        # Priority: compute > communication > memory > misc > idle\n",
      "        if i > 0 and next_time != current_time:\n",
      "            duration = next_time - current_time\n",
      "            if active[CUDAKernelTimeCategory.COMPUTE] > 0:\n",
      "                times[CUDAKernelTimeCategory.COMPUTE] += duration\n",
      "            elif active[CUDAKernelTimeCategory.COLL_COMM] > 0:\n",
      "                times[CUDAKernelTimeCategory.COLL_COMM] += duration\n",
      "            elif active[CUDAKernelTimeCategory.P2P_COMM] > 0:\n",
      "                times[CUDAKernelTimeCategory.P2P_COMM] += duration\n",
      "            elif active[CUDAKernelTimeCategory.MEM] > 0:\n",
      "                times[CUDAKernelTimeCategory.MEM] += duration\n",
      "            elif active[CUDAKernelTimeCategory.MISC] > 0:\n",
      "                times[CUDAKernelTimeCategory.MISC] += duration\n",
      "            else:\n",
      "                times[CUDAKernelTimeCategory.IDLE] += duration\n",
      "        active[events[i].category] += 1 if events[i].is_start else -1\n",
      "        current_time = next_time\n",
      "\n",
      "    assert all(v == 0 for v in active.values()), active\n",
      "    return CUDAKernelTimeStat(world_size=1, **{k.value: v for k, v in times.items()})\n",
      "\n",
      "\n",
      "async def _load_events_async(file_path, semaphore) -> List[Dict]:\n",
      "    import aiofiles\n",
      "\n",
      "    async with semaphore:\n",
      "        pid = int(file_path.rstrip(\".json\").split(\"_r\")[-1])\n",
      "        async with aiofiles.open(file_path, \"r\") as f:\n",
      "            content = await f.read()\n",
      "        events = json.loads(content)[\"traceEvents\"]\n",
      "        events = list(\n",
      "            filter(\n",
      "                lambda x: \"cat\" in x and x[\"cat\"] in [\"gpu_user_annotation\", \"kernel\"],\n",
      "                events,\n",
      "            )\n",
      "        )\n",
      "        # Replace with the actual process id, starting from 0 to #gpus-1\n",
      "        for ev in events:\n",
      "            ev[\"pid\"] = pid\n",
      "    return events, pid\n",
      "\n",
      "\n",
      "async def _load_all_events(root_dir, mfc_name) -> List[Dict]:\n",
      "    trace_file_paths = []\n",
      "    for fn in os.listdir(root_dir):\n",
      "        if not fn.startswith(mfc_name):\n",
      "            continue\n",
      "        trace_file_paths.append(os.path.join(root_dir, fn))\n",
      "\n",
      "    # The JSON file can be large, up to 2GB. Load them concurrently.\n",
      "    semaphore = asyncio.Semaphore(8)\n",
      "    tasks = [_load_events_async(file_path, semaphore) for file_path in trace_file_paths]\n",
      "\n",
      "    all_events = {}\n",
      "    for coro in tqdm.asyncio.tqdm(\n",
      "        asyncio.as_completed(tasks), total=len(tasks), desc=\"Loading JSON files\"\n",
      "    ):\n",
      "        try:\n",
      "            events, pid = await coro\n",
      "            all_events[pid] = events\n",
      "        except Exception as e:\n",
      "            print(f\"Error loading JSON file: {e}\")\n",
      "\n",
      "    return all_events\n",
      "\n",
      "\n",
      "def kernelStatFromTrace(root_dir: str, mfc_name: str):\n",
      "    cache_file = os.path.join(root_dir, f\"_cached_{mfc_name}.json\")\n",
      "    if os.path.exists(cache_file):\n",
      "        logger.info(f'Loading trace JSON files of MFC \"{mfc_name}\" from cache...')\n",
      "        with open(cache_file, \"r\") as f:\n",
      "            all_events = json.load(f)\n",
      "        all_events = {int(pid): v for pid, v in all_events.items()}\n",
      "    else:\n",
      "        if not any(fn.startswith(mfc_name) for fn in os.listdir(root_dir)):\n",
      "            raise RuntimeError(\n",
      "                f\"No trace file found for the given MFC name: {mfc_name}.\"\n",
      "            )\n",
      "\n",
      "        load_json_tik = time.perf_counter()\n",
      "        logger.info(\n",
      "            f'Loading trace JSON files of MFC \"{mfc_name}\" concurrently from {root_dir}...'\n",
      "        )\n",
      "        all_events: Dict[int, List[Dict]] = asyncio.run(\n",
      "            _load_all_events(root_dir, mfc_name)\n",
      "        )\n",
      "        logger.info(\n",
      "            f\"{len(all_events)} JSON file loaded. \"\n",
      "            f\"Time consumption: {time.perf_counter() - load_json_tik:.2f} secs. \"\n",
      "            f\"Processing...\"\n",
      "        )\n",
      "\n",
      "        with open(cache_file, \"w\") as f:\n",
      "            json.dump(all_events, f)\n",
      "\n",
      "    # To remove the wait time from nccl send/recv, collect send/recv kernels annotations.\n",
      "    # These annotations look like \"nccl:send 0->1\". For each annotation, find the execution time\n",
      "    # of its pair. The actual execution time should the minimum of the two.\n",
      "    send_recv_annotations = {\n",
      "        pid: [\n",
      "            ev\n",
      "            for ev in events\n",
      "            if ev[\"cat\"] == \"gpu_user_annotation\"\n",
      "            and ev[\"name\"].startswith(\"nccl:recv\")\n",
      "            or ev[\"name\"].startswith(\"nccl:send\")\n",
      "        ]\n",
      "        for pid, events in all_events.items()\n",
      "    }\n",
      "    for events in send_recv_annotations.values():\n",
      "        events.sort(key=lambda x: x[\"ts\"])\n",
      "\n",
      "    send_recv_time = {pid: [] for pid, events in send_recv_annotations.items()}\n",
      "\n",
      "    def _matches_next_sr(type_, src, dst):\n",
      "        if type_ == \"send\":\n",
      "            annot = send_recv_annotations[dst][0]\n",
      "            m = re.match(r\"nccl:recv (\\d+)<-(\\d+)\", annot[\"name\"])\n",
      "            if not m:\n",
      "                return False\n",
      "            peer_dst, peer_src = map(int, m.groups())\n",
      "            if peer_src != src or peer_dst != dst:\n",
      "                return False\n",
      "            return True\n",
      "        else:\n",
      "            assert type_ == \"recv\"\n",
      "            annot = send_recv_annotations[src][0]\n",
      "            m = re.match(r\"nccl:send (\\d+)->(\\d+)\", annot[\"name\"])\n",
      "            if not m:\n",
      "                return False\n",
      "            peer_src, peer_dst = map(int, m.groups())\n",
      "            if peer_src != src or peer_dst != dst:\n",
      "                return False\n",
      "            return True\n",
      "\n",
      "    def resolve_next_sr_time(pid):\n",
      "        # Resolve send/recv time recursively, just like a Tetris game\n",
      "        annot = send_recv_annotations[pid][0]\n",
      "        if annot[\"name\"].startswith(\"nccl:send\"):\n",
      "            src, dst = map(\n",
      "                int, re.match(r\"nccl:send (\\d+)->(\\d+)\", annot[\"name\"]).groups()\n",
      "            )\n",
      "            assert src == pid, (src, pid)\n",
      "            while not _matches_next_sr(\"send\", src, dst):\n",
      "                resolve_next_sr_time(dst)\n",
      "        else:\n",
      "            assert annot[\"name\"].startswith(\"nccl:recv\")\n",
      "            dst, src = map(\n",
      "                int, re.match(r\"nccl:recv (\\d+)<-(\\d+)\", annot[\"name\"]).groups()\n",
      "            )\n",
      "            assert dst == pid, (dst, pid)\n",
      "            while not _matches_next_sr(\"recv\", src, dst):\n",
      "                resolve_next_sr_time(src)\n",
      "        ev1, ev2 = send_recv_annotations[src].pop(0), send_recv_annotations[dst].pop(0)\n",
      "        dur = min(ev1[\"dur\"], ev2[\"dur\"])\n",
      "        send_recv_time[src].append(dur)\n",
      "        send_recv_time[dst].append(dur)\n",
      "\n",
      "    for pid in tqdm.tqdm(all_events, desc=\"Resolving send/recv times\"):\n",
      "        while len(send_recv_annotations[pid]) > 0:\n",
      "            resolve_next_sr_time(pid)\n",
      "\n",
      "    kernel_events: Dict[int, List[KernelEventEntry]] = defaultdict(list)\n",
      "    global_start = min(ev[\"ts\"] for events in all_events.values() for ev in events)\n",
      "    global_end = max(ev[\"ts\"] for events in all_events.values() for ev in events)\n",
      "    for pid in tqdm.tqdm(all_events, desc=\"Processing events\"):\n",
      "        for ev in all_events[pid]:\n",
      "            if ev[\"cat\"] != \"kernel\":\n",
      "                continue\n",
      "            assert ev[\"dur\"] > 0, ev\n",
      "            cat = CUDAKernelTimeCategory.from_name(ev[\"name\"])\n",
      "            if cat == CUDAKernelTimeCategory.P2P_COMM:\n",
      "                assert len(send_recv_time[pid]) > 0\n",
      "                ev[\"dur\"] = send_recv_time[pid].pop(0)\n",
      "            kernel_events[pid].append(\n",
      "                KernelEventEntry(\n",
      "                    ts=ev[\"ts\"], tid=ev[\"tid\"], dur=ev[\"dur\"], category=cat\n",
      "                )\n",
      "            )\n",
      "    assert all(len(times) == 0 for times in send_recv_time.values()), [\n",
      "        len(times) == 0 for times in send_recv_time.values()\n",
      "    ]\n",
      "    for events in kernel_events.values():\n",
      "        events.sort(key=lambda x: x.ts)\n",
      "\n",
      "    x = None\n",
      "    for events in tqdm.tqdm(\n",
      "        kernel_events.values(),\n",
      "        total=len(kernel_events),\n",
      "        desc=\"Gathering kernel time stats for all processes...\",\n",
      "    ):\n",
      "        stats = kernelStatFromEvents(events, global_start, global_end)\n",
      "        if x is None:\n",
      "            x = stats\n",
      "        else:\n",
      "            x = x + stats\n",
      "    return x\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/ray_utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "import subprocess\n",
      "\n",
      "\n",
      "def check_ray_availability():\n",
      "    return (\n",
      "        int(\n",
      "            subprocess.run(\n",
      "                [\"ray\", \"--help\"],\n",
      "                stdout=open(os.devnull, \"wb\"),\n",
      "                stderr=open(os.devnull, \"wb\"),\n",
      "            ).returncode\n",
      "        )\n",
      "        == 0\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/saveload_utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "import shutil\n",
      "from typing import Dict\n",
      "\n",
      "import torch\n",
      "import tqdm\n",
      "from safetensors import safe_open\n",
      "\n",
      "from realhf.base import logging\n",
      "\n",
      "logger = logging.getLogger(\"SaveLoad\")\n",
      "\n",
      "\n",
      "def split_state_dict_into_shards(state_dict: Dict, n_shards: int) -> Dict:\n",
      "    if n_shards == 1:\n",
      "        return [state_dict]\n",
      "\n",
      "    keys = list(state_dict.keys())\n",
      "    if len(keys) < n_shards:\n",
      "        raise ValueError(f\"state_dict has {len(keys)} keys, but n_shards={n_shards}\")\n",
      "\n",
      "    shard_size = len(keys) // n_shards\n",
      "    extra = len(keys) % n_shards\n",
      "    shard_size_list = [shard_size for _ in range(n_shards)]\n",
      "    shard_size_list[-1] = shard_size + extra\n",
      "    start, shards = 0, []\n",
      "    for i, size in enumerate(\n",
      "        tqdm.tqdm(\n",
      "            shard_size_list,\n",
      "            desc=f\"Splitting state dict into {len(shard_size_list)} shards...\",\n",
      "        )\n",
      "    ):\n",
      "        shard = {}\n",
      "        for j in range(start, start + size):\n",
      "            shard[keys[j]] = state_dict[keys[j]]\n",
      "        start += size\n",
      "        shards.append(shard)\n",
      "    return shards\n",
      "\n",
      "\n",
      "HF_MODEL_CONFIG_FILES = [\n",
      "    \"generation_config.json\",\n",
      "    \"tokenizer_config.json\",\n",
      "    \"vocab.json\",\n",
      "    \"merges.txt\",\n",
      "    \"special_tokens_map.json\",\n",
      "    \"tokenizer.json\",\n",
      "]\n",
      "\n",
      "\n",
      "def copy_hf_configs(src_model_dir, dst_model_dir):\n",
      "    for file in HF_MODEL_CONFIG_FILES:\n",
      "        try:\n",
      "            shutil.copy(\n",
      "                os.path.join(src_model_dir, file),\n",
      "                os.path.join(dst_model_dir, file),\n",
      "            )\n",
      "            logger.info(f\"copied {file} from {src_model_dir} to {dst_model_dir}\")\n",
      "        except FileNotFoundError:\n",
      "            logger.info(f\"{file} not exist in {src_model_dir} skipping.\")\n",
      "    # Copy remote codes\n",
      "    for file in os.listdir(src_model_dir):\n",
      "        for prefix in [\"chat_format\", \"configuration_\", \"modeling_\", \"tokenization_\"]:\n",
      "            if file.startswith(prefix) and file.endswith(\".py\"):\n",
      "                shutil.copy(\n",
      "                    os.path.join(src_model_dir, file),\n",
      "                    os.path.join(dst_model_dir, file),\n",
      "                )\n",
      "                logger.info(f\"copied {file} from {src_model_dir} to {dst_model_dir}\")\n",
      "\n",
      "\n",
      "def load_safetensor(fn: str) -> Dict[str, torch.Tensor]:\n",
      "    assert fn.endswith(\".safetensors\")\n",
      "    state_dict = {}\n",
      "    with safe_open(fn, framework=\"pt\", device=\"cpu\") as f:\n",
      "        for key in f.keys():\n",
      "            state_dict[key] = f.get_tensor(key)\n",
      "    return state_dict\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/gpu_utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import itertools\n",
      "import os\n",
      "import platform\n",
      "import socket\n",
      "import time\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "import realhf.base.name_resolve as name_resolve\n",
      "import realhf.base.names as names\n",
      "import realhf.base.network as network\n",
      "\n",
      "logger = logging.getLogger(\"System-GPU\", \"system\")\n",
      "\n",
      "GPU_DEVICES_ISOLATED = False\n",
      "GLOBAL_PROCESS_GROUP_NAME = \"master\"\n",
      "\n",
      "\n",
      "def gpu_count():\n",
      "    \"\"\"Returns the number of gpus on a node.\n",
      "\n",
      "    Ad-hoc to frl cluster.\n",
      "    \"\"\"\n",
      "    if platform.system() == \"Darwin\":\n",
      "        return 0\n",
      "    elif platform.system() == \"Windows\":\n",
      "        try:\n",
      "            import torch\n",
      "\n",
      "            return torch.cuda.device_count()\n",
      "        except ImportError:\n",
      "            return 0\n",
      "    else:\n",
      "        dev_directories = list(os.listdir(\"/dev/\"))\n",
      "        for cnt in itertools.count():\n",
      "            if \"nvidia\" + str(cnt) in dev_directories:\n",
      "                continue\n",
      "            else:\n",
      "                break\n",
      "        return cnt\n",
      "\n",
      "\n",
      "def set_cuda_device(device):\n",
      "    \"\"\"Set the default cuda-device.\n",
      "\n",
      "    Useful on multi-gpu nodes. Should be called in every gpu-thread.\n",
      "    \"\"\"\n",
      "    # logger.info(f\"Setting device to {device}.\")\n",
      "    if device != \"cpu\":\n",
      "        import torch\n",
      "\n",
      "        torch.cuda.set_device(device)\n",
      "\n",
      "\n",
      "def reveal_pg_identity(expr_name, trial_name, worker_index):\n",
      "    master_group_name = names.distributed_peer(\n",
      "        expr_name, trial_name, GLOBAL_PROCESS_GROUP_NAME\n",
      "    )\n",
      "    name_resolve.add_subentry(master_group_name, str(worker_index))\n",
      "\n",
      "\n",
      "def isolate_cuda_device(\n",
      "    worker_type: str,\n",
      "    rank: int,\n",
      "    world_size: int,\n",
      "    experiment_name: str,\n",
      "    trial_name: str,\n",
      "):\n",
      "    \"\"\"Isolate CUDA_VISIBLE_DEVICES for each Slurm jobstep.\n",
      "\n",
      "    To distinguish the concept of job/jobstep/worker/task, check scheduler/slurm/utils.py.\n",
      "    A slurm job with multiple jobsteps will not set CUDA_VISIBLE_DEVICES properly.\n",
      "    For example, if a job has 2 jobsteps, each with 1 GPU, and is allocated onto GPU 0 and 1,\n",
      "    then CUDA_VISIBLE_DEVICES of these jobsteps will be 0,1, instead of 0 and 1.\n",
      "    We use this function in `apps.remote` to isolate CUDA_VISIBLE_DEVICES for each jobstep.\n",
      "\n",
      "    Args:\n",
      "        worker_type (str): .\n",
      "        rank (int): Rank of the **jobstep**.\n",
      "        world_size (int): Size of the **jobsteps**, aka SLURM_NPROCS. However, we may call this function\n",
      "            in other schedulers (e.g. local scheduler), so we don't use SLURM_NPROCS directly.\n",
      "        experiment_name (str): .\n",
      "        trial_name (str): .\n",
      "    \"\"\"\n",
      "    if not os.environ.get(\"CUDA_VISIBLE_DEVICES\"):\n",
      "        return\n",
      "\n",
      "    name_resolve_identifier = f\"__type_{worker_type}\"\n",
      "    name_resolve.add_subentry(\n",
      "        names.distributed_local_peer(\n",
      "            experiment_name,\n",
      "            trial_name,\n",
      "            socket.gethostname(),\n",
      "            name_resolve_identifier,\n",
      "        ),\n",
      "        rank,\n",
      "    )\n",
      "    name_resolve.add_subentry(\n",
      "        names.distributed_peer(experiment_name, trial_name, name_resolve_identifier),\n",
      "        rank,\n",
      "    )\n",
      "    logger.debug(\n",
      "        f\"Worker type {worker_type} rank {rank} waiting for peers, world size {world_size}...\"\n",
      "    )\n",
      "    while (\n",
      "        len(\n",
      "            name_resolve.get_subtree(\n",
      "                names.distributed_peer(\n",
      "                    experiment_name, trial_name, name_resolve_identifier\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "        < world_size\n",
      "    ):\n",
      "        time.sleep(0.1)\n",
      "    # logger.info(f\"Rank {rank} discovers all peers, resolving local rank...\")\n",
      "    local_peer_name = names.distributed_local_peer(\n",
      "        experiment_name,\n",
      "        trial_name,\n",
      "        socket.gethostname(),\n",
      "        name_resolve_identifier,\n",
      "    )\n",
      "    local_peers = list(\n",
      "        [\n",
      "            str(x)\n",
      "            for x in sorted([int(x) for x in name_resolve.get_subtree(local_peer_name)])\n",
      "        ]\n",
      "    )\n",
      "    # logger.info(f\"Rank {rank} discovers local peers with global ranks {local_peers}\")\n",
      "\n",
      "    local_peer_index = local_peers.index(str(rank))\n",
      "    n_local_peers = len(local_peers)\n",
      "    visible_devices = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
      "    n_visible_devices = len(visible_devices)\n",
      "    if n_visible_devices == 0:\n",
      "        raise RuntimeError(\n",
      "            f\"No visible cuda devices: {os.environ['CUDA_VISIBLE_DEVICES']}\"\n",
      "        )\n",
      "    if n_visible_devices == n_local_peers:\n",
      "        local_gpu_id = visible_devices[local_peer_index]\n",
      "    elif n_visible_devices == 1:\n",
      "        local_gpu_id = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
      "    elif n_visible_devices % n_local_peers == 0:\n",
      "        # A process occupies multiple GPUs, e.g., TP generation server\n",
      "        factor = n_visible_devices // n_local_peers\n",
      "        local_gpu_id = visible_devices[factor * local_peer_index]\n",
      "    else:\n",
      "        if not os.environ.get(\"REAL_MODE\") == \"LOCAL\":\n",
      "            raise RuntimeError(\n",
      "                f\"Unresolvable CUDA_VISIBLE_DEVICES {os.environ['CUDA_VISIBLE_DEVICES']} on host {network.gethostname()}, \"\n",
      "                f\"local peers (global ranks) {local_peers}, local peer index {local_peer_index}.\"\n",
      "            )\n",
      "        # In the local mode, all processes use GPUs in a round-robin manner\n",
      "        devices = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
      "        local_gpu_id = int(devices[local_peer_index % len(devices)])\n",
      "\n",
      "    # logger.info(\n",
      "    #     f\"Worker type {worker_type} rank {rank} running on host {socket.gethostname()}, \"\n",
      "    #     f\"local peer index: {local_peer_index}, local gpu id {local_gpu_id}.\"\n",
      "    # )\n",
      "\n",
      "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(local_gpu_id)\n",
      "    os.environ[\"GPU_DEVICES_ISOLATED\"] = \"1\"\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/pkg_version.py ====\n",
      "\n",
      "from importlib.metadata import PackageNotFoundError\n",
      "from importlib.metadata import version as get_version\n",
      "\n",
      "from packaging.version import Version\n",
      "\n",
      "\n",
      "def is_available(pkg_name):\n",
      "    try:\n",
      "        return bool(get_version(pkg_name))\n",
      "    except PackageNotFoundError:\n",
      "        return False\n",
      "\n",
      "\n",
      "def compare_versions(version1: str, version2: str) -> int:\n",
      "    \"\"\"\n",
      "    Compare two version strings.\n",
      "\n",
      "    :param version1: First version string.\n",
      "    :param version2: Second version string.\n",
      "    :return: -1 if version1 < version2, 0 if version1 == version2, 1 if version1 > version2.\n",
      "    \"\"\"\n",
      "    v1 = Version(version1)\n",
      "    v2 = Version(version2)\n",
      "    if v1 < v2:\n",
      "        return -1\n",
      "    elif v1 == v2:\n",
      "        return 0\n",
      "    else:\n",
      "        return 1\n",
      "\n",
      "\n",
      "def is_version_greater_or_equal(package_name: str, target_version: str) -> bool:\n",
      "    \"\"\"\n",
      "    Check if the installed version of a package is greater than or equal to the target version.\n",
      "\n",
      "    :param package_name: Name of the package.\n",
      "    :param target_version: Target version to compare against.\n",
      "    :return: True if the installed version is greater than or equal to the target version, False otherwise.\n",
      "    \"\"\"\n",
      "    installed_version = get_version(package_name)\n",
      "    return compare_versions(installed_version, target_version) >= 0\n",
      "\n",
      "\n",
      "def is_version_less(package_name: str, target_version: str) -> bool:\n",
      "    \"\"\"\n",
      "    Check if the installed version of a package is less than the target version.\n",
      "\n",
      "    :param package_name: Name of the package.\n",
      "    :param target_version: Target version to compare against.\n",
      "    :return: True if the installed version is less than the target version, False otherwise.\n",
      "    \"\"\"\n",
      "    installed_version = get_version(package_name)\n",
      "    return compare_versions(installed_version, target_version) < 0\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/cluster.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import TYPE_CHECKING, Dict\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from realhf.api.cli_args import BaseExperimentConfig\n",
      "\n",
      "\n",
      "class ClusterSpec:\n",
      "    def __init__(self):\n",
      "        # Set default values to comfort ray\n",
      "        from realhf.api.cli_args import BaseExperimentConfig\n",
      "\n",
      "        self.load_spec_from_args(BaseExperimentConfig())\n",
      "\n",
      "        self.__loaded = False\n",
      "\n",
      "    def load_spec_from_file(self, file_path: str):\n",
      "        if not os.path.exists(file_path):\n",
      "            raise FileNotFoundError(f\"Cluster spec file not found: {file_path}\")\n",
      "\n",
      "        with open(file_path, \"r\") as f:\n",
      "            spec: Dict = json.load(f)\n",
      "\n",
      "        self.__cluster_type = spec[\"cluster_type\"]\n",
      "        self.__cluster_name = spec[\"cluster_name\"]\n",
      "        self.__fileroot = spec[\"fileroot\"]\n",
      "        self.__gpu_type = spec.get(\"gpu_type\", None)\n",
      "        self.__mount = spec.get(\"default_mount\", None)\n",
      "        self.__gpu_image = spec.get(\"gpu_image\", None)\n",
      "        self.__gpu_infer_image = spec.get(\"gpu_infer_image\", self.__gpu_image)\n",
      "        self.__cpu_image = spec.get(\"cpu_image\", None)\n",
      "        self.__node_name_prefix = spec.get(\"node_name_prefix\", \"slurmd-\")\n",
      "        # self.__n_nodes decides number of digits in slurm hostnames\n",
      "        # e.g. if __n_nodes = 32, then the hostnames will be slurmd-{:02d}\n",
      "        #      if __n_nodes = 128, then the hostnames will be slurmd-{:03d}\n",
      "        self.__n_nodes = int(spec.get(\"n_nodes\", 32))\n",
      "        self.__n_gpus_per_node = int(spec.get(\"n_gpus_per_node\", 8))\n",
      "        assert isinstance(self.__n_nodes, int)\n",
      "\n",
      "        self.__loaded = True\n",
      "\n",
      "    def load_spec_from_args(self, args: \"BaseExperimentConfig\"):\n",
      "        self.__cluster_type = args.mode\n",
      "        self.__cluster_name = args.cluster.cluster_name\n",
      "        self.__fileroot = args.cluster.fileroot\n",
      "        self.__gpu_type = args.cluster.gpu_type\n",
      "        self.__mount = args.cluster.mount\n",
      "        self.__gpu_image = args.cluster.gpu_image\n",
      "        self.__gpu_infer_image = args.cluster.gpu_infer_image\n",
      "        self.__cpu_image = args.cluster.cpu_image\n",
      "        self.__node_name_prefix = args.cluster.node_name_prefix\n",
      "        self.__n_nodes = args.cluster.n_nodes\n",
      "        self.__n_gpus_per_node = args.cluster.n_gpus_per_node\n",
      "        self.__loaded = True\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        assert self.__loaded\n",
      "        return self.__cluster_name\n",
      "\n",
      "    @property\n",
      "    def gpu_type(self):\n",
      "        assert self.__loaded\n",
      "        return self.__gpu_type\n",
      "\n",
      "    @property\n",
      "    def fileroot(self) -> str:\n",
      "        \"\"\"Return the root directory of the file system in the cluster.\n",
      "\n",
      "        When running experiments, files such as logs, checkpoints,\n",
      "        caches will be saved under this directory.\n",
      "        \"\"\"\n",
      "        assert self.__loaded\n",
      "        return self.__fileroot\n",
      "\n",
      "    @fileroot.setter\n",
      "    def fileroot(self, root: str):\n",
      "        # Used for testing\n",
      "        self.__fileroot = root\n",
      "\n",
      "    @property\n",
      "    def mount(self) -> str:\n",
      "        \"\"\"Directories that should be mounted to container that runs\n",
      "        workers.\"\"\"\n",
      "        assert self.__loaded\n",
      "        return self.__mount\n",
      "\n",
      "    @property\n",
      "    def gpu_image(self) -> str:\n",
      "        \"\"\"Return the default image for containers of GPU trainer workers.\"\"\"\n",
      "        assert self.__loaded\n",
      "        return self.__gpu_image\n",
      "\n",
      "    @property\n",
      "    def gpu_infer_image(self) -> str:\n",
      "        \"\"\"Return the default image for containers of GPU inference workers.\"\"\"\n",
      "        assert self.__loaded\n",
      "        return self.__gpu_infer_image\n",
      "\n",
      "    @property\n",
      "    def cpu_image(self) -> str:\n",
      "        \"\"\"Return the default image for containers of CPU workers.\"\"\"\n",
      "        assert self.__loaded\n",
      "        return self.__cpu_image\n",
      "\n",
      "    @property\n",
      "    def node_name_prefix(self) -> str:\n",
      "        \"\"\"Return the prefix of node names in slurm format.\"\"\"\n",
      "        assert self.__loaded\n",
      "        return self.__node_name_prefix\n",
      "\n",
      "    @property\n",
      "    def n_nodes(self) -> int:\n",
      "        return self.__n_nodes\n",
      "\n",
      "    @property\n",
      "    def suffix_n_digits(self) -> int:\n",
      "        return len(str(self.__n_nodes))\n",
      "\n",
      "    @property\n",
      "    def n_gpus_per_node(self) -> int:\n",
      "        return self.__n_gpus_per_node\n",
      "\n",
      "    @property\n",
      "    def cluster_type(self) -> str:\n",
      "        return self.__cluster_type\n",
      "\n",
      "\n",
      "spec = ClusterSpec()\n",
      "\n",
      "\n",
      "def init_cluster_spec(args: \"BaseExperimentConfig\"):\n",
      "    global spec\n",
      "    CLUSTER_SPEC_PATH = os.environ.get(\"CLUSTER_SPEC_PATH\", \"\")\n",
      "    if args.cluster.config_path:\n",
      "        spec.load_spec_from_file(args.cluster.config_path)\n",
      "    elif CLUSTER_SPEC_PATH:\n",
      "        spec.load_spec_from_file(CLUSTER_SPEC_PATH)\n",
      "    else:\n",
      "        spec.load_spec_from_args(args)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/name_resolve.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# Implements a simple name resolving service, which can be considered as a distributed key-value dict.\n",
      "import dataclasses\n",
      "import os\n",
      "import queue\n",
      "import random\n",
      "import shutil\n",
      "import threading\n",
      "import time\n",
      "import uuid\n",
      "from typing import Callable, List, Optional\n",
      "\n",
      "import ray\n",
      "\n",
      "try:\n",
      "    import etcd3\n",
      "except Exception:\n",
      "    etcd3 = None\n",
      "\n",
      "from realhf.base import logging, security, timeutil\n",
      "\n",
      "logger = logging.getLogger(\"name-resolve\")\n",
      "\n",
      "\n",
      "class ArgumentError(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class NameEntryExistsError(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class NameEntryNotFoundError(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class NameRecordRepository:\n",
      "\n",
      "    def __del__(self):\n",
      "        try:\n",
      "            self.reset()\n",
      "        except Exception as e:\n",
      "            logger.info(f\"Exception ignore when deleting NameResolveRepo {e}\")\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "        self.reset()\n",
      "\n",
      "    def add(\n",
      "        self,\n",
      "        name,\n",
      "        value,\n",
      "        delete_on_exit=True,\n",
      "        keepalive_ttl=None,\n",
      "        replace=False,\n",
      "    ):\n",
      "        \"\"\"Creates a name record in the central repository.\n",
      "\n",
      "        In our semantics, the name record repository is essentially a multimap (i.e. Dict[str, Set[str]]).\n",
      "        This class keeps a single name->value map, where the name can be non-unique while the value has to be.\n",
      "        The class also deletes the (name, value) pair on exits (__exit__/__del__) if opted-in. In case of\n",
      "        preventing unexpected exits (i.e. process crashes without calling graceful exits), an user may also\n",
      "        want to specify time_to_live and call touch() regularly to allow a more consistent\n",
      "\n",
      "        Args:\n",
      "            name: The key of the record. It has to be a valid path-like string; e.g. \"a/b/c\". If the name\n",
      "                already exists, the behaviour is defined by the `replace` argument.\n",
      "            value: The value of the record. This can be any valid string.\n",
      "            delete_on_exit: If the record shall be deleted when the repository closes.\n",
      "            keepalive_ttl: If not None, adds a time-to-live in seconds for the record. The repository\n",
      "                shall keep pinging the backend service with at least this frequency to make sure the name\n",
      "                entry is alive during the lifetime of the repository. On the other hand, specifying this\n",
      "                prevents stale keys caused by the scenario that a Python process accidentally crashes before\n",
      "                calling delete().\n",
      "            replace: If the name already exists, then replaces the current value with the supplied value if\n",
      "                `replace` is True, or raises exception if `replace` is False.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def add_subentry(self, name, value, **kwargs):\n",
      "        \"\"\"Adds a sub-entry to the key-root `name`.\n",
      "\n",
      "        The values is retrievable by get_subtree() given that no other\n",
      "        entries use the name prefix.\n",
      "        \"\"\"\n",
      "        sub_name = os.path.join(os.path.normpath(name), str(uuid.uuid4())[:8])\n",
      "        self.add(sub_name, value, **kwargs)\n",
      "        return sub_name\n",
      "\n",
      "    def delete(self, name):\n",
      "        \"\"\"Deletes an existing record.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def clear_subtree(self, name_root):\n",
      "        \"\"\"Deletes all records whose names start with the path root name_root;\n",
      "        specifically, whose name either is `name_root`, or starts with\n",
      "        `name_root.rstrip(\"/\") + \"/\"`.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def get(self, name):\n",
      "        \"\"\"Returns the value of the key.\n",
      "\n",
      "        Raises NameEntryNotFoundError if not found.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def get_subtree(self, name_root):\n",
      "        \"\"\"Returns all values whose names start with the path root name_root;\n",
      "        specifically, whose name either is `name_root`, or starts with\n",
      "        `name_root.rstrip(\"/\") + \"/\"`.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def find_subtree(self, name_root):\n",
      "        \"\"\"Returns all KEYS whose names start with the path root name_root.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def wait(self, name, timeout=None, poll_frequency=1):\n",
      "        \"\"\"Waits until a name appears.\n",
      "\n",
      "        Raises:\n",
      "             TimeoutError: if timeout exceeds.\n",
      "        \"\"\"\n",
      "        start = time.monotonic()\n",
      "        while True:\n",
      "            try:\n",
      "                return self.get(name)\n",
      "            except NameEntryNotFoundError:\n",
      "                pass\n",
      "            if timeout is None or timeout > 0:\n",
      "                time.sleep(\n",
      "                    poll_frequency + random.random() * 0.1\n",
      "                )  # To reduce concurrency.\n",
      "            if timeout is not None and time.monotonic() - start > timeout:\n",
      "                raise TimeoutError(\n",
      "                    f\"Timeout waiting for key '{name}' ({self.__class__.__name__})\"\n",
      "                )\n",
      "\n",
      "    def reset(self):\n",
      "        \"\"\"Deletes all entries added via this repository instance's\n",
      "        add(delete_on_exit=True).\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def watch_names(\n",
      "        self,\n",
      "        names: List,\n",
      "        call_back: Callable,\n",
      "        poll_frequency=15,\n",
      "        wait_timeout=300,\n",
      "    ):\n",
      "        \"\"\"Watch a name, execute call_back when key is deleted.\"\"\"\n",
      "        if isinstance(names, str):\n",
      "            names = [names]\n",
      "\n",
      "        q = queue.Queue(maxsize=len(names))\n",
      "        for _ in range(len(names) - 1):\n",
      "            q.put(0)\n",
      "\n",
      "        def wrap_call_back():\n",
      "            try:\n",
      "                q.get_nowait()\n",
      "            except queue.Empty:\n",
      "                logger.info(f\"Key {names} is gone. Executing callback {call_back}\")\n",
      "                call_back()\n",
      "\n",
      "        for name in names:\n",
      "            t = threading.Thread(\n",
      "                target=self._watch_thread_run,\n",
      "                args=(name, wrap_call_back, poll_frequency, wait_timeout),\n",
      "                daemon=True,\n",
      "            )\n",
      "            t.start()\n",
      "\n",
      "    def _watch_thread_run(self, name, call_back, poll_frequency, wait_timeout):\n",
      "        self.wait(name, timeout=wait_timeout, poll_frequency=poll_frequency)\n",
      "        while True:\n",
      "            try:\n",
      "                self.get(name)\n",
      "                time.sleep(poll_frequency + random.random())\n",
      "            except NameEntryNotFoundError:\n",
      "                call_back()\n",
      "                break\n",
      "\n",
      "\n",
      "class MemoryNameRecordRepository(NameRecordRepository):\n",
      "    \"\"\"Stores all the records in a thread-local memory.\n",
      "\n",
      "    Note that this is most likely for testing purposes:\n",
      "    any distributed application is impossible to use this.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, log_events=False):\n",
      "        self.__store = {}\n",
      "        self.__to_delete = set()\n",
      "        self.__log_events = log_events\n",
      "\n",
      "    def add(\n",
      "        self,\n",
      "        name,\n",
      "        value,\n",
      "        delete_on_exit=True,\n",
      "        keepalive_ttl=None,\n",
      "        replace=False,\n",
      "    ):\n",
      "        if not name:\n",
      "            raise ValueError(f\"Invalid name: {name}\")\n",
      "        name = os.path.normpath(name)\n",
      "        if self.__log_events:\n",
      "            print(f\"NameResolve: add {name} {value}\")\n",
      "        if name in self.__store and not replace:\n",
      "            raise NameEntryExistsError(f\"K={name} V={self.__store[name]} V2={value}\")\n",
      "        self.__store[name] = str(value)\n",
      "        if delete_on_exit:\n",
      "            self.__to_delete.add(name)\n",
      "\n",
      "    def touch(self, name, value, new_time_to_live):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def delete(self, name):\n",
      "        if self.__log_events:\n",
      "            print(f\"NameResolve: delete {name}\")\n",
      "        if name not in self.__store:\n",
      "            raise NameEntryNotFoundError(f\"K={name}\")\n",
      "        if name in self.__to_delete:\n",
      "            self.__to_delete.remove(name)\n",
      "        del self.__store[name]\n",
      "\n",
      "    def clear_subtree(self, name_root):\n",
      "        if self.__log_events:\n",
      "            print(f\"NameResolve: clear_subtree {name_root}\")\n",
      "        name_root = os.path.normpath(name_root)\n",
      "        for name in list(self.__store):\n",
      "            if (\n",
      "                name_root == \"/\"\n",
      "                or name == name_root\n",
      "                or name.startswith(name_root + \"/\")\n",
      "            ):\n",
      "                if name in self.__to_delete:\n",
      "                    self.__to_delete.remove(name)\n",
      "                del self.__store[name]\n",
      "\n",
      "    def get(self, name):\n",
      "        name = os.path.normpath(name)\n",
      "        if name not in self.__store:\n",
      "            raise NameEntryNotFoundError(f\"K={name}\")\n",
      "        r = self.__store[name]\n",
      "        if self.__log_events:\n",
      "            print(f\"NameResolve: get {name} -> {r}\")\n",
      "        return r\n",
      "\n",
      "    def get_subtree(self, name_root):\n",
      "        if self.__log_events:\n",
      "            print(f\"NameResolve: get_subtree {name_root}\")\n",
      "        name_root = os.path.normpath(name_root)\n",
      "        rs = []\n",
      "        for name, value in self.__store.items():\n",
      "            if (\n",
      "                name_root == \"/\"\n",
      "                or name == name_root\n",
      "                or name.startswith(name_root + \"/\")\n",
      "            ):\n",
      "                rs.append(value)\n",
      "        return rs\n",
      "\n",
      "    def find_subtree(self, name_root):\n",
      "        if self.__log_events:\n",
      "            print(f\"NameResolve: find_subtree {name_root}\")\n",
      "        rs = []\n",
      "        for name in self.__store:\n",
      "            if (\n",
      "                name_root == \"/\"\n",
      "                or name == name_root\n",
      "                or name.startswith(name_root + \"/\")\n",
      "            ):\n",
      "                rs.append(name)\n",
      "        rs.sort()\n",
      "        return rs\n",
      "\n",
      "    def reset(self):\n",
      "        for name in self.__to_delete:\n",
      "            self.__store.pop(name)\n",
      "        self.__to_delete = set()\n",
      "\n",
      "\n",
      "class NfsNameRecordRepository(NameRecordRepository):\n",
      "    RECORD_ROOT = \"\"\n",
      "\n",
      "    def __init__(self, **kwargs):\n",
      "        self.__to_delete = set()\n",
      "\n",
      "    @staticmethod\n",
      "    def __dir_path(name):\n",
      "        if not NfsNameRecordRepository.RECORD_ROOT:\n",
      "            from realhf.base.cluster import spec as cluster_spec\n",
      "\n",
      "            RECORD_ROOT = f\"{cluster_spec.fileroot}/name_resolve/\"\n",
      "            os.makedirs(RECORD_ROOT, exist_ok=True)\n",
      "            NfsNameRecordRepository.RECORD_ROOT = RECORD_ROOT\n",
      "        return os.path.join(NfsNameRecordRepository.RECORD_ROOT, name)\n",
      "\n",
      "    @staticmethod\n",
      "    def __file_path(name):\n",
      "        return os.path.join(NfsNameRecordRepository.__dir_path(name), \"ENTRY\")\n",
      "\n",
      "    def add(\n",
      "        self,\n",
      "        name,\n",
      "        value,\n",
      "        delete_on_exit=True,\n",
      "        keepalive_ttl=None,\n",
      "        replace=False,\n",
      "    ):\n",
      "        if not name:\n",
      "            raise ValueError(\"Name cannot be empty\")\n",
      "        name = os.path.normpath(name)\n",
      "        path = self.__file_path(name)\n",
      "        while True:\n",
      "            # To avoid concurrency issues when multiple processes\n",
      "            # call makedirs on the same dirname of CPFS.\n",
      "            try:\n",
      "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
      "                break\n",
      "            except (NotADirectoryError, FileNotFoundError):\n",
      "                pass\n",
      "        if os.path.isfile(path) and not replace:\n",
      "            raise NameEntryExistsError(path)\n",
      "        local_id = str(uuid.uuid4())[:8]\n",
      "        with open(path + f\".tmp.{local_id}\", \"w\") as f:\n",
      "            f.write(str(value))\n",
      "        os.rename(path + f\".tmp.{local_id}\", path)\n",
      "        if delete_on_exit:\n",
      "            self.__to_delete.add(name)\n",
      "\n",
      "    def delete(self, name):\n",
      "        path = self.__file_path(name)\n",
      "        if not os.path.isfile(path):\n",
      "            raise NameEntryNotFoundError(path)\n",
      "        os.remove(path)\n",
      "        while True:\n",
      "            path = os.path.dirname(path)\n",
      "            if path == NfsNameRecordRepository.RECORD_ROOT:\n",
      "                break\n",
      "            if len(os.listdir(path)) > 0:\n",
      "                break\n",
      "            shutil.rmtree(path, ignore_errors=True)\n",
      "        if name in self.__to_delete:\n",
      "            self.__to_delete.remove(name)\n",
      "\n",
      "    def clear_subtree(self, name_root):\n",
      "        dir_path = self.__dir_path(name_root)\n",
      "        if os.path.isdir(dir_path):\n",
      "            logger.info(\"Removing name resolve path: %s\", dir_path)\n",
      "            shutil.rmtree(dir_path)\n",
      "        else:\n",
      "            logger.info(\"No such name resolve path: %s\", dir_path)\n",
      "\n",
      "    def get(self, name):\n",
      "        name = os.path.normpath(name)\n",
      "        path = self.__file_path(name)\n",
      "        if not os.path.isfile(path):\n",
      "            raise NameEntryNotFoundError(path)\n",
      "        for _ in range(100):\n",
      "            # HACK: dealing with the possible OSError: Stale file handle\n",
      "            try:\n",
      "                with open(path, \"r\") as f:\n",
      "                    return f.read().strip()\n",
      "            except OSError as e:\n",
      "                if e.errno == 116:\n",
      "                    time.sleep(5e-3)\n",
      "                    continue\n",
      "                raise e\n",
      "        raise RuntimeError(\"Failed to read value for %s\" % name)\n",
      "\n",
      "    def get_subtree(self, name_root):\n",
      "        dir_path = self.__dir_path(name_root)\n",
      "        rs = []\n",
      "        if os.path.isdir(dir_path):\n",
      "            for root, _, files in os.walk(dir_path):\n",
      "                try:\n",
      "                    if len(files) != 1:\n",
      "                        continue\n",
      "                    if files[0] != \"ENTRY\":\n",
      "                        continue\n",
      "                    key = root.removeprefix(self.RECORD_ROOT)\n",
      "                    key = key.removeprefix(\"/\")\n",
      "                    rs.append(self.get(key))\n",
      "                except NameEntryNotFoundError:\n",
      "                    pass\n",
      "        return rs\n",
      "\n",
      "    def find_subtree(self, name_root):\n",
      "        dir_path = self.__dir_path(name_root)\n",
      "        rs = []\n",
      "        if os.path.isdir(dir_path):\n",
      "            for root, _, files in os.walk(dir_path):\n",
      "                try:\n",
      "                    if len(files) != 1:\n",
      "                        continue\n",
      "                    if files[0] != \"ENTRY\":\n",
      "                        continue\n",
      "                    key = root.removeprefix(self.RECORD_ROOT)\n",
      "                    key = key.removeprefix(\"/\")\n",
      "                    rs.append(key)\n",
      "                except NameEntryNotFoundError:\n",
      "                    pass\n",
      "        rs.sort()\n",
      "        return rs\n",
      "\n",
      "    def reset(self):\n",
      "        for name in list(self.__to_delete):\n",
      "            try:\n",
      "                self.delete(name)\n",
      "            except:\n",
      "                pass\n",
      "        self.__to_delete = set()\n",
      "\n",
      "\n",
      "class RedisNameRecordRepository(NameRecordRepository):\n",
      "    _IS_FRL = False\n",
      "    REDIS_HOST = \"redis\" if _IS_FRL else \"localhost\"\n",
      "    REDIS_PASSWORD = security.read_key(\"redis\") if _IS_FRL else None\n",
      "    REDIS_DB = 0\n",
      "    KEEPALIVE_POLL_FREQUENCY = 1\n",
      "\n",
      "    @dataclasses.dataclass\n",
      "    class _Entry:\n",
      "        value: str\n",
      "        keepalive_ttl: Optional[int] = None\n",
      "        keeper: Optional[timeutil.FrequencyControl] = None\n",
      "\n",
      "    def __init__(self, **kwargs):\n",
      "        import redis\n",
      "        from redis.backoff import ExponentialBackoff\n",
      "        from redis.retry import Retry\n",
      "\n",
      "        super().__init__()\n",
      "        self.__lock = threading.Lock()\n",
      "        self.__redis = redis.Redis(\n",
      "            host=RedisNameRecordRepository.REDIS_HOST,\n",
      "            password=RedisNameRecordRepository.REDIS_PASSWORD,\n",
      "            db=RedisNameRecordRepository.REDIS_DB,\n",
      "            socket_timeout=60,\n",
      "            retry_on_timeout=True,\n",
      "            retry=Retry(ExponentialBackoff(180, 60), 3),\n",
      "        )\n",
      "        self.__entries = {}\n",
      "        self.__keepalive_running = True\n",
      "        self.__keepalive_thread = threading.Thread(\n",
      "            target=self.__keepalive_thread_run, daemon=True\n",
      "        )\n",
      "        self.__keepalive_thread.start()\n",
      "\n",
      "    def __del__(self):\n",
      "        self.__keepalive_running = False\n",
      "        self.__keepalive_thread.join(timeout=5)\n",
      "        self.reset()\n",
      "        self.__redis.close()\n",
      "\n",
      "    def add(self, name, value, delete_on_exit=True, keepalive_ttl=10, replace=False):\n",
      "        # deprecated parameter: delete_on_exit, now every entry has a default keepalive_ttl=10 seconds\n",
      "        if name.endswith(\"/\"):\n",
      "            raise ValueError(f\"Entry name cannot end with '/': {name}\")\n",
      "\n",
      "        with self.__lock:\n",
      "            keepalive_ttl = int(keepalive_ttl * 1000)\n",
      "            assert (\n",
      "                keepalive_ttl > 0\n",
      "            ), f\"keepalive_ttl in milliseconds must >0: {keepalive_ttl}\"\n",
      "            if self.__redis.set(name, value, px=keepalive_ttl, nx=not replace) is None:\n",
      "                raise NameEntryExistsError(f\"Cannot set Redis key: K={name} V={value}\")\n",
      "\n",
      "            # touch every 1/3 of keepalive_ttl to prevent Redis from deleting the key\n",
      "            # after program exit, redis will automatically delete key in keepalive_ttl\n",
      "            self.__entries[name] = self._Entry(\n",
      "                value=value,\n",
      "                keepalive_ttl=keepalive_ttl,\n",
      "                keeper=timeutil.FrequencyControl(\n",
      "                    frequency_seconds=keepalive_ttl / 1000 / 3\n",
      "                ),\n",
      "            )\n",
      "\n",
      "    def delete(self, name):\n",
      "        with self.__lock:\n",
      "            self.__delete_locked(name)\n",
      "\n",
      "    def __delete_locked(self, name):\n",
      "        if name in self.__entries:\n",
      "            del self.__entries[name]\n",
      "        if self.__redis.delete(name) == 0:\n",
      "            raise NameEntryNotFoundError(f\"No such Redis entry to delete: {name}\")\n",
      "\n",
      "    def clear_subtree(self, name_root):\n",
      "        with self.__lock:\n",
      "            count = 0\n",
      "            for name in list(self.__find_subtree_locked(name_root)):\n",
      "                try:\n",
      "                    self.__delete_locked(name)\n",
      "                    count += 1\n",
      "                except NameEntryNotFoundError:\n",
      "                    pass\n",
      "            logger.info(\"Deleted %d Redis entries under %s\", count, name_root)\n",
      "\n",
      "    def get(self, name):\n",
      "        with self.__lock:\n",
      "            return self.__get_locked(name)\n",
      "\n",
      "    def __get_locked(self, name):\n",
      "        r = self.__redis.get(name)\n",
      "        if r is None:\n",
      "            raise NameEntryNotFoundError(f\"No such Redis entry: {name}\")\n",
      "        return r.decode()\n",
      "\n",
      "    def get_subtree(self, name_root):\n",
      "        with self.__lock:\n",
      "            rs = []\n",
      "            for name in self.__find_subtree_locked(name_root):\n",
      "                rs.append(self.__get_locked(name))\n",
      "            rs.sort()\n",
      "            return rs\n",
      "\n",
      "    def find_subtree(self, name_root):\n",
      "        with self.__lock:\n",
      "            return list(sorted(self.__find_subtree_locked(name_root)))\n",
      "\n",
      "    def reset(self):\n",
      "        with self.__lock:\n",
      "            count = 0\n",
      "            for name in list(self.__entries):\n",
      "                try:\n",
      "                    self.__delete_locked(name)\n",
      "                    count += 1\n",
      "                except NameEntryNotFoundError:\n",
      "                    pass\n",
      "            self.__entries = {}\n",
      "            logger.info(\"Reset %d saved Redis entries\", count)\n",
      "\n",
      "    def __keepalive_thread_run(self):\n",
      "        while self.__keepalive_running:\n",
      "            time.sleep(self.KEEPALIVE_POLL_FREQUENCY)\n",
      "            with self.__lock:\n",
      "                for name, entry in self.__entries.items():\n",
      "                    if entry.keeper is not None and entry.keeper.check():\n",
      "                        r = self.__redis.set(name, entry.value, px=entry.keepalive_ttl)\n",
      "                        if r is None:\n",
      "                            logger.error(\n",
      "                                \"Failed touching Redis key: K=%s V=%s\",\n",
      "                                name,\n",
      "                                entry.value,\n",
      "                            )\n",
      "\n",
      "    def __find_subtree_locked(self, name_root):\n",
      "        pattern = name_root + \"*\"\n",
      "        return [k.decode() for k in self.__redis.keys(pattern=pattern)]\n",
      "\n",
      "    def _testonly_drop_cached_entry(self, name):\n",
      "        \"\"\"Used by unittest only to simulate the case that the Python process\n",
      "        crashes and the key is automatically removed after TTL.\"\"\"\n",
      "        with self.__lock:\n",
      "            del self.__entries[name]\n",
      "            print(\"Testonly: dropped key:\", name)\n",
      "\n",
      "\n",
      "class Etcd3NameRecordRepository(NameRecordRepository):\n",
      "    \"\"\"Implements a name record repository using etcd3 as the backend storage.\n",
      "\n",
      "    This implementation provides distributed key-value storage with support for\n",
      "    TTL-based expiration, atomic operations, and key watching functionality.\n",
      "    \"\"\"\n",
      "\n",
      "    # Default configuration\n",
      "    try:\n",
      "        host, port = os.getenv(\"REAL_ETCD_ADDR\", \"\").split(\":\")\n",
      "    except ValueError:\n",
      "        host, port = \"localhost\", 2379\n",
      "    ETCD_HOST = host\n",
      "    ETCD_PORT = int(port)\n",
      "    ETCD_USER = None\n",
      "    ETCD_PASSWORD = None\n",
      "    KEEPALIVE_POLL_FREQUENCY = 1\n",
      "\n",
      "    @dataclasses.dataclass\n",
      "    class _Entry:\n",
      "        value: str\n",
      "        lease_id: Optional[int] = None\n",
      "        keepalive_ttl: Optional[int] = None\n",
      "        keeper: Optional[timeutil.FrequencyControl] = None\n",
      "\n",
      "    def __init__(self, host=None, port=None, user=None, password=None, **kwargs):\n",
      "        \"\"\"Initialize the etcd3 name record repository.\n",
      "\n",
      "        Args:\n",
      "            host: etcd server host (defaults to ETCD_HOST)\n",
      "            port: etcd server port (defaults to ETCD_PORT)\n",
      "            user: etcd username for authentication (defaults to ETCD_USER)\n",
      "            password: etcd password for authentication (defaults to ETCD_PASSWORD)\n",
      "            **kwargs: Additional configuration parameters\n",
      "        \"\"\"\n",
      "\n",
      "        super().__init__()\n",
      "        self._lock = threading.Lock()\n",
      "\n",
      "        # Set connection parameters\n",
      "        self._host = host or self.ETCD_HOST\n",
      "        self._port = port or self.ETCD_PORT\n",
      "        self._user = user or self.ETCD_USER\n",
      "        self._password = password or self.ETCD_PASSWORD\n",
      "\n",
      "        # Connect to etcd\n",
      "        self._client = etcd3.client(\n",
      "            host=self._host, port=self._port, user=self._user, password=self._password\n",
      "        )\n",
      "\n",
      "        # Keep track of entries for cleanup and keepalive\n",
      "        self._entries = {}\n",
      "        self._keepalive_running = True\n",
      "        self._keepalive_thread = threading.Thread(\n",
      "            target=self._keepalive_thread_run, daemon=True\n",
      "        )\n",
      "        self._keepalive_thread.start()\n",
      "\n",
      "        self._to_delete = set()\n",
      "\n",
      "        logger.debug(f\"Connected to etcd3 at {self._host}:{self._port}\")\n",
      "\n",
      "    def __del__(self):\n",
      "        \"\"\"Clean up resources when the object is deleted.\"\"\"\n",
      "        self._keepalive_running = False\n",
      "        if hasattr(self, \"_keepalive_thread\"):\n",
      "            self._keepalive_thread.join(timeout=5)\n",
      "        self.reset()\n",
      "        if hasattr(self, \"_client\"):\n",
      "            self._client.close()\n",
      "\n",
      "    def _create_lease(self, ttl_seconds):\n",
      "        \"\"\"Create an etcd lease with the specified TTL.\n",
      "\n",
      "        Args:\n",
      "            ttl_seconds: Time-to-live in seconds\n",
      "\n",
      "        Returns:\n",
      "            The lease ID\n",
      "        \"\"\"\n",
      "        lease = self._client.lease(ttl_seconds)\n",
      "        return lease.id\n",
      "\n",
      "    def add(\n",
      "        self,\n",
      "        name,\n",
      "        value,\n",
      "        delete_on_exit=True,\n",
      "        keepalive_ttl=None,\n",
      "        replace=False,\n",
      "    ):\n",
      "        \"\"\"Add a key-value pair to etcd with optional TTL.\n",
      "\n",
      "        Args:\n",
      "            name: Key name\n",
      "            value: Value to store\n",
      "            delete_on_exit: Whether to delete the key when this object is destroyed\n",
      "            keepalive_ttl: TTL in seconds for the key (default: 10)\n",
      "            replace: Whether to replace an existing key\n",
      "\n",
      "        Raises:\n",
      "            NameEntryExistsError: If the key already exists and replace is False\n",
      "        \"\"\"\n",
      "        if not name:\n",
      "            raise ValueError(f\"Invalid name: {name}\")\n",
      "        name = os.path.normpath(name)\n",
      "        value = str(value)\n",
      "\n",
      "        with self._lock:\n",
      "            # Check if key exists when replace=False\n",
      "            if not replace:\n",
      "                existing_value, _ = self._client.get(name)\n",
      "                if existing_value is not None:\n",
      "                    raise NameEntryExistsError(\n",
      "                        f\"Key already exists: K={name} V={existing_value.decode()}\"\n",
      "                    )\n",
      "\n",
      "            # Create lease for TTL if specified\n",
      "            lease_id = None\n",
      "            if keepalive_ttl is not None and keepalive_ttl > 0:\n",
      "                lease_id = self._create_lease(keepalive_ttl)\n",
      "                # Encode the string value to bytes\n",
      "                self._client.put(name, value.encode(\"utf-8\"), lease=lease_id)\n",
      "                self._to_delete.add(name)\n",
      "            else:\n",
      "                # Encode the string value to bytes\n",
      "                self._client.put(name, value.encode(\"utf-8\"))\n",
      "                if delete_on_exit:\n",
      "                    self._to_delete.add(name)\n",
      "\n",
      "            # Store entry information for keepalive management\n",
      "            self._entries[name] = self._Entry(\n",
      "                value=value,\n",
      "                lease_id=lease_id,\n",
      "                keepalive_ttl=keepalive_ttl,\n",
      "                keeper=(\n",
      "                    timeutil.FrequencyControl(frequency_seconds=keepalive_ttl / 3)\n",
      "                    if keepalive_ttl\n",
      "                    else None\n",
      "                ),\n",
      "            )\n",
      "\n",
      "    def delete(self, name):\n",
      "        \"\"\"Delete a key from etcd.\n",
      "\n",
      "        Args:\n",
      "            name: Key to delete\n",
      "\n",
      "        Raises:\n",
      "            NameEntryNotFoundError: If the key doesn't exist\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            self._delete_locked(name)\n",
      "            if name in self._to_delete:\n",
      "                self._to_delete.remove(name)\n",
      "\n",
      "    def _delete_locked(self, name):\n",
      "        \"\"\"Delete a key from etcd with lock already acquired.\n",
      "\n",
      "        Args:\n",
      "            name: Key to delete\n",
      "\n",
      "        Raises:\n",
      "            NameEntryNotFoundError: If the key doesn't exist\n",
      "        \"\"\"\n",
      "        # First check if the key exists\n",
      "        value, _ = self._client.get(name)\n",
      "        if value is None:\n",
      "            raise NameEntryNotFoundError(f\"No such etcd entry to delete: {name}\")\n",
      "\n",
      "        # Clean up entry tracking\n",
      "        if name in self._entries:\n",
      "            del self._entries[name]\n",
      "\n",
      "        # Delete from etcd\n",
      "        self._client.delete(name)\n",
      "\n",
      "    def clear_subtree(self, name_root):\n",
      "        \"\"\"Delete all keys with the given prefix.\n",
      "\n",
      "        Args:\n",
      "            name_root: Prefix to match keys against\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            count = 0\n",
      "            name_root = os.path.normpath(name_root)\n",
      "            # Get all keys with the prefix\n",
      "            for key_metadata_tuple in self._client.get_prefix(name_root):\n",
      "                key = key_metadata_tuple[1].key.decode(\n",
      "                    \"utf-8\"\n",
      "                )  # Extract the key from metadata\n",
      "                # Remove from our tracking\n",
      "                if key in self._entries:\n",
      "                    del self._entries[key]\n",
      "                # Delete from etcd\n",
      "                self._client.delete(key)\n",
      "                count += 1\n",
      "\n",
      "            logger.debug(f\"Deleted {count} etcd entries under {name_root}\")\n",
      "\n",
      "    def get_subtree(self, name_root):\n",
      "        \"\"\"Get all values with keys having the given prefix.\n",
      "\n",
      "        Args:\n",
      "            name_root: Prefix to match keys against\n",
      "\n",
      "        Returns:\n",
      "            List of values\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            rs = []\n",
      "            name_root = os.path.normpath(name_root)\n",
      "            for value_metadata_tuple in self._client.get_prefix(name_root):\n",
      "                value = value_metadata_tuple[0].decode(\"utf-8\")  # Extract the value\n",
      "                rs.append(value)\n",
      "            return sorted(rs)\n",
      "\n",
      "    def find_subtree(self, name_root):\n",
      "        \"\"\"Find all keys with the given prefix.\n",
      "\n",
      "        Args:\n",
      "            name_root: Prefix to match keys against\n",
      "\n",
      "        Returns:\n",
      "            List of keys\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            rs = []\n",
      "            for key_metadata_tuple in self._client.get_prefix(name_root):\n",
      "                key = key_metadata_tuple[1].key.decode(\n",
      "                    \"utf-8\"\n",
      "                )  # Extract the key from metadata\n",
      "                rs.append(key)\n",
      "            return sorted(rs)\n",
      "\n",
      "    def get(self, name):\n",
      "        \"\"\"Get the value for a key.\n",
      "\n",
      "        Args:\n",
      "            name: Key to retrieve\n",
      "\n",
      "        Returns:\n",
      "            The value as a string\n",
      "\n",
      "        Raises:\n",
      "            NameEntryNotFoundError: If the key doesn't exist\n",
      "        \"\"\"\n",
      "        name = os.path.normpath(name)\n",
      "        with self._lock:\n",
      "            return self._get_locked(name)\n",
      "\n",
      "    def _get_locked(self, name):\n",
      "        \"\"\"Get a value with lock already acquired.\n",
      "\n",
      "        Args:\n",
      "            name: Key to retrieve\n",
      "\n",
      "        Returns:\n",
      "            The value as a string\n",
      "\n",
      "        Raises:\n",
      "            NameEntryNotFoundError: If the key doesn't exist\n",
      "        \"\"\"\n",
      "        value, _ = self._client.get(name)\n",
      "        if value is None:\n",
      "            raise NameEntryNotFoundError(f\"No such etcd entry: {name}\")\n",
      "        return value.decode(\"utf-8\")\n",
      "\n",
      "    def reset(self):\n",
      "        \"\"\"Delete all keys added via this repository instance.\"\"\"\n",
      "        with self._lock:\n",
      "            count = 0\n",
      "            for name in self._to_delete:\n",
      "                if name in self._entries:\n",
      "                    try:\n",
      "                        self._delete_locked(name)\n",
      "                        count += 1\n",
      "                    except NameEntryNotFoundError:\n",
      "                        pass\n",
      "            self._to_delete = set()\n",
      "            logger.info(f\"Reset {count} saved etcd entries\")\n",
      "\n",
      "    def _keepalive_thread_run(self):\n",
      "        \"\"\"Background thread to keep leases alive.\"\"\"\n",
      "        while self._keepalive_running:\n",
      "            time.sleep(self.KEEPALIVE_POLL_FREQUENCY)\n",
      "            with self._lock:\n",
      "                for name, entry in list(self._entries.items()):\n",
      "                    if (\n",
      "                        entry.keeper is not None\n",
      "                        and entry.keepalive_ttl is not None\n",
      "                        and entry.lease_id is not None\n",
      "                        and entry.keeper.check()\n",
      "                    ):\n",
      "                        try:\n",
      "                            # Refresh the lease\n",
      "                            self._client.refresh_lease(entry.lease_id)\n",
      "                        except Exception as e:\n",
      "                            logger.error(\n",
      "                                f\"Failed to refresh lease for key: K={name} V={entry.value}. Error: {e}\"\n",
      "                            )\n",
      "\n",
      "    def watch_names(\n",
      "        self,\n",
      "        names: List,\n",
      "        call_back: Callable,\n",
      "        poll_frequency=15,\n",
      "        wait_timeout=300,\n",
      "    ):\n",
      "        \"\"\"Watch keys and call back when they are deleted.\n",
      "\n",
      "        Args:\n",
      "            names: Keys to watch\n",
      "            call_back: Function to call when any key is deleted\n",
      "            poll_frequency: How often to check in seconds\n",
      "            wait_timeout: Maximum time to wait for keys to exist\n",
      "        \"\"\"\n",
      "        if isinstance(names, str):\n",
      "            names = [names]\n",
      "\n",
      "        q = queue.Queue(maxsize=len(names))\n",
      "        for _ in range(len(names) - 1):\n",
      "            q.put(0)\n",
      "\n",
      "        def wrap_call_back():\n",
      "            try:\n",
      "                q.get_nowait()\n",
      "            except queue.Empty:\n",
      "                logger.info(f\"Key {names} is gone. Executing callback {call_back}\")\n",
      "                call_back()\n",
      "\n",
      "        # Use etcd's native watch capability for more efficient watching\n",
      "        for name in names:\n",
      "            # First wait for the key to exist\n",
      "            self.wait(name, timeout=wait_timeout, poll_frequency=poll_frequency)\n",
      "\n",
      "            # Start watching for key deletion\n",
      "            watch_id = self._client.add_watch_callback(\n",
      "                name, lambda event: self._watch_callback(event, wrap_call_back)\n",
      "            )\n",
      "\n",
      "            # Store watch ID for cleanup\n",
      "            if not hasattr(self, \"_watch_ids\"):\n",
      "                self._watch_ids = []\n",
      "            self._watch_ids.append(watch_id)\n",
      "\n",
      "    def _watch_callback(self, event, callback):\n",
      "        \"\"\"Process watch events and call back on deletion.\n",
      "\n",
      "        Args:\n",
      "            event: The etcd watch response (WatchResponse object)\n",
      "            callback: Function to call when a key is deleted\n",
      "        \"\"\"\n",
      "        # Iterate through the events in the WatchResponse\n",
      "        for ev in event.events:\n",
      "            # Check if this is a delete event\n",
      "            if isinstance(ev, etcd3.events.DeleteEvent):\n",
      "                logger.debug(f\"Key {ev.key.decode()} was deleted. Executing callback.\")\n",
      "                callback()\n",
      "\n",
      "    def _testonly_drop_cached_entry(self, name):\n",
      "        \"\"\"Used by unittest only to simulate the case that the process crashes.\n",
      "\n",
      "        Args:\n",
      "            name: Key to drop from local cache\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            if name in self._entries:\n",
      "                del self._entries[name]\n",
      "                logger.debug(f\"Testonly: dropped key: {name}\")\n",
      "\n",
      "\n",
      "@ray.remote\n",
      "class DistributedKVStore:\n",
      "    \"\"\"Ray actor implementing a distributed key-value store with TTL support.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.store = {}\n",
      "        self.ttl_store = {}  # key -> expiry_time\n",
      "        self.lease_store = {}  # key -> lease_id\n",
      "        self.lease_counter = 0\n",
      "\n",
      "    def put(self, key: str, value: str, lease_id: Optional[int] = None):\n",
      "        \"\"\"Store a key-value pair with optional lease.\"\"\"\n",
      "        self.store[key] = value\n",
      "        if lease_id is not None:\n",
      "            self.lease_store[key] = lease_id\n",
      "        return True\n",
      "\n",
      "    def get(self, key: str):\n",
      "        \"\"\"Get value for a key, checking TTL expiry.\"\"\"\n",
      "        self._cleanup_expired()\n",
      "        if key not in self.store:\n",
      "            return None\n",
      "        return self.store[key]\n",
      "\n",
      "    def delete(self, key: str):\n",
      "        \"\"\"Delete a key and its associated metadata.\"\"\"\n",
      "        deleted = key in self.store\n",
      "        self.store.pop(key, None)\n",
      "        self.ttl_store.pop(key, None)\n",
      "        self.lease_store.pop(key, None)\n",
      "        return deleted\n",
      "\n",
      "    def get_prefix(self, prefix: str):\n",
      "        \"\"\"Get all key-value pairs with keys matching the prefix.\"\"\"\n",
      "        self._cleanup_expired()\n",
      "        result = []\n",
      "        normalized_prefix = os.path.normpath(prefix)\n",
      "\n",
      "        for key, value in self.store.items():\n",
      "            normalized_key = os.path.normpath(key)\n",
      "            # Check if key matches prefix (exact match or starts with prefix/)\n",
      "            if normalized_key == normalized_prefix or normalized_key.startswith(\n",
      "                normalized_prefix.rstrip(\"/\") + \"/\"\n",
      "            ):\n",
      "                result.append((key, value))\n",
      "        return result\n",
      "\n",
      "    def delete_prefix(self, prefix: str):\n",
      "        \"\"\"Delete all keys matching the prefix.\"\"\"\n",
      "        self._cleanup_expired()\n",
      "        normalized_prefix = os.path.normpath(prefix)\n",
      "        keys_to_delete = []\n",
      "\n",
      "        for key in self.store.keys():\n",
      "            normalized_key = os.path.normpath(key)\n",
      "            if normalized_key == normalized_prefix or normalized_key.startswith(\n",
      "                normalized_prefix.rstrip(\"/\") + \"/\"\n",
      "            ):\n",
      "                keys_to_delete.append(key)\n",
      "\n",
      "        for key in keys_to_delete:\n",
      "            self.delete(key)\n",
      "        return len(keys_to_delete)\n",
      "\n",
      "    def create_lease(self, ttl_seconds: int):\n",
      "        \"\"\"Create a lease with TTL.\"\"\"\n",
      "        self.lease_counter += 1\n",
      "        lease_id = self.lease_counter\n",
      "        expiry_time = time.time() + ttl_seconds\n",
      "        return lease_id, expiry_time\n",
      "\n",
      "    def put_with_lease(self, key: str, value: str, ttl_seconds: int):\n",
      "        \"\"\"Store key-value with TTL lease.\"\"\"\n",
      "        lease_id, expiry_time = self.create_lease(ttl_seconds)\n",
      "        self.store[key] = value\n",
      "        self.ttl_store[key] = expiry_time\n",
      "        self.lease_store[key] = lease_id\n",
      "        return lease_id\n",
      "\n",
      "    def refresh_lease(self, key: str, ttl_seconds: int):\n",
      "        \"\"\"Refresh the lease for a key.\"\"\"\n",
      "        if key in self.store and key in self.lease_store:\n",
      "            self.ttl_store[key] = time.time() + ttl_seconds\n",
      "            return True\n",
      "        return False\n",
      "\n",
      "    def _cleanup_expired(self):\n",
      "        \"\"\"Remove expired keys.\"\"\"\n",
      "        current_time = time.time()\n",
      "        expired_keys = []\n",
      "\n",
      "        for key, expiry_time in self.ttl_store.items():\n",
      "            if current_time > expiry_time:\n",
      "                expired_keys.append(key)\n",
      "\n",
      "        for key in expired_keys:\n",
      "            self.delete(key)\n",
      "\n",
      "    def get_all_keys(self):\n",
      "        \"\"\"Get all keys in the store.\"\"\"\n",
      "        self._cleanup_expired()\n",
      "        return list(self.store.keys())\n",
      "\n",
      "\n",
      "class RayNameResolveRepository:\n",
      "    \"\"\"Ray-based implementation of NameRecordRepository using distributed actors.\"\"\"\n",
      "\n",
      "    KEEPALIVE_POLL_FREQUENCY = 1\n",
      "\n",
      "    @dataclasses.dataclass\n",
      "    class _Entry:\n",
      "        value: str\n",
      "        lease_id: Optional[int] = None\n",
      "        keepalive_ttl: Optional[int] = None\n",
      "        keeper: Optional[timeutil.FrequencyControl] = None\n",
      "\n",
      "    def __init__(self, actor_name: str = \"distributed_kv_store\", **kwargs):\n",
      "        \"\"\"Initialize Ray-based name record repository.\n",
      "\n",
      "        Args:\n",
      "            actor_name: Name for the Ray actor (for sharing across processes)\n",
      "            **kwargs: Additional configuration parameters\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self._lock = threading.Lock()\n",
      "        self._actor_name = actor_name\n",
      "\n",
      "        # Initialize Ray if not already done\n",
      "        if not ray.is_initialized():\n",
      "            ray.init(ignore_reinit_error=True)\n",
      "\n",
      "        # Try to get existing actor or create new one\n",
      "        try:\n",
      "            self._kv_store = ray.get_actor(self._actor_name)\n",
      "            logger.debug(\n",
      "                f\"Connected to existing Ray KV store actor: {self._actor_name}\"\n",
      "            )\n",
      "        except ValueError:\n",
      "            # Actor doesn't exist, create it\n",
      "            self._kv_store = DistributedKVStore.options(\n",
      "                name=self._actor_name, lifetime=\"detached\"\n",
      "            ).remote()\n",
      "            logger.debug(f\"Created new Ray KV store actor: {self._actor_name}\")\n",
      "\n",
      "        # Track entries for cleanup and keepalive\n",
      "        self._entries = {}\n",
      "        self._keepalive_running = True\n",
      "        self._keepalive_thread = threading.Thread(\n",
      "            target=self._keepalive_thread_run, daemon=True\n",
      "        )\n",
      "        self._keepalive_thread.start()\n",
      "\n",
      "        self._to_delete = set()\n",
      "\n",
      "    def __del__(self):\n",
      "        \"\"\"Clean up resources when the object is deleted.\"\"\"\n",
      "        try:\n",
      "            self.reset()\n",
      "        except Exception as e:\n",
      "            logger.info(\n",
      "                f\"Exception ignored when deleting RayNameResolveRepository: {e}\"\n",
      "            )\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "        self.reset()\n",
      "\n",
      "    def add(\n",
      "        self,\n",
      "        name: str,\n",
      "        value: str,\n",
      "        delete_on_exit: bool = True,\n",
      "        keepalive_ttl: Optional[int] = None,\n",
      "        replace: bool = False,\n",
      "    ):\n",
      "        \"\"\"Add a key-value pair to the distributed store.\n",
      "\n",
      "        Args:\n",
      "            name: Key name\n",
      "            value: Value to store\n",
      "            delete_on_exit: Whether to delete the key when this object is destroyed\n",
      "            keepalive_ttl: TTL in seconds for the key\n",
      "            replace: Whether to replace an existing key\n",
      "\n",
      "        Raises:\n",
      "            NameEntryExistsError: If the key already exists and replace is False\n",
      "        \"\"\"\n",
      "        if not name:\n",
      "            raise ValueError(f\"Invalid name: {name}\")\n",
      "        name = os.path.normpath(name)\n",
      "        value = str(value)\n",
      "\n",
      "        with self._lock:\n",
      "            # Check if key exists when replace=False\n",
      "            if not replace:\n",
      "                existing_value = ray.get(self._kv_store.get.remote(name))\n",
      "                if existing_value is not None:\n",
      "                    raise NameEntryExistsError(\n",
      "                        f\"Key already exists: K={name} V={existing_value}\"\n",
      "                    )\n",
      "\n",
      "            # Store with or without TTL\n",
      "            lease_id = None\n",
      "            if keepalive_ttl is not None and keepalive_ttl > 0:\n",
      "                lease_id = ray.get(\n",
      "                    self._kv_store.put_with_lease.remote(name, value, keepalive_ttl)\n",
      "                )\n",
      "                self._to_delete.add(name)\n",
      "            else:\n",
      "                ray.get(self._kv_store.put.remote(name, value))\n",
      "                if delete_on_exit:\n",
      "                    self._to_delete.add(name)\n",
      "\n",
      "            # Store entry information for keepalive management\n",
      "            self._entries[name] = self._Entry(\n",
      "                value=value,\n",
      "                lease_id=lease_id,\n",
      "                keepalive_ttl=keepalive_ttl,\n",
      "                keeper=(\n",
      "                    timeutil.FrequencyControl(frequency_seconds=keepalive_ttl / 3)\n",
      "                    if keepalive_ttl\n",
      "                    else None\n",
      "                ),\n",
      "            )\n",
      "\n",
      "    def add_subentry(self, name: str, value: str, **kwargs):\n",
      "        \"\"\"Add a sub-entry to the key-root `name`.\"\"\"\n",
      "        sub_name = os.path.join(os.path.normpath(name), str(uuid.uuid4())[:8])\n",
      "        self.add(sub_name, value, **kwargs)\n",
      "        return sub_name\n",
      "\n",
      "    def delete(self, name: str):\n",
      "        \"\"\"Delete a key from the distributed store.\n",
      "\n",
      "        Args:\n",
      "            name: Key to delete\n",
      "\n",
      "        Raises:\n",
      "            NameEntryNotFoundError: If the key doesn't exist\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            self._delete_locked(name)\n",
      "            if name in self._to_delete:\n",
      "                self._to_delete.remove(name)\n",
      "\n",
      "    def _delete_locked(self, name: str):\n",
      "        \"\"\"Delete a key with lock already acquired.\"\"\"\n",
      "        # Check if key exists\n",
      "        existing_value = ray.get(self._kv_store.get.remote(name))\n",
      "        if existing_value is None:\n",
      "            raise NameEntryNotFoundError(f\"No such entry to delete: {name}\")\n",
      "\n",
      "        # Clean up entry tracking\n",
      "        if name in self._entries:\n",
      "            del self._entries[name]\n",
      "\n",
      "        # Delete from store\n",
      "        ray.get(self._kv_store.delete.remote(name))\n",
      "\n",
      "    def clear_subtree(self, name_root: str):\n",
      "        \"\"\"Delete all keys with the given prefix.\"\"\"\n",
      "        with self._lock:\n",
      "            name_root = os.path.normpath(name_root)\n",
      "            count = ray.get(self._kv_store.delete_prefix.remote(name_root))\n",
      "\n",
      "            # Clean up local tracking for deleted keys\n",
      "            keys_to_remove = []\n",
      "            for key in self._entries.keys():\n",
      "                normalized_key = os.path.normpath(key)\n",
      "                if normalized_key == name_root or normalized_key.startswith(\n",
      "                    name_root.rstrip(\"/\") + \"/\"\n",
      "                ):\n",
      "                    keys_to_remove.append(key)\n",
      "\n",
      "            for key in keys_to_remove:\n",
      "                del self._entries[key]\n",
      "\n",
      "            logger.debug(f\"Deleted {count} entries under {name_root}\")\n",
      "\n",
      "    def get(self, name: str):\n",
      "        \"\"\"Get the value for a key.\n",
      "\n",
      "        Args:\n",
      "            name: Key to retrieve\n",
      "\n",
      "        Returns:\n",
      "            The value as a string\n",
      "\n",
      "        Raises:\n",
      "            NameEntryNotFoundError: If the key doesn't exist\n",
      "        \"\"\"\n",
      "        name = os.path.normpath(name)\n",
      "        with self._lock:\n",
      "            return self._get_locked(name)\n",
      "\n",
      "    def _get_locked(self, name: str):\n",
      "        \"\"\"Get a value with lock already acquired.\"\"\"\n",
      "        value = ray.get(self._kv_store.get.remote(name))\n",
      "        if value is None:\n",
      "            raise NameEntryNotFoundError(f\"No such entry: {name}\")\n",
      "        return value\n",
      "\n",
      "    def get_subtree(self, name_root: str):\n",
      "        \"\"\"Get all values with keys having the given prefix.\"\"\"\n",
      "        with self._lock:\n",
      "            name_root = os.path.normpath(name_root)\n",
      "            pairs = ray.get(self._kv_store.get_prefix.remote(name_root))\n",
      "            values = [value for key, value in pairs]\n",
      "            return sorted(values)\n",
      "\n",
      "    def find_subtree(self, name_root: str):\n",
      "        \"\"\"Find all keys with the given prefix.\"\"\"\n",
      "        with self._lock:\n",
      "            name_root = os.path.normpath(name_root)\n",
      "            pairs = ray.get(self._kv_store.get_prefix.remote(name_root))\n",
      "            keys = [key for key, value in pairs]\n",
      "            return sorted(keys)\n",
      "\n",
      "    def wait(\n",
      "        self, name: str, timeout: Optional[float] = None, poll_frequency: float = 1\n",
      "    ):\n",
      "        \"\"\"Wait until a name appears.\n",
      "\n",
      "        Raises:\n",
      "            TimeoutError: if timeout exceeds.\n",
      "        \"\"\"\n",
      "        start = time.monotonic()\n",
      "        while True:\n",
      "            try:\n",
      "                return self.get(name)\n",
      "            except NameEntryNotFoundError:\n",
      "                pass\n",
      "            if timeout is None or timeout > 0:\n",
      "                time.sleep(\n",
      "                    poll_frequency + random.random() * 0.1\n",
      "                )  # To reduce concurrency.\n",
      "            if timeout is not None and time.monotonic() - start > timeout:\n",
      "                raise TimeoutError(\n",
      "                    f\"Timeout waiting for key '{name}' ({self.__class__.__name__})\"\n",
      "                )\n",
      "\n",
      "    def reset(self):\n",
      "        \"\"\"Delete all keys added via this repository instance.\"\"\"\n",
      "        self._keepalive_running = False\n",
      "        if hasattr(self, \"_keepalive_thread\"):\n",
      "            self._keepalive_thread.join(timeout=5)\n",
      "\n",
      "        with self._lock:\n",
      "            count = 0\n",
      "            for name in list(self._to_delete):\n",
      "                try:\n",
      "                    self._delete_locked(name)\n",
      "                    count += 1\n",
      "                except NameEntryNotFoundError:\n",
      "                    pass\n",
      "            self._to_delete = set()\n",
      "            self._entries = {}\n",
      "            logger.debug(f\"Reset {count} saved entries\")\n",
      "\n",
      "    def watch_names(\n",
      "        self,\n",
      "        names: List[str],\n",
      "        call_back: Callable,\n",
      "        poll_frequency: float = 15,\n",
      "        wait_timeout: float = 300,\n",
      "    ):\n",
      "        \"\"\"Watch keys and call back when they are deleted.\n",
      "\n",
      "        Args:\n",
      "            names: Keys to watch\n",
      "            call_back: Function to call when any key is deleted\n",
      "            poll_frequency: How often to check in seconds\n",
      "            wait_timeout: Maximum time to wait for keys to exist\n",
      "        \"\"\"\n",
      "        if isinstance(names, str):\n",
      "            names = [names]\n",
      "\n",
      "        q = queue.Queue(maxsize=len(names))\n",
      "        for _ in range(len(names) - 1):\n",
      "            q.put(0)\n",
      "\n",
      "        def wrap_call_back():\n",
      "            try:\n",
      "                q.get_nowait()\n",
      "            except queue.Empty:\n",
      "                logger.info(f\"Key {names} is gone. Executing callback {call_back}\")\n",
      "                call_back()\n",
      "\n",
      "        for name in names:\n",
      "            t = threading.Thread(\n",
      "                target=self._watch_thread_run,\n",
      "                args=(name, wrap_call_back, poll_frequency, wait_timeout),\n",
      "                daemon=True,\n",
      "            )\n",
      "            t.start()\n",
      "\n",
      "    def _watch_thread_run(\n",
      "        self, name: str, call_back: Callable, poll_frequency: float, wait_timeout: float\n",
      "    ):\n",
      "        \"\"\"Background thread to watch a key for deletion.\"\"\"\n",
      "        self.wait(name, timeout=wait_timeout, poll_frequency=poll_frequency)\n",
      "        while True:\n",
      "            try:\n",
      "                self.get(name)\n",
      "                time.sleep(poll_frequency + random.random())\n",
      "            except NameEntryNotFoundError:\n",
      "                call_back()\n",
      "                break\n",
      "\n",
      "    def _keepalive_thread_run(self):\n",
      "        \"\"\"Background thread to keep leases alive.\"\"\"\n",
      "        while self._keepalive_running:\n",
      "            time.sleep(self.KEEPALIVE_POLL_FREQUENCY)\n",
      "            with self._lock:\n",
      "                for name, entry in list(self._entries.items()):\n",
      "                    if (\n",
      "                        entry.keeper is not None\n",
      "                        and entry.keepalive_ttl is not None\n",
      "                        and entry.lease_id is not None\n",
      "                        and entry.keeper.check()\n",
      "                    ):\n",
      "                        try:\n",
      "                            # Refresh the lease\n",
      "                            success = ray.get(\n",
      "                                self._kv_store.refresh_lease.remote(\n",
      "                                    name, entry.keepalive_ttl\n",
      "                                )\n",
      "                            )\n",
      "                            if not success:\n",
      "                                logger.warning(\n",
      "                                    f\"Failed to refresh lease for key: {name}\"\n",
      "                                )\n",
      "                        except Exception as e:\n",
      "                            logger.error(\n",
      "                                f\"Failed to refresh lease for key: K={name} V={entry.value}. Error: {e}\"\n",
      "                            )\n",
      "\n",
      "\n",
      "def make_repository(type_=\"nfs\", **kwargs):\n",
      "    if type_ == \"memory\":\n",
      "        return MemoryNameRecordRepository(**kwargs)\n",
      "    elif type_ == \"nfs\":\n",
      "        return NfsNameRecordRepository(**kwargs)\n",
      "    elif type_ == \"redis\":\n",
      "        return RedisNameRecordRepository(**kwargs)\n",
      "    elif type_ == \"etcd3\":\n",
      "        return Etcd3NameRecordRepository(**kwargs)\n",
      "    elif type_ == \"ray\":\n",
      "        return RayNameResolveRepository(**kwargs)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"No such name resolver: {type_}\")\n",
      "\n",
      "\n",
      "# DEFAULT_REPOSITORY_TYPE = \"redis\" if socket.gethostname().startswith(\"frl\") else \"nfs\"\n",
      "DEFAULT_REPOSITORY_TYPE = \"nfs\"\n",
      "if etcd3 is not None and os.getenv(\"REAL_ETCD_ADDR\", \"\"):\n",
      "    DEFAULT_REPOSITORY_TYPE = \"etcd3\"\n",
      "if os.getenv(\"REAL_ETCD_ADDR\", \"\") and etcd3 is None:\n",
      "    logger.warning(\n",
      "        f\"Detected REAL_ETCD_ADDR but etcd3 client is not available. \"\n",
      "        \"Please run `pip install -r requirements.txt` if you want to use etcd name resolve.\"\n",
      "    )\n",
      "DEFAULT_REPOSITORY = make_repository(DEFAULT_REPOSITORY_TYPE)\n",
      "add = DEFAULT_REPOSITORY.add\n",
      "add_subentry = DEFAULT_REPOSITORY.add_subentry\n",
      "delete = DEFAULT_REPOSITORY.delete\n",
      "clear_subtree = DEFAULT_REPOSITORY.clear_subtree\n",
      "get = DEFAULT_REPOSITORY.get\n",
      "get_subtree = DEFAULT_REPOSITORY.get_subtree\n",
      "find_subtree = DEFAULT_REPOSITORY.find_subtree\n",
      "wait = DEFAULT_REPOSITORY.wait\n",
      "reset = DEFAULT_REPOSITORY.reset\n",
      "watch_names = DEFAULT_REPOSITORY.watch_names\n",
      "\n",
      "\n",
      "def reconfigure(*args, **kwargs):\n",
      "    global DEFAULT_REPOSITORY, DEFAULT_REPOSITORY_TYPE\n",
      "    global add, add_subentry, delete, clear_subtree, get, get_subtree, find_subtree, wait, reset, watch_names\n",
      "    DEFAULT_REPOSITORY = make_repository(*args, **kwargs)\n",
      "    DEFAULT_REPOSITORY_TYPE = args[0]\n",
      "    add = DEFAULT_REPOSITORY.add\n",
      "    add_subentry = DEFAULT_REPOSITORY.add_subentry\n",
      "    delete = DEFAULT_REPOSITORY.delete\n",
      "    clear_subtree = DEFAULT_REPOSITORY.clear_subtree\n",
      "    get = DEFAULT_REPOSITORY.get\n",
      "    get_subtree = DEFAULT_REPOSITORY.get_subtree\n",
      "    find_subtree = DEFAULT_REPOSITORY.find_subtree\n",
      "    wait = DEFAULT_REPOSITORY.wait\n",
      "    reset = DEFAULT_REPOSITORY.reset\n",
      "    watch_names = DEFAULT_REPOSITORY.watch_names\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/constants.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# log format constants\n",
      "import contextlib\n",
      "import copy\n",
      "import datetime\n",
      "import getpass\n",
      "import os\n",
      "import pathlib\n",
      "import subprocess\n",
      "from collections import defaultdict\n",
      "from pathlib import Path\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from realhf.api.cli_args import BaseExperimentConfig\n",
      "    from realhf.api.core.config import ModelName\n",
      "    from realhf.api.core.system_api import ModelShardID\n",
      "    from realhf.base.topology import ParallelGrid, ProcessTopology\n",
      "\n",
      "\n",
      "class GlobalMemoryBuffer:\n",
      "    \"\"\"Global buffer to avoid dynamic memory allocations.\n",
      "\n",
      "    Caller should ensure that buffers of the same name are not used\n",
      "    concurrently.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.buffer = {}\n",
      "\n",
      "    def get_tensor(self, tensor_shape, dtype, name, force_zero: bool = False):\n",
      "        device = current_device()\n",
      "        required_len = int(np.prod(tensor_shape))\n",
      "        if self.buffer.get((name, dtype), None) is None:\n",
      "            self.buffer[(name, dtype)] = torch.empty(\n",
      "                required_len,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "                requires_grad=False,\n",
      "            )\n",
      "        elif self.buffer[(name, dtype)].numel() < required_len:\n",
      "            self.buffer[(name, dtype)] = torch.nn.functional.pad(\n",
      "                self.buffer[(name, dtype)],\n",
      "                (0, required_len - self.buffer[(name, dtype)].numel()),\n",
      "                value=0,\n",
      "            )\n",
      "        res = self.buffer[(name, dtype)][0:required_len].view(*tensor_shape)\n",
      "        if force_zero:\n",
      "            res.zero_()\n",
      "        return res\n",
      "\n",
      "\n",
      "# For large models, generation may consume more than 3600s.\n",
      "# We set a large value to avoid NCCL timeout issues during generaiton.\n",
      "NCCL_DEFAULT_TIMEOUT = datetime.timedelta(seconds=7200)\n",
      "\n",
      "# We may want to use CPU for testing even when CUDA is available.\n",
      "TORCH_FORCE_CPU = False\n",
      "\n",
      "# constants in experiment instance scope\n",
      "LOCAL_CACHE_DIR = \"/tmp/realhf\"\n",
      "PYTORCH_KERNEL_CACHE_PATH = (\n",
      "    f\"{LOCAL_CACHE_DIR}/.cache/{getpass.getuser()}/torch/kernels\"\n",
      ")\n",
      "TRITON_CACHE_PATH = f\"{LOCAL_CACHE_DIR}/.cache/{getpass.getuser()}/triton\"\n",
      "QUICKSTART_EXPR_CACHE_PATH = str(Path(__file__).parent.parent.parent / \".cache\")\n",
      "os.makedirs(PYTORCH_KERNEL_CACHE_PATH, exist_ok=True)\n",
      "os.makedirs(TRITON_CACHE_PATH, exist_ok=True)\n",
      "os.makedirs(QUICKSTART_EXPR_CACHE_PATH, exist_ok=True)\n",
      "\n",
      "# Global constants that should be initialized after cluster initialization.\n",
      "MODEL_SAVE_ROOT = None\n",
      "LOG_ROOT = None\n",
      "RECOVER_ROOT = None\n",
      "SLURM_LOCK_FILE_NAME = None\n",
      "PORT_LOCK_FILE_ROOT = None\n",
      "DATASET_CACHE_PATH = None\n",
      "PROFILER_CACHE_PATH = None\n",
      "PARAM_REALLOC_PATH = None\n",
      "SGLANG_CACHE_PATH = None\n",
      "TORCH_EXTENSIONS_DIR = None\n",
      "BASE_ENVIRONS = None\n",
      "\n",
      "\n",
      "def init_constants(args: \"BaseExperimentConfig\"):\n",
      "    from realhf.base.cluster import init_cluster_spec\n",
      "    from realhf.base.cluster import spec as cluster_spec\n",
      "\n",
      "    init_cluster_spec(args)\n",
      "\n",
      "    globals_dict = globals()  # Get module's global variables\n",
      "\n",
      "    kwargs = dict(\n",
      "        MODEL_SAVE_ROOT=f\"{cluster_spec.fileroot}/checkpoints/{getpass.getuser()}\",\n",
      "        LOG_ROOT=f\"{cluster_spec.fileroot}/logs/{getpass.getuser()}\",\n",
      "        RECOVER_ROOT=f\"{cluster_spec.fileroot}/recover/{getpass.getuser()}\",\n",
      "        SLURM_LOCK_FILE_NAME=f\"{cluster_spec.fileroot}/logs/slurm_scheduler.lock\",\n",
      "        PORT_LOCK_FILE_ROOT=f\"{cluster_spec.fileroot}/.cache/{getpass.getuser()}/ports\",\n",
      "        DATASET_CACHE_PATH=f\"{cluster_spec.fileroot}/.cache/{getpass.getuser()}/datasets\",\n",
      "        PROFILER_CACHE_PATH=f\"{cluster_spec.fileroot}/.cache/{getpass.getuser()}/profiler\",\n",
      "        PARAM_REALLOC_PATH=f\"{cluster_spec.fileroot}/.cache/{getpass.getuser()}/param_realloc\",\n",
      "        SGLANG_CACHE_PATH=f\"{cluster_spec.fileroot}/.cache/{getpass.getuser()}/sglang\",\n",
      "        TORCH_EXTENSIONS_DIR=(\n",
      "            f\"{cluster_spec.fileroot}/.cache/{getpass.getuser()}/torch/extensions\"\n",
      "        ),\n",
      "    )\n",
      "    BASE_ENVIRONS = {\n",
      "        # \"PYTHONPATH\": \"/realhf\",\n",
      "        \"REAL_IS_REMOTE\": \"1\",\n",
      "        # \"NCCL_P2P_DISABLE\": \"1\",\n",
      "        # \"NCCL_IB_DISABLE\": \"1\",\n",
      "        \"TRANSFORMERS_OFFLINE\": \"1\",\n",
      "        \"PYTORCH_KERNEL_CACHE_PATH\": PYTORCH_KERNEL_CACHE_PATH,\n",
      "        \"TRITON_CACHE_DIR\": TRITON_CACHE_PATH,\n",
      "        \"TOKENIZERS_PARALLELISM\": \"true\",\n",
      "        \"TORCH_EXTENSIONS_DIR\": kwargs[\"TORCH_EXTENSIONS_DIR\"],\n",
      "        # \"TORCH_DISTRIBUTED_DEBUG\": \"DETAIL\",\n",
      "        # \"NCCL_SOCKET_IFNAME\": \"ibp71s0\",\n",
      "        # \"GLOO_SOCKET_IFNAME\": \"ibp71s0\",\n",
      "        # \"TORCH_USE_CUDA_DSA\": \"1\",\n",
      "        # \"NCCL_IGNORE_DISABLED_P2P\": \"1\",\n",
      "        # \"CUDA_LAUNCH_BLOCKING\": \"1\",  # NOTE: CUDAGraph Capturing will not work if CUDA_LAUNCH_BLOCKING is set to 1.\n",
      "        # \"NCCL_COMM_BLOCKING\": \"1\",  # NOTE: CUDAGraph Capturing will not work if NCCL_COMM_BLOCKING is set to 1.\n",
      "        # \"NCCL_BLOCKING_WAIT\": \"1\",  # NOTE: CUDAGraph Capturing will not work if NCCL_BLOCKING_WAIT is set to 1.\n",
      "        # \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
      "        # \"RAY_DEDUP_LOGS\": \"0\",  # disable ray log deduplication\n",
      "        \"CUDA_DEVICE_MAX_CONNECTIONS\": \"1\",\n",
      "        \"OMP_NUM_THREADS\": str(min(os.cpu_count(), 32)),\n",
      "        # torch.distributed.all_reduce does not free the input tensor until\n",
      "        # the synchronization point. This causes the memory usage to grow\n",
      "        # as the number of all_reduce calls increases. This env var disables\n",
      "        # this behavior.\n",
      "        # Related issue:\n",
      "        # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n",
      "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
      "        # Whether to enable time mark to plot timelines.\n",
      "        \"REAL_CUDA_TMARK\": os.getenv(\"REAL_CUDA_TMARK\", \"0\"),\n",
      "        \"REAL_DUMP_TRACE\": os.getenv(\"REAL_DUMP_TRACE\", \"0\"),\n",
      "        \"REAL_DUMP_MEMORY\": os.getenv(\"REAL_DUMP_MEMORY\", \"0\"),\n",
      "        \"REAL_GPU_MEMORY_KILL_THRESHOLD\": os.getenv(\n",
      "            \"REAL_GPU_MEMORY_KILL_THRESHOLD\", \"1.0\"\n",
      "        ),\n",
      "        \"LC_ALL\": \"C\",\n",
      "        \"LANG\": \"C\",\n",
      "        \"NCCL_DEBUG\": \"WARN\",\n",
      "    }\n",
      "    kwargs[\"BASE_ENVIRONS\"] = BASE_ENVIRONS\n",
      "    # Set PPU-specific environment variables for stable training.\n",
      "    if cluster_spec.name == \"wa180\":\n",
      "        logger.warning(\"Detected PPU. Amending PPU-related environment variables.\")\n",
      "        PPU_ENVIRONS = {\n",
      "            \"NCCL_DEBUG\": \"INFO\",\n",
      "            \"NCCL_IB_DISABLE\": \"1\",\n",
      "            \"NCCL_DEBUG_SUBSYS\": \"INIT\",\n",
      "            \"NCCL_SET_THREAD_NAME\": \"1\",\n",
      "            \"NCCL_IB_HCA\": \"\",\n",
      "            \"NCCL_SOCKET_IFNAME\": \"bond0\",\n",
      "            \"PCCL_STATE_MONITOR_DISABLE\": \"1\",\n",
      "        }\n",
      "        kwargs[\"BASE_ENVIRONS\"].update(PPU_ENVIRONS)\n",
      "    elif cluster_spec.name == \"na132\":\n",
      "        # Specific environment variable for h800 cluster na132\n",
      "        NV_ENVIRONS = {\n",
      "            \"NCCL_SOCKET_IFNAME\": \"bond0\",\n",
      "            \"NCCL_NET_PLUGIN\": \"\",\n",
      "            \"NCCL_IB_GID_INDEX\": \"3\",\n",
      "            \"NCCL_IB_TIMEOUT\": \"2\",\n",
      "            \"NCCL_IB_RETRY_CNT\": \"7\",\n",
      "            \"NCCL_IB_SL\": \"5\",\n",
      "            \"NCCL_IB_TC\": \"136\",\n",
      "            \"NCCL_IB_HCA\": \"mlx5_bond\",\n",
      "            \"NCCL_IB_QPS_PER_CONNECTION\": \"8\",\n",
      "            \"NCCL_SET_THREAD_NAME\": \"1\",\n",
      "            \"NCCL_DEBUG_SUBSYS\": \"INIT,TUNING,GRAPH\",\n",
      "        }\n",
      "        kwargs[\"BASE_ENVIRONS\"].update(NV_ENVIRONS)\n",
      "\n",
      "    for key, value in kwargs.items():\n",
      "        if key not in globals_dict:\n",
      "            raise ValueError(f\"Invalid constant name: {key}\")\n",
      "        if globals_dict[key] is not None and globals_dict[key] != value:\n",
      "            raise RuntimeError(f\"Constant '{key}' already initialized!\")\n",
      "        globals_dict[key] = value\n",
      "\n",
      "    # make directories if does not exist\n",
      "    os.makedirs(globals_dict[\"PARAM_REALLOC_PATH\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"MODEL_SAVE_ROOT\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"LOG_ROOT\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"RECOVER_ROOT\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"DATASET_CACHE_PATH\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"PROFILER_CACHE_PATH\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"TORCH_EXTENSIONS_DIR\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"PORT_LOCK_FILE_ROOT\"], exist_ok=True)\n",
      "    os.makedirs(globals_dict[\"SGLANG_CACHE_PATH\"], exist_ok=True)\n",
      "\n",
      "\n",
      "# _model_name will be changed in the model_scope context manager\n",
      "_model_name: \"ModelName\" = None\n",
      "\n",
      "# constants in worker/process scope\n",
      "_experiment_name = None\n",
      "_trial_name = None\n",
      "\n",
      "_grids: Dict[\"ModelName\", \"ParallelGrid\"] = {}\n",
      "_pgroups: Dict[\"ModelName\", Any] = (\n",
      "    {}\n",
      ")  # torch.distributed.ProcessGroup, not type hint here to avoid importing torch\n",
      "_cpu_pgroups: Dict[\"ModelName\", Any] = (\n",
      "    {}\n",
      ")  # torch.distributed.ProcessGroup, not type hint here to avoid importing torch\n",
      "_pgroup_ranks: Dict[\"ModelName\", List[int]] = {}\n",
      "_self_group = None\n",
      "_rank_mapping: Dict[\"ModelName\", Dict[\"ModelShardID\", int]] = {}\n",
      "_global_memory_buffer: GlobalMemoryBuffer = GlobalMemoryBuffer()\n",
      "\n",
      "\n",
      "# TODO: As in Megatron, we can set NCCL group options. Is it necessary?\n",
      "\n",
      "\n",
      "def reset_run():\n",
      "    global _model_name, _grids, _pgroups, _pgroup_ranks, _self_group, _rank_mapping, _global_memory_buffer\n",
      "    _model_name = None\n",
      "    _grids = {}\n",
      "    _pgroups = {}\n",
      "    _pgroup_ranks = {}\n",
      "    _self_group = None\n",
      "    _rank_mapping = {}\n",
      "    _global_memory_buffer = GlobalMemoryBuffer()\n",
      "\n",
      "\n",
      "@contextlib.contextmanager\n",
      "def model_scope(model_name: \"ModelName\"):\n",
      "    global _model_name\n",
      "    assert _model_name is None\n",
      "    _model_name = model_name\n",
      "    yield\n",
      "    assert _model_name == model_name\n",
      "    _model_name = None\n",
      "\n",
      "\n",
      "@contextlib.contextmanager\n",
      "def model_scope_disabled():\n",
      "    global _model_name\n",
      "    assert _model_name is not None\n",
      "    t, _model_name = _model_name, None\n",
      "    yield\n",
      "    _model_name = t\n",
      "\n",
      "\n",
      "################# setter functions #################\n",
      "def set_force_cpu(val: bool):\n",
      "    global TORCH_FORCE_CPU\n",
      "    TORCH_FORCE_CPU = val\n",
      "\n",
      "\n",
      "def set_experiment_trial_names(expr_name: str, trial_name: str):\n",
      "    global _experiment_name, _trial_name\n",
      "    if _experiment_name is not None and _experiment_name != expr_name:\n",
      "        raise RuntimeError(\"Experiment name has been set.\")\n",
      "    if _trial_name is not None and _trial_name != trial_name:\n",
      "        raise RuntimeError(\"Trial name has been set.\")\n",
      "    _experiment_name = expr_name\n",
      "    _trial_name = trial_name\n",
      "\n",
      "\n",
      "def set_grid(model_name: \"ModelName\", grid: \"ParallelGrid\"):\n",
      "    global _grids\n",
      "    if model_name in _grids:\n",
      "        raise RuntimeError(f\"Grid for model {model_name} is already set.\")\n",
      "    _grids[model_name] = grid\n",
      "\n",
      "\n",
      "def set_parallelism_group(model_name: \"ModelName\", pgroup, ranks):\n",
      "    global _pgroups\n",
      "    if model_name in _pgroups:\n",
      "        raise RuntimeError(f\"Parallelism group for model {model_name} is already set.\")\n",
      "    _pgroups[model_name] = pgroup\n",
      "    _pgroup_ranks[model_name] = ranks\n",
      "\n",
      "\n",
      "def set_cpu_parallelism_group(model_name: \"ModelName\", pgroup):\n",
      "    global _cpu_pgroups\n",
      "    if model_name in _cpu_pgroups:\n",
      "        raise RuntimeError(f\"Parallelism group for model {model_name} is already set.\")\n",
      "    _cpu_pgroups[model_name] = pgroup\n",
      "\n",
      "\n",
      "def set_self_group(pgroup):\n",
      "    global _self_group\n",
      "    if _self_group is not None:\n",
      "        raise RuntimeError(\"Self group is already set.\")\n",
      "    _self_group = pgroup\n",
      "\n",
      "\n",
      "def set_rank_mapping(\n",
      "    model_name: \"ModelName\",\n",
      "    topo: \"ProcessTopology\",\n",
      "    msid2mwid: Optional[Dict[\"ModelShardID\", int]] = None,\n",
      "):\n",
      "    global _rank_mapping\n",
      "    if model_name in _rank_mapping:\n",
      "        raise RuntimeError(f\"Rank mapping for model {model_name} is already set.\")\n",
      "    if msid2mwid is None:\n",
      "        _rank_mapping[model_name] = {i: i for i in range(topo.world_size())}\n",
      "    else:\n",
      "        msid2mwid = {k: v for k, v in msid2mwid.items() if k.model_name == model_name}\n",
      "        _rank_mapping[model_name] = {\n",
      "            topo.get_rank(data=s.dp_rank, tensor=s.tp_rank, pipe=s.pp_rank): mw_id\n",
      "            for s, mw_id in msid2mwid.items()\n",
      "        }\n",
      "\n",
      "\n",
      "################# attribute functions #################\n",
      "def current_device() -> torch.device:\n",
      "    global TORCH_FORCE_CPU\n",
      "    if TORCH_FORCE_CPU or not torch.cuda.is_available():\n",
      "        return torch.device(\"cpu\")\n",
      "    return torch.cuda.current_device()\n",
      "\n",
      "\n",
      "def use_cuda() -> bool:\n",
      "    return not TORCH_FORCE_CPU and torch.cuda.is_available()\n",
      "\n",
      "\n",
      "def use_te_impl() -> bool:\n",
      "    try:\n",
      "        import transformer_engine.pytorch as te\n",
      "\n",
      "        TE_ENABLED = True\n",
      "    except ImportError:\n",
      "        TE_ENABLED = False\n",
      "    return TE_ENABLED and os.getenv(\"REAL_LLM_USE_TE\") == \"1\"\n",
      "\n",
      "\n",
      "def sequence_parallel() -> bool:\n",
      "    return grid().topology().sequence_parallel\n",
      "\n",
      "\n",
      "def gradient_accumulation_fusion() -> bool:\n",
      "    _grad_accum_fusion_available = True\n",
      "    try:\n",
      "        import fused_weight_gradient_mlp_cuda\n",
      "    except ImportError:\n",
      "        _grad_accum_fusion_available = False\n",
      "    return _grad_accum_fusion_available and getattr(\n",
      "        grid().topology(), \"gradient_accumulation_fusion\", False\n",
      "    )\n",
      "\n",
      "\n",
      "def max_prompt_len() -> int:\n",
      "    return grid().topology().max_prompt_len\n",
      "\n",
      "\n",
      "def gradient_checkpointing() -> bool:\n",
      "    return getattr(grid().topology(), \"gradient_checkpointing\", False)\n",
      "\n",
      "\n",
      "def has_model_name(name: str) -> bool:\n",
      "    return name in _grids and _grids[name].global_rank != -1\n",
      "\n",
      "\n",
      "def self_group():\n",
      "    global _self_group\n",
      "    assert _self_group is not None\n",
      "    return _self_group\n",
      "\n",
      "\n",
      "def model_name():\n",
      "    if _model_name == None:\n",
      "        raise RuntimeError(\n",
      "            \"Global constant `model_name` should be accessed in the `model_scope` context.\"\n",
      "        )\n",
      "    return _model_name\n",
      "\n",
      "\n",
      "def experiment_name():\n",
      "    if _experiment_name == None:\n",
      "        raise RuntimeError(\"Global constant `experiment_name` is accessed before set.\")\n",
      "    return _experiment_name\n",
      "\n",
      "\n",
      "def trial_name():\n",
      "    if _trial_name == None:\n",
      "        raise RuntimeError(\"Global constant `trial_name` is accessed before set.\")\n",
      "    return _trial_name\n",
      "\n",
      "\n",
      "def grid() -> \"ParallelGrid\":\n",
      "    if _model_name is None:\n",
      "        raise RuntimeError(\"Global constant `model_name` is accessed before set.\")\n",
      "    if _grids.get(_model_name, None) is None:\n",
      "        raise RuntimeError(f\"Grid for model {_model_name} is not set.\")\n",
      "    return _grids[_model_name]\n",
      "\n",
      "\n",
      "def grid_of_model(model_name: str) -> \"ParallelGrid\":\n",
      "    if _grids.get(model_name, None) is None:\n",
      "        raise RuntimeError(f\"Grid for model {model_name} is not set.\")\n",
      "    return _grids[model_name]\n",
      "\n",
      "\n",
      "def parallelism_group():\n",
      "    \"\"\"Returns the 3D parallelism group of a specific model.\"\"\"\n",
      "    if _model_name is None:\n",
      "        raise RuntimeError(\"Global constant `model_name` is accessed before set.\")\n",
      "    if _pgroups.get(_model_name, None) is None:\n",
      "        raise RuntimeError(f\"Parallelism group for model {_model_name} is not set.\")\n",
      "    return _pgroups[_model_name]\n",
      "\n",
      "\n",
      "def cpu_parallelism_group():\n",
      "    \"\"\"Returns the GLOO 3D parallelism group of a specific model.\"\"\"\n",
      "    if _model_name is None:\n",
      "        raise RuntimeError(\"Global constant `model_name` is accessed before set.\")\n",
      "    if _cpu_pgroups.get(_model_name, None) is None:\n",
      "        raise RuntimeError(f\"Parallelism group for model {_model_name} is not set.\")\n",
      "    return _cpu_pgroups[_model_name]\n",
      "\n",
      "\n",
      "def parallelism_group_ranks():\n",
      "    if _model_name is None:\n",
      "        raise RuntimeError(\"Global constant `model_name` is accessed before set.\")\n",
      "    if _pgroup_ranks.get(_model_name, None) is None:\n",
      "        raise RuntimeError(\n",
      "            f\"Parallelism group ranks for model {_model_name} is not set.\"\n",
      "        )\n",
      "    return _pgroup_ranks[_model_name]\n",
      "\n",
      "\n",
      "def parallelism_group_size() -> int:\n",
      "    \"\"\"The 3D parallelism group size of a specific model, normally dp_size *\n",
      "    pp_size * tp_size.\"\"\"\n",
      "    import torch.distributed as dist\n",
      "\n",
      "    return dist.get_world_size(group=parallelism_group())\n",
      "\n",
      "\n",
      "def parallelism_rank() -> int:\n",
      "    \"\"\"Return the rank of a specific model in its 3D parallelism group.\"\"\"\n",
      "    import torch.distributed as dist\n",
      "\n",
      "    return dist.get_rank(group=parallelism_group())\n",
      "\n",
      "\n",
      "def to_global_pg_rank(local_rank: int) -> int:\n",
      "    global _rank_mapping\n",
      "    if _rank_mapping is None or model_name() not in _rank_mapping:\n",
      "        raise RuntimeError(\"Rank mapping is not set.\")\n",
      "    return _rank_mapping[model_name()][local_rank]\n",
      "\n",
      "\n",
      "def rank_mapping_of_model(model_name: str) -> Dict[\"ModelShardID\", int]:\n",
      "    global _rank_mapping\n",
      "    if _rank_mapping is None or _rank_mapping.get(model_name, None) is None:\n",
      "        raise RuntimeError(f\"Rank mapping for model {model_name} is not set.\")\n",
      "    return _rank_mapping[model_name]\n",
      "\n",
      "\n",
      "def pipe_parallel_rank() -> int:\n",
      "    return grid().get_pipe_parallel_rank()\n",
      "\n",
      "\n",
      "def pipe_parallel_world_size() -> int:\n",
      "    return grid().get_pipe_parallel_world_size()\n",
      "\n",
      "\n",
      "def pipe_parallel_group():\n",
      "    return grid().get_pipe_parallel_group()\n",
      "\n",
      "\n",
      "def pipe_parallel_cpu_group():\n",
      "    return grid().pp_proc_group_gloo\n",
      "\n",
      "\n",
      "def is_last_pipe_stage():\n",
      "    return pipe_parallel_rank() == pipe_parallel_world_size() - 1\n",
      "\n",
      "\n",
      "def is_first_pipe_stage():\n",
      "    return pipe_parallel_rank() == 0\n",
      "\n",
      "\n",
      "def next_pipe_stage():\n",
      "    return (pipe_parallel_rank() + 1) % pipe_parallel_world_size()\n",
      "\n",
      "\n",
      "def prev_pipe_stage():\n",
      "    return (\n",
      "        pipe_parallel_world_size() + pipe_parallel_rank() - 1\n",
      "    ) % pipe_parallel_world_size()\n",
      "\n",
      "\n",
      "def is_dp_head():\n",
      "    return is_last_pipe_stage() and tensor_parallel_rank() == 0\n",
      "\n",
      "\n",
      "def tensor_parallel_rank() -> int:\n",
      "    \"\"\"Return the rank inside the tensor parallelism group.\"\"\"\n",
      "    return grid().get_tensor_model_parallel_rank()\n",
      "\n",
      "\n",
      "def tensor_parallel_world_size() -> int:\n",
      "    \"\"\"Return the world size of the tensor parallelism group.\"\"\"\n",
      "    return grid().get_tensor_model_parallel_world_size()\n",
      "\n",
      "\n",
      "def tensor_parallel_group():\n",
      "    \"\"\"Return the NCCL tensor parallelism process group.\"\"\"\n",
      "    return grid().get_tensor_model_parallel_group()\n",
      "\n",
      "\n",
      "def tensor_parallel_cpu_group():\n",
      "    \"\"\"Return the GLOO tensor parallelism process group.\"\"\"\n",
      "    return grid().get_tensor_model_parallel_cpu_group()\n",
      "\n",
      "\n",
      "def tp_and_pp_group():\n",
      "    \"\"\"Used as the world group of vLLM.\"\"\"\n",
      "    return grid().get_model_parallel_group()\n",
      "\n",
      "\n",
      "def tp_and_pp_cpu_group():\n",
      "    return grid().ds_model_proc_group_gloo\n",
      "\n",
      "\n",
      "def tp_and_pp_rank():\n",
      "    \"\"\"Used as the rank in the world group of vLLM.\"\"\"\n",
      "    return grid().get_model_parallel_rank()\n",
      "\n",
      "\n",
      "def tp_and_pp_world_size():\n",
      "    \"\"\"Used as the world size of vLLM.\"\"\"\n",
      "    return grid().get_model_parallel_world_size()\n",
      "\n",
      "\n",
      "def data_parallel_rank() -> int:\n",
      "    return grid().get_data_parallel_rank()\n",
      "\n",
      "\n",
      "def data_parallel_world_size() -> int:\n",
      "    return grid().get_data_parallel_world_size()\n",
      "\n",
      "\n",
      "def data_parallel_group():\n",
      "    return grid().get_data_parallel_group()\n",
      "\n",
      "\n",
      "def get_global_memory_buffer():\n",
      "    global _global_memory_buffer\n",
      "    assert _global_memory_buffer is not None, \"global memory buffer is not set\"\n",
      "    return _global_memory_buffer\n",
      "\n",
      "\n",
      "def clear_global_memory_buffer():\n",
      "    global _global_memory_buffer\n",
      "    _global_memory_buffer = GlobalMemoryBuffer()\n",
      "\n",
      "\n",
      "def get_repo_path() -> pathlib.Path:\n",
      "    return pathlib.Path(__file__).resolve().parent.parent.parent\n",
      "\n",
      "\n",
      "def get_env_vars(**kwargs):\n",
      "    kwargs.update(\n",
      "        CLUSTER_SPEC_PATH=os.environ.get(\"CLUSTER_SPEC_PATH\", \"\"),\n",
      "        REAL_DUMP_TRACE=os.environ.get(\"REAL_DUMP_TRACE\", \"0\"),\n",
      "        REAL_RECORD_PERFORMANCE=os.environ.get(\"REAL_RECORD_PERFORMANCE\", \"0\"),\n",
      "        FUNCTIONCALL_SERVICE_DOMAIN=os.getenv(\"FUNCTIONCALL_SERVICE_DOMAIN\", \"\"),\n",
      "        REAL_DUMP_MEMORY=os.environ.get(\"REAL_DUMP_MEMORY\", \"0\"),\n",
      "        REAL_ETCD_ADDR=os.getenv(\"REAL_ETCD_ADDR\", \"localhost:2379\"),\n",
      "        REAL_OSS_TESTCASE_PATH=os.getenv(\"REAL_OSS_TESTCASE_PATH\", \"\"),\n",
      "    )\n",
      "    return {\n",
      "        **kwargs,\n",
      "        \"REAL_PACKAGE_PATH\": str(get_repo_path()),\n",
      "        **BASE_ENVIRONS,\n",
      "    }\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/importing.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import importlib\n",
      "import importlib.util\n",
      "import os\n",
      "import re\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "from .logging import getLogger\n",
      "\n",
      "logger = getLogger(\"importing\")\n",
      "\n",
      "\n",
      "def import_module(path: str, pattern: re.Pattern):\n",
      "    dirname = Path(path)\n",
      "    for x in os.listdir(dirname.absolute()):\n",
      "        if not pattern.match(x):\n",
      "            continue\n",
      "        module_path = os.path.splitext(os.path.join(dirname, x))[0]\n",
      "        assert \"realhf\" in module_path\n",
      "        start_idx = path.rindex(\"realhf\")\n",
      "        module_path = module_path[start_idx:]\n",
      "        module_path = \"realhf.\" + module_path.replace(os.sep, \".\").replace(\n",
      "            \"realhf.\", \"\"\n",
      "        )\n",
      "        # logger.info(f\"Automatically importing module {module_path}.\")\n",
      "        importlib.import_module(module_path)\n",
      "\n",
      "\n",
      "def import_usercode(module_path: str, module_name: str):\n",
      "    # Create a module spec\n",
      "    spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
      "    # Create a module object\n",
      "    module = importlib.util.module_from_spec(spec)\n",
      "    # Add the module to sys.modules\n",
      "    sys.modules[module_name] = module\n",
      "    # Execute the module in its own namespace\n",
      "    spec.loader.exec_module(module)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/testing.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import gc\n",
      "import multiprocessing as mp\n",
      "import os\n",
      "import pickle\n",
      "import queue\n",
      "import random\n",
      "import time\n",
      "import traceback\n",
      "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
      "\n",
      "import pynvml\n",
      "import pytest\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.utils.data\n",
      "\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "from realhf.base import constants, gpu_utils, logging, name_resolve, names, topology\n",
      "from realhf.base.topology import (\n",
      "    DataPipeTensorParallelTopology,\n",
      "    ParallelGrid,\n",
      "    PipeDataTensorParallelTopology,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"testing\")\n",
      "\n",
      "MODEL_NAME = \"default\"\n",
      "_DEFAULT_EXPR_NAME = \"test\"\n",
      "_DEFAULT_TRIAL_NAME = \"test\"\n",
      "\n",
      "TESTING_MODEL_VOCAB_SIZE = 128\n",
      "TESTING_MODEL_N_POSITIONS = 128\n",
      "TESTING_MODEL_INTERMEDIATE_SIZE = 32\n",
      "TESTING_MODEL_HIDDEN_SIZE = 16\n",
      "TESTING_MODEL_HEAD_DIM = 2\n",
      "TESTING_MODEL_N_LAYERS = 8\n",
      "TESTING_MODEL_N_HEADS = 8\n",
      "\n",
      "TESTING_DATASET_SIZE = 100\n",
      "\n",
      "\n",
      "class StandaloneTestingProcess(mp.Process):\n",
      "    \"\"\"Aims for defining this class:\n",
      "    + Removing duplicate setup GPU codes in each test.\n",
      "\n",
      "    Note that `init_global_constants` should be called in `func`.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        rank: int,\n",
      "        world_size: int,\n",
      "        barrier: mp.Barrier,  # type: ignore\n",
      "        err_queue: mp.Queue,\n",
      "        func: Callable,\n",
      "        *args,\n",
      "        expr_name: str = None,\n",
      "        dist_backend: Optional[str] = None,\n",
      "        trial_name: str = None,\n",
      "        setup_dist_torch: bool = True,\n",
      "        use_cpu: bool = False,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.rank = rank\n",
      "        self.world_size = world_size\n",
      "        self.err_queue = err_queue\n",
      "        self.barrier = barrier\n",
      "\n",
      "        self.expr_name = expr_name if expr_name is not None else _DEFAULT_EXPR_NAME\n",
      "        self.trial_name = trial_name if trial_name is not None else _DEFAULT_TRIAL_NAME\n",
      "        self.dist_backend = dist_backend\n",
      "\n",
      "        self.func = func\n",
      "        self.args = args\n",
      "        self.kwargs = kwargs\n",
      "        self.dist_backend = dist_backend\n",
      "        self.setup_dist_torch = setup_dist_torch\n",
      "        self.use_cpu = use_cpu\n",
      "\n",
      "    def _run(self):\n",
      "        return self.func(*self.args, **self.kwargs)\n",
      "\n",
      "    def run(self) -> None:\n",
      "        assert not torch.cuda.is_initialized()\n",
      "        constants.set_force_cpu(self.use_cpu)\n",
      "        if constants.use_cuda():\n",
      "            torch.cuda.set_device(0)\n",
      "\n",
      "        self.barrier.wait()\n",
      "\n",
      "        if self.setup_dist_torch:\n",
      "            # init process group\n",
      "            gpu_utils.reveal_pg_identity(self.expr_name, self.trial_name, self.rank)\n",
      "            self.barrier.wait()\n",
      "            from realhf.impl.model.comm.global_comm import setup_global_comm\n",
      "\n",
      "            if self.dist_backend is None:\n",
      "                self.dist_backend = \"gloo\" if not constants.use_cuda() else \"nccl\"\n",
      "            setup_global_comm(\n",
      "                self.expr_name, self.trial_name, self.rank, backend=self.dist_backend\n",
      "            )\n",
      "\n",
      "        # misc setup\n",
      "        if constants.use_cuda():\n",
      "            pynvml.nvmlInit()\n",
      "            pytorch_memory_burnin(self.rank)\n",
      "\n",
      "        try:\n",
      "            self._run()\n",
      "        except Exception as e:\n",
      "            print(traceback.format_exc(), flush=True)\n",
      "            self.err_queue.put(e)\n",
      "            raise e\n",
      "        if dist.is_initialized():\n",
      "            dist.destroy_process_group()\n",
      "\n",
      "\n",
      "class LocalMultiProcessTest:\n",
      "    \"\"\"Aims for defining this class:\n",
      "    1. Defining a barrier and a queue for all sub-processes.\n",
      "    2. Error handling after launch.\n",
      "    \"\"\"\n",
      "\n",
      "    # NOTE: This is necessary for running pytest, otherwise\n",
      "    # pytest will exit early before subprocesses terminate.\n",
      "    mp.set_start_method(\"spawn\", force=True)\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        world_size: int,\n",
      "        func: Callable | List[Callable],\n",
      "        *args,\n",
      "        expr_name: str = None,\n",
      "        trial_name: str = None,\n",
      "        dist_backend: Optional[str] = None,\n",
      "        timeout_secs: int = 300,\n",
      "        setup_dist_torch: bool = True,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        self.barrier = mp.Barrier(world_size)\n",
      "        self.err_queue = mp.Queue(world_size)\n",
      "        os.environ[\"REAL_MODE\"] = \"LOCAL\"\n",
      "        if torch.cuda.is_available():\n",
      "            os.environ[\"CUDA_DEVICE_MAX_CONNECTIONS\"] = \"1\"\n",
      "            os.environ[\"GPU_DEVICES_ISOLATED\"] = str(1)\n",
      "        clear_name_resolve(expr_name, trial_name)\n",
      "        self.timeout_secs = timeout_secs\n",
      "        self.processes = []\n",
      "        if isinstance(func, list):\n",
      "            assert len(func) == world_size, (len(func), world_size)\n",
      "        for rank in range(world_size):\n",
      "            if torch.cuda.is_available():\n",
      "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank)\n",
      "            p = StandaloneTestingProcess(\n",
      "                rank,\n",
      "                world_size,\n",
      "                self.barrier,\n",
      "                self.err_queue,\n",
      "                func[rank] if isinstance(func, list) else func,\n",
      "                *args,\n",
      "                expr_name=expr_name,\n",
      "                trial_name=trial_name,\n",
      "                dist_backend=dist_backend,\n",
      "                setup_dist_torch=setup_dist_torch,\n",
      "                use_cpu=not constants.use_cuda(),\n",
      "                **kwargs,\n",
      "            )\n",
      "            self.processes.append(p)\n",
      "\n",
      "    def start(self):\n",
      "        [p.start() for p in self.processes]\n",
      "\n",
      "    def wait(self, timeout=None):\n",
      "        timeout = timeout or self.timeout_secs\n",
      "        tik = time.time()\n",
      "        while any([p.is_alive() for p in self.processes]):\n",
      "            try:\n",
      "                err = self.err_queue.get_nowait()\n",
      "                raise err\n",
      "            except queue.Empty:\n",
      "                time.sleep(0.1)\n",
      "            if time.time() - tik > self.timeout_secs:\n",
      "                raise TimeoutError(\"Timeout\")\n",
      "\n",
      "    def launch(self, timeout=None):\n",
      "        self.start()\n",
      "        try:\n",
      "            self.wait(timeout)\n",
      "        except TimeoutError as e:\n",
      "            self.terminate()\n",
      "            raise e\n",
      "        self.terminate()\n",
      "\n",
      "    def terminate(self):\n",
      "        [p.terminate() for p in self.processes]\n",
      "        [p.join() for p in self.processes]\n",
      "\n",
      "\n",
      "def init_global_constants(\n",
      "    num_dp=1,\n",
      "    num_tp=1,\n",
      "    num_pp=1,\n",
      "    topo=None,\n",
      "    model_name=None,\n",
      "    msid2mwid=None,\n",
      "    sequence_parallel=False,\n",
      "    gradient_checkpointing=True,\n",
      "    gradient_accumulation_fusion=False,\n",
      "    max_prompt_len=None,\n",
      "    is_train: bool = True,\n",
      "    expr_name=None,\n",
      "    trial_name=None,\n",
      "):\n",
      "    expr_name = expr_name if expr_name is not None else _DEFAULT_EXPR_NAME\n",
      "    trial_name = trial_name if trial_name is not None else _DEFAULT_TRIAL_NAME\n",
      "    constants.set_experiment_trial_names(expr_name, trial_name)\n",
      "    model_name = model_name if model_name is not None else MODEL_NAME\n",
      "\n",
      "    if topo is None:\n",
      "        if is_train:\n",
      "            topo = PipeDataTensorParallelTopology(\n",
      "                num_dp=num_dp,\n",
      "                num_tp=num_tp,\n",
      "                num_pp=num_pp,\n",
      "                sequence_parallel=sequence_parallel,\n",
      "                gradient_checkpointing=gradient_checkpointing,\n",
      "                gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "                max_prompt_len=max_prompt_len,\n",
      "            )\n",
      "        else:\n",
      "            topo = DataPipeTensorParallelTopology(\n",
      "                num_dp=num_dp,\n",
      "                num_tp=num_tp,\n",
      "                num_pp=num_pp,\n",
      "                sequence_parallel=sequence_parallel,\n",
      "            )\n",
      "        ws = num_dp * num_tp * num_pp\n",
      "    else:\n",
      "        ws = topo.world_size()\n",
      "\n",
      "    with constants.model_scope(model_name):\n",
      "        constants.set_rank_mapping(model_name, topo, msid2mwid=msid2mwid)\n",
      "        wg_ranks = [constants.to_global_pg_rank(i) for i in range(ws)]\n",
      "        wg = topology.new_or_get_group(ranks=wg_ranks)\n",
      "\n",
      "        constants.set_parallelism_group(\n",
      "            model_name=model_name, pgroup=wg, ranks=wg_ranks\n",
      "        )\n",
      "        grid = ParallelGrid(\n",
      "            process_group=wg,\n",
      "            topology=topo,\n",
      "            rank_mapping=constants.rank_mapping_of_model(model_name),\n",
      "        )\n",
      "        constants.set_grid(model_name=model_name, grid=grid)\n",
      "\n",
      "\n",
      "def clear_name_resolve(expr_name=None, trial_name=None):\n",
      "    expr_name = expr_name if expr_name is not None else _DEFAULT_EXPR_NAME\n",
      "    trial_name = trial_name if trial_name is not None else _DEFAULT_TRIAL_NAME\n",
      "    name_resolve.clear_subtree(\n",
      "        names.trial_root(experiment_name=expr_name, trial_name=trial_name)\n",
      "    )\n",
      "\n",
      "\n",
      "def pytorch_memory_burnin(rank):\n",
      "    torch.cuda.set_device(0)\n",
      "    torch.cuda.init()\n",
      "    x = torch.randn(1, device=\"cuda\", dtype=torch.float64, requires_grad=True)\n",
      "    y = x * torch.randn(1000, device=\"cuda\", dtype=torch.float64)\n",
      "    y.mean().backward()\n",
      "    del x, y\n",
      "    gc.collect()\n",
      "    torch.cuda.empty_cache()\n",
      "    gc.collect()\n",
      "\n",
      "\n",
      "def clear_gpu_cache():\n",
      "    gc.collect()\n",
      "    torch.cuda.empty_cache()\n",
      "    gc.collect()\n",
      "\n",
      "\n",
      "def get_memory(rank):\n",
      "    handle = pynvml.nvmlDeviceGetHandleByIndex(rank)\n",
      "    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
      "    used_memory = memory_info.used / (1024**2)  # Convert bytes to megabytes\n",
      "    return used_memory\n",
      "\n",
      "\n",
      "def get_pytorch_profiler(save_fn: str):\n",
      "\n",
      "    def trace_handler(p: torch.profiler._KinetoProfile):\n",
      "        # print(\n",
      "        #     p.key_averages().table(\n",
      "        #         sort_by=\"cuda_memory_usage\", row_limit=20, max_name_column_width=30, max_src_column_width=30\n",
      "        #     )\n",
      "        # )\n",
      "        p.export_chrome_trace(save_fn)\n",
      "\n",
      "    return torch.profiler.profile(\n",
      "        activities=[\n",
      "            torch.profiler.ProfilerActivity.CPU,\n",
      "            torch.profiler.ProfilerActivity.CUDA,\n",
      "        ],\n",
      "        record_shapes=True,\n",
      "        profile_memory=True,\n",
      "        with_stack=True,\n",
      "        on_trace_ready=trace_handler,\n",
      "        with_flops=True,\n",
      "    )\n",
      "\n",
      "\n",
      "def random_sample(num_sequences: int, seq_len: int, vocab_size: int, seed: int = 1):\n",
      "    torch.manual_seed(seed)\n",
      "    return torch.randint(0, vocab_size, (num_sequences, seq_len), dtype=torch.long)\n",
      "\n",
      "\n",
      "def make_random_packed_batches(\n",
      "    n_batches,\n",
      "    batch_size,\n",
      "    seq_len,\n",
      "    vocab_size,\n",
      "    seed: int = 1,\n",
      "    dp_rank=None,\n",
      "    dp_size=None,\n",
      ") -> List[SequenceSample]:\n",
      "    assert (dp_rank is None and dp_size is None) or (\n",
      "        dp_rank is not None and dp_size is not None\n",
      "    )\n",
      "    if dp_rank is None:\n",
      "        dp_rank = constants.data_parallel_rank()\n",
      "        dp_size = constants.data_parallel_world_size()\n",
      "    assert batch_size % dp_size == 0\n",
      "    n_seqs = batch_size * n_batches\n",
      "    seqs = random_sample(batch_size * n_batches, seq_len, vocab_size, seed)\n",
      "    seqs = seqs[n_seqs * dp_rank // dp_size : n_seqs * (dp_rank + 1) // dp_size]\n",
      "    x = SequenceSample.from_default(\n",
      "        seqlens=[seq_len for _ in range(seqs.shape[0])],\n",
      "        data=dict(\n",
      "            packed_input_ids=seqs.view(-1),\n",
      "            prompt_mask=torch.zeros_like(seqs.view(-1), dtype=torch.bool),\n",
      "        ),\n",
      "        ids=list(range(seqs.shape[0])),\n",
      "    )\n",
      "    return x.split(n_batches)\n",
      "\n",
      "\n",
      "def make_random_unpacked_batches(\n",
      "    n_batches,\n",
      "    batch_size,\n",
      "    seq_len,\n",
      "    vocab_size,\n",
      "    seed: int = 1,\n",
      "    dp_rank=None,\n",
      "    dp_size=None,\n",
      "):\n",
      "    n_seqs = batch_size * n_batches\n",
      "    dp_batch_size = batch_size // dp_size\n",
      "    assert (dp_rank is None and dp_size is None) or (\n",
      "        dp_rank is not None and dp_size is not None\n",
      "    )\n",
      "    if dp_rank is None:\n",
      "        dp_rank = constants.data_parallel_rank()\n",
      "        dp_size = constants.data_parallel_world_size()\n",
      "    assert batch_size % dp_size == 0\n",
      "    seqs = random_sample(batch_size * n_batches, seq_len, vocab_size, seed)\n",
      "    seqs = seqs[n_seqs * dp_rank // dp_size : n_seqs * (dp_rank + 1) // dp_size]\n",
      "    batches = [\n",
      "        seqs[j * dp_batch_size : (j + 1) * dp_batch_size] for j in range(n_batches)\n",
      "    ]\n",
      "    batches = [\n",
      "        dict(\n",
      "            input_ids=batch,\n",
      "            attention_mask=torch.ones_like(batch),\n",
      "        )\n",
      "        for batch in batches\n",
      "    ]\n",
      "    return batches\n",
      "\n",
      "\n",
      "def get_free_mem_gb() -> int:\n",
      "    with open(\"/proc/meminfo\") as file:\n",
      "        for line in file:\n",
      "            if \"MemFree\" in line:\n",
      "                return int(line.split()[1]) >> 20\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/seeding.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import hashlib\n",
      "import os\n",
      "import random\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "_SEED = None\n",
      "_BASE_SEED = None\n",
      "_SHUFFLER = None\n",
      "\n",
      "\n",
      "def _seed_from_key(key: str) -> int:\n",
      "    return int(hashlib.sha256(key.encode()).hexdigest(), 16) & 0xFFFFFFFF\n",
      "\n",
      "\n",
      "def set_random_seed(base_seed, key):\n",
      "    global _SEED, _BASE_SEED\n",
      "    _BASE_SEED = base_seed\n",
      "    seed = base_seed + _seed_from_key(key)\n",
      "    _SEED = seed\n",
      "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
      "    transformers.set_seed(seed)\n",
      "    random.seed(seed)\n",
      "    np.random.seed(seed)\n",
      "    torch.manual_seed(seed)\n",
      "    if torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed(seed)\n",
      "        torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\n",
      "def get_seed() -> int:\n",
      "    global _SEED\n",
      "    assert _SEED is not None\n",
      "    return _SEED\n",
      "\n",
      "\n",
      "class Shuffler:\n",
      "    def __init__(self, key=\"default\"):\n",
      "        self.cnt = 0\n",
      "        self.base_key = key\n",
      "\n",
      "    def next_shuffle(self) -> int:\n",
      "        shuffle_key = f\"{self.base_key}_{self.cnt}\"\n",
      "        self.cnt += 1\n",
      "        return _seed_from_key(shuffle_key)\n",
      "\n",
      "\n",
      "def get_shuffle_seed() -> int:\n",
      "    global _BASE_SEED, _SHUFFLER\n",
      "    if _SHUFFLER is None:\n",
      "        _SHUFFLER = Shuffler(f\"AReaL-seed{_BASE_SEED}\")\n",
      "    return _SHUFFLER.next_shuffle()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/logging.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import logging.config\n",
      "import os\n",
      "from logging import WARNING, Logger, Manager, RootLogger\n",
      "from typing import Literal, Optional\n",
      "\n",
      "import colorlog\n",
      "\n",
      "LOG_FORMAT = \"%(asctime)s.%(msecs)03d %(name)s %(levelname)s: %(message)s\"\n",
      "DATE_FORMAT = \"%Y%m%d-%H:%M:%S\"\n",
      "LOGLEVEL = logging.INFO\n",
      "\n",
      "# NOTE: To use colorlog we should not call colorama.init() anywhere.\n",
      "# The available color names are black, red, green, yellow, blue, purple, cyan and white\n",
      "log_config = {\n",
      "    \"version\": 1,\n",
      "    \"formatters\": {\n",
      "        \"plain\": {\n",
      "            \"()\": colorlog.ColoredFormatter,\n",
      "            \"format\": \"%(log_color)s\" + LOG_FORMAT,\n",
      "            \"datefmt\": DATE_FORMAT,\n",
      "            \"log_colors\": {\n",
      "                \"DEBUG\": \"white\",\n",
      "                \"INFO\": \"white\",\n",
      "                \"WARNING\": \"yellow\",\n",
      "                \"ERROR\": \"red\",\n",
      "                \"CRITICAL\": \"bold_white,bg_red\",\n",
      "            },\n",
      "        },\n",
      "        \"colored\": {\n",
      "            \"()\": colorlog.ColoredFormatter,\n",
      "            \"format\": \"%(log_color)s\" + LOG_FORMAT,\n",
      "            \"datefmt\": DATE_FORMAT,\n",
      "            \"log_colors\": {\n",
      "                \"DEBUG\": \"blue\",\n",
      "                \"INFO\": \"light_purple\",\n",
      "                \"WARNING\": \"yellow\",\n",
      "                \"ERROR\": \"red\",\n",
      "                \"CRITICAL\": \"bold_white,bg_red\",\n",
      "            },\n",
      "        },\n",
      "        \"colored_system\": {\n",
      "            \"()\": colorlog.ColoredFormatter,\n",
      "            \"format\": \"%(log_color)s\" + LOG_FORMAT,\n",
      "            \"datefmt\": DATE_FORMAT,\n",
      "            \"log_colors\": {\n",
      "                \"DEBUG\": \"blue\",\n",
      "                \"INFO\": \"light_green\",\n",
      "                \"WARNING\": \"yellow\",\n",
      "                \"ERROR\": \"red\",\n",
      "                \"CRITICAL\": \"bold_white,bg_red\",\n",
      "            },\n",
      "        },\n",
      "        \"colored_benchmark\": {\n",
      "            \"()\": colorlog.ColoredFormatter,\n",
      "            \"format\": \"%(log_color)s\" + LOG_FORMAT,\n",
      "            \"datefmt\": DATE_FORMAT,\n",
      "            \"log_colors\": {\n",
      "                \"DEBUG\": \"light_black\",\n",
      "                \"INFO\": \"light_cyan\",\n",
      "                \"WARNING\": \"yellow\",\n",
      "                \"ERROR\": \"red\",\n",
      "                \"CRITICAL\": \"bold_white,bg_red\",\n",
      "            },\n",
      "        },\n",
      "    },\n",
      "    \"handlers\": {\n",
      "        \"plainHandler\": {\n",
      "            \"class\": \"logging.StreamHandler\",\n",
      "            \"level\": LOGLEVEL,\n",
      "            \"formatter\": \"plain\",\n",
      "            \"stream\": \"ext://sys.stdout\",\n",
      "        },\n",
      "        \"benchmarkHandler\": {\n",
      "            \"class\": \"logging.StreamHandler\",\n",
      "            \"level\": \"DEBUG\",\n",
      "            \"formatter\": \"colored_benchmark\",\n",
      "            \"stream\": \"ext://sys.stdout\",\n",
      "        },\n",
      "        \"systemHandler\": {\n",
      "            \"class\": \"logging.StreamHandler\",\n",
      "            \"level\": \"INFO\",\n",
      "            \"formatter\": \"colored_system\",\n",
      "            \"stream\": \"ext://sys.stdout\",\n",
      "        },\n",
      "        \"coloredHandler\": {\n",
      "            \"class\": \"logging.StreamHandler\",\n",
      "            \"level\": LOGLEVEL,\n",
      "            \"formatter\": \"colored\",\n",
      "            \"stream\": \"ext://sys.stdout\",\n",
      "        },\n",
      "    },\n",
      "    \"loggers\": {\n",
      "        \"plain\": {\n",
      "            \"handlers\": [\"plainHandler\"],\n",
      "            \"level\": LOGLEVEL,\n",
      "        },\n",
      "        \"benchmark\": {\n",
      "            \"handlers\": [\"benchmarkHandler\"],\n",
      "            \"level\": \"DEBUG\",\n",
      "        },\n",
      "        \"colored\": {\n",
      "            \"handlers\": [\"coloredHandler\"],\n",
      "            \"level\": LOGLEVEL,\n",
      "        },\n",
      "        \"system\": {\n",
      "            \"handlers\": [\"systemHandler\"],\n",
      "            \"level\": LOGLEVEL,\n",
      "        },\n",
      "    },\n",
      "    \"disable_existing_loggers\": True,\n",
      "}\n",
      "\n",
      "\n",
      "def getLogger(\n",
      "    name: Optional[str] = None,\n",
      "    type_: Optional[Literal[\"plain\", \"benchmark\", \"colored\", \"system\"]] = None,\n",
      "):\n",
      "    # Fix the logging config automatically set by transformer_engine\n",
      "    # by reset config everytime getLogger is called.\n",
      "    root = RootLogger(WARNING)\n",
      "    Logger.root = root\n",
      "    Logger.manager = Manager(Logger.root)\n",
      "\n",
      "    logging.config.dictConfig(log_config)\n",
      "\n",
      "    if name is None:\n",
      "        name = \"plain\"\n",
      "    if type_ is None:\n",
      "        type_ = \"plain\"\n",
      "    assert type_ in [\"plain\", \"benchmark\", \"colored\", \"system\"]\n",
      "    if name not in log_config[\"loggers\"]:\n",
      "        log_config[\"loggers\"][name] = {\n",
      "            \"handlers\": [f\"{type_}Handler\"],\n",
      "            \"level\": LOGLEVEL,\n",
      "        }\n",
      "        logging.config.dictConfig(log_config)\n",
      "    return logging.getLogger(name)\n",
      "\n",
      "\n",
      "_LATEST_WANDB_STEP = 0\n",
      "\n",
      "\n",
      "def log_wandb_tensorboard(data, step=None, summary_writer=None):\n",
      "    import wandb\n",
      "\n",
      "    global _LATEST_WANDB_STEP\n",
      "    if step is None:\n",
      "        step = _LATEST_WANDB_STEP\n",
      "    else:\n",
      "        _LATEST_WANDB_STEP = max(_LATEST_WANDB_STEP, step)\n",
      "\n",
      "    wandb.log(data, step=step)\n",
      "    if summary_writer is not None:\n",
      "        for key, val in data.items():\n",
      "            summary_writer.add_scalar(f\"{key}\", val, step)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # The following serves as a color visualization test.\n",
      "    # The available color names are black, red, green, yellow, blue, purple, cyan and white\n",
      "    log_config = {\n",
      "        \"version\": 1,\n",
      "        \"formatters\": {\n",
      "            \"colored\": {\n",
      "                \"()\": colorlog.ColoredFormatter,\n",
      "                \"format\": \"%(log_color)s\" + LOG_FORMAT,\n",
      "                \"datefmt\": DATE_FORMAT,\n",
      "                \"log_colors\": {\n",
      "                    \"DEBUG\": \"purple\",\n",
      "                    \"INFO\": \"light_purple\",\n",
      "                    \"WARNING\": \"yellow\",\n",
      "                    \"ERROR\": \"red\",\n",
      "                    \"CRITICAL\": \"bold_white,bg_red\",\n",
      "                },\n",
      "            },\n",
      "        },\n",
      "        \"handlers\": {\n",
      "            \"coloredHandler\": {\n",
      "                \"class\": \"logging.StreamHandler\",\n",
      "                \"level\": \"DEBUG\",\n",
      "                \"formatter\": \"colored\",\n",
      "                \"stream\": \"ext://sys.stdout\",\n",
      "            },\n",
      "        },\n",
      "        \"loggers\": {\n",
      "            \"\": {\n",
      "                \"handlers\": [\"coloredHandler\"],\n",
      "                \"level\": \"DEBUG\",\n",
      "            },\n",
      "        },\n",
      "    }\n",
      "    logging.config.dictConfig(log_config)\n",
      "    logging.debug(\"This is a debug message\")\n",
      "    logging.info(\"This is an info message\")\n",
      "    logging.warning(\"This is a warning message\")\n",
      "    logging.error(\"This is an error message\")\n",
      "    logging.critical(\"This is a critical message\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/recover.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import os\n",
      "import pathlib\n",
      "import pickle\n",
      "from typing import Dict, List, Optional, Tuple\n",
      "\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"recover\")\n",
      "\n",
      "RECOVER_INFO_PATH = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class StepInfo:\n",
      "    epoch: int = 0\n",
      "    epoch_step: int = 0\n",
      "    global_step: int = 0\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RecoverInfo:\n",
      "    # Recover start is the counter of the next RLHF interation\n",
      "    # w.r.t. the counter of the saved checkpoint\n",
      "    recover_start: StepInfo\n",
      "    # Last step info is the counter of the saved checkpoint.\n",
      "    # It exactly lags beind recover_start by 1 iteration.\n",
      "    last_step_info: StepInfo\n",
      "\n",
      "    save_ctl_info: Dict\n",
      "    ckpt_ctl_info: Dict\n",
      "    eval_ctl_info: Dict\n",
      "\n",
      "    data_loading_dp_idx: int\n",
      "\n",
      "    hash_vals_to_ignore: List[int] = dataclasses.field(default_factory=list)\n",
      "\n",
      "\n",
      "def dump_recover_info(recover_info: RecoverInfo):\n",
      "    global RECOVER_INFO_PATH\n",
      "    if RECOVER_INFO_PATH is None:\n",
      "        RECOVER_INFO_PATH = os.path.join(\n",
      "            constants.RECOVER_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"recover_info.pkl\",\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(RECOVER_INFO_PATH), exist_ok=True)\n",
      "    with open(RECOVER_INFO_PATH, \"wb\") as f:\n",
      "        pickle.dump(recover_info, f)\n",
      "\n",
      "\n",
      "def load_recover_info() -> Tuple[int, Optional[RecoverInfo]]:\n",
      "    if os.environ.get(\"REAL_RECOVER_RUN\", \"0\") != \"1\":\n",
      "        return False, None\n",
      "    global RECOVER_INFO_PATH\n",
      "    if RECOVER_INFO_PATH is None:\n",
      "        RECOVER_INFO_PATH = os.path.join(\n",
      "            constants.RECOVER_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"recover_info.pkl\",\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(RECOVER_INFO_PATH), exist_ok=True)\n",
      "    try:\n",
      "        with open(RECOVER_INFO_PATH, \"rb\") as f:\n",
      "            return True, pickle.load(f)\n",
      "    except FileNotFoundError:\n",
      "        logger.warning(\n",
      "            f\"Resume info not found at {RECOVER_INFO_PATH}. \"\n",
      "            f\"This should not be a resumed experiment!\"\n",
      "        )\n",
      "        return False, None\n",
      "\n",
      "\n",
      "class InValidRecoverCkpt(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "def discover_ckpt(\n",
      "    expr_name: str, trial_name: str\n",
      ") -> Tuple[str, List[str], RecoverInfo]:\n",
      "    recover_info_file = (\n",
      "        pathlib.Path(constants.RECOVER_ROOT)\n",
      "        / expr_name\n",
      "        / trial_name\n",
      "        / \"recover_info.pkl\"\n",
      "    )\n",
      "    if os.path.exists(str(recover_info_file)):\n",
      "        with open(recover_info_file, \"rb\") as f:\n",
      "            info: RecoverInfo = pickle.load(f)\n",
      "        if info.last_step_info.epoch < 0:\n",
      "            msg = (\n",
      "                f\"Recover checkpoint is not valid. \"\n",
      "                f\"Expected last_step_info.epoch >= 0, \"\n",
      "                f\"but found {info.last_step_info.epoch}\"\n",
      "            )\n",
      "            raise InValidRecoverCkpt(msg)\n",
      "        model_save_dir = (\n",
      "            pathlib.Path(constants.MODEL_SAVE_ROOT) / expr_name / trial_name\n",
      "        )\n",
      "        model_ckpt_dirs = []\n",
      "        for role in os.listdir(model_save_dir):\n",
      "            if \"dataset_indices\" in role:\n",
      "                continue\n",
      "            if not os.path.isdir(model_save_dir / role):\n",
      "                continue\n",
      "            ckpt_dir = (\n",
      "                model_save_dir\n",
      "                / role\n",
      "                / f\"epoch{info.last_step_info.epoch + 1}epochstep{info.last_step_info.epoch_step + 1}globalstep{info.last_step_info.global_step + 1}\"\n",
      "            )\n",
      "            if not ckpt_dir.exists():\n",
      "                raise InValidRecoverCkpt(\n",
      "                    f\"Guessed checkpoint path does not exist: {ckpt_dir}.\"\n",
      "                )\n",
      "            model_ckpt_dirs.append(str(ckpt_dir))\n",
      "        return str(recover_info_file), model_ckpt_dirs, info\n",
      "    raise InValidRecoverCkpt(f\"Recover checkpoint not found at: {recover_info_file}\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/numpy_utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import Dict, List, Tuple\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def shape_leq(shape1: Tuple, shape2: Tuple) -> bool:\n",
      "    assert len(shape1) == len(shape2)\n",
      "    return all(x1 <= x2 for x1, x2 in zip(shape1, shape2))\n",
      "\n",
      "\n",
      "def shape_union(*shapes: List[Tuple]) -> Tuple:\n",
      "    if len(shapes) == 1:\n",
      "        return shapes[0]\n",
      "    for s in shapes:\n",
      "        assert len(s) == len(shapes[0])\n",
      "    return tuple(max(*dims) for dims in zip(*shapes))\n",
      "\n",
      "\n",
      "def split_to_shapes(x: np.ndarray, shapes: Dict, axis: int = -1):\n",
      "    \"\"\"Split an array and reshape to desired shapes.\n",
      "\n",
      "    Args:\n",
      "        x (np.ndarray): The array to be splitted\n",
      "        shapes (Dict): Dict of shapes (tuples) specifying how to split.\n",
      "        axis (int): Split dimension.\n",
      "\n",
      "    Returns:\n",
      "        List: Splitted observations.\n",
      "    \"\"\"\n",
      "    axis = len(x.shape) + axis if axis < 0 else axis\n",
      "    split_lengths = [np.prod(shape) for shape in shapes.values()]\n",
      "    assert x.shape[axis] == sum(split_lengths)\n",
      "    accum_split_lengths = [sum(split_lengths[:i]) for i in range(1, len(split_lengths))]\n",
      "    splitted_x = np.split(x, accum_split_lengths, axis)\n",
      "    return {\n",
      "        k: x.reshape(*x.shape[:axis], *shape, *x.shape[axis + 1 :])\n",
      "        for x, (k, shape) in zip(splitted_x, shapes.items())\n",
      "    }\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/slurm_utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "import re\n",
      "import subprocess\n",
      "from typing import List\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "\n",
      "\n",
      "def parse_node_id(node_name: str, prefix: str) -> int:\n",
      "    return int(node_name.split(prefix)[-1])\n",
      "\n",
      "\n",
      "def parse_nodelist(nodelist: str, prefix: str) -> List[str]:\n",
      "    if not nodelist.startswith(prefix):\n",
      "        raise ValueError(\n",
      "            f\"Node list `{nodelist}` does not start with hostname prefix `{prefix}`.\"\n",
      "        )\n",
      "    n = cluster_spec.suffix_n_digits\n",
      "    nodelist = nodelist.replace(prefix, \"\")\n",
      "    if \"[\" not in nodelist:\n",
      "        return [prefix + nodelist]\n",
      "    else:\n",
      "        nodelist = nodelist.strip(\"[]\")\n",
      "        node_ids = []\n",
      "        nodelist = nodelist.split(\",\")\n",
      "        for node_repr in nodelist:\n",
      "            if \"-\" not in node_repr:\n",
      "                node_ids.append(int(node_repr))\n",
      "            else:\n",
      "                start, end = map(int, node_repr.split(\"-\"))\n",
      "                node_ids += list(range(start, end + 1))\n",
      "        return [f\"{prefix}{node_id:0{n}d}\" for node_id in node_ids]\n",
      "\n",
      "\n",
      "def nodelist_from_nodes(nodes: List[str], prefix: str) -> str:\n",
      "    n = cluster_spec.suffix_n_digits\n",
      "    node_ids = sorted([parse_node_id(node, prefix) for node in nodes])\n",
      "    assert len(node_ids) > 0\n",
      "    if len(node_ids) == 1:\n",
      "        return f\"{prefix}{node_ids[0]:02d}\"\n",
      "    else:\n",
      "        node_reprs = []\n",
      "        start, end = node_ids[0], node_ids[0]\n",
      "        for i in range(len(node_ids)):\n",
      "            node_id = node_ids[i]\n",
      "            next_node_id = node_ids[i + 1] if i + 1 < len(node_ids) else -1\n",
      "            if node_id + 1 == next_node_id:\n",
      "                end = next_node_id\n",
      "            else:\n",
      "                if start == end:\n",
      "                    node_reprs.append(f\"{start:0{n}d}\")\n",
      "                else:\n",
      "                    node_reprs.append(f\"{start:0{n}d}-{end:0{n}d}\")\n",
      "                start = next_node_id\n",
      "                end = next_node_id\n",
      "        return f\"{prefix}[{','.join(node_reprs)}]\"\n",
      "\n",
      "\n",
      "def are_ones_contiguous(binary_array: np.ndarray):\n",
      "    one_indices = np.where(binary_array == 1)[0]\n",
      "    if len(one_indices) == 0:\n",
      "        return False\n",
      "    return np.all(np.diff(one_indices) == 1)\n",
      "\n",
      "\n",
      "def slurm_hostname_key(hostname):\n",
      "    \"\"\"Custom sorting key function to sort Slurm hostnames.\"\"\"\n",
      "    # Extract node number from hostname\n",
      "    match = re.match(r\"(\\D+)(\\d+)\", hostname)\n",
      "    if match:\n",
      "        prefix, number = match.groups()\n",
      "        return (prefix, int(number))\n",
      "    else:\n",
      "        return (hostname,)\n",
      "\n",
      "\n",
      "def check_slurm_availability():\n",
      "\n",
      "    slurm_available = (\n",
      "        int(\n",
      "            subprocess.run(\n",
      "                \"squeue\",\n",
      "                shell=True,\n",
      "                stdout=open(os.devnull, \"wb\"),\n",
      "                stderr=open(os.devnull, \"wb\"),\n",
      "            ).returncode\n",
      "        )\n",
      "        == 0\n",
      "    )\n",
      "    return slurm_available\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/topology.py ====\n",
      "\n",
      "# Modified from DeepSpeed.\n",
      "# Copyright [2025] Microsoft Corporation\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import os\n",
      "from itertools import permutations\n",
      "from itertools import product as cartesian_product\n",
      "from typing import Dict, List, NamedTuple, Optional, Tuple\n",
      "\n",
      "import torch.distributed as dist\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.base.constants import NCCL_DEFAULT_TIMEOUT\n",
      "\n",
      "logger = logging.getLogger(\"Topology\")\n",
      "\n",
      "GLOBAL_PROCESS_GROUP_REGISTRY: Dict[Tuple[Tuple[int], str], dist.ProcessGroup] = {}\n",
      "\n",
      "\n",
      "def new_or_get_group(ranks: List[int], backend=None):\n",
      "    if backend is None:\n",
      "        backend = dist.get_backend()\n",
      "    ranks = tuple(sorted(ranks))\n",
      "    global GLOBAL_PROCESS_GROUP_REGISTRY\n",
      "    key = (ranks, backend)\n",
      "    if key not in GLOBAL_PROCESS_GROUP_REGISTRY:\n",
      "        GLOBAL_PROCESS_GROUP_REGISTRY[key] = dist.new_group(\n",
      "            ranks, backend=backend, timeout=NCCL_DEFAULT_TIMEOUT\n",
      "        )\n",
      "    return GLOBAL_PROCESS_GROUP_REGISTRY[key]\n",
      "\n",
      "\n",
      "def destroy_all_comm_groups():\n",
      "    if not dist.is_initialized():\n",
      "        return\n",
      "    global GLOBAL_PROCESS_GROUP_REGISTRY\n",
      "    for group in GLOBAL_PROCESS_GROUP_REGISTRY.values():\n",
      "        dist.destroy_process_group(group)\n",
      "    GLOBAL_PROCESS_GROUP_REGISTRY = {}\n",
      "    # Destroying the default group will raise an error\n",
      "    # for the latest pytorch release. Omit it as a workaround.\n",
      "    # dist.destroy_process_group()\n",
      "\n",
      "\n",
      "def decompose_to_three_factors(n: int) -> List[Tuple[int, int, int]]:\n",
      "    factors = []\n",
      "    for i in range(1, int(n ** (1 / 2)) + 1):\n",
      "        if n % i == 0:\n",
      "            for j in range(i, int((n // i) ** (1 / 2)) + 1):\n",
      "                if (n // i) % j == 0:\n",
      "                    k = (n // i) // j\n",
      "                    factors += list(set(permutations([i, j, k])))\n",
      "    return factors\n",
      "\n",
      "\n",
      "class PipeDataTensorProcessCoord(NamedTuple):\n",
      "    pipe: int\n",
      "    data: int\n",
      "    tensor: int\n",
      "\n",
      "\n",
      "class DataPipeTensorProcessCoord(NamedTuple):\n",
      "    data: int\n",
      "    pipe: int\n",
      "    tensor: int\n",
      "\n",
      "\n",
      "# Explicitly define these class to allow pickling.\n",
      "PROCESS_COORD_REGISTRY = {\n",
      "    \"pipe#data#tensor\": PipeDataTensorProcessCoord,\n",
      "    \"data#pipe#tensor\": DataPipeTensorProcessCoord,\n",
      "}\n",
      "\n",
      "\n",
      "class ProcessTopology:\n",
      "    \"\"\"Manages the mapping of n-dimensional Cartesian coordinates to linear\n",
      "    indices. This mapping is used to map the rank of processes to the grid for\n",
      "    various forms of parallelism.\n",
      "\n",
      "    Each axis of the tensor is accessed by its name. The provided\n",
      "    ordering of the axes defines the layout of the topology.\n",
      "    ProcessTopology uses a \"row-major\" layout of the tensor axes, and so\n",
      "    axes=['x', 'y'] would map coordinates (x,y) and (x,y+1) to adjacent\n",
      "    linear indices. If instead axes=['y', 'x'] was used, coordinates\n",
      "    (x,y) and (x+1,y) would be adjacent.\n",
      "\n",
      "    Some methods return ProcessCoord namedtuples.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, axes, dims):\n",
      "        \"\"\"Create a mapping of n-dimensional tensor coordinates to linear\n",
      "        indices.\n",
      "\n",
      "        Arguments:\n",
      "            axes (list): the names of the tensor axes\n",
      "            dims (list): the dimension (length) of each axis of the topology tensor\n",
      "        \"\"\"\n",
      "\n",
      "        self.axes = axes  # names of each topology axis\n",
      "        self.dims = dims  # length of each topology axis\n",
      "\n",
      "        # This is actually a class that lets us hash {'row':3, 'col':2} mappings\n",
      "        try:\n",
      "            self.ProcessCoord = PROCESS_COORD_REGISTRY[\"#\".join(axes)]\n",
      "        except KeyError as e:\n",
      "            raise KeyError(\n",
      "                f\"Corresponding coordinate namedtuple not implemented for axes {axes}. \"\n",
      "                \"Check base/topology.py and implement explicitly.\"\n",
      "            ) from e\n",
      "\n",
      "        self.mapping = {}\n",
      "        ranges = [range(d) for d in dims]\n",
      "        # example: 1, (0,0,1)\n",
      "        for global_rank, coord in enumerate(cartesian_product(*ranges)):\n",
      "            key = {axis: coord[self.axes.index(axis)] for axis in self.axes}\n",
      "            key = self.ProcessCoord(**key)\n",
      "            # for example, {ProcessCoord(row=0, col=1) : 1}\n",
      "            self.mapping[key] = global_rank\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        if not isinstance(other, ProcessTopology):\n",
      "            return False\n",
      "        return self.mapping == other.mapping\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"ProcessTopology(axes={self.axes}, dims={self.dims})\"\n",
      "\n",
      "    def get_rank(self, **coord_kwargs):\n",
      "        \"\"\"Return the global rank of a process via its coordinates.\n",
      "\n",
      "        Coordinates are specified as kwargs. For example:\n",
      "\n",
      "            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n",
      "            >>> X.get_rank(x=0, y=1)\n",
      "            1\n",
      "        \"\"\"\n",
      "        if len(coord_kwargs) != len(self.axes):\n",
      "            raise ValueError(\"get_rank() does not support slices. Use filter_match())\")\n",
      "\n",
      "        key = self.ProcessCoord(**coord_kwargs)\n",
      "        assert (\n",
      "            key in self.mapping\n",
      "        ), f\"key {coord_kwargs} invalid, mapping: {self.mapping}, key: {key}\"\n",
      "        return self.mapping[key]\n",
      "\n",
      "    def get_axis_names(self):\n",
      "        \"\"\"Return a list of the axis names in the ordering of the topology.\"\"\"\n",
      "        return self.axes\n",
      "\n",
      "    def get_rank_repr(\n",
      "        self, rank, omit_axes=[\"data\", \"pipe\"], inner_sep=\"_\", outer_sep=\"-\"\n",
      "    ):\n",
      "        \"\"\"Return a string representation of a rank.\n",
      "\n",
      "        This method is primarily used for checkpointing model data.\n",
      "\n",
      "        For example:\n",
      "            >>> topo = Topo(axes=['a', 'b'], dims=[2, 2])\n",
      "            >>> topo.get_rank_repr(rank=3)\n",
      "            'a_01-b_01'\n",
      "            >>> topo.get_rank_repr(rank=3, omit_axes=['a'])\n",
      "            'b_01'\n",
      "\n",
      "        Args:\n",
      "            rank (int): A rank in the topology.\n",
      "            omit_axes (list, optional): Axes that should not be in the representation. Defaults to ['data', 'pipe'].\n",
      "            inner_sep (str, optional): [description]. Defaults to '_'.\n",
      "            outer_sep (str, optional): [description]. Defaults to '-'.\n",
      "\n",
      "        Returns:\n",
      "            str: A string representation of the coordinate owned by ``rank``.\n",
      "        \"\"\"\n",
      "        omit_axes = frozenset(omit_axes)\n",
      "        axes = [a for a in self.get_axis_names() if a not in omit_axes]\n",
      "        names = []\n",
      "        n = cluster_spec.suffix_n_digits\n",
      "        for ax in axes:\n",
      "            ax_rank = getattr(self.get_coord(rank=rank), ax)\n",
      "            names.append(f\"{ax}{inner_sep}{ax_rank:0{n}d}\")\n",
      "        return outer_sep.join(names)\n",
      "\n",
      "    def get_dim(self, axis):\n",
      "        \"\"\"Return the number of processes along the given axis.\n",
      "\n",
      "        For example:\n",
      "            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n",
      "            >>> X.get_dim('y')\n",
      "            3\n",
      "        \"\"\"\n",
      "        if axis not in self.axes:\n",
      "            return 0\n",
      "        return self.dims[self.axes.index(axis)]\n",
      "\n",
      "    def get_coord(self, rank):\n",
      "        \"\"\"Return the coordinate owned by a process rank.\n",
      "\n",
      "        The axes of the returned namedtuple can be directly accessed as members. For\n",
      "        example:\n",
      "            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n",
      "            >>> coord = X.get_coord(rank=1)\n",
      "            >>> coord.x\n",
      "            0\n",
      "            >>> coord.y\n",
      "            1\n",
      "        \"\"\"\n",
      "        for coord, idx in self.mapping.items():\n",
      "            if idx == rank:\n",
      "                return coord\n",
      "        raise ValueError(f\"rank {rank} not found in topology.\")\n",
      "\n",
      "    def get_axis_comm_lists(self, axis):\n",
      "        \"\"\"Construct lists suitable for a communicator group along axis\n",
      "        ``axis``.\n",
      "\n",
      "        Example:\n",
      "            >>> topo = Topo(axes=['pipe', 'data', 'model'], dims=[2, 2, 2])\n",
      "            >>> topo.get_axis_comm_lists('pipe')\n",
      "            [\n",
      "                [0, 4], # data=0, model=0\n",
      "                [1, 5], # data=0, model=1\n",
      "                [2, 6], # data=1, model=0\n",
      "                [3, 7], # data=1, model=1\n",
      "            ]\n",
      "\n",
      "        Returns:\n",
      "            A list of lists whose coordinates match in all axes *except* ``axis``.\n",
      "        \"\"\"\n",
      "\n",
      "        # We don't want to RuntimeError because it allows us to write more generalized\n",
      "        # code for hybrid parallelisms.\n",
      "        if axis not in self.axes:\n",
      "            return []\n",
      "\n",
      "        # Grab all axes but `axis`\n",
      "        other_axes = [a for a in self.axes if a != axis]\n",
      "\n",
      "        lists = []\n",
      "\n",
      "        # Construct all combinations of coords with other_axes\n",
      "        ranges = [range(self.get_dim(a)) for a in other_axes]\n",
      "        for coord in cartesian_product(*ranges):\n",
      "            other_keys = {a: coord[other_axes.index(a)] for a in other_axes}\n",
      "            # now go over all ranks in `axis`.\n",
      "            sub_list = []\n",
      "            for axis_key in range(self.get_dim(axis)):\n",
      "                key = self.ProcessCoord(**other_keys, **{axis: axis_key})\n",
      "                sub_list.append(self.mapping[key])\n",
      "            lists.append(sub_list)\n",
      "\n",
      "        return lists\n",
      "\n",
      "    def filter_match(self, **filter_kwargs):\n",
      "        \"\"\"Return the list of ranks whose coordinates match the provided\n",
      "        criteria.\n",
      "\n",
      "        Example:\n",
      "            >>> X = ProcessTopology(axes=['pipe', 'data', 'model'], dims=[2, 2, 2])\n",
      "            >>> X.filter_match(pipe=0, data=1)\n",
      "            [2, 3]\n",
      "            >>> [X.get_coord(rank) for rank in X.filter_match(pipe=0, data=1)]\n",
      "            [ProcessCoord(pipe=0, data=1, model=0), ProcessCoord(pipe=0, data=1, model=1)]\n",
      "\n",
      "        Arguments:\n",
      "            **filter_kwargs (dict): criteria used to select coordinates.\n",
      "\n",
      "        Returns:\n",
      "            The list of ranks whose coordinates match filter_kwargs.\n",
      "        \"\"\"\n",
      "\n",
      "        def _filter_helper(x):\n",
      "            for key, val in filter_kwargs.items():\n",
      "                if getattr(x, key) != val:\n",
      "                    return False\n",
      "            return True\n",
      "\n",
      "        coords = filter(_filter_helper, self.mapping.keys())\n",
      "        return [self.mapping[coord] for coord in coords]\n",
      "\n",
      "    def get_axis_list(self, axis, idx):\n",
      "        \"\"\"Returns the list of global ranks whose coordinate in an axis is idx.\n",
      "\n",
      "        For example:\n",
      "            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n",
      "            >>> X.get_axis_list(axis='x', idx=0)\n",
      "            [0, 1, 2]\n",
      "            >>> X.get_axis_list(axis='y', idx=0)\n",
      "            [0, 3]\n",
      "        \"\"\"\n",
      "\n",
      "        # This could be faster by generating the desired keys directly instead of\n",
      "        # filtering.\n",
      "        axis_num = self.axes.index(axis)\n",
      "        ranks = [self.mapping[k] for k in self.mapping.keys() if k[axis_num] == idx]\n",
      "        return ranks\n",
      "\n",
      "    def world_size(self):\n",
      "        return len(self.mapping)\n",
      "\n",
      "    def __str__(self):\n",
      "        return str(self.mapping)\n",
      "\n",
      "\n",
      "def _prime_factors(N):\n",
      "    \"\"\"Returns the prime factorization of positive integer N.\"\"\"\n",
      "    if N <= 0:\n",
      "        raise ValueError(\"Values must be strictly positive.\")\n",
      "\n",
      "    primes = []\n",
      "    while N != 1:\n",
      "        for candidate in range(2, N + 1):\n",
      "            if N % candidate == 0:\n",
      "                primes.append(candidate)\n",
      "                N //= candidate\n",
      "                break\n",
      "    return primes\n",
      "\n",
      "\n",
      "class PipeDataTensorParallelTopology(ProcessTopology):\n",
      "    \"\"\"A topology for hybrid pipeline, model, and data parallelism.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        num_pp: int,\n",
      "        num_tp: int,\n",
      "        num_dp: int,\n",
      "        sequence_parallel: bool,\n",
      "        gradient_checkpointing: bool,\n",
      "        gradient_accumulation_fusion: bool,\n",
      "        max_prompt_len: Optional[int] = None,\n",
      "    ):\n",
      "        super().__init__(axes=[\"pipe\", \"data\", \"tensor\"], dims=[num_pp, num_dp, num_tp])\n",
      "\n",
      "        self.sequence_parallel = sequence_parallel\n",
      "        self.gradient_checkpointing = gradient_checkpointing\n",
      "        self.max_prompt_len = max_prompt_len\n",
      "        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n",
      "\n",
      "\n",
      "class DataPipeTensorParallelTopology(ProcessTopology):\n",
      "    \"\"\"A topology for hybrid data, pipeline, and tensor parallelism.\n",
      "\n",
      "    Note that DP is the most outer dimension. Used for inference only.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        num_pp: int,\n",
      "        num_tp: int,\n",
      "        num_dp: int,\n",
      "        sequence_parallel: bool,\n",
      "        max_prompt_len: Optional[int] = None,\n",
      "    ):\n",
      "        super().__init__(axes=[\"data\", \"pipe\", \"tensor\"], dims=[num_dp, num_pp, num_tp])\n",
      "        self.sequence_parallel = sequence_parallel\n",
      "        self.max_prompt_len = max_prompt_len\n",
      "\n",
      "\n",
      "class ParallelGrid:\n",
      "    \"\"\"Implements a grid object that stores the data parallel ranks\n",
      "    corresponding to each of the model parallel stages.\n",
      "\n",
      "    The grid object organizes the processes in a distributed pytorch job\n",
      "    into a 2D grid, of stage_id and data_parallel_id.\n",
      "\n",
      "    self.stage_id and self.data_parallel_id stores the stage id and the\n",
      "    data parallel id of current process.\n",
      "\n",
      "    self.dp_group groups the processes by stage_id. self.dp_group[i], is\n",
      "    a list containing all process ranks whose stage_id is i.\n",
      "\n",
      "    self.p2p_groups stores a list of tuple, where each tuple stores\n",
      "    process ranks of adjacent stages for a given data_parallel_id. For\n",
      "    example if num_stage is 5 then a tuple [7,8] represents stages [3,\n",
      "    4], with data_parallel id = 1. A stage wrap around will appear as\n",
      "    non-adjacent ranks, for example tuple [4,0] with representing wrap-\n",
      "    around stage 4 and 0, for data_parallel_id = 0, or similarly [9,5]\n",
      "    represents wrapped around stages [4,0] for data_parallel_id = 1.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        topology: ProcessTopology,\n",
      "        process_group: dist.ProcessGroup,\n",
      "        rank_mapping: Optional[Dict[int, int]] = None,\n",
      "    ):\n",
      "        # NOTE: DeepSpeed will access *EVERY* attribute they defined. We need to remain\n",
      "        # the original semantics, so the attribute name may not truthfully mean what the attribute is.\n",
      "        # E.g., self.global_rank is not the global rank of the whole world including the master worker,\n",
      "        # but the rank in the 3D parallelism group of this specific model.\n",
      "\n",
      "        if rank_mapping is None:\n",
      "            rank_mapping = {i: i for i in range(topology.world_size())}\n",
      "        self.rank_mapping = rank_mapping\n",
      "\n",
      "        self.global_rank = dist.get_rank(group=process_group)\n",
      "        # NOTE: we don't use dist.get_world_size(process_group) because it will only return the true\n",
      "        # world size when this process belongs to the process group, otherwise it will return -1.\n",
      "        # self.world_size=-1 will trigger assertion errors.\n",
      "        self.world_size = topology.world_size()\n",
      "\n",
      "        self._topo = topology\n",
      "\n",
      "        self.data_parallel_size = max(self._topo.get_dim(\"data\"), 1)\n",
      "        self.pipe_parallel_size = max(self._topo.get_dim(\"pipe\"), 1)\n",
      "        self.model_parallel_size = max(self._topo.get_dim(\"tensor\"), 1)\n",
      "        self.slice_parallel_size = self.model_parallel_size\n",
      "        assert self._is_grid_valid(), (\n",
      "            \"Invalid Grid\",\n",
      "            topology,\n",
      "            self.world_size,\n",
      "        )\n",
      "\n",
      "        self.stage_id = self.get_stage_id()\n",
      "        self.data_parallel_id = self.get_data_parallel_id()\n",
      "\n",
      "        # Create new ProcessGroups for TP + PP parallelism approaches.\n",
      "        self.ds_model_proc_group = None\n",
      "        self.ds_model_proc_group_gloo = None\n",
      "        self.ds_model_rank = -1\n",
      "        for dp in range(self.data_parallel_size):\n",
      "            ranks = sorted(self._topo.get_axis_list(axis=\"data\", idx=dp))\n",
      "            proc_group = new_or_get_group(ranks=[rank_mapping[rank] for rank in ranks])\n",
      "            proc_group_gloo = new_or_get_group(\n",
      "                ranks=[rank_mapping[rank] for rank in ranks], backend=\"gloo\"\n",
      "            )\n",
      "            if self.global_rank in ranks:\n",
      "                self.ds_model_proc_group = proc_group\n",
      "                self.ds_model_proc_group_gloo = proc_group_gloo\n",
      "                self.ds_model_world_size = len(ranks)\n",
      "                self.ds_model_rank = ranks.index(self.global_rank)\n",
      "        if self.global_rank != -1:\n",
      "            assert self.ds_model_rank > -1\n",
      "            assert self.ds_model_proc_group is not None\n",
      "        else:\n",
      "            assert self.ds_model_rank == -1\n",
      "            assert self.ds_model_proc_group is None\n",
      "\n",
      "        # Create new ProcessGroup for gradient all-reduces - these are the data parallel groups\n",
      "        self.dp_group = []\n",
      "        self.dp_groups = self._topo.get_axis_comm_lists(\"data\")\n",
      "        self.dp_proc_group = self.dp_proc_group_gloo = None\n",
      "        for g in self.dp_groups:\n",
      "            proc_group = new_or_get_group(ranks=[rank_mapping[x] for x in g])\n",
      "            # NOTE: We must create the GLOO group for vLLM's usage.\n",
      "            # It will be used to broadcast CPU objects.\n",
      "            proc_group_gloo = new_or_get_group(\n",
      "                ranks=[rank_mapping[x] for x in g], backend=\"gloo\"\n",
      "            )\n",
      "            if self.global_rank in g:\n",
      "                self.dp_group = g\n",
      "                self.dp_proc_group = proc_group\n",
      "                self.dp_proc_group_gloo = proc_group_gloo\n",
      "\n",
      "        self.is_first_stage = self.stage_id == 0\n",
      "        self.is_last_stage = self.stage_id == (self.pipe_parallel_size - 1)\n",
      "\n",
      "        # These ranks will be used only when NCCL P2P is not supported (torch version <= 1.8).\n",
      "        # Otherwise we don't need to create groups for P2P communication.\n",
      "        self.p2p_groups: List[List[int]] = self._build_p2p_groups()\n",
      "\n",
      "        # Create new ProcessGroup for pipeline collectives - these are pipe parallel groups\n",
      "        self.pp_group = []\n",
      "        self.pp_proc_group = self.pp_proc_group_gloo = None\n",
      "        self.embedding_proc_group = None\n",
      "        self.position_embedding_proc_group = None\n",
      "        self.pipe_groups = self._topo.get_axis_comm_lists(\"pipe\")\n",
      "        for ranks in self.pipe_groups:\n",
      "            proc_group = new_or_get_group(ranks=[rank_mapping[rank] for rank in ranks])\n",
      "            # NOTE: We must create the GLOO group for vLLM's usage.\n",
      "            # It will be used to broadcast CPU objects.\n",
      "            cpu_group = new_or_get_group(\n",
      "                ranks=[rank_mapping[rank] for rank in ranks], backend=\"gloo\"\n",
      "            )\n",
      "            if self.global_rank in ranks:\n",
      "                self.pp_group = ranks\n",
      "                self.pp_proc_group = proc_group\n",
      "                # NOTE: we must create the GLOO group for vLLM's usage\n",
      "                self.pp_proc_group_gloo = cpu_group\n",
      "\n",
      "            # Create embedding groups for communicating gradients between LM head and the embedding.\n",
      "            # Used to tie gradients of the embedding layer, e.g. for GPT.\n",
      "            if len(ranks) > 1:\n",
      "                embedding_ranks = [ranks[0], ranks[-1]]\n",
      "                position_embedding_ranks = [ranks[0]]\n",
      "            else:\n",
      "                embedding_ranks = position_embedding_ranks = ranks\n",
      "            embedding_group = new_or_get_group(\n",
      "                ranks=[rank_mapping[rank] for rank in embedding_ranks]\n",
      "            )\n",
      "            if self.global_rank in embedding_ranks:\n",
      "                self.embedding_proc_group = embedding_group\n",
      "\n",
      "            # TODO: support pipline_model_parallel_split_rank\n",
      "            position_embedding_group = new_or_get_group(\n",
      "                ranks=[rank_mapping[rank] for rank in position_embedding_ranks]\n",
      "            )\n",
      "            if self.global_rank in position_embedding_ranks:\n",
      "                self.position_embedding_proc_group = position_embedding_group\n",
      "\n",
      "        if self.global_rank != -1:\n",
      "            assert self.pp_proc_group is not None\n",
      "        else:\n",
      "            assert self.pp_proc_group is None\n",
      "\n",
      "        # Create new ProcessGroup for model (tensor-slicing) collectives\n",
      "\n",
      "        # Short circuit case without model parallelism.\n",
      "        self.slice_group = None\n",
      "        self.slice_proc_group = self.slice_proc_group_gloo = None\n",
      "        self.mp_group = []\n",
      "        self.model_groups = self._topo.get_axis_comm_lists(\"tensor\")\n",
      "        for g in self.model_groups:\n",
      "            proc_group = new_or_get_group(ranks=[rank_mapping[x] for x in g])\n",
      "            # NOTE: We must create the GLOO group for vLLM's usage.\n",
      "            # It will be used to broadcast CPU objects.\n",
      "            cpu_group = new_or_get_group(\n",
      "                ranks=[rank_mapping[x] for x in g], backend=\"gloo\"\n",
      "            )\n",
      "            if self.global_rank in g:\n",
      "                self.slice_group = g\n",
      "                self.slice_proc_group = proc_group\n",
      "                self.slice_proc_group_gloo = cpu_group\n",
      "\n",
      "        # Create tp+dp group to be consistent with Megatron.\n",
      "        self.tp_dp_proc_group = None\n",
      "        for pp in range(self.pipe_parallel_size):\n",
      "            ranks = sorted(self._topo.get_axis_list(axis=\"pipe\", idx=pp))\n",
      "            proc_group = new_or_get_group(ranks=[rank_mapping[rank] for rank in ranks])\n",
      "            if self.global_rank in ranks:\n",
      "                self.tp_dp_proc_group = proc_group\n",
      "\n",
      "    def get_stage_id(self):\n",
      "        if self.global_rank == -1:\n",
      "            return -1\n",
      "        return self._topo.get_coord(rank=self.global_rank).pipe\n",
      "\n",
      "    def get_data_parallel_id(self):\n",
      "        if self.global_rank == -1:\n",
      "            return -1\n",
      "        return self._topo.get_coord(rank=self.global_rank).data\n",
      "\n",
      "    def _build_p2p_groups(self):\n",
      "        \"\"\"Groups for sending and receiving activations and gradients across\n",
      "        model parallel stages.\"\"\"\n",
      "        comm_lists = self._topo.get_axis_comm_lists(\"pipe\")\n",
      "        p2p_lists = []\n",
      "        for rank in range(self.world_size):\n",
      "            for l in comm_lists:\n",
      "                assert len(l) == self.pipe_parallel_size\n",
      "                if rank in l:\n",
      "                    idx = l.index(rank)\n",
      "                    buddy_rank = l[(idx + 1) % self.pipe_parallel_size]\n",
      "                    p2p_lists.append(\n",
      "                        [self.rank_mapping[rank], self.rank_mapping[buddy_rank]]\n",
      "                    )\n",
      "                    break  # next global rank\n",
      "        assert len(p2p_lists) == self.world_size\n",
      "        return p2p_lists\n",
      "\n",
      "    def _is_grid_valid(self):\n",
      "        ranks = 1\n",
      "        for ax in self._topo.get_axis_names():\n",
      "            ranks *= self._topo.get_dim(ax)\n",
      "        return ranks == self.world_size\n",
      "\n",
      "    # returns the global rank of the process with the provided stage id\n",
      "    # which has the same data_parallel_id as caller process\n",
      "    def stage_to_global(self, stage_id, **kwargs):\n",
      "        if self.global_rank == -1:\n",
      "            return -1\n",
      "        me = self._topo.get_coord(self.global_rank)\n",
      "        transform = me._replace(pipe=stage_id, **kwargs)._asdict()\n",
      "        return self._topo.get_rank(**transform)\n",
      "\n",
      "    def topology(self) -> ProcessTopology:\n",
      "        return self._topo\n",
      "\n",
      "    # MPU functions for DeepSpeed integration\n",
      "    def get_global_rank(self):\n",
      "        return self.global_rank\n",
      "\n",
      "    def get_pipe_parallel_rank(self):\n",
      "        \"\"\"The stage of the pipeline this rank resides in.\"\"\"\n",
      "        return self.get_stage_id()\n",
      "\n",
      "    def get_pipe_parallel_world_size(self):\n",
      "        \"\"\"The number of stages in the pipeline.\"\"\"\n",
      "        return self.pipe_parallel_size\n",
      "\n",
      "    def get_pipe_parallel_group(self):\n",
      "        \"\"\"The group of ranks within the same pipeline.\"\"\"\n",
      "        return self.pp_proc_group\n",
      "\n",
      "    def get_data_parallel_rank(self):\n",
      "        \"\"\"Which pipeline this rank resides in.\"\"\"\n",
      "        return self.data_parallel_id\n",
      "\n",
      "    def get_data_parallel_world_size(self):\n",
      "        \"\"\"The number of pipelines.\"\"\"\n",
      "        return self.data_parallel_size\n",
      "\n",
      "    def get_data_parallel_group(self):\n",
      "        \"\"\"The group of ranks within the same stage of all pipelines.\"\"\"\n",
      "        return self.dp_proc_group\n",
      "\n",
      "    def get_data_parallel_group_gloo(self):\n",
      "        return self.dp_proc_group_gloo\n",
      "\n",
      "    # These are model parallel groups across all types of model parallelism.\n",
      "    # Deepspeed uses them to detect overflow, etc.\n",
      "    # group of ranks with same dp rank\n",
      "    def get_model_parallel_rank(self):\n",
      "        return self.ds_model_rank\n",
      "\n",
      "    def get_model_parallel_world_size(self):\n",
      "        return self.ds_model_world_size\n",
      "\n",
      "    def get_model_parallel_group(self):\n",
      "        return self.ds_model_proc_group\n",
      "\n",
      "    # For Megatron-style tensor slicing\n",
      "    def get_tensor_model_parallel_rank(self):\n",
      "        if self.global_rank == -1:\n",
      "            return -1\n",
      "        if \"tensor\" in self._topo.get_axis_names():\n",
      "            return self._topo.get_coord(rank=self.global_rank).tensor\n",
      "        else:\n",
      "            return 0\n",
      "\n",
      "    def get_tensor_model_parallel_world_size(self):\n",
      "        return self.slice_parallel_size\n",
      "\n",
      "    def get_tensor_model_parallel_group(self):\n",
      "        return self.slice_proc_group\n",
      "\n",
      "    def get_tensor_model_parallel_cpu_group(self):\n",
      "        return self.slice_proc_group_gloo\n",
      "\n",
      "    @property\n",
      "    def topo(self):\n",
      "        return self._topo\n",
      "\n",
      "\n",
      "class FakeGrid:\n",
      "    \"\"\"Used for testing dynamic scheduling in none-GPU environment.\"\"\"\n",
      "\n",
      "    def __init__(self, rank: int, topo: ProcessTopology):\n",
      "        self.rank = rank\n",
      "        self._topo = topo\n",
      "\n",
      "        self.data_parallel_size = max(self._topo.get_dim(\"data\"), 1)\n",
      "        self.pipe_parallel_size = max(self._topo.get_dim(\"pipe\"), 1)\n",
      "        self.model_parallel_size = max(self._topo.get_dim(\"tensor\"), 1)\n",
      "\n",
      "        self.coord = self._topo.get_coord(self.rank)\n",
      "        self.dp_id = self.coord.data\n",
      "        self.pp_id = self.coord.pipe\n",
      "        self.mp_id = self.coord.tensor\n",
      "\n",
      "        self.world_size = (\n",
      "            self.data_parallel_size * self.pipe_parallel_size * self.model_parallel_size\n",
      "        )\n",
      "\n",
      "    def get_pipe_parallel_group(self):\n",
      "        raise RuntimeError(\"No groups in fake grid.\")\n",
      "\n",
      "    def get_pipe_parallel_world_size(self):\n",
      "        return self.pipe_parallel_size\n",
      "\n",
      "    def get_pipe_parallel_rank(self):\n",
      "        return self.pp_id\n",
      "\n",
      "    def get_data_parallel_group(self):\n",
      "        raise RuntimeError(\"No groups in fake grid.\")\n",
      "\n",
      "    def get_data_parallel_world_size(self):\n",
      "        return self.data_parallel_size\n",
      "\n",
      "    def get_data_parallel_rank(self):\n",
      "        return self.dp_id\n",
      "\n",
      "    def get_tensor_model_parallel_group(self):\n",
      "        raise RuntimeError(\"No groups in fake grid.\")\n",
      "\n",
      "    def get_tensor_model_parallel_world_size(self):\n",
      "        return self.model_parallel_size\n",
      "\n",
      "    def get_tensor_model_parallel_rank(self):\n",
      "        return self.mp_id\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/timeutil.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import math\n",
      "import threading\n",
      "from abc import ABC\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "INFINITE_DURATION = 60 * 60 * 24 * 365 * 1000\n",
      "\n",
      "\n",
      "class FrequencyControl:\n",
      "    \"\"\"An utility to control the execution of code with a time or/and step\n",
      "    frequency.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self, frequency_seconds=None, frequency_steps=None, initial_value=False\n",
      "    ):\n",
      "        \"\"\"Initialization method of FrequencyControl.\n",
      "        Args:\n",
      "            frequency_seconds: Minimal interval between two trigger.\n",
      "            frequency_steps: Minimal number of steps between two triggers.\n",
      "            initial_value: In true, the first call of check() returns True.\n",
      "\n",
      "        NOTE:\n",
      "            - If both frequency_seconds and frequency_steps are None, the checking will always return False except\n",
      "             for the specified initial value.\n",
      "            - If passed both, both frequency and steps conditions have to be met for check() to return True.\n",
      "            - If one is passed, checking on the other condition will be ignored.\n",
      "        \"\"\"\n",
      "        self.frequency_seconds = frequency_seconds\n",
      "        self.frequency_steps = frequency_steps\n",
      "        self.__start_time = datetime.now()\n",
      "        self.__steps = 0\n",
      "        self.__last_time = datetime.now()\n",
      "        self.__last_steps = 0\n",
      "        self.__interval_seconds = self.__interval_steps = None\n",
      "        self.__initial_value = initial_value\n",
      "        self.__lock = threading.Lock()\n",
      "\n",
      "    def state_dict(self):\n",
      "        return dict(\n",
      "            frequence_seconds=self.frequency_seconds,\n",
      "            frequency_steps=self.frequency_steps,\n",
      "            start_time=self.__start_time,\n",
      "            steps=self.__steps,\n",
      "            last_time=self.__last_time,\n",
      "            last_steps=self.__last_steps,\n",
      "            interval_steps=self.__interval_steps,\n",
      "            interval_seconds=self.__interval_seconds,\n",
      "            initial_value=self.__initial_value,\n",
      "        )\n",
      "\n",
      "    def load_state_dict(self, state_dict):\n",
      "        self.frequency_seconds = state_dict[\"frequence_seconds\"]\n",
      "        self.frequency_steps = state_dict[\"frequency_steps\"]\n",
      "        self.__start_time = state_dict[\"start_time\"]\n",
      "        self.__steps = state_dict[\"steps\"]\n",
      "        self.__last_time = state_dict[\"last_time\"]\n",
      "        self.__last_steps = state_dict[\"last_steps\"]\n",
      "        self.__interval_steps = state_dict[\"interval_steps\"]\n",
      "        self.__interval_seconds = state_dict[\"interval_seconds\"]\n",
      "        self.__initial_value = state_dict[\"initial_value\"]\n",
      "\n",
      "    @property\n",
      "    def total_seconds(self):\n",
      "        now = datetime.now()\n",
      "        return (now - self.__start_time).total_seconds()\n",
      "\n",
      "    @property\n",
      "    def total_steps(self):\n",
      "        return self.__steps\n",
      "\n",
      "    @property\n",
      "    def interval_seconds(self):\n",
      "        return self.__interval_seconds\n",
      "\n",
      "    @property\n",
      "    def interval_steps(self):\n",
      "        return self.__interval_steps\n",
      "\n",
      "    def check(self, steps=1):\n",
      "        \"\"\"Check whether frequency condition is met.\n",
      "        Args:\n",
      "            steps: number of step between this and the last call of check()\n",
      "\n",
      "        Returns:\n",
      "            flag: True if condition is met, False other wise\n",
      "        \"\"\"\n",
      "        with self.__lock:\n",
      "            now = datetime.now()\n",
      "            self.__steps += steps\n",
      "\n",
      "            if self.__initial_value:\n",
      "                self.__last_time = now\n",
      "                self.__last_steps = self.__steps\n",
      "                self.__initial_value = False\n",
      "                return True\n",
      "\n",
      "            self.__interval_seconds = (now - self.__last_time).total_seconds()\n",
      "            self.__interval_steps = self.__steps - self.__last_steps\n",
      "            if self.frequency_steps is None and self.frequency_seconds is None:\n",
      "                return False\n",
      "            if (\n",
      "                self.frequency_seconds is not None\n",
      "                and self.__interval_seconds < self.frequency_seconds\n",
      "            ):\n",
      "                return False\n",
      "            if (\n",
      "                self.frequency_steps is not None\n",
      "                and self.__interval_steps < self.frequency_steps\n",
      "            ):\n",
      "                return False\n",
      "            self.__last_time = now\n",
      "            self.__last_steps = self.__steps\n",
      "\n",
      "            return True\n",
      "\n",
      "    def reset_time(self):\n",
      "        self.__last_time = datetime.now()\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class EpochStepTimeFreqCtl:\n",
      "    freq_epoch: int | None\n",
      "    freq_step: int | None\n",
      "    freq_sec: int | None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        self.epoch_ctl = FrequencyControl(frequency_steps=self.freq_epoch)\n",
      "        self.step_ctl = FrequencyControl(frequency_steps=self.freq_step)\n",
      "        self.time_ctl = FrequencyControl(frequency_seconds=self.freq_sec)\n",
      "\n",
      "    def check(self, epochs: int, steps: int):\n",
      "        x, y, z = (\n",
      "            self.epoch_ctl.check(epochs),\n",
      "            self.step_ctl.check(steps),\n",
      "            self.time_ctl.check(),\n",
      "        )\n",
      "        return x or y or z\n",
      "\n",
      "    def state_dict(self):\n",
      "        return dict(\n",
      "            epoch=self.epoch_ctl.state_dict(),\n",
      "            step=self.step_ctl.state_dict(),\n",
      "            time=self.time_ctl.state_dict(),\n",
      "        )\n",
      "\n",
      "    def load_state_dict(self, state_dict):\n",
      "        self.epoch_ctl.load_state_dict(state_dict[\"epoch\"])\n",
      "        self.step_ctl.load_state_dict(state_dict[\"step\"])\n",
      "        self.time_ctl.load_state_dict(state_dict[\"time\"])\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class Scheduler(ABC):\n",
      "    init_value: float\n",
      "    total_iters: int\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.total_iters <= 0:\n",
      "            raise ValueError(\"total_iters should be a positive number.\")\n",
      "\n",
      "    def get(self, step: int) -> float:\n",
      "        \"\"\"Get the scheduled value at the current `step`.\"\"\"\n",
      "        if step < 0 or step > self.total_iters:\n",
      "            raise ValueError(\n",
      "                f\"Scheduler step should be in the interval [0, {self.total_iters}]. Input {step}.\"\n",
      "            )\n",
      "        return self._get(step)\n",
      "\n",
      "    def _get(self, step):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def final_value(self):\n",
      "        return self.get(step=self.total_iters)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ConstantScheduler(Scheduler):\n",
      "\n",
      "    def _get(self, *args, **kwargs) -> float:\n",
      "        return self.init_value\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class LinearScheduler(Scheduler):\n",
      "    end_value: float\n",
      "\n",
      "    def _get(self, step: int) -> float:\n",
      "        return (\n",
      "            self.end_value - self.init_value\n",
      "        ) / self.total_iters * step + self.init_value\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ExponentialScheduler(Scheduler):\n",
      "    decay: float\n",
      "\n",
      "    def _get(self, step: int) -> float:\n",
      "        return self.init_value * self.decay**step\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class CosineDecayScheduler(Scheduler):\n",
      "    end_value: float\n",
      "\n",
      "    def __post_init__(self):\n",
      "        super().__post_init__()\n",
      "        if self.end_value >= self.init_value:\n",
      "            raise ValueError(\"end_value should be smaller than init_value!\")\n",
      "\n",
      "    def _get(self, step: int) -> float:\n",
      "        delta = self.init_value - self.end_value\n",
      "        return (\n",
      "            delta * 0.5 * (1 + math.cos(math.pi / self.total_iters * step))\n",
      "            + self.end_value\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ChainedScheduler:\n",
      "    schedulers: List[Scheduler]\n",
      "\n",
      "    @property\n",
      "    def total_iters(self):\n",
      "        return sum(x.total_iters for x in self.schedulers)\n",
      "\n",
      "    @property\n",
      "    def init_value(self):\n",
      "        return self.schedulers[0].init_value\n",
      "\n",
      "    @property\n",
      "    def final_value(self):\n",
      "        return self.schedulers[-1].final_value\n",
      "\n",
      "    def __post_init__(self):\n",
      "        for i in range(len(self.schedulers) - 1):\n",
      "            # Float point err 1e-8.\n",
      "            if (\n",
      "                abs(self.schedulers[i + 1].get(0) - self.schedulers[i].final_value)\n",
      "                > 1e-8\n",
      "            ):\n",
      "                raise ValueError(\n",
      "                    f\"Values should be consecutive between \"\n",
      "                    f\"the {i}-th ({type(self.schedulers[i])}) and \"\n",
      "                    f\"the {i+1}-th {type(self.schedulers[i+1])} schedulers! \"\n",
      "                    f\"End value is {self.schedulers[i].final_value} and the \"\n",
      "                    f\"next init value is {self.schedulers[i + 1].get(0)}.\"\n",
      "                )\n",
      "\n",
      "    def get(self, step: int) -> float:\n",
      "        for s in self.schedulers:\n",
      "            if step > s.total_iters:\n",
      "                step -= s.total_iters\n",
      "            else:\n",
      "                return s.get(step)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/prologue.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import argparse\n",
      "import datetime\n",
      "import getpass\n",
      "import json\n",
      "import os\n",
      "import sys\n",
      "\n",
      "from omegaconf import OmegaConf\n",
      "\n",
      "PROLOGUE_FLAG_NAME = \"--config\"\n",
      "PROLOGUE_FLAG_VAR_NAME = \"config\"\n",
      "PROLOGUE_EXTERNAL_CONFIG_NAME = \"external_configs\"\n",
      "\n",
      "\n",
      "def global_init():\n",
      "    parser = argparse.ArgumentParser(add_help=False)\n",
      "    parser.add_argument(PROLOGUE_FLAG_NAME)\n",
      "    args = vars(parser.parse_known_args()[0])\n",
      "    if args[PROLOGUE_FLAG_VAR_NAME] is None:\n",
      "        return\n",
      "    prologue_path = args[PROLOGUE_FLAG_VAR_NAME]\n",
      "\n",
      "    config = OmegaConf.load(prologue_path)\n",
      "    external_configs = config.get(PROLOGUE_EXTERNAL_CONFIG_NAME)\n",
      "\n",
      "    if external_configs is None:\n",
      "        return\n",
      "\n",
      "    # add externel envs.\n",
      "    if external_configs.get(\"envs\"):\n",
      "        for key, value in external_configs.envs.items():\n",
      "            if key not in os.environ:\n",
      "                os.environ[key] = value\n",
      "\n",
      "    # resolve config path for cluster spec.\n",
      "    cluster_spec_path = os.environ.get(\"CLUSTER_SPEC_PATH\", \"\")\n",
      "    if cluster_spec_path == \"\":\n",
      "        if external_configs.get(\"cluster_config\"):\n",
      "            fileroot = external_configs.cluster_config.get(\"fileroot\")\n",
      "            if fileroot is not None and fileroot != \"\":\n",
      "                experiment_name = get_experiment_name(config.get(\"experiment_name\"))\n",
      "                trial_name = get_trial_name(config.get(\"trial_name\"))\n",
      "                config_dir = f\"{fileroot}/configs/{getpass.getuser()}/{experiment_name}/{trial_name}\"\n",
      "                os.makedirs(config_dir, exist_ok=True)\n",
      "                cluster_spec_path = f\"{config_dir}/cluster_config.json\"\n",
      "                cluster_spec = OmegaConf.to_container(external_configs.cluster_config)\n",
      "                if \"cluster_type\" not in cluster_spec:\n",
      "                    cluster_spec[\"cluster_type\"] = config.mode\n",
      "                if \"cluster_name\" not in cluster_spec:\n",
      "                    cluster_spec[\"cluster_name\"] = f\"{config.mode}_cluster\"\n",
      "                with open(cluster_spec_path, \"w\") as f:\n",
      "                    json.dump(cluster_spec, f)\n",
      "                os.environ[\"CLUSTER_SPEC_PATH\"] = cluster_spec_path\n",
      "\n",
      "\n",
      "def get_experiment_name(default_name: str = \"\"):\n",
      "    if any(\"experiment_name=\" in x for x in sys.argv):\n",
      "        experiment_name = next(x for x in sys.argv if \"experiment_name=\" in x).split(\n",
      "            \"=\"\n",
      "        )[1]\n",
      "    else:\n",
      "        experiment_name = default_name\n",
      "        if experiment_name == \"\":\n",
      "            experiment_name = f\"quickstart-experiment\"\n",
      "\n",
      "    if \"_\" in experiment_name:\n",
      "        raise RuntimeError(\"experiment_name should not contain `_`.\")\n",
      "    return experiment_name\n",
      "\n",
      "\n",
      "def get_trial_name(default_name: str = \"\"):\n",
      "    if any(\"trial_name=\" in x for x in sys.argv):\n",
      "        trial_name = next(x for x in sys.argv if \"trial_name=\" in x).split(\"=\")[1]\n",
      "    else:\n",
      "        trial_name = default_name\n",
      "        if trial_name == \"\":\n",
      "            trial_name = f\"run{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
      "\n",
      "    if \"_\" in trial_name:\n",
      "        raise RuntimeError(\"trial_name should not contain `_`.\")\n",
      "    return trial_name\n",
      "\n",
      "\n",
      "global_init()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "from .prologue import *  # isort: skip\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/names.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# This file standardizes the name-resolve names used by different components of the system.\n",
      "import getpass\n",
      "\n",
      "USER_NAMESPACE = getpass.getuser()\n",
      "\n",
      "\n",
      "def registry_root(user):\n",
      "    return f\"trial_registry/{user}\"\n",
      "\n",
      "\n",
      "def trial_registry(experiment_name, trial_name):\n",
      "    return f\"trial_registry/{USER_NAMESPACE}/{experiment_name}/{trial_name}\"\n",
      "\n",
      "\n",
      "def trial_root(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}\"\n",
      "\n",
      "\n",
      "def worker_status(experiment_name, trial_name, worker_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/status/{worker_name}\"\n",
      "\n",
      "\n",
      "def worker_root(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/worker/\"\n",
      "\n",
      "\n",
      "def worker(experiment_name, trial_name, worker_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/worker/{worker_name}\"\n",
      "\n",
      "\n",
      "def worker_key(experiment_name, trial_name, key):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/worker_key/{key}\"\n",
      "\n",
      "\n",
      "def request_reply_stream(experiment_name, trial_name, stream_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/request_reply_stream/{stream_name}\"\n",
      "\n",
      "\n",
      "def request_reply_stream_root(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/request_reply_stream/\"\n",
      "\n",
      "\n",
      "def distributed_root(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/distributed/\"\n",
      "\n",
      "\n",
      "def distributed_peer(experiment_name, trial_name, model_name):\n",
      "    return (\n",
      "        f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/distributed/peer/{model_name}\"\n",
      "    )\n",
      "\n",
      "\n",
      "def distributed_local_peer(experiment_name, trial_name, host_name, model_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/distributed/local_peer/{host_name}/{model_name}\"\n",
      "\n",
      "\n",
      "def distributed_master(experiment_name, trial_name, model_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/distributed/master/{model_name}\"\n",
      "\n",
      "\n",
      "def model_version(experiment_name, trial_name, model_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/model_version/{model_name}\"\n",
      "\n",
      "\n",
      "def metric_server_root(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/metrics\"\n",
      "\n",
      "\n",
      "def metric_server(experiment_name, trial_name, group, name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/metrics/{group}/{name}\"\n",
      "\n",
      "\n",
      "def push_pull_stream(experiment_name, trial_name, stream_name):\n",
      "    # Used to write addresses\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/push_pull_stream/{stream_name}\"\n",
      "\n",
      "\n",
      "def push_pull_stream_root(experiment_name, trial_name):\n",
      "    # Used to collect addresses\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/push_pull_stream/\"\n",
      "\n",
      "\n",
      "def stream_pullers(experiment_name, trial_name):\n",
      "    # Used to claim identities so that pushers know the number of pullers\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/push_pull_stream_peers/\"\n",
      "\n",
      "\n",
      "def gen_servers(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/gen_servers\"\n",
      "\n",
      "\n",
      "def used_ports(experiment_name, trial_name, host_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/{host_name}/\"\n",
      "\n",
      "\n",
      "def gen_server_manager(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/gen_server_manager\"\n",
      "\n",
      "\n",
      "def training_samples(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/training_samples\"\n",
      "\n",
      "\n",
      "def experiment_status(experiment_name, trial_name):\n",
      "    return f\"{USER_NAMESPACE}/{experiment_name}/{trial_name}/experiment_status\"\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/base/network.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import fcntl\n",
      "import os\n",
      "import socket\n",
      "import time\n",
      "from contextlib import closing\n",
      "from functools import wraps\n",
      "\n",
      "from realhf.base import constants, logging, name_resolve, names\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def gethostname():\n",
      "    return socket.gethostname()\n",
      "\n",
      "\n",
      "def gethostip():\n",
      "    return socket.gethostbyname(socket.gethostname())\n",
      "\n",
      "\n",
      "def find_free_port(\n",
      "    low=1, high=65536, exclude_ports=None, experiment_name=\"port\", trial_name=\"port\"\n",
      "):\n",
      "    \"\"\"Find a free port within the specified range, excluding certain ports.\"\"\"\n",
      "\n",
      "    ports_name = names.used_ports(experiment_name, trial_name, gethostip())\n",
      "\n",
      "    free_port = None\n",
      "    lockfile = os.path.join(constants.PORT_LOCK_FILE_ROOT, gethostip())\n",
      "    while True:\n",
      "        with open(lockfile, \"w\") as fd:\n",
      "            # This will block until lock is acquired\n",
      "            fcntl.flock(fd, fcntl.LOCK_EX)\n",
      "            used_ports = list(map(int, name_resolve.get_subtree(ports_name)))\n",
      "            assert len(used_ports) == len(set(used_ports))\n",
      "            if exclude_ports is None:\n",
      "                exclude_ports = set(used_ports)\n",
      "            else:\n",
      "                exclude_ports = exclude_ports.union(set(used_ports))\n",
      "            try:\n",
      "                with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
      "                    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
      "                    s.bind((\"\", 0))\n",
      "                    port = s.getsockname()[1]\n",
      "                    if low <= port <= high and port not in exclude_ports:\n",
      "                        name_resolve.add_subentry(ports_name, str(port))\n",
      "                        logger.info(f\"Found free port {port}\")\n",
      "                        free_port = port\n",
      "                        break\n",
      "            finally:\n",
      "                fcntl.flock(fd, fcntl.LOCK_UN)\n",
      "        time.sleep(0.05)\n",
      "    return free_port\n",
      "\n",
      "\n",
      "def find_multiple_free_ports(\n",
      "    count, low=1, high=65536, experiment_name=\"port\", trial_name=\"port\"\n",
      "):\n",
      "    \"\"\"Find multiple mutually exclusive free ports.\"\"\"\n",
      "    free_ports = set()\n",
      "    for _ in range(count):\n",
      "        port = find_free_port(\n",
      "            low=low,\n",
      "            high=high,\n",
      "            exclude_ports=free_ports,\n",
      "            experiment_name=experiment_name,\n",
      "            trial_name=trial_name,\n",
      "        )\n",
      "        free_ports.add(port)\n",
      "    return list(free_ports)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/scheduler/evaluator.py ====\n",
      "\n",
      "import dataclasses\n",
      "import enum\n",
      "import json\n",
      "import os\n",
      "import pathlib\n",
      "import re\n",
      "import subprocess\n",
      "import time\n",
      "from typing import Dict, Optional\n",
      "\n",
      "import wandb\n",
      "\n",
      "import realhf.api.core.system_api as config_pkg\n",
      "from realhf.base import cluster, constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"AutomaticEvaluator\", \"colored\")\n",
      "\n",
      "\n",
      "class EvaluationStepStatus(enum.Enum):\n",
      "    PENDING = 0\n",
      "    RUNNING = 1\n",
      "    FAILED = 2\n",
      "    DONE = 3\n",
      "    LOGGED = 4\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class EvaluationStep:\n",
      "    global_step: int\n",
      "    status: EvaluationStepStatus\n",
      "    start_time: Optional[float] = None\n",
      "    ckpt_dir: Optional[str] = None\n",
      "    process: Optional[subprocess.Popen] = None\n",
      "\n",
      "    @staticmethod\n",
      "    def from_ckpt_dir(ckpt_dir):\n",
      "        # NOTE: ckpt_dir should be absolute path\n",
      "        if pathlib.Path(ckpt_dir).is_symlink():\n",
      "            return None\n",
      "        _dir = os.path.basename(ckpt_dir)\n",
      "        match = re.match(r\"epoch(\\d+)epochstep(\\d+)globalstep(\\d+)\", _dir)\n",
      "        if not match:\n",
      "            return None\n",
      "        _, _, global_step = map(int, match.groups())\n",
      "        return EvaluationStep(\n",
      "            global_step=global_step,\n",
      "            status=EvaluationStepStatus.PENDING,\n",
      "            ckpt_dir=ckpt_dir,\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def from_output_dir(output_dir):\n",
      "        # NOTE: output_dir should be absolute path\n",
      "        # Should only be called in recover.\n",
      "        _dir = os.path.basename(output_dir)\n",
      "        match = re.match(r\"globalstep(\\d+)\", _dir)\n",
      "        if not match:\n",
      "            return None\n",
      "        global_step = int(match.groups()[0])\n",
      "        return EvaluationStep(\n",
      "            global_step=global_step, status=EvaluationStepStatus.LOGGED\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def output_dir(self):\n",
      "        return os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"eval_output\",\n",
      "            f\"globalstep{self.global_step}\",\n",
      "        )\n",
      "\n",
      "    def slurm_eval_cmd(self, config: config_pkg.AutomaticEvaluator):\n",
      "        slurm_job_name = f\"{constants.experiment_name()}_{constants.trial_name()}:eval_globalstep{self.global_step}\"\n",
      "        cmd = (\n",
      "            f\"srun --mpi=pmi2 -J {slurm_job_name} --ntasks=1 --cpus-per-task=128 --gres=gpu:8 --mem-per-cpu=12G \"\n",
      "            f\"singularity exec --no-home --nv --pid --writable-tmpfs --bind /storage:/storage \"\n",
      "            f\"{config.eval_job_image or cluster.spec.gpu_image} \"\n",
      "            f\"bash ./evaluation/sh/install_deps_and_eval.sh {self.ckpt_dir} {self.output_dir} \"\n",
      "            f\"{config.data_names} {config.max_gen_tokens} {config.prompt_type}\"\n",
      "        )\n",
      "        return cmd\n",
      "\n",
      "    def submit(self, config: config_pkg.AutomaticEvaluator):\n",
      "        os.makedirs(self.output_dir, exist_ok=True)\n",
      "        log_file = open(os.path.join(self.output_dir, \"output.log\"), \"w\")\n",
      "        if cluster.spec.cluster_type == \"slurm\":\n",
      "            cmd = self.slurm_eval_cmd(config)\n",
      "        else:\n",
      "            raise NotImplementedError(\n",
      "                \"AutomaticEvaluator does only support slurm cluster.\"\n",
      "            )\n",
      "\n",
      "        logger.info(\n",
      "            f\"Submitting evaluation job of checkpoint at {self.ckpt_dir} (globalstep{self.global_step}), \"\n",
      "            f\"command: {cmd}\"\n",
      "        )\n",
      "        self.process = subprocess.Popen(\n",
      "            cmd,\n",
      "            stdout=log_file,\n",
      "            stderr=log_file,\n",
      "            shell=True,\n",
      "        )\n",
      "        self.start_time = time.perf_counter()\n",
      "        self.status = EvaluationStepStatus.RUNNING\n",
      "\n",
      "    def log(self, config: config_pkg.AutomaticEvaluator) -> bool:\n",
      "        result_path = os.path.join(\n",
      "            self.output_dir,\n",
      "            f\"math_eval_{config.max_gen_tokens}\",\n",
      "            f\"aggregate_parallel_{config.prompt_type}.json\",\n",
      "        )\n",
      "        # NOTE: If decoding json failed or not found,\n",
      "        # evaluation step will be marked as failed.\n",
      "        try:\n",
      "            with open(result_path, \"r\") as fp:\n",
      "                data = json.load(fp)\n",
      "        except json.JSONDecodeError:\n",
      "            logger.warning(f\"JSON file {result_path} decoding failed.\")\n",
      "            self.status = EvaluationStepStatus.FAILED\n",
      "            return False\n",
      "        except FileNotFoundError:\n",
      "            logger.warning(f\"JSON file {result_path} does not exist.\")\n",
      "            self.status = EvaluationStepStatus.FAILED\n",
      "            return False\n",
      "\n",
      "        wandb_data = {}\n",
      "        for data_name, d in data.items():\n",
      "            for k, v in d.items():\n",
      "                wandb_data[f\"{data_name}_{k}\"] = v\n",
      "        wandb.log(wandb_data, step=self.global_step)\n",
      "        self.status = EvaluationStepStatus.LOGGED\n",
      "        logger.info(f\"Logging eval result {wandb_data} to step {self.global_step}\")\n",
      "        return True\n",
      "\n",
      "    def check(self):\n",
      "        assert self.process is not None\n",
      "        result = self.process.poll()\n",
      "        if not result is None:\n",
      "            logger.info(\n",
      "                f\"Evaluation of checkpoint (globalstep{self.global_step}) is done, returncode={self.process.returncode}, \"\n",
      "                f\"time passed {time.perf_counter() - self.start_time:.3f} s.\"\n",
      "            )\n",
      "            if self.process.returncode == 0:\n",
      "                self.status = EvaluationStepStatus.DONE\n",
      "            else:\n",
      "                self.status = EvaluationStepStatus.FAILED\n",
      "\n",
      "\n",
      "class AutomaticEvaluator:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: config_pkg.AutomaticEvaluator,\n",
      "        wandb_config: config_pkg.WandBConfig,\n",
      "    ):\n",
      "        self.__eval_steps: Dict[int, EvaluationStep] = {}\n",
      "        self.__max_concurrent_jobs = config.max_concurrent_jobs\n",
      "        self.__wandb_config = wandb_config\n",
      "        self.__config = config\n",
      "        self.__wandb_initialized = False\n",
      "\n",
      "        # Check evaluated checkpoints by logs in recover\n",
      "        # NOTE: All previous evaluation steps with output will be marked\n",
      "        # as logged, even if it is not really logged in wandb.\n",
      "        # This is because we do not know the status of evaluation jobs\n",
      "        # submitted before recover.\n",
      "        # Resubmiting or waiting for these jobs will probably result in\n",
      "        # unexpected behaviors.\n",
      "        output_parent = os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"eval_output\",\n",
      "        )\n",
      "        if os.path.exists(output_parent):\n",
      "            for output_dir in os.listdir(output_parent):\n",
      "                output_dir = os.path.join(output_parent, output_dir)\n",
      "                eval_step = EvaluationStep.from_output_dir(output_dir)\n",
      "                if eval_step:\n",
      "                    self.__eval_steps[eval_step.global_step] = eval_step\n",
      "\n",
      "        logger.info(\n",
      "            f\"Initializing AutomaticEvaluator: \\n\"\n",
      "            f\"eval_job_image: {config.eval_job_image}\\n\"\n",
      "            f\"data_names: {config.data_names}\\n\"\n",
      "            f\"max_gen_tokens: {config.max_gen_tokens}\\n\"\n",
      "            f\"max_concurrent_jobs: {config.max_concurrent_jobs}\\n\"\n",
      "            f\"Existing eval outputs for global steps: \"\n",
      "            f\"{list(self.__eval_steps.keys())}\"\n",
      "        )\n",
      "        if self.__config.initial_checkpoint_path and 0 not in self.__eval_steps:\n",
      "            self.__eval_steps[0] = EvaluationStep(\n",
      "                global_step=0,\n",
      "                status=EvaluationStepStatus.PENDING,\n",
      "                ckpt_dir=self.__config.initial_checkpoint_path,\n",
      "            )\n",
      "\n",
      "        if not cluster.spec.cluster_type == \"slurm\":\n",
      "            raise NotImplementedError(\n",
      "                \"Currently only support automatic evaluation for slurm\"\n",
      "            )\n",
      "\n",
      "    def __lazy_wandb_init(self):\n",
      "        # Initializing wandb for evaluator.\n",
      "        # Here we use lazy init because if this wandb instance is launched\n",
      "        # with wandb instance on master worker without a time interval,\n",
      "        # one of them will fail.\n",
      "        wandb.login()\n",
      "        wandb.init(\n",
      "            mode=self.__wandb_config.mode,\n",
      "            entity=self.__wandb_config.entity,\n",
      "            project=self.__wandb_config.project or constants.experiment_name(),\n",
      "            name=self.__wandb_config.name or f\"{constants.trial_name()}_eval\",\n",
      "            job_type=self.__wandb_config.job_type,\n",
      "            group=self.__wandb_config.group\n",
      "            or f\"{constants.experiment_name()}_{constants.trial_name()}\",\n",
      "            notes=self.__wandb_config.notes,\n",
      "            tags=self.__wandb_config.tags,\n",
      "            config=self.__wandb_config.config,\n",
      "            dir=os.path.join(\n",
      "                constants.LOG_ROOT, constants.experiment_name(), constants.trial_name()\n",
      "            ),\n",
      "            force=True,\n",
      "            id=f\"{constants.experiment_name()}_{constants.trial_name()}_eval\",\n",
      "            resume=\"allow\",\n",
      "            settings=wandb.Settings(start_method=\"fork\"),\n",
      "        )\n",
      "\n",
      "    def step(self):\n",
      "        # Check whether a new evaluation step should be created\n",
      "        ckpt_parent = os.path.join(\n",
      "            constants.MODEL_SAVE_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"actor\",\n",
      "        )\n",
      "        if os.path.exists(ckpt_parent):\n",
      "            for ckpt_dir in os.listdir(ckpt_parent):\n",
      "                ckpt_dir = os.path.join(ckpt_parent, ckpt_dir)\n",
      "                eval_step = EvaluationStep.from_ckpt_dir(ckpt_dir)\n",
      "                if eval_step is None:\n",
      "                    continue\n",
      "                if eval_step.global_step in self.__eval_steps:\n",
      "                    continue\n",
      "                self.__eval_steps[eval_step.global_step] = eval_step\n",
      "                logger.info(\n",
      "                    f\"Found new checkpoint (globalstep{eval_step.global_step}) \"\n",
      "                    f\"at {ckpt_dir}\"\n",
      "                )\n",
      "\n",
      "        # Submit pending evaluation step\n",
      "        if self.__running_jobs < self.__max_concurrent_jobs:\n",
      "            # Submit in global_step order\n",
      "            pending_steps = list(\n",
      "                filter(\n",
      "                    lambda x: self.__eval_steps[x].status\n",
      "                    == EvaluationStepStatus.PENDING,\n",
      "                    self.__eval_steps.keys(),\n",
      "                )\n",
      "            )\n",
      "            if pending_steps:\n",
      "                min_pending = min(pending_steps)\n",
      "                self.__eval_steps[min_pending].submit(self.__config)\n",
      "\n",
      "        # Check if any eval job is done or failed\n",
      "        running_steps = filter(\n",
      "            lambda x: self.__eval_steps[x].status == EvaluationStepStatus.RUNNING,\n",
      "            self.__eval_steps.keys(),\n",
      "        )\n",
      "        for global_step in running_steps:\n",
      "            self.__eval_steps[global_step].check()\n",
      "\n",
      "        # Check whether the **minimal global step**, not logged or failed, is done,\n",
      "        # and log this step to wandb if done.\n",
      "        # NOTE: LOGGED and FAILED steps have identical behaviors now.\n",
      "        # But in future versions that supports multi-node eval they could be different.\n",
      "        log_steps = list(\n",
      "            filter(\n",
      "                lambda x: self.__eval_steps[x].status\n",
      "                not in [\n",
      "                    EvaluationStepStatus.LOGGED,\n",
      "                    EvaluationStepStatus.FAILED,\n",
      "                ],\n",
      "                self.__eval_steps.keys(),\n",
      "            )\n",
      "        )\n",
      "        if log_steps:\n",
      "            log_step = min(log_steps)\n",
      "            if self.__eval_steps[log_step].status == EvaluationStepStatus.DONE:\n",
      "                if not self.__wandb_initialized:\n",
      "                    self.__lazy_wandb_init()\n",
      "                    self.__wandb_initialized = True\n",
      "                self.__eval_steps[log_step].log(self.__config)\n",
      "\n",
      "    @property\n",
      "    def __running_jobs(self):\n",
      "        return len(\n",
      "            list(\n",
      "                filter(\n",
      "                    lambda x: self.__eval_steps[x].status\n",
      "                    == EvaluationStepStatus.RUNNING,\n",
      "                    self.__eval_steps.keys(),\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/scheduler/client.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import enum\n",
      "import subprocess\n",
      "from typing import List, Optional\n",
      "\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "\n",
      "\n",
      "class JobState(enum.Enum):\n",
      "    NOT_FOUND = 0\n",
      "    PENDING = 1\n",
      "    RUNNING = 2\n",
      "    COMPLETED = 3\n",
      "    FAILED = 4\n",
      "    CANCELLED = 5\n",
      "\n",
      "    def active(self):\n",
      "        return self == self.PENDING or self == self.RUNNING\n",
      "\n",
      "\n",
      "class SchedulerError(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class JobException(Exception):\n",
      "\n",
      "    def __init__(self, run_name, worker_type, host, reason: JobState):\n",
      "        super().__init__(f\"Job {run_name}:{worker_type} {reason} at node {host}\")\n",
      "        self.run_name = run_name\n",
      "        self.worker_type = worker_type\n",
      "        self.host = host\n",
      "        self.reason = reason\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class JobInfo:\n",
      "    name: str\n",
      "    state: JobState\n",
      "    host: str = (\n",
      "        None  # The host on which the job is/was running. None if the job had not run.\n",
      "    )\n",
      "    submit_time: str = None\n",
      "    start_time: str = None\n",
      "    slurm_id: str = None  # Slurm only. The Slurm id of the job.\n",
      "\n",
      "\n",
      "class SchedulerClient:\n",
      "\n",
      "    def __init__(self, expr_name, trial_name):\n",
      "        self.expr_name = expr_name\n",
      "        self.trial_name = trial_name\n",
      "        self.run_name = f\"{expr_name}_{trial_name}\"\n",
      "\n",
      "    def submit(self, worker_type, cmd, **kwargs):\n",
      "        \"\"\"Submits a job to the scheduler. Raises exception if the job is\n",
      "        already running.\n",
      "\n",
      "        Args:\n",
      "            worker_type: The worker type to be submitted. The job name is specified when initializing the client.\n",
      "            cmd (str or List[str]): The command of this job. If this is str, the command is parsed by\n",
      "                shell; otherwise it is executed directly.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def submit_array(self, worker_type, cmd, count, **kwargs):\n",
      "        \"\"\"Submits an array of jobs to the scheduler.\n",
      "\n",
      "        Args:\n",
      "            worker_type: The worker type to be submitted, shared by all jobs.\n",
      "            cmd: Command template of the jobs that may contain an \"{index}\" format placeholder.\n",
      "            count: Number of jobs. The indices of the jobs shall be 0..count-1.\n",
      "        \"\"\"\n",
      "        for index in range(count):\n",
      "            self.submit(\n",
      "                worker_type + \"_\" + str(index),\n",
      "                cmd.format(index=index, count=count),\n",
      "                **kwargs,\n",
      "            )\n",
      "\n",
      "    def stop(self, job_name):\n",
      "        \"\"\"Stops a running job.\n",
      "\n",
      "        Raises exception if there is no such job, but passes if the job\n",
      "        has stopped either successfully or not.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def stop_all(self, signal=None):\n",
      "        \"\"\"Stops the whole job.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def find(self, job_name) -> Optional[JobInfo]:\n",
      "        \"\"\"Gets the status of a job of this job.\n",
      "\n",
      "        Args:\n",
      "            job_name: Name of the job.\n",
      "\n",
      "        Returns:\n",
      "            A JobInfo if the job is found, or None otherwise.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def find_all(self, job_name_regex=\".*\") -> List[JobInfo]:\n",
      "        \"\"\"Finds jobs.\n",
      "\n",
      "        Args:\n",
      "            job_name_regex: job name regex.\n",
      "\n",
      "        Returns:\n",
      "            A list of found JobInfo.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def wait(self, timeout=None, **kwargs):\n",
      "        \"\"\"Waits until all jobs submitted via this client instance finish.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "def get_python3_path():\n",
      "    if cluster_spec.cluster_type == \"ray\":\n",
      "        return subprocess.check_output([\"which\", \"python3\"]).decode(\"utf-8\").strip()\n",
      "    return \"python3\"\n",
      "\n",
      "\n",
      "def remote_worker_cmd(expr_name, trial_name, debug, worker_type):\n",
      "    # requires information in scheduler package\n",
      "    return (\n",
      "        f\"{get_python3_path()} {'' if debug else '-O'} -m realhf.apps.remote worker -w {worker_type} \"\n",
      "        f\"-e {expr_name} -f {trial_name} -i {{jobstep_id}} -g {{n_jobsteps}} -r {{worker_submission_index}} \"\n",
      "        f\"-p {{wprocs_per_jobstep}} -j {{wprocs_in_job}} -o {{wproc_offset}}\"\n",
      "    )\n",
      "\n",
      "\n",
      "def setup_cmd(expr_name, trial_name, debug):\n",
      "    bash_cmd = (  # f\"pip3 install -e $REAL_PACKAGE_PATH --no-build-isolation && \"\n",
      "        f\"{get_python3_path()} {'' if debug else '-O'} -m realhf.apps.remote \"\n",
      "        f\"reset_name_resolve -e {expr_name} -f {trial_name}\"\n",
      "    )\n",
      "    # return f\"bash -c \\\"{bash_cmd}\\\"\"\n",
      "    return bash_cmd\n",
      "\n",
      "\n",
      "def control_cmd(expr_name, trial_name, debug, ignore_worker_error, controller_type):\n",
      "    bash_cmd = (  # f\"pip3 install -e $REAL_PACKAGE_PATH --no-build-isolation && \"\n",
      "        f\"{get_python3_path()} {'' if debug else '-O'} -m realhf.apps.remote controller \"\n",
      "        f\"-e {expr_name} -f {trial_name} \"\n",
      "        f\"--{'ignore_worker_error' if ignore_worker_error else 'raise_worker_error'} \"\n",
      "        f\"--type {controller_type}\"\n",
      "    )\n",
      "    # return f\"bash -c \\\"{bash_cmd}\\\"\"\n",
      "    return bash_cmd\n",
      "\n",
      "\n",
      "def make(mode, expr_name, trial_name, **kwargs) -> SchedulerClient:\n",
      "    if mode == \"slurm\":\n",
      "        from realhf.scheduler.slurm.client import SlurmSchedulerClient\n",
      "\n",
      "        schedule_strategy = kwargs.get(\"schedule_strategy\", \"empty_first\")\n",
      "        evaluator = kwargs.get(\"evaluator\", None)\n",
      "        job_group_id = kwargs.get(\"job_group_id\", None)\n",
      "        job_group_index = kwargs.get(\"job_group_index\", None)\n",
      "        return SlurmSchedulerClient(\n",
      "            expr_name,\n",
      "            trial_name,\n",
      "            schedule_strategy,\n",
      "            evaluator,\n",
      "            job_group_id,\n",
      "            job_group_index,\n",
      "        )\n",
      "    elif mode == \"local\":\n",
      "        from realhf.scheduler.local.client import LocalSchedulerClient\n",
      "\n",
      "        return LocalSchedulerClient(expr_name, trial_name)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Scheduler {mode} not found\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/scheduler/slurm/client.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import fcntl\n",
      "import os\n",
      "import re\n",
      "import select\n",
      "import subprocess\n",
      "import threading\n",
      "import time\n",
      "from collections import defaultdict\n",
      "from typing import Dict, List, Literal, Optional, Tuple\n",
      "\n",
      "import colorama\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.base.constants import LOG_ROOT\n",
      "from realhf.base.constants import SLURM_LOCK_FILE_NAME as LOCK_FILE_NAME\n",
      "from realhf.scheduler.client import JobException, JobInfo, JobState, SchedulerClient\n",
      "from realhf.scheduler.evaluator import AutomaticEvaluator\n",
      "from realhf.scheduler.slurm.utils import (\n",
      "    SlurmLaunchInfo,\n",
      "    SlurmResource,\n",
      "    SlurmResourceNotEnoughException,\n",
      "    allocate_resources,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"Slurm-scheduler\")\n",
      "\n",
      "SCHEDULING_RETRY_INTERVAL_SECONDS = 30\n",
      "SCHEDULING_TIMEOUT_MAX_SECONDS = 3600 * 24\n",
      "SCHEDULER_WAIT_CHECK_TIME_INTERVAL = 5\n",
      "\n",
      "\n",
      "def monitor_log(\n",
      "    job_name: str, log_path: str, output_file: str, stop_event: threading.Event\n",
      "):\n",
      "    \"\"\"Monitor a log file and write its contents to the output file with job name prefix.\"\"\"\n",
      "    # Wait for log file to be created\n",
      "    while not os.path.exists(log_path) and not stop_event.is_set():\n",
      "        time.sleep(0.1)\n",
      "\n",
      "    if stop_event.is_set():\n",
      "        return\n",
      "\n",
      "    # Open the log file and follow it\n",
      "    with open(log_path, \"r\") as log_file, open(output_file, \"a\") as out_file:\n",
      "        # Store last position\n",
      "        position = 0\n",
      "        line_pos = 0\n",
      "\n",
      "        while not stop_event.is_set():\n",
      "            log_file.seek(position)\n",
      "            try:\n",
      "                new_lines = log_file.readlines()\n",
      "            except UnicodeDecodeError:\n",
      "                time.sleep(0.5)\n",
      "                continue\n",
      "\n",
      "            if new_lines:\n",
      "                # Update position\n",
      "                position = log_file.tell()\n",
      "\n",
      "                worker_type = job_name.split(\":\")[1]\n",
      "                # Write new lines to output file with job name prefix\n",
      "                for line in new_lines:\n",
      "                    if line.strip():  # Skip empty lines\n",
      "                        out_file.write(\n",
      "                            f\"{colorama.Fore.YELLOW + colorama.Style.DIM}({worker_type} Line {line_pos}){colorama.Style.RESET_ALL} {line}\"\n",
      "                        )\n",
      "                    line_pos += 1\n",
      "                out_file.flush()\n",
      "\n",
      "            # Sleep briefly to avoid CPU spinning\n",
      "            time.sleep(0.1)\n",
      "\n",
      "\n",
      "class SlurmSchedulerClient(SchedulerClient):\n",
      "    \"\"\"Uses Slurm (https://slurm.schedmd.com/overview.html).\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        expr_name: str,\n",
      "        trial_name: str,\n",
      "        schedule_strategy: str,\n",
      "        evaluator: Optional[AutomaticEvaluator],\n",
      "        job_group_id: str,\n",
      "        job_group_index: int,\n",
      "    ):\n",
      "        super().__init__(expr_name, trial_name)\n",
      "\n",
      "        self.__schedule_strategy = schedule_strategy\n",
      "\n",
      "        self.__pending_jobs: Dict[str, SlurmLaunchInfo] = dict()\n",
      "        self.__committed_jobs: Dict[str, SlurmLaunchInfo] = dict()\n",
      "\n",
      "        self.__submission_counter = defaultdict(int)\n",
      "        self.__wprocs_counter = defaultdict(int)\n",
      "        self.__evaluator = evaluator\n",
      "        self.__job_group_id = job_group_id\n",
      "        self.__job_group_index = job_group_index\n",
      "\n",
      "    def submit(self, worker_type, cmd, **kwargs):\n",
      "        self.submit_array(worker_type, cmd, count=1, **kwargs)\n",
      "\n",
      "    def submit_array(\n",
      "        self,\n",
      "        worker_type: str,\n",
      "        cmd: str,  # XXX: should be None for workers\n",
      "        count: int,\n",
      "        cpu: int = 1,\n",
      "        gpu: int = 0,\n",
      "        mem: int = 1024,  # MB\n",
      "        env_vars: Optional[Dict] = None,\n",
      "        container_image: Optional[str] = None,\n",
      "        container_mounts: Optional[str] = None,\n",
      "        nodelist: Optional[str] = None,\n",
      "        exclude: Optional[str] = None,\n",
      "        hostfile: bool = True,\n",
      "        multiprog: bool = True,\n",
      "        begin: str = None,\n",
      "        deadline: str = None,\n",
      "        time_limit: str = None,\n",
      "    ):\n",
      "        container_image = container_image or cluster_spec.cpu_image\n",
      "        container_mounts = container_mounts or cluster_spec.mount\n",
      "        # record launch information, do not submit to slurm until `wait()` is called\n",
      "        # NOTE: fractional GPU requirement will be resolved automatically in `__post_init__` of SlurnLaunchInfo\n",
      "        launch_info = SlurmLaunchInfo(\n",
      "            worker_type=worker_type,\n",
      "            wprocs_in_job=count,\n",
      "            resource_requirement=SlurmResource(mem=mem, cpu=cpu, gpu=gpu),\n",
      "            cmd=cmd,\n",
      "            run_name=self.run_name,\n",
      "            exper_name=self.expr_name,\n",
      "            trial_name=self.trial_name,\n",
      "            container_image=container_image,\n",
      "            container_mounts=container_mounts,\n",
      "            env_vars=env_vars,\n",
      "            nodelist=nodelist,\n",
      "            exclude=exclude,\n",
      "            hostfile=hostfile,\n",
      "            multiprog=multiprog,\n",
      "            worker_submission_idx=self.__submission_counter[worker_type],\n",
      "            begin=begin,\n",
      "            deadline=deadline,\n",
      "            time_limit=time_limit,\n",
      "            job_group_id=self.__job_group_id,\n",
      "            job_group_index=self.__job_group_index,\n",
      "        )\n",
      "\n",
      "        if (\n",
      "            launch_info.slurm_name in self.__pending_jobs\n",
      "            or launch_info.slurm_name in self.__committed_jobs\n",
      "        ):\n",
      "            raise ValueError(f\"job name {launch_info.slurm_name} already existed.\")\n",
      "\n",
      "        if launch_info.multiprog:\n",
      "            launch_info = self.__resolve_multiprog_file(launch_info)\n",
      "\n",
      "        self.__submission_counter[worker_type] += 1\n",
      "        self.__wprocs_counter[worker_type] += count\n",
      "\n",
      "        self.__pending_jobs[launch_info.slurm_name] = launch_info\n",
      "        logger.info(f\"Registered Slurm job {launch_info.slurm_name} to scheduler.\")\n",
      "\n",
      "    def __resolve_multiprog_file(self, launch_info: SlurmLaunchInfo):\n",
      "        worker_type = launch_info.worker_type\n",
      "        cmd = launch_info.cmd.format(\n",
      "            jobstep_id=\"$SLURM_PROCID\",\n",
      "            n_jobsteps=launch_info.n_jobsteps,\n",
      "            worker_submission_index=self.__submission_counter[worker_type],\n",
      "            wprocs_per_jobstep=launch_info.wprocs_per_jobstep,\n",
      "            wprocs_in_job=launch_info.wprocs_in_job,\n",
      "            wproc_offset=self.__wprocs_counter[worker_type],\n",
      "        )\n",
      "        wrap_cmd = \"singularity exec \"\n",
      "        if cluster_spec.name == \"na132\":\n",
      "            wrap_cmd += \"--pid \"\n",
      "        if cluster_spec.gpu_type == \"tesla\":\n",
      "            wrap_cmd += \"--nv \"\n",
      "        wrap_cmd += \"--no-home --writable-tmpfs \"\n",
      "        if len(launch_info.env_vars) > 0:\n",
      "            wrap_cmd += f\"{' '.join([f'--env {k}={v}' for k, v in launch_info.env_vars.items()])} \"\n",
      "        if len(launch_info.container_mounts) > 0:\n",
      "            wrap_cmd += f\"--bind {launch_info.container_mounts} \"\n",
      "        wrap_cmd += f\"{launch_info.container_image} \"\n",
      "        wrap_cmd += \"bash -c '{}'\".format(cmd)\n",
      "        launch_info.multiprog_content = f\"0-{launch_info.n_jobsteps - 1} {wrap_cmd}\\n\"\n",
      "        return launch_info\n",
      "\n",
      "    def __allocate_and_commit_pending_jobs(self):\n",
      "        \"\"\"Allocate resources to all pending job specs.\n",
      "\n",
      "        Generate hostfiles for each job info\n",
      "        \"\"\"\n",
      "        start_time = time.monotonic()\n",
      "        while True:\n",
      "            try:\n",
      "                fp = open(LOCK_FILE_NAME, \"w\")\n",
      "                fcntl.flock(fp, fcntl.LOCK_EX)\n",
      "                infos = list(self.__pending_jobs.values())\n",
      "                infos = allocate_resources(infos, strategy=self.__schedule_strategy)\n",
      "                self.__pending_jobs = {info.slurm_name: info for info in infos}\n",
      "                # logger.info(\"Allocated jobs: \")\n",
      "                # for info in infos:\n",
      "                #     logger.info(info)\n",
      "                break\n",
      "            except SlurmResourceNotEnoughException:\n",
      "                logger.critical(\n",
      "                    \"Not enough resources to allocate all pending jobs. Retrying ...\"\n",
      "                )\n",
      "                logger.warning(\n",
      "                    \"Time since start: %d seconds\",\n",
      "                    time.monotonic() - start_time,\n",
      "                )\n",
      "                fcntl.flock(fp, fcntl.LOCK_UN)\n",
      "                time.sleep(SCHEDULING_RETRY_INTERVAL_SECONDS)\n",
      "                if time.monotonic() - start_time > SCHEDULING_TIMEOUT_MAX_SECONDS:\n",
      "                    raise TimeoutError(\n",
      "                        f\"Timeout waiting for {self.run_name} to schedule.\"\n",
      "                    )\n",
      "            except Exception as e:\n",
      "                fcntl.flock(fp, fcntl.LOCK_UN)\n",
      "                raise e\n",
      "        try:\n",
      "            for slurm_name, launch_info in self.__pending_jobs.items():\n",
      "                launch_info.commit()\n",
      "                self.__committed_jobs[slurm_name] = launch_info\n",
      "            self.__pending_jobs = dict()\n",
      "            states = [None for _ in self.__committed_jobs]\n",
      "            while JobState.PENDING in states or None in states:\n",
      "                time.sleep(0.1)\n",
      "                states = self.__update_all()\n",
      "            # time.sleep(2)\n",
      "            fcntl.flock(fp, fcntl.LOCK_UN)\n",
      "        except Exception as e:\n",
      "            for launch_info in self.__committed_jobs.values():\n",
      "                launch_info.cancel()\n",
      "            fcntl.flock(fp, fcntl.LOCK_UN)\n",
      "            raise e\n",
      "\n",
      "    def stop(self, slurm_name: str):\n",
      "        launch_info = self.__committed_jobs.get(slurm_name, None)\n",
      "        if launch_info:\n",
      "            launch_info.cancel()\n",
      "\n",
      "    def stop_all(self, signal: Literal[\"SIGINT\", \"SIGKILL\"] = \"SIGKILL\"):\n",
      "        for launch_info in self.__committed_jobs.values():\n",
      "            logger.info(f\"Canceling job {launch_info.slurm_name}\")\n",
      "            launch_info.cancel(signal)\n",
      "        time.sleep(0.2)\n",
      "        self.wait(\n",
      "            check_status=(),\n",
      "            remove_status=(\n",
      "                JobState.CANCELLED,\n",
      "                JobState.NOT_FOUND,\n",
      "                JobState.FAILED,\n",
      "                JobState.COMPLETED,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    def find(self, slurm_name: str) -> JobInfo:\n",
      "        launch_info = self.__committed_jobs.get(slurm_name, None)\n",
      "        if launch_info is None or launch_info.job_info is None:\n",
      "            return JobInfo(name=slurm_name, state=JobState.NOT_FOUND)\n",
      "        else:\n",
      "            return launch_info.job_info\n",
      "\n",
      "    def find_all(self, job_name_regex: str = \".*\") -> List[JobInfo]:\n",
      "        self.__update_all()\n",
      "        infos = []\n",
      "        for r in self.__committed_jobs.values():\n",
      "            if r.job_info is None:\n",
      "                continue\n",
      "            if re.fullmatch(job_name_regex, r.slurm_name):\n",
      "                infos.append(r.job_info)\n",
      "        return infos\n",
      "\n",
      "    def wait(\n",
      "        self,\n",
      "        timeout=None,\n",
      "        check_status: Tuple[JobState, ...] = (\n",
      "            JobState.CANCELLED,\n",
      "            JobState.FAILED,\n",
      "            JobState.NOT_FOUND,\n",
      "        ),\n",
      "        remove_status: Tuple[JobState, ...] = (JobState.COMPLETED,),\n",
      "        update=False,\n",
      "    ):\n",
      "        # before wait, commit all remaining pending jobs\n",
      "        # TODO: grab global file lock to avoid multi-experiment deadlocks\n",
      "        self.__allocate_and_commit_pending_jobs()\n",
      "        # Start monitoring threads\n",
      "        threads = []\n",
      "        stop_events = []\n",
      "\n",
      "        merged_log_path = os.path.join(\n",
      "            LOG_ROOT, self.expr_name, self.trial_name, \"main.log\"\n",
      "        )\n",
      "\n",
      "        for job_name, launch_info in self.__committed_jobs.items():\n",
      "            stop_event = threading.Event()\n",
      "            stop_events.append(stop_event)\n",
      "\n",
      "            # Thread for monitoring the log file\n",
      "            log_thread = threading.Thread(\n",
      "                target=monitor_log,\n",
      "                args=(job_name, launch_info.log_path, merged_log_path, stop_event),\n",
      "            )\n",
      "            threads.append(log_thread)\n",
      "            log_thread.start()\n",
      "\n",
      "        # begin wait\n",
      "        deadline = None if timeout is None else time.time() + timeout\n",
      "        left = set(self.__committed_jobs.keys())\n",
      "        num_jobs_left = len(left)\n",
      "        logger.info(\n",
      "            f\"Waiting for {num_jobs_left} jobs. Jobs IDs: \"\n",
      "            f\"{','.join(sorted([x.job_info.slurm_id for x in self.__committed_jobs.values()]))}.\"\n",
      "        )\n",
      "        logger.info(\n",
      "            f\"All slurm logs will be merged. To check the real-time output, \"\n",
      "            f\"run\\n\\t`tail -f {merged_log_path}`.\"\n",
      "        )\n",
      "        try:\n",
      "            while len(left) > 0:\n",
      "                if len(left) < num_jobs_left:\n",
      "                    num_jobs_left = len(left)\n",
      "                    logger.info(f\"Waiting for {num_jobs_left} jobs.\")\n",
      "                if self.__evaluator is not None:\n",
      "                    self.__evaluator.step()\n",
      "                if deadline is not None and time.time() > deadline:\n",
      "                    raise TimeoutError(\n",
      "                        f\"Timeout waiting for {self.run_name}: {', '.join(sorted(left))}\"\n",
      "                    )\n",
      "                try:\n",
      "                    self.__update_all()\n",
      "                except subprocess.CalledProcessError:\n",
      "                    logger.warning(\n",
      "                        \"Calling squeue failed. Check slurm manually if you continue to see this warning.\"\n",
      "                    )\n",
      "                    time.sleep(30)\n",
      "                    continue\n",
      "                for job_slurm_name in list(left):\n",
      "                    launch_info = self.__committed_jobs[job_slurm_name]\n",
      "                    if launch_info.slurm_id is None:\n",
      "                        continue\n",
      "                    if launch_info.job_info.state in check_status:\n",
      "                        launch_info.show_log()\n",
      "                        raise JobException(\n",
      "                            run_name=self.run_name,\n",
      "                            worker_type=launch_info.worker_type,\n",
      "                            host=launch_info.job_info.host,\n",
      "                            reason=launch_info.job_info.state,\n",
      "                        )\n",
      "                    if launch_info.job_info.state in remove_status:\n",
      "                        logger.info(\n",
      "                            f\"Job {launch_info.slurm_name} is {launch_info.job_info.state}.(Removed)\"\n",
      "                        )\n",
      "                        left.remove(job_slurm_name)\n",
      "                        if update:\n",
      "                            self.__committed_jobs.pop(job_slurm_name)\n",
      "                time.sleep(SCHEDULER_WAIT_CHECK_TIME_INTERVAL)\n",
      "        finally:\n",
      "            [s.set() for s in stop_events]\n",
      "            [t.join() for t in threads]\n",
      "\n",
      "    def __update_all(self):\n",
      "        states = []\n",
      "        for launch_info in self.__committed_jobs.values():\n",
      "            state = launch_info.update()\n",
      "            states.append(state)\n",
      "        return states\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/scheduler/slurm/utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from __future__ import (\n",
      "    annotations,  # python3.7+ feature to allow self-referencing type hints\n",
      ")\n",
      "\n",
      "import collections\n",
      "import dataclasses\n",
      "import datetime\n",
      "import getpass\n",
      "import json\n",
      "import math\n",
      "import os\n",
      "import shutil\n",
      "import socket\n",
      "import subprocess\n",
      "from typing import Callable, Dict, List, Literal, Optional, Union\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import realhf.base.cluster as cluster\n",
      "import realhf.base.logging as logging\n",
      "import realhf.version as version\n",
      "from realhf.base.constants import LOG_ROOT\n",
      "from realhf.scheduler.client import JobException, JobInfo, JobState\n",
      "\n",
      "logger = logging.getLogger(\"scheduler.slurm.utils\")\n",
      "\n",
      "SQUEUE_FIELDS = [\n",
      "    \"JobID\",\n",
      "    \"State\",\n",
      "    \"SubmitTime\",\n",
      "    \"StartTime\",\n",
      "    \"Name\",\n",
      "    \"NodeList\",\n",
      "    \"UserName\",\n",
      "    \"MaxCPUs\",\n",
      "    \"cpus-per-task\",\n",
      "    \"NumTasks\",\n",
      "    \"tres-alloc\",\n",
      "]\n",
      "STATUS_MAPPING = {\n",
      "    \"RUNNING\": JobState.RUNNING,\n",
      "    \"COMPLETING\": JobState.RUNNING,\n",
      "    \"PENDING\": JobState.PENDING,\n",
      "    \"CANCELLED\": JobState.CANCELLED,\n",
      "    \"FAILED\": JobState.FAILED,\n",
      "    \"COMPLETED\": JobState.COMPLETED,\n",
      "    \"OUT_OF_MEMORY\": JobState.FAILED,\n",
      "    \"DEADLINE\": JobState.COMPLETED,\n",
      "    \"TIMEOUT\": JobState.COMPLETED,\n",
      "}\n",
      "\n",
      "\n",
      "class SlurmResourceNotEnoughException(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class InvalidGPUTypeException(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class SlurmResource:\n",
      "    # a data class that represents a slurm resource quota\n",
      "    mem: int = 0\n",
      "    cpu: int = 0\n",
      "    gpu: Union[float, int] = 0\n",
      "\n",
      "    def __str__(self):\n",
      "        return (\n",
      "            \"SlurmResource: \\n\"\n",
      "            + \"mem: \"\n",
      "            + str(self.mem)\n",
      "            + \" MB \\n\"\n",
      "            + \"cpu: \"\n",
      "            + str(self.cpu)\n",
      "            + \" \\n\"\n",
      "            + \"gpu: \"\n",
      "            + str(self.gpu)\n",
      "        )\n",
      "\n",
      "    def __mul__(self, other: int) -> SlurmResource:\n",
      "        assert isinstance(\n",
      "            other, int\n",
      "        ), \"ResourceRequirement can only be multiplied by int.\"\n",
      "        return SlurmResource(\n",
      "            mem=self.mem * other,\n",
      "            cpu=self.cpu * other,\n",
      "            gpu=self.gpu * other,\n",
      "        )\n",
      "\n",
      "    def __rmul__(self, other: int) -> SlurmResource:\n",
      "        return self.__mul__(other)\n",
      "\n",
      "    def __add__(self, other: SlurmResource) -> SlurmResource:\n",
      "        assert isinstance(\n",
      "            other, SlurmResource\n",
      "        ), \"SlurmResource can only add another SlurmResource instance.\"\n",
      "        return SlurmResource(\n",
      "            mem=self.mem + other.mem,\n",
      "            cpu=self.cpu + other.cpu,\n",
      "            gpu=self.gpu + other.gpu,\n",
      "        )\n",
      "\n",
      "    def __sub__(self, other: SlurmResource) -> SlurmResource:\n",
      "        assert isinstance(\n",
      "            other, SlurmResource\n",
      "        ), \"SlurmResource can only subtract another SlurmResource instance.\"\n",
      "        return SlurmResource(\n",
      "            mem=self.mem - other.mem,\n",
      "            cpu=self.cpu - other.cpu,\n",
      "            gpu=self.gpu - other.gpu,\n",
      "        )\n",
      "\n",
      "    def __neg__(self) -> SlurmResource:\n",
      "        return SlurmResource(\n",
      "            mem=-self.mem,\n",
      "            cpu=-self.cpu,\n",
      "            gpu=-self.gpu,\n",
      "        )\n",
      "\n",
      "    def __eq__(self, other: SlurmResource) -> bool:\n",
      "        return self.mem == other.mem and self.cpu == other.cpu and self.gpu == other.gpu\n",
      "\n",
      "    def __lt__(self, other: SlurmResource) -> bool:\n",
      "        if self.gpu != other.gpu:\n",
      "            return self.gpu < other.gpu\n",
      "        if self.cpu != other.cpu:\n",
      "            return self.cpu < other.cpu\n",
      "        if self.mem != other.mem:\n",
      "            return self.mem < other.mem\n",
      "\n",
      "    def valid(self) -> bool:\n",
      "        # check if it is a valid resource requirement\n",
      "        if self.mem < 0 or self.cpu < 0 or self.gpu < 0:\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class SlurmLaunchInfo:\n",
      "    \"\"\"A SlurmLaunchInfo contains all informantion required to **launch** a\n",
      "    slurm job.\n",
      "\n",
      "    Matching one `TasksGroup` in `SchedulingConfig` and one slurm job.\n",
      "\n",
      "    The naming conventions:\n",
      "        - `job`: Literally a slurm job with a (maybe non-unique) job name and an unique job ID,\n",
      "            which may contain multiple job steps and processes. It corresponds an `sbatch` or `srun` call.\n",
      "            Job names are guaranteed to be unique using the scheduler within this repo.\n",
      "        - `jobstep`: Literally a slurm job step with a unique job step ID, i.e., ${jobID}.${stepID},\n",
      "            which corresponds to a running instance `apps.remote` script, but may still contain multiple processes.\n",
      "            A job step occupies at most one GPU. Processes in the same job step must share the same GPU.\n",
      "        - `wproc`: A single worker process launched by `apps.remote` script, which may occupy less than 1 GPU.\n",
      "            A worker just corresponds to a process.\n",
      "        - `task`: The alias of `jobstep`. It is easier to under stand this concept in the context of `srun` command.\n",
      "            `--ntasks' is just the number of jobsteps. We use the alternative term `jobstep` to avoid confusion.\n",
      "\n",
      "    Attributes:\n",
      "        run_name (str): Identifier of this run, typically ${exp_name}_${trial_name}.\n",
      "        worker_type (str): Type of workers to be launched, e.g. model_worker, data_worker, etc.\n",
      "        worker_submission_idx (int): For heterogeneous scheduling, we submit jobs of the same worker_type to slurm\n",
      "            for multiple times. `worker_submission_idx` is used to distinguish them, so the (global) slurm job name will\n",
      "            be ${run_name}:${worker_type}:${worker_submission_idx}.\n",
      "        wprocs_in_job: The number of worker processes in this slurm job (of all job steps).\n",
      "        n_jobsteps (int): The number of job steps of this slurm job. This is also the group size of the multiprog file.\n",
      "            Will be resolved automatically according to GPU requirement.\n",
      "        wprocs_per_jobstep: The number of worker processes in each job step, as well as the number of sub-processes\n",
      "            spawned by `apps.remote`. Will be resolved automatically according to GPU requirement.\n",
      "\n",
      "        resource_requirement (SlurmResource): The resource requirement of this job, including all job steps.\n",
      "        cmd (str): The command to be executed.\n",
      "        container_image (str): In current PPU setup, container_image should match the format provided by singularity.\n",
      "            If the image is a file, this string should be the path. If the image is a remote docker image,\n",
      "            this string should be of format 'docker://<image>'.\n",
      "        container_mounts (str): .\n",
      "        env_vars (dict): .\n",
      "        nodelist (str): .\n",
      "        exclude (str): .\n",
      "        partition (str, optional): default to \"all\".\n",
      "        time_limit (str, optional): Slurm job time limit.\n",
      "        begin (str, optional): Scheduled worker start time.\n",
      "        deadline (str, optional): Scheduled worker end time.\n",
      "        hostfile (bool): Whether to use hostfile for `--distribution=arbitrary` scheduling.\n",
      "        hostfile_content (str, optional): The content of the hostfile.\n",
      "        multiprog (bool): Whether to use multiprog file for `--multi-prog` job submission.\n",
      "        multiprog_content (str, optional): The content of the multiprog file.\n",
      "    \"\"\"\n",
      "\n",
      "    run_name: str\n",
      "    exper_name: str\n",
      "    trial_name: str\n",
      "    worker_type: str\n",
      "    worker_submission_idx: int\n",
      "    wprocs_in_job: int\n",
      "    job_group_id: str\n",
      "    job_group_index: str\n",
      "\n",
      "    resource_requirement: SlurmResource\n",
      "    cmd: str\n",
      "    container_image: str\n",
      "    container_mounts: str\n",
      "    env_vars: dict\n",
      "    nodelist: str\n",
      "    exclude: str\n",
      "    partition: Optional[str] = \"all\"\n",
      "    time_limit: Optional[str] = None\n",
      "    begin: Optional[str] = None\n",
      "    deadline: Optional[str] = None\n",
      "    # hostfile\n",
      "    hostfile: bool = True\n",
      "    hostfile_content: Optional[str] = None\n",
      "    # multiprog options, override cmd\n",
      "    multiprog: bool = True\n",
      "    multiprog_content: Optional[str] = None\n",
      "\n",
      "    n_jobsteps: int = None\n",
      "    wprocs_per_jobstep: int = None\n",
      "\n",
      "    job_info: Optional[JobInfo] = None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        \"\"\"Resolve fractional GPU resource requirement.\"\"\"\n",
      "        gpu_per_worker = self.resource_requirement.gpu\n",
      "        # assert gpu_per_worker <= 1 and gpu_per_worker >= 0\n",
      "        if gpu_per_worker < 1 and gpu_per_worker > 0:\n",
      "            self.resource_requirement.gpu = 1\n",
      "            self.wprocs_per_jobstep = math.floor(1 / gpu_per_worker)\n",
      "            self.resource_requirement.cpu *= self.wprocs_per_jobstep\n",
      "            self.resource_requirement.mem *= self.wprocs_per_jobstep\n",
      "            self.n_jobsteps = math.ceil(self.wprocs_in_job / self.wprocs_per_jobstep)\n",
      "            logger.info(f\"Resolved fractional GPU requirement for {self.slurm_name}\")\n",
      "            logger.info(\n",
      "                f\"GPU per worker {gpu_per_worker}, workers per jobstep (process size in `apps.remote`) {self.wprocs_per_jobstep}, \"\n",
      "                f\"number of jobsteps (instance of running `apps.remote`) {self.n_jobsteps}\"\n",
      "            )\n",
      "        else:\n",
      "            self.n_jobsteps = self.wprocs_in_job\n",
      "            self.wprocs_per_jobstep = 1\n",
      "\n",
      "    @property\n",
      "    def slurm_name(self) -> str:\n",
      "        return f\"{self.run_name}:{self.worker_type}:{self.worker_submission_idx}\"\n",
      "\n",
      "    @property\n",
      "    def slurm_id(self) -> Optional[str]:\n",
      "        if self.job_info:\n",
      "            return self.job_info.slurm_id\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    @property\n",
      "    def log_path(self) -> str:\n",
      "        return os.path.join(\n",
      "            LOG_ROOT,\n",
      "            self.exper_name,\n",
      "            self.trial_name,\n",
      "            f\"{self.worker_type}-{self.worker_submission_idx}.log\",\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def multiprog_path(self) -> str:\n",
      "        path = os.path.join(\n",
      "            LOG_ROOT,\n",
      "            self.exper_name,\n",
      "            self.trial_name,\n",
      "            \"slurm\",\n",
      "            \"multiprog\",\n",
      "            f\"{self.worker_type}-{self.worker_submission_idx}.multiprog\",\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
      "        return path\n",
      "\n",
      "    @property\n",
      "    def hostfile_path(self) -> str:\n",
      "        path = os.path.join(\n",
      "            LOG_ROOT,\n",
      "            self.exper_name,\n",
      "            self.trial_name,\n",
      "            \"slurm\",\n",
      "            \"hostfile\",\n",
      "            f\"{self.worker_type}-{self.worker_submission_idx}.hostfile\",\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
      "        return path\n",
      "\n",
      "    def show_log(self):\n",
      "        try:\n",
      "            terminal_columns = os.get_terminal_size().columns\n",
      "        except OSError:\n",
      "            terminal_columns = shutil.get_terminal_size().columns\n",
      "        logger.info(\n",
      "            f\"Showing log of slurm job: {self.worker_type}-{self.worker_submission_idx}\\n\\n{'-'*terminal_columns}\"\n",
      "        )\n",
      "        subprocess.Popen([\"tail\", \"-n50\", self.log_path]).wait(timeout=3)\n",
      "        logger.info(\n",
      "            f\"End of log: {self.worker_type}-{self.worker_submission_idx}\\n\\n{'-'*terminal_columns}\"\n",
      "        )\n",
      "\n",
      "    def update(self):\n",
      "        job_infos = query_jobs(slurm_names=[self.slurm_name])\n",
      "        job_infos = sorted(\n",
      "            job_infos,\n",
      "            key=lambda x: parse_formatted_time(x.submit_time),\n",
      "            reverse=True,\n",
      "        )\n",
      "        self.job_info = job_infos[0] if len(job_infos) > 0 else None\n",
      "        if self.job_info:\n",
      "            return self.job_info.state\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def cancel(self, signal: Literal[\"SIGINT\", \"SIGKILL\"] = \"SIGKILL\"):\n",
      "        cancel_jobs(slurm_names=[self.slurm_name], signal=signal)\n",
      "        self.job_info = JobInfo(name=self.slurm_name, state=JobState.CANCELLED)\n",
      "\n",
      "    def __str__(self):\n",
      "        s = f\"SlurmLaunchInfo [{self.slurm_name}] \\n\"\n",
      "        s += f\"Resources: [\\n{self.resource_requirement}\\n]\\n\"\n",
      "        s += f\"Multiprog Filepath: [{self.multiprog_path}]\\n\"\n",
      "        s += f\"Multiprog Content: [\\n{self.multiprog_content}\\n]\\n\"\n",
      "        s += f\"Hostfile Filepath: [{self.hostfile_path}]\\n\"\n",
      "        s += f\"Hostfile Content: [\\n{self.hostfile_content}\\n]\\n\"\n",
      "        if self.job_info is None:\n",
      "            job_info_str = \"None\"\n",
      "        else:\n",
      "            job_info_str = \"\\n\".join(\n",
      "                [f\"{k}: {v}\" for k, v in self.job_info.__dict__.items()]\n",
      "            )\n",
      "        s += f\"Runtime JobInfo: [\\n{job_info_str}\\n]\\n\"\n",
      "        env_var_str = \"\\n\".join([f\"{k}: {v}\" for k, v in self.env_vars.items()])\n",
      "        s += f\"Env vars: [\\n{env_var_str}\\n]\\n\"\n",
      "        return s\n",
      "\n",
      "    def commit(self):\n",
      "        os.makedirs(os.path.dirname(self.log_path), exist_ok=True, mode=0o775)\n",
      "\n",
      "        ntasks = self.n_jobsteps\n",
      "        mem = self.resource_requirement.mem\n",
      "        cpu = self.resource_requirement.cpu\n",
      "        gpu = self.resource_requirement.gpu\n",
      "\n",
      "        cmd = self.cmd\n",
      "\n",
      "        # assert gpu == 1 or gpu == 0, \"Slurm job GPU requirement should be resolved to a integer.\"\n",
      "\n",
      "        if self.multiprog:\n",
      "            with open(self.multiprog_path, \"w\") as f:\n",
      "                f.write(self.multiprog_content)\n",
      "        if self.hostfile:\n",
      "            with open(self.hostfile_path, \"w\") as f:\n",
      "                f.write(self.hostfile_content)\n",
      "\n",
      "        logger.info(\n",
      "            f'Allocating {ntasks} jobstep(s) \"{self.worker_type}\" submssion index {self.worker_submission_idx}'\n",
      "            f\" with {cpu} cpu, {gpu} gpu and {mem} MB memory.\"\n",
      "        )\n",
      "        logger.info(f\"To check the output, run \\n\\t`tail -f {self.log_path}`.\")\n",
      "\n",
      "        # Setup sbatch\n",
      "        # head\n",
      "        gres_line = \"\"\n",
      "        if gpu >= 1:\n",
      "            assert (gpu * ntasks) % cluster.spec.n_gpus_per_node == 0\n",
      "            # In current slurm cluster setup, we can only use \"--gres\" to\n",
      "            # allocate PPUs per node. There are no options to allocate customized\n",
      "            # gres per tasks.\n",
      "            if cluster.spec.gpu_type == \"ppu\":\n",
      "                gres_line = f\"--gres=ppu:{cluster.spec.n_gpus_per_node}\"\n",
      "            else:\n",
      "                gres_line = f\"--gres=gpu:{cluster.spec.n_gpus_per_node}\"\n",
      "\n",
      "        srun_env = os.environ.copy()\n",
      "        job_metadata = {\n",
      "            \"user\": srun_env.get(\"EMAILPREFIX\", \"\"),\n",
      "            \"version\": version.__version__,\n",
      "            \"branch\": version.__branch__,\n",
      "            \"commit\": version.__commit__,\n",
      "            \"dirty\": version.__is_dirty__,\n",
      "            \"job_group_id\": self.job_group_id,\n",
      "            \"job_group_index\": self.job_group_index,\n",
      "        }\n",
      "        job_metadata_json = json.dumps(job_metadata)\n",
      "\n",
      "        lines = [\n",
      "            \"#!/bin/bash\",\n",
      "            f\"#SBATCH --job-name={self.slurm_name}\",\n",
      "            f\"#SBATCH --output={self.log_path}\",\n",
      "            \"#SBATCH --open-mode=append\",\n",
      "            f\"#SBATCH --ntasks={ntasks}\",\n",
      "            f\"#SBATCH {gres_line}\",\n",
      "            f\"#SBATCH --cpus-per-task={cpu}\",\n",
      "            f\"#SBATCH --mem-per-cpu={mem // max(1, cpu)}M\",\n",
      "            \"#SBATCH --distribution=arbitrary\" if self.hostfile else \"\",\n",
      "            # f'#SBATCH --nodelist={spec.nodelist}' if spec.nodelist is not None else \"\",\n",
      "            # f'#SBATCH --exclude={spec.exclude}' if spec.exclude is not None else \"\",\n",
      "            f\"#SBATCH --time={self.time_limit}\" if self.time_limit else \"\",\n",
      "            f\"#SBATCH --begin={self.begin}\" if self.begin else \"\",\n",
      "            f\"#SBATCH --deadline={self.deadline}\" if self.deadline else \"\",\n",
      "            f\"#SBATCH --comment='{job_metadata_json}'\",\n",
      "        ]\n",
      "\n",
      "        if self.hostfile:\n",
      "            srun_env[\"SLURM_HOSTFILE\"] = self.hostfile_path\n",
      "        # Setup step command.\n",
      "        # add current directory into container mounts to ensure editable mode for realhf package\n",
      "        srun_flags = [\n",
      "            f\"--ntasks={ntasks}\",\n",
      "            f\"--cpus-per-task={cpu}\",\n",
      "            gres_line,\n",
      "            f\"--mem-per-cpu={mem // max(1, cpu)}\",\n",
      "            f\"--multi-prog {self.multiprog_path}\" if self.multiprog else \"\",\n",
      "        ]\n",
      "\n",
      "        # The `-K` option ensures all job steps within the same job id would be killed if\n",
      "        # one of them exited with error. This is necessary for recovery.\n",
      "        if self.multiprog:\n",
      "            srun_cmd = (\n",
      "                f'srun --mpi=pmi2 -K -l {\" \".join(srun_flags)} {self.multiprog_path}'\n",
      "            )\n",
      "        else:\n",
      "            srun_cmd = f'srun --mpi=pmi2 -K -l {\" \".join(srun_flags)} {cmd}'\n",
      "\n",
      "        lines += [\n",
      "            'echo \"[Runner] StartTime: $(date -u)\"',\n",
      "            'echo \"[Runner] Host: $(hostname)\"',\n",
      "            \"echo '[Runner] Command: {}'\".format(srun_cmd),\n",
      "            \"echo '[Runner] Log: {}'\".format(self.log_path),\n",
      "            'echo \"[Runner] CudaVisible: $CUDA_VISIBLE_DEVICES\"',\n",
      "            'echo \"[Runner] CudaMpsPerc: $CUDA_MPS_ACTIVE_THREAD_PERCENTAGE\"',\n",
      "            srun_cmd,\n",
      "            \"RETCODE=$?\",\n",
      "            'echo \"[Runner] FinishTime: $(date -u)\"',\n",
      "            'echo \"[Runner] RetCode: $RETCODE\"',\n",
      "            'echo \"[Runner] ------------\"',\n",
      "            \"exit $RETCODE\",\n",
      "        ]\n",
      "\n",
      "        script_strs = \"\\n\".join(list(filter(lambda x: x, lines))) + \"\\n\"\n",
      "        script = script_strs.encode(\"ascii\")\n",
      "\n",
      "        def pad_output_str_to_length(s: str, pad_s: str, length: int):\n",
      "            assert len(pad_s) == 1\n",
      "            assert len(s) + 2 <= length\n",
      "            n_pads = (length - len(s) - 2) // 2\n",
      "            return pad_s * n_pads + \" \" + s + \" \" + pad_s * n_pads\n",
      "\n",
      "        with open(self.log_path, \"a\") as f:\n",
      "            f.write(pad_output_str_to_length(\"SBATCH SCRIPT BEGIN\", \"=\", 80) + \"\\n\")\n",
      "            f.write(script_strs)\n",
      "            f.write(pad_output_str_to_length(\"SBATCH SCRIPT END\", \"=\", 80) + \"\\n\")\n",
      "            f.write(pad_output_str_to_length(\"SBATCH JOB INFO BEGIN\", \"=\", 80) + \"\\n\")\n",
      "            f.write(str(self))\n",
      "            f.write(pad_output_str_to_length(\"SBATCH JOB INFO END\", \"=\", 80) + \"\\n\")\n",
      "            f.write(pad_output_str_to_length(\"JOB OUTPUT BEGIN\", \"=\", 80) + \"\\n\")\n",
      "        r = (\n",
      "            subprocess.check_output(\n",
      "                [\"sbatch\", \"--parsable\"], input=script, env=srun_env\n",
      "            )\n",
      "            .decode(\"ascii\")\n",
      "            .strip()\n",
      "        )\n",
      "        self.job_info = JobInfo(name=self.slurm_name, state=JobState.PENDING)\n",
      "\n",
      "\n",
      "def parse_formatted_time(time_string: str) -> int:\n",
      "    if time_string == \"N/A\":\n",
      "        return -1\n",
      "    d = datetime.datetime.strptime(time_string, \"%Y-%m-%dT%H:%M:%S\")\n",
      "    return int(datetime.datetime.timestamp(d))\n",
      "\n",
      "\n",
      "def unparse_formatted_time(timestamp: int) -> str:\n",
      "    if timestamp == -1:\n",
      "        return \"N/A\"\n",
      "    d = datetime.datetime.fromtimestamp(timestamp)\n",
      "    return d.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
      "\n",
      "\n",
      "# slurm command execute and output parsing\n",
      "def query_jobs(\n",
      "    slurm_names: Optional[List[str]] = None,\n",
      "    slurm_ids: Optional[List[str]] = None,\n",
      "    status: str = \"all\",\n",
      "    delimiter: str = \"__PSI__\",\n",
      ") -> List[JobInfo]:\n",
      "    squeue_format = f\":.{delimiter},\".join(SQUEUE_FIELDS)\n",
      "    cmd = [\"squeue\", \"-O\", squeue_format, f\"-t{status}\"]\n",
      "    if slurm_names is not None:\n",
      "        cmd += [\"-n\", \",\".join(slurm_names)]\n",
      "    if slurm_ids is not None:\n",
      "        cmd += [\"-j\", \",\".join([str(s) for s in slurm_ids])]\n",
      "    output = (\n",
      "        subprocess.check_output(cmd, stderr=subprocess.DEVNULL).decode(\"ascii\").strip()\n",
      "    )\n",
      "    rs = []\n",
      "    for line in output.split(\"\\n\")[1:]:\n",
      "        job_id, state, submit_time, start_time, slurm_name, nodelist, *_ = line.split(\n",
      "            delimiter\n",
      "        )\n",
      "        rs.append(\n",
      "            JobInfo(\n",
      "                name=slurm_name,\n",
      "                state=STATUS_MAPPING[state],\n",
      "                host=nodelist,\n",
      "                submit_time=submit_time,\n",
      "                start_time=start_time,\n",
      "                slurm_id=job_id.strip(),\n",
      "            )\n",
      "        )\n",
      "    return rs\n",
      "\n",
      "\n",
      "def cancel_jobs(\n",
      "    slurm_names: Optional[List[str]] = None,\n",
      "    slurm_ids: Optional[List[str]] = None,\n",
      "    signal: Literal[\"SIGINT\", \"SIGKILL\"] = \"SIGKILL\",\n",
      "):\n",
      "    assert (\n",
      "        slurm_names is not None or slurm_ids is not None\n",
      "    ), \"Must specify slurm_names or slurm_ids.\"\n",
      "    assert not (\n",
      "        slurm_names and slurm_ids\n",
      "    ), \"Cannot specify both slurm_names and slurm_ids.\"\n",
      "    cmd = [\"scancel\", \"-s\", signal]\n",
      "    if slurm_names is not None:\n",
      "        cmd += [\"-n\", \",\".join(slurm_names)]\n",
      "    elif slurm_ids is not None:\n",
      "        cmd += [\"-j\", \",\".join([str(s) for s in slurm_ids])]\n",
      "    subprocess.check_call(cmd)\n",
      "    logger.info(\n",
      "        f\"Cancelled Slurm job with signal {signal}: \"\n",
      "        f\"slurm identifiers {slurm_names if slurm_ids is None else slurm_ids}\"\n",
      "    )\n",
      "\n",
      "\n",
      "def _parse_output_status_line(status):\n",
      "    assert status.startswith(\"State=\")\n",
      "    status = status.split(\" \")[0]\n",
      "    status = status.split(\"=\")[1]\n",
      "    return status.split(\"+\")\n",
      "\n",
      "\n",
      "def _parse_output_tres_line(tres):\n",
      "    tres = tres.split(\"=\", maxsplit=1)[1]\n",
      "    tres = tres.split(\",\")\n",
      "    res = SlurmResource()\n",
      "    if len(tres) == 0 or (len(tres) == 1 and tres[0] == \"\"):\n",
      "        return SlurmResource()\n",
      "    for t in tres:\n",
      "        if t.startswith(\"mem\"):\n",
      "            if t.endswith(\"M\"):\n",
      "                res.mem = int(t.split(\"=\")[1].strip(\"M\"))\n",
      "            elif t.endswith(\"G\"):\n",
      "                res.mem = int(float(t.split(\"=\")[1].strip(\"G\")) * 1024)\n",
      "            elif t.endswith(\"T\"):\n",
      "                res.mem = int(float(t.split(\"=\")[1].strip(\"T\")) * 1024 * 1024)\n",
      "            else:\n",
      "                raise ValueError(\"Unknown memory unit.\")\n",
      "        elif t.startswith(\"cpu\"):\n",
      "            res.cpu = int(t.split(\"=\")[1])\n",
      "        elif t.startswith(\"gres/gpu\"):\n",
      "            prefix, sgpu = t.split(\"=\")\n",
      "            res.gpu = int(sgpu)\n",
      "        elif t.startswith(\"gres/ppu\"):\n",
      "            prefix, sgpu = t.split(\"=\")\n",
      "            res.gpu = int(sgpu)\n",
      "        elif t.startswith(\"billing\"):\n",
      "            # slurm default resource to limit number of\n",
      "            # tasks in one node\n",
      "            pass\n",
      "        else:\n",
      "            raise NotImplementedError(f\"Unknown resource type: {repr(t)}\")\n",
      "    return res\n",
      "\n",
      "\n",
      "def available_hostnames(\n",
      "    nodelist: Optional[str] = None,\n",
      "    exclude: Optional[str] = None,\n",
      "    partition: Optional[str] = None,\n",
      ") -> List[str]:\n",
      "    sinfo_cmd = 'sinfo -o \"%N\" --noheader'\n",
      "    if partition:\n",
      "        sinfo_cmd += f\" --partition={partition}\"\n",
      "    all_nodelist: str = (\n",
      "        subprocess.check_output(sinfo_cmd, shell=True).decode(\"utf-8\").strip()\n",
      "    )\n",
      "    all_hostnames: List[str] = (\n",
      "        subprocess.check_output(\n",
      "            [\n",
      "                \"scontrol\",\n",
      "                \"show\",\n",
      "                \"hostnames\",\n",
      "                all_nodelist,\n",
      "            ]\n",
      "        )\n",
      "        .decode(\"utf-8\")\n",
      "        .strip()\n",
      "        .split(\"\\n\")\n",
      "    )\n",
      "\n",
      "    if nodelist is not None:\n",
      "        valid_hostnames: List[str] = (\n",
      "            subprocess.check_output(\n",
      "                [\n",
      "                    \"scontrol\",\n",
      "                    \"show\",\n",
      "                    \"hostnames\",\n",
      "                    nodelist,\n",
      "                ]\n",
      "            )\n",
      "            .decode(\"utf-8\")\n",
      "            .strip()\n",
      "            .split(\"\\n\")\n",
      "        )\n",
      "    else:\n",
      "        valid_hostnames = all_hostnames\n",
      "\n",
      "    if exclude is not None:\n",
      "        excluded_hostnames: List[str] = (\n",
      "            subprocess.check_output(\n",
      "                [\n",
      "                    \"scontrol\",\n",
      "                    \"show\",\n",
      "                    \"hostnames\",\n",
      "                    exclude,\n",
      "                ]\n",
      "            )\n",
      "            .decode(\"utf-8\")\n",
      "            .strip()\n",
      "            .split(\"\\n\")\n",
      "        )\n",
      "        for hn in excluded_hostnames:\n",
      "            if hn in valid_hostnames:\n",
      "                valid_hostnames.remove(hn)\n",
      "\n",
      "    invalid_hostnames = []\n",
      "    for hn in valid_hostnames:\n",
      "        if hn not in all_hostnames:\n",
      "            logger.warning(\n",
      "                f\"Invalid host name: {hn}. Maybe it is not in this partition/cluster.\"\n",
      "            )\n",
      "            invalid_hostnames.append(hn)\n",
      "\n",
      "    for hn in invalid_hostnames:\n",
      "        valid_hostnames.remove(hn)\n",
      "\n",
      "    return valid_hostnames\n",
      "\n",
      "\n",
      "def get_all_node_resources() -> Dict[str, SlurmResource]:\n",
      "    \"\"\"Execute `scontrol show node` to get all node resources available in the\n",
      "    slurm cluster.\n",
      "\n",
      "    Return a list of SlurmResource\n",
      "    \"\"\"\n",
      "    o = subprocess.check_output([\"scontrol\", \"show\", \"node\"]).decode(\"utf-8\")\n",
      "    nodes = o.split(\"\\n\\n\")\n",
      "    all_rres = {}\n",
      "    for node in nodes:\n",
      "        if len(node) <= 1:\n",
      "            continue\n",
      "        ls = node.split(\"\\n\")\n",
      "        node_name = ls[0].split(\" \")[0].split(\"=\")[1]\n",
      "        ctres = SlurmResource()\n",
      "        atres = SlurmResource()\n",
      "        for l in ls:\n",
      "            l = l.strip(\"\\n\").strip()\n",
      "            if l.startswith(\"State\"):\n",
      "                status = _parse_output_status_line(l)\n",
      "                if any(\n",
      "                    x in status\n",
      "                    for x in [\"DOWN\", \"DRAIN\", \"NOT_RESPONDING\", \"COMPLETING\"]\n",
      "                ):\n",
      "                    break\n",
      "            if l.startswith(\"CfgTRES\"):\n",
      "                ctres = _parse_output_tres_line(l)\n",
      "            if l.startswith(\"AllocTRES\"):\n",
      "                atres = _parse_output_tres_line(l)\n",
      "        rres = ctres - atres\n",
      "        if rres.valid():\n",
      "            all_rres[node_name] = rres\n",
      "        else:\n",
      "            all_rres[node_name] = SlurmResource()\n",
      "\n",
      "    return all_rres\n",
      "\n",
      "\n",
      "def resource_to_string(resources: Dict[str, SlurmResource]) -> str:\n",
      "    resource_list = [\n",
      "        {\n",
      "            **{\"NodeName\": k},\n",
      "            **{\n",
      "                field.name: getattr(r, field.name)\n",
      "                for field in r.__dataclass_fields__.values()\n",
      "            },\n",
      "        }\n",
      "        for k, r in resources.items()\n",
      "    ]\n",
      "    return pd.DataFrame(resource_list).to_string(index=False)\n",
      "\n",
      "\n",
      "def allocate_resources(\n",
      "    infos: List[SlurmLaunchInfo],\n",
      "    strategy: Literal[\"empty_first\", \"allocated_first\"] = \"empty_first\",\n",
      ") -> List[SlurmLaunchInfo]:\n",
      "    \"\"\"Allocate all slurm task specs, fill in the hostfile field of the specs.\n",
      "\n",
      "    All slurm tasks are scheduled in pack. There are two choices of allocating\n",
      "    strategies. The first is `empty_first`, which means we first allocate\n",
      "    tasks to nodes with more free resources. The second is `allocated_first`,\n",
      "    which allocate tasks to nodes with less free resources without exceeding\n",
      "    resource capacity.\n",
      "    \"\"\"\n",
      "    assert strategy in [\"empty_first\", \"allocated_first\"]\n",
      "    all_resources = get_all_node_resources()\n",
      "    # sorted by requirements in descending order\n",
      "    infos = sorted(\n",
      "        infos, key=lambda x: x.n_jobsteps * x.resource_requirement, reverse=True\n",
      "    )\n",
      "    prioritized_hosts = set()\n",
      "    for info_idx, info in enumerate(infos):\n",
      "        valid_hostnames = available_hostnames(\n",
      "            nodelist=info.nodelist,\n",
      "            exclude=info.exclude,\n",
      "            partition=info.partition,\n",
      "        )\n",
      "        valid_hostnames = list(filter(lambda x: x in all_resources, valid_hostnames))\n",
      "        prioritized_resources = {\n",
      "            hn: all_resources[hn] for hn in valid_hostnames if hn in prioritized_hosts\n",
      "        }\n",
      "        other_resources = {\n",
      "            hn: all_resources[hn]\n",
      "            for hn in valid_hostnames\n",
      "            if hn not in prioritized_hosts\n",
      "        }\n",
      "        # sorted by available resources according to chosen strategy\n",
      "        prioritized_resources = sorted(\n",
      "            prioritized_resources.items(),\n",
      "            key=lambda x: x[1],\n",
      "            reverse=strategy != \"allocated_first\",\n",
      "        )\n",
      "        # if all of the allocated nodes cannot satisfy the requirement,\n",
      "        # find the new node according to chosen strategy\n",
      "        other_resources = sorted(\n",
      "            other_resources.items(),\n",
      "            key=lambda x: x[1],\n",
      "            reverse=strategy != \"allocated_first\",\n",
      "        )\n",
      "        valid_resources = prioritized_resources + other_resources\n",
      "        task_left = info.n_jobsteps\n",
      "        allocated = dict()\n",
      "        for hostname, resource in valid_resources:\n",
      "            tmp = task_left\n",
      "            while task_left > 0:\n",
      "                # In current slurm cluster GRES setting,\n",
      "                # we can only allocate tasks in the granularity of nodes\n",
      "                # (16 PPUs/8 GPUs by default)\n",
      "                batched_requirement = info.resource_requirement\n",
      "                batched_ntasks = 1\n",
      "                gpu_per_task = info.resource_requirement.gpu\n",
      "                if gpu_per_task > 0:\n",
      "                    assert (\n",
      "                        task_left * gpu_per_task % cluster.spec.n_gpus_per_node == 0\n",
      "                    ), (task_left, gpu_per_task)\n",
      "                    assert (\n",
      "                        cluster.spec.n_gpus_per_node % gpu_per_task == 0\n",
      "                    ), gpu_per_task\n",
      "                    batched_ntasks = int(cluster.spec.n_gpus_per_node // gpu_per_task)\n",
      "                    batched_requirement = batched_ntasks * info.resource_requirement\n",
      "                try:\n",
      "                    resource = resource - batched_requirement\n",
      "                except InvalidGPUTypeException:\n",
      "                    # InvalidGPUTypeException will be raised when\n",
      "                    # `resource` and `batched_requirement`\n",
      "                    # do not have the same GPU type.\n",
      "                    break\n",
      "                if not resource.valid():\n",
      "                    resource += batched_requirement\n",
      "                    break\n",
      "                task_left -= batched_ntasks\n",
      "                prioritized_hosts.add(hostname)\n",
      "            if tmp - task_left > 0:\n",
      "                allocated[hostname] = tmp - task_left\n",
      "            all_resources[hostname] = resource\n",
      "        if task_left > 0:\n",
      "            if cluster.spec.gpu_type == \"ppu\" and info.resource_requirement.gpu > 0:\n",
      "                logger.warning(\n",
      "                    \"For PPU resources, we can only allocate tasks in the \"\n",
      "                    f\"granularity of nodes ({cluster.spec.n_gpus_per_node} PPUs)\"\n",
      "                )\n",
      "            logger.warning(\n",
      "                f'Unable to allocate {info.n_jobsteps} Jobs with name \"{info.slurm_name}\". '\n",
      "                f\"Resource Requirement of this job is: {dataclasses.asdict(info.resource_requirement)}. \"\n",
      "                f\"Valid resources for this job is \"\n",
      "                f\"(according to NodeList={info.nodelist}, \"\n",
      "                f\"and Exclude={info.exclude}):\\n {resource_to_string({k: v for k, v in get_all_node_resources().items() if k in valid_hostnames})}\"\n",
      "            )\n",
      "            for pinfo in infos[:info_idx]:\n",
      "                if (\n",
      "                    len(\n",
      "                        set(pinfo.hostfile_content.split(\"\\n\")).intersection(\n",
      "                            set(valid_hostnames)\n",
      "                        )\n",
      "                    )\n",
      "                    == 0\n",
      "                ):\n",
      "                    continue\n",
      "                palloc = collections.defaultdict(lambda: 0)\n",
      "                for _n in pinfo.hostfile_content.split(\"\\n\"):\n",
      "                    palloc[_n] += 1\n",
      "                logger.warning(\n",
      "                    f'Found previous job \"{pinfo.slurm_name}\" (ntasks={pinfo.n_jobsteps}) '\n",
      "                    f\"has been allocated to the same set of nodes. \"\n",
      "                    f\"Resource requirement of this job is: {dataclasses.asdict(pinfo.resource_requirement)}, \"\n",
      "                    f\"allocation of this job is {dict(palloc)}.\"\n",
      "                )\n",
      "            raise SlurmResourceNotEnoughException()\n",
      "        hostlist = []\n",
      "        for hostname, task_num in allocated.items():\n",
      "            hostlist += [hostname] * task_num\n",
      "        info.hostfile_content = \"\\n\".join(hostlist)\n",
      "    return infos\n",
      "\n",
      "\n",
      "def show_tesla():\n",
      "    all_rres = get_all_node_resources()\n",
      "    hostname = socket.gethostname()\n",
      "    for k in available_hostnames():\n",
      "        print(k, all_rres[k])\n",
      "\n",
      "\n",
      "def show_all():\n",
      "    all_rres = get_all_node_resources()\n",
      "    for k, v in all_rres.items():\n",
      "        print(k, v)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    show_all()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/scheduler/local/client.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "import re\n",
      "import signal as signal_module\n",
      "import subprocess\n",
      "import time\n",
      "from collections import defaultdict\n",
      "from typing import Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import psutil\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.base import gpu_utils\n",
      "from realhf.base.constants import LOG_ROOT\n",
      "from realhf.scheduler.client import (\n",
      "    JobException,\n",
      "    JobInfo,\n",
      "    JobState,\n",
      "    SchedulerClient,\n",
      "    SchedulerError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"Local Scheduler\")\n",
      "\n",
      "JOB_STATE_TO_PROCESS_STATUS = {\n",
      "    JobState.NOT_FOUND: [],\n",
      "    JobState.PENDING: [psutil.STATUS_PARKED],\n",
      "    JobState.RUNNING: [\n",
      "        psutil.STATUS_RUNNING,\n",
      "        psutil.STATUS_SLEEPING,\n",
      "        psutil.STATUS_DISK_SLEEP,\n",
      "        psutil.STATUS_TRACING_STOP,\n",
      "        psutil.STATUS_WAKING,\n",
      "        psutil.STATUS_WAITING,\n",
      "        psutil.STATUS_LOCKED,\n",
      "        psutil.STATUS_IDLE,\n",
      "    ],\n",
      "    JobState.COMPLETED: [\n",
      "        psutil.STATUS_DEAD,\n",
      "        psutil.STATUS_STOPPED,\n",
      "        psutil.STATUS_ZOMBIE,\n",
      "    ],\n",
      "    JobState.FAILED: [],\n",
      "    JobState.CANCELLED: [],\n",
      "}\n",
      "\n",
      "PROCESS_STATUS_TO_JOB_STATE = {}\n",
      "for job_state, process_statuses in JOB_STATE_TO_PROCESS_STATUS.items():\n",
      "    for process_status in process_statuses:\n",
      "        PROCESS_STATUS_TO_JOB_STATE[process_status] = job_state\n",
      "\n",
      "\n",
      "def terminate_process_and_children(pid: int, signal: Optional[Union[str, int]] = None):\n",
      "    if signal is None:\n",
      "        signal = signal_module.SIGKILL\n",
      "    if isinstance(signal, str):\n",
      "        signal = getattr(signal_module, signal)\n",
      "    try:\n",
      "        parent = psutil.Process(pid)\n",
      "        children = parent.children(recursive=True)\n",
      "        for child in children:\n",
      "            terminate_process_and_children(child.pid)\n",
      "        parent.send_signal(signal)\n",
      "    except psutil.NoSuchProcess:\n",
      "        pass\n",
      "\n",
      "\n",
      "class LocalSchedulerClient(SchedulerClient):\n",
      "    \"\"\"Instead of talking to the scheduler server (the typical behaviour), this\n",
      "    client starts jobs directly on the local host and keeps a collection of job\n",
      "    processes.\"\"\"\n",
      "\n",
      "    def log_path_of(self, worker_type) -> str:\n",
      "        return os.path.join(\n",
      "            LOG_ROOT,\n",
      "            self.expr_name,\n",
      "            self.trial_name,\n",
      "            f\"{worker_type}-0\",\n",
      "        )\n",
      "\n",
      "    def __init__(self, expr_name, trial_name):\n",
      "        super().__init__(expr_name, trial_name)\n",
      "        self._jobs: Dict[str, subprocess.Popen] = {}\n",
      "        self._running_worker_types = []\n",
      "\n",
      "        self._gpu_counter = 0\n",
      "        self._cuda_devices: List[str] = os.environ.get(\n",
      "            \"CUDA_VISIBLE_DEVICES\", \",\".join(map(str, range(gpu_utils.gpu_count())))\n",
      "        ).split(\",\")\n",
      "\n",
      "        self._job_counter: Dict[str, int] = defaultdict(int)\n",
      "        self._job_with_gpu: Dict[str, bool] = defaultdict(int)\n",
      "        self._job_env_vars: Dict[str, Dict] = defaultdict(int)\n",
      "        self._job_cmd = {}\n",
      "        self._job_states = {}\n",
      "\n",
      "        if len(self._cuda_devices) < 1:\n",
      "            raise RuntimeError(\n",
      "                f\"Local mode can only run when there is at least one GPU. \"\n",
      "                f\"CUDA_VISIBLE_DEVICES is currently set to {os.environ['CUDA_VISIBLE_DEVICES']}.\"\n",
      "            )\n",
      "\n",
      "    def __del__(self):\n",
      "        self.wait(commit=False)\n",
      "\n",
      "    def submit_array(\n",
      "        self,\n",
      "        worker_type: str,\n",
      "        cmd: str,\n",
      "        count: int = 1,\n",
      "        gpu: int = 0,\n",
      "        env_vars: Optional[Dict] = None,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        if env_vars is None:\n",
      "            env_vars = {}\n",
      "\n",
      "        self._job_counter[worker_type] += count\n",
      "        if worker_type in self._job_with_gpu:\n",
      "            assert self._job_with_gpu[worker_type] == (\n",
      "                gpu > 0\n",
      "            ), \"All workers of the same type must either use GPU or not use GPU.\"\n",
      "        else:\n",
      "            self._job_with_gpu[worker_type] = gpu > 0\n",
      "\n",
      "        if worker_type in self._job_env_vars:\n",
      "            assert (\n",
      "                self._job_env_vars[worker_type] == env_vars\n",
      "            ), \"All workers of the same type must have the same env vars.\"\n",
      "        else:\n",
      "            self._job_env_vars[worker_type] = env_vars\n",
      "\n",
      "        if worker_type in self._job_cmd:\n",
      "            assert (\n",
      "                self._job_cmd[worker_type] == cmd\n",
      "            ), \"All workers of the same type must have the same cmd.\"\n",
      "        else:\n",
      "            self._job_cmd[worker_type] = cmd\n",
      "\n",
      "    def submit(self, worker_type, cmd, **kwargs):\n",
      "        self.submit_array(worker_type, cmd, count=1, **kwargs)\n",
      "\n",
      "    def __commit_all(self):\n",
      "        for worker_type, count, use_gpu, env_vars in zip(\n",
      "            self._job_counter.keys(),\n",
      "            self._job_counter.values(),\n",
      "            self._job_with_gpu.values(),\n",
      "            self._job_env_vars.values(),\n",
      "        ):\n",
      "            os.makedirs(\n",
      "                os.path.dirname(self.log_path_of(worker_type)),\n",
      "                exist_ok=True,\n",
      "                mode=0o775,\n",
      "            )\n",
      "            for i in range(count):\n",
      "                if use_gpu:\n",
      "                    available_device_id = self._gpu_counter % len(self._cuda_devices)\n",
      "                    env_vars[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
      "                        self._cuda_devices[available_device_id]\n",
      "                    )\n",
      "                    self._gpu_counter += 1\n",
      "                cmd = (\n",
      "                    \" \".join(str(k) + \"=\" + str(v) for k, v in env_vars.items())\n",
      "                    + \" stdbuf -oL \"\n",
      "                    + self._job_cmd[worker_type]\n",
      "                )\n",
      "                # Run `apps.remote` with a single process.\n",
      "                # This simulates a multi-prog slurm job with `count` jobsteps, with each jobstep having a single process.\n",
      "                cmd = cmd.format(\n",
      "                    jobstep_id=i,\n",
      "                    n_jobsteps=count,\n",
      "                    worker_submission_index=0,\n",
      "                    wprocs_per_jobstep=1,\n",
      "                    wprocs_in_job=count,\n",
      "                    wproc_offset=0,\n",
      "                )\n",
      "                logger.debug(\"Starting local process with command: %s\", cmd)\n",
      "                cmd = f\"{cmd} | tee -a {self.log_path_of(worker_type)}\"\n",
      "                process = subprocess.Popen(cmd, shell=isinstance(cmd, str))\n",
      "                self._jobs[f\"{worker_type}/{i}\"] = process\n",
      "            self._running_worker_types.append(worker_type)\n",
      "\n",
      "    def stop(self, worker_type, signal=None):\n",
      "        assert any(k.startswith(worker_type) for k in self._jobs)\n",
      "        keys = [k for k, p in self._jobs.items() if k.startswith(worker_type)]\n",
      "        procs = [p for k, p in self._jobs.items() if k.startswith(worker_type)]\n",
      "        logger.info(\n",
      "            f\"Stopping local process with signal {signal if signal else 'SIGKILL'}, \"\n",
      "            f\"pid: {[p.pid for p in procs]}\"\n",
      "        )\n",
      "        for p in procs:\n",
      "            terminate_process_and_children(p.pid, signal=signal)\n",
      "        for p in procs:\n",
      "            p.wait()\n",
      "        for k, p in zip(keys, procs):\n",
      "            self._jobs.pop(k)\n",
      "            del p\n",
      "        self._running_worker_types.remove(worker_type)\n",
      "\n",
      "    def stop_all(self, signal=None):\n",
      "        # signal argument is ignored in local stop_all\n",
      "        for name in self._job_counter:\n",
      "            self.stop(name, signal=signal)\n",
      "\n",
      "    def find(self, job_name):\n",
      "        if job_name in self._jobs:\n",
      "            return JobInfo(name=job_name, state=JobState.RUNNING, host=\"localhost\")\n",
      "        else:\n",
      "            return JobInfo(name=job_name, state=JobState.NOT_FOUND)\n",
      "\n",
      "    def find_all(self, job_name_regex=\".*\"):\n",
      "        rs = []\n",
      "        for name in self._jobs:\n",
      "            if re.fullmatch(job_name_regex, name):\n",
      "                rs.append(self.find(name))\n",
      "        return rs\n",
      "\n",
      "    def wait(\n",
      "        self,\n",
      "        timeout=None,\n",
      "        check_status: Tuple[JobState, ...] = (\n",
      "            JobState.CANCELLED,\n",
      "            JobState.FAILED,\n",
      "            JobState.NOT_FOUND,\n",
      "        ),\n",
      "        remove_status: Tuple[JobState, ...] = (JobState.COMPLETED,),\n",
      "        update=False,\n",
      "        commit=True,\n",
      "    ):\n",
      "        if commit:\n",
      "            self.__commit_all()\n",
      "        deadline = None if timeout is None else time.time() + timeout\n",
      "        logger.info(\n",
      "            \"Waiting for %d local running processes, pids: %s\",\n",
      "            len(self._jobs),\n",
      "            \" \".join(str(job.pid) for job in self._jobs.values()),\n",
      "        )\n",
      "        left = set(self._jobs.keys())\n",
      "        num_jobs_left = len(left)\n",
      "\n",
      "        while len(left) > 0:\n",
      "            to_remove = []\n",
      "            if len(left) < num_jobs_left:\n",
      "                num_jobs_left = len(left)\n",
      "                logger.info(f\"Waiting for {num_jobs_left} jobs.\")\n",
      "            if deadline is not None and time.time() > deadline:\n",
      "                raise TimeoutError(\n",
      "                    f\"Timeout waiting for {self.run_name}: {', '.join(sorted(left))}\"\n",
      "                )\n",
      "            # update job states\n",
      "            for job_name in list(left):\n",
      "                job = self._jobs[job_name]\n",
      "                pid = job.pid\n",
      "                process = psutil.Process(pid)\n",
      "                self._job_states[job_name] = PROCESS_STATUS_TO_JOB_STATE.get(\n",
      "                    process.status(), JobState.NOT_FOUND\n",
      "                )\n",
      "\n",
      "            for job_name in list(left):\n",
      "                state = self._job_states[job_name]\n",
      "                if state in check_status:\n",
      "                    raise JobException(\n",
      "                        run_name=self.run_name,\n",
      "                        worker_type=job_name.split(\"/\")[0],\n",
      "                        host=\"local\",\n",
      "                        reason=state,\n",
      "                    )\n",
      "                if state in remove_status:\n",
      "                    logger.info(f\"Job {job_name} is {state}.(Removed)\")\n",
      "                    left.remove(job_name)\n",
      "                    to_remove.append(job_name)\n",
      "\n",
      "            if update:\n",
      "                for k in to_remove:\n",
      "                    self._jobs.pop(k)\n",
      "                    worker_type = k.split(\"/\")[0]\n",
      "                    assert worker_type in self._job_counter\n",
      "                    self._job_counter[worker_type] -= 1\n",
      "                    if self._job_counter[worker_type] <= 0:\n",
      "                        assert worker_type in self._job_with_gpu\n",
      "                        assert worker_type in self._job_env_vars\n",
      "                        assert worker_type in self._job_cmd\n",
      "                        self._job_counter.pop(worker_type)\n",
      "                        self._job_with_gpu.pop(worker_type)\n",
      "                        self._job_env_vars.pop(worker_type)\n",
      "                        self._job_cmd.pop(worker_type)\n",
      "\n",
      "            time.sleep(2)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/apps/quickstart.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import argparse\n",
      "import datetime\n",
      "import getpass\n",
      "import os\n",
      "import pathlib\n",
      "import re\n",
      "import sys\n",
      "\n",
      "import hydra\n",
      "from omegaconf import DictConfig, OmegaConf\n",
      "from rich.panel import Panel\n",
      "\n",
      "from realhf.api.cli_args import console, highlighter, print_config_help\n",
      "from realhf.api.quickstart.entrypoint import QUICKSTART_CONFIG_CLASSES, QUICKSTART_FN\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.base.importing import import_module\n",
      "from realhf.base.prologue import (\n",
      "    PROLOGUE_EXTERNAL_CONFIG_NAME,\n",
      "    PROLOGUE_FLAG_NAME,\n",
      "    PROLOGUE_FLAG_VAR_NAME,\n",
      "    get_experiment_name,\n",
      "    get_trial_name,\n",
      ")\n",
      "from realhf.version import get_full_version_with_dirty_description\n",
      "\n",
      "# NOTE: Register all implemented experiments inside ReaL.\n",
      "import_module(\n",
      "    str(pathlib.Path(__file__).resolve().parent.parent / \"experiments\" / \"common\"),\n",
      "    re.compile(r\".*_exp\\.py$\"),\n",
      ")\n",
      "import_module(\n",
      "    str(pathlib.Path(__file__).resolve().parent.parent / \"experiments\" / \"async_exp\"),\n",
      "    re.compile(r\".*_exp\\.py$\"),\n",
      ")\n",
      "import realhf.experiments.benchmark.profile_exp\n",
      "\n",
      "\n",
      "def print_help(exp_type):\n",
      "    \"\"\"Print comprehensive help with rich formatting\"\"\"\n",
      "    config_class = QUICKSTART_CONFIG_CLASSES[exp_type]()\n",
      "\n",
      "    # Main help panel\n",
      "    console.print(\n",
      "        Panel.fit(\n",
      "            f\"[header]Configuration Help for {exp_type}[/header]\", border_style=\"border\"\n",
      "        )\n",
      "    )\n",
      "\n",
      "    # Configuration options section\n",
      "    console.print(\"\\n[title]CONFIGURATION OPTIONS[/title]\")\n",
      "    print_config_help(config_class)\n",
      "\n",
      "    # Usage section\n",
      "    console.print(\"\\n[title]USAGE[/title]\")\n",
      "    usage_code = f\"python -m realhf.apps.quickstart {exp_type} --config ./your/config.yaml [OPTIONS]\"\n",
      "    console.print(highlighter(usage_code))\n",
      "\n",
      "    # Examples section\n",
      "    console.print(\"\\n[title]EXAMPLE OVERRIDES[/title]\")\n",
      "    example_code = f\"python -m realhf.apps.quickstart {exp_type} --config ./your/config.yaml dataset.path=/my/dataset.jsonl actor.optimizer.lr=2e-5\"\n",
      "    console.print(highlighter(example_code))\n",
      "\n",
      "    # Footer\n",
      "    console.print(\"\\n[dim]Use [bold]--help[/bold] to show this message again[/dim]\")\n",
      "\n",
      "\n",
      "def print_version():\n",
      "    console.print(f\"AReaL Version: {get_full_version_with_dirty_description()}\")\n",
      "\n",
      "\n",
      "def main():\n",
      "    # Create parser with add_help=False to disable automatic --help\n",
      "    parser = argparse.ArgumentParser(prog=\"ReaL Quickstart\", add_help=False)\n",
      "\n",
      "    # Add custom help argument that won't conflict\n",
      "    parser.add_argument(\n",
      "        \"--help\", action=\"store_true\", help=\"Show this help message and exit\"\n",
      "    )\n",
      "    parser.add_argument(\"--version\", action=\"store_true\", help=\"Show AReaL version\")\n",
      "\n",
      "    subparsers = parser.add_subparsers(dest=\"cmd\", help=\"sub-command help\")\n",
      "    subparsers.required = True\n",
      "\n",
      "    for k, v in QUICKSTART_FN.items():\n",
      "        # Create subparser with add_help=False\n",
      "        subparser = subparsers.add_parser(k, add_help=False)\n",
      "\n",
      "        # Add custom help to subparser\n",
      "        subparser.add_argument(\n",
      "            \"--help\", action=\"store_true\", help=\"Show help for this command\"\n",
      "        )\n",
      "        subparser.add_argument(\n",
      "            PROLOGUE_FLAG_NAME,\n",
      "            type=str,\n",
      "            help=\"Set config (*.yaml) for this experiment.\",\n",
      "        )\n",
      "\n",
      "        subparser.set_defaults(func=v)\n",
      "\n",
      "    # Parse known args first to check for help\n",
      "    args = vars(parser.parse_known_args()[0])\n",
      "\n",
      "    if args[\"version\"]:\n",
      "        print_version()\n",
      "        return\n",
      "\n",
      "    # Handle help at both main and subcommand levels\n",
      "    if args[\"help\"]:\n",
      "        if args[\"cmd\"]:\n",
      "            # Subcommand help\n",
      "            print_help(args[\"cmd\"])\n",
      "        else:\n",
      "            # Main help\n",
      "            parser.print_help()\n",
      "        return\n",
      "\n",
      "    # Continue with normal execution\n",
      "    if not args[\"cmd\"]:\n",
      "        parser.print_help()\n",
      "    experiment_name = \"\"\n",
      "    trial_name = \"\"\n",
      "\n",
      "    if args[PROLOGUE_FLAG_VAR_NAME]:\n",
      "        config_dir, experiment_name, trial_name = prepare_hydra_config(\n",
      "            args[\"cmd\"], args[PROLOGUE_FLAG_VAR_NAME]\n",
      "        )\n",
      "        sys.argv.remove(PROLOGUE_FLAG_NAME)\n",
      "        sys.argv.remove(args[PROLOGUE_FLAG_VAR_NAME])\n",
      "        sys.argv += [f\"--config-path\", f\"{config_dir}\"]\n",
      "    else:\n",
      "        experiment_name = get_experiment_name()\n",
      "        trial_name = get_trial_name()\n",
      "\n",
      "    launch_hydra_task(\n",
      "        args[\"cmd\"], experiment_name, trial_name, QUICKSTART_FN[args[\"cmd\"]]\n",
      "    )\n",
      "\n",
      "\n",
      "def prepare_hydra_config(name: str, prologue_path: str):\n",
      "    config = OmegaConf.load(prologue_path)\n",
      "    experiment_name = get_experiment_name(config.get(\"experiment_name\"))\n",
      "    trial_name = get_trial_name(config.get(\"trial_name\"))\n",
      "    config_dir = f\"{cluster_spec.fileroot}/configs/{getpass.getuser()}/{experiment_name}/{trial_name}\"\n",
      "    os.makedirs(config_dir, exist_ok=True)\n",
      "\n",
      "    config.pop(PROLOGUE_EXTERNAL_CONFIG_NAME, {})\n",
      "    with open(f\"{config_dir}/{name}.yaml\", \"w\") as f:\n",
      "        f.write(OmegaConf.to_yaml(config))\n",
      "\n",
      "    return (config_dir, experiment_name, trial_name)\n",
      "\n",
      "\n",
      "def launch_hydra_task(\n",
      "    name: str, experiment_name: str, trial_name: str, func: hydra.TaskFunction\n",
      "):\n",
      "    # Disable hydra logging.\n",
      "    if not any(\"hydra/job_logging=disabled\" in x for x in sys.argv):\n",
      "        sys.argv.insert(2, \"hydra/job_logging=disabled\")\n",
      "\n",
      "    sys.argv.pop(1)\n",
      "\n",
      "    func()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/apps/remote.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import argparse\n",
      "import functools\n",
      "import json\n",
      "import multiprocessing\n",
      "import os\n",
      "\n",
      "try:\n",
      "    import uvloop\n",
      "\n",
      "    uvloop.install()\n",
      "except (ModuleNotFoundError, ImportError):\n",
      "    pass\n",
      "\n",
      "import torch\n",
      "from omegaconf import OmegaConf\n",
      "\n",
      "multiprocessing.set_start_method(\"spawn\", force=True)\n",
      "\n",
      "from realhf.api.quickstart.entrypoint import QUICKSTART_CONFIG_CLASSES\n",
      "from realhf.base import gpu_utils, importing, logging\n",
      "from realhf.version import get_full_version_with_dirty_description\n",
      "\n",
      "logger = logging.getLogger(\"Main-Workers\")\n",
      "\n",
      "\n",
      "def _patch_external_impl(exp_name, trial_name):\n",
      "    import realhf.api.core.system_api as system_api\n",
      "    from realhf.base.constants import QUICKSTART_EXPR_CACHE_PATH\n",
      "\n",
      "    if os.path.exists(QUICKSTART_EXPR_CACHE_PATH):\n",
      "        for exp_cache in os.listdir(QUICKSTART_EXPR_CACHE_PATH):\n",
      "            target_cache_name = f\"{exp_name}_{trial_name}.json\"\n",
      "            if exp_cache != target_cache_name:\n",
      "                continue\n",
      "            cache_file = os.path.join(QUICKSTART_EXPR_CACHE_PATH, target_cache_name)\n",
      "            with open(cache_file, \"r\") as f:\n",
      "                cache = json.load(f)\n",
      "            usercode_path = cache[\"usercode_path\"]\n",
      "            exp_cls_args = OmegaConf.create(cache[\"args\"])\n",
      "            config_name = cache[\"config_name\"]\n",
      "            # Import user code to register quickstart experiments.\n",
      "            importing.import_usercode(usercode_path, \"_realhf_user_code\")\n",
      "            # Register the internal experiment.\n",
      "            exp_cls = QUICKSTART_CONFIG_CLASSES[config_name]\n",
      "            system_api.register_experiment(\n",
      "                exp_name, functools.partial(exp_cls, **exp_cls_args)\n",
      "            )\n",
      "\n",
      "\n",
      "def main_worker(args):\n",
      "    import realhf.base.constants as constants\n",
      "\n",
      "    constants.set_experiment_trial_names(args.experiment_name, args.trial_name)\n",
      "    _patch_external_impl(args.experiment_name, args.trial_name)\n",
      "\n",
      "    # Initialize cluster infor from ENV or CLI args.\n",
      "    import realhf.api.core.system_api as system_api\n",
      "\n",
      "    experiment = system_api.make_experiment(name=args.experiment_name)\n",
      "    constants.init_constants(experiment)\n",
      "\n",
      "    worker_index_start = args.jobstep_id * args.wprocs_per_jobstep + args.wproc_offset\n",
      "    worker_index_end = min(\n",
      "        worker_index_start + args.wprocs_per_jobstep,\n",
      "        args.wprocs_in_job + args.wproc_offset,\n",
      "    )\n",
      "    if args.worker_type in [\"master_worker\", \"rollout_worker\", \"gserver_manager\"]:\n",
      "        try:\n",
      "            # CUDA_VISIBLE_DEVICES is set by slurm on PPU nodes\n",
      "            # we need to remove it on CPU workers\n",
      "            del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
      "        except KeyError:\n",
      "            pass\n",
      "\n",
      "    group_name = f\"{args.worker_type}-{args.worker_submission_index}\"\n",
      "    logger.debug(\n",
      "        f\"{args.worker_type} group id {args.jobstep_id}, \"\n",
      "        f\"worker index {worker_index_start}:{worker_index_end}\"\n",
      "    )\n",
      "\n",
      "    # Isolate within the same slurm job, among different jobsteps.\n",
      "    if torch.cuda.is_initialized():\n",
      "        raise RuntimeError(\n",
      "            \"CUDA already initialized before isolating CUDA devices. This should not happen.\"\n",
      "        )\n",
      "    gpu_utils.isolate_cuda_device(\n",
      "        group_name,\n",
      "        args.jobstep_id,\n",
      "        args.n_jobsteps,\n",
      "        args.experiment_name,\n",
      "        args.trial_name,\n",
      "    )\n",
      "    if os.environ.get(\"CUDA_VISIBLE_DEVICES\", None):\n",
      "        logger.debug(\"CUDA_VISIBLE_DEVICES: %s\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
      "\n",
      "    # NOTE: Importing these will initialize DeepSpeed/CUDA devices.\n",
      "    # profiler.import_profiler_registers()\n",
      "    import realhf.system\n",
      "\n",
      "    logger.debug(f\"Run {args.worker_type} worker with args: %s\", args)\n",
      "    assert not args.experiment_name.startswith(\n",
      "        \"/\"\n",
      "    ), f'Invalid experiment_name \"{args.experiment_name}\" starts with \"/\"'\n",
      "    if args.wprocs_per_jobstep == 1:\n",
      "        realhf.system.run_worker(\n",
      "            worker_type=args.worker_type,\n",
      "            experiment_name=args.experiment_name,\n",
      "            trial_name=args.trial_name,\n",
      "            worker_name=f\"{args.worker_type}/{worker_index_start}\",\n",
      "            worker_server_type=\"zmq\",\n",
      "        )\n",
      "    else:\n",
      "        workers = []\n",
      "        for wid in range(worker_index_start, worker_index_end):\n",
      "            worker_args = dict(\n",
      "                worker_type=args.worker_type,\n",
      "                experiment_name=args.experiment_name,\n",
      "                trial_name=args.trial_name,\n",
      "                worker_name=f\"{args.worker_type}/{wid}\",\n",
      "                worker_server_type=\"zmq\",\n",
      "            )\n",
      "            p = multiprocessing.Process(\n",
      "                target=realhf.system.run_worker, kwargs=worker_args\n",
      "            )\n",
      "            p.name = f\"{args.worker_type}/{wid}\"\n",
      "            p.start()\n",
      "            workers.append(p)\n",
      "\n",
      "        logger.info(\n",
      "            f\"Waiting for {args.wprocs_per_jobstep} {args.worker_type} workers of group id {args.jobstep_id}.\"\n",
      "        )\n",
      "        worker_exit_code = 0\n",
      "        while not worker_exit_code:\n",
      "            for p in workers:\n",
      "                if p.exitcode is None:\n",
      "                    p.join(timeout=5)\n",
      "                elif p.exitcode == 0:\n",
      "                    pass\n",
      "                else:\n",
      "                    logger.error(f\"{p.name} exitcode: {p.exitcode}\")\n",
      "                    worker_exit_code = p.exitcode\n",
      "                    break\n",
      "        for p in workers:\n",
      "            if p.is_alive():\n",
      "                p.kill()\n",
      "        exit(worker_exit_code)\n",
      "\n",
      "\n",
      "def main_controller(args):\n",
      "    \"\"\"\n",
      "    Args:\n",
      "        args: argparse result including:\n",
      "            experiment_name:\n",
      "            trial_name:\n",
      "            config_index: the index of experiment configuration (experiment may return multiple configurations)\n",
      "            ignore_worker_error: bool, if False, stop the experiment when any worker(s) fail.\n",
      "    \"\"\"\n",
      "    import realhf.api.core.system_api as system_api\n",
      "    import realhf.base.constants as constants\n",
      "    import realhf.system as system\n",
      "\n",
      "    constants.set_experiment_trial_names(args.experiment_name, args.trial_name)\n",
      "    _patch_external_impl(args.experiment_name, args.trial_name)\n",
      "\n",
      "    logger.debug(\"Running controller with args: %s\", args)\n",
      "    assert not args.experiment_name.startswith(\"/\"), args.experiment_name\n",
      "    try:\n",
      "        del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
      "    except KeyError:\n",
      "        pass\n",
      "    controller = system.make_controller(\n",
      "        type_=args.type,\n",
      "        experiment_name=args.experiment_name,\n",
      "        trial_name=args.trial_name,\n",
      "    )\n",
      "    experiment = system_api.make_experiment(args.experiment_name)\n",
      "\n",
      "    # Initialize cluster infor from ENV or CLI args.\n",
      "    constants.init_constants(experiment)\n",
      "\n",
      "    controller.start(\n",
      "        experiment=experiment,\n",
      "        ignore_worker_error=args.ignore_worker_error,\n",
      "    )\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(prog=\"marl\")\n",
      "    subparsers = parser.add_subparsers(dest=\"cmd\", help=\"sub-command help\")\n",
      "    subparsers.required = True\n",
      "\n",
      "    subparser = subparsers.add_parser(\n",
      "        \"controller\", help=\"run a controller of experiment\"\n",
      "    )\n",
      "    subparser.add_argument(\"--experiment_name\", \"-e\", type=str, required=True)\n",
      "    subparser.add_argument(\"--trial_name\", \"-f\", type=str, required=True)\n",
      "    subparser.add_argument(\"--ignore_worker_error\", action=\"store_true\")\n",
      "    subparser.add_argument(\n",
      "        \"--raise_worker_error\", dest=\"ignore_worker_error\", action=\"store_false\"\n",
      "    )\n",
      "    subparser.add_argument(\"--type\", type=str, default=\"zmq\")\n",
      "    subparser.set_defaults(feature=False)\n",
      "    subparser.set_defaults(func=main_controller)\n",
      "\n",
      "    subparser = subparsers.add_parser(\"worker\", help=\"run a standalone worker\")\n",
      "    subparser.add_argument(\"--experiment_name\", \"-e\", type=str, required=True)\n",
      "    subparser.add_argument(\"--trial_name\", \"-f\", type=str, required=True)\n",
      "    subparser.add_argument(\"--worker_type\", \"-w\", type=str, required=True)\n",
      "    subparser.add_argument(\n",
      "        \"--jobstep_id\",\n",
      "        \"-i\",\n",
      "        type=int,\n",
      "        required=True,\n",
      "        help=\"jobstep/task ID in a slurm job.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--n_jobsteps\",\n",
      "        \"-g\",\n",
      "        type=int,\n",
      "        required=True,\n",
      "        help=\"`--ntasks` of `srun`, aka SLURM_NPROCS.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--worker_submission_index\",\n",
      "        \"-r\",\n",
      "        type=int,\n",
      "        required=True,\n",
      "        help=\"Submission index to slurm for this worker. Used for locating job name and logs.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--wprocs_per_jobstep\",\n",
      "        \"-p\",\n",
      "        type=int,\n",
      "        required=True,\n",
      "        help=\"Number of worker processes launched by multiprocessing in this script.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--wprocs_in_job\",\n",
      "        \"-j\",\n",
      "        type=int,\n",
      "        required=True,\n",
      "        help=\"Number of worker processes in this slurm job.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--wproc_offset\",\n",
      "        \"-o\",\n",
      "        type=int,\n",
      "        required=True,\n",
      "        help=\"Offset of worker processes of this slurm job. \"\n",
      "        \"For example, we may allocate 4 type `A` workers with 1 GPU each and 2 with 0.5 GPU each. \"\n",
      "        \"This launches 2 jobs, the former with 4 job steps and the latter with 2 job steps. \"\n",
      "        \"The offset is 0 for the 1st job and 4 for the 2nd job.\",\n",
      "    )\n",
      "    subparser.set_defaults(func=main_worker)\n",
      "\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    logger.info(f\"AReaL Version: {get_full_version_with_dirty_description()}\")\n",
      "    args.func(args)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/apps/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/apps/main.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import argparse\n",
      "import getpass\n",
      "import os\n",
      "import re\n",
      "import time\n",
      "import uuid\n",
      "from typing import Dict, List, Optional\n",
      "\n",
      "import realhf.api.core.system_api as config_package\n",
      "import realhf.scheduler.client as sched_client\n",
      "import realhf.system as system\n",
      "from realhf.api.core.system_api import ExpStatus\n",
      "from realhf.base import constants, logging, name_resolve, names, recover\n",
      "from realhf.scheduler.client import JobException, JobState\n",
      "from realhf.scheduler.evaluator import AutomaticEvaluator\n",
      "from realhf.version import get_full_version_with_dirty_description\n",
      "\n",
      "logger = logging.getLogger(\"main\", \"system\")\n",
      "\n",
      "CONTROLLER_TIME_LIMIT = None\n",
      "\n",
      "\n",
      "def scheduler_mode(mode: str) -> str:\n",
      "    return mode if mode == \"slurm\" else \"local\"\n",
      "\n",
      "\n",
      "def _submit_workers(\n",
      "    sched: sched_client.SchedulerClient,\n",
      "    expr_name: str,\n",
      "    trial_name: str,\n",
      "    debug: bool,\n",
      "    worker_type: str,\n",
      "    scheduling_configs: List[config_package.TasksGroup],\n",
      "    environs: Dict[str, str],\n",
      ") -> List[str]:\n",
      "    if len(scheduling_configs) == 0:\n",
      "        return []\n",
      "\n",
      "    scheduled_jobs = []\n",
      "    for sch_cfg in scheduling_configs:\n",
      "        if sch_cfg is None:\n",
      "            continue\n",
      "        job_environs = {**environs, **sch_cfg.scheduling.env_vars}\n",
      "        cmd = sched_client.remote_worker_cmd(expr_name, trial_name, debug, worker_type)\n",
      "\n",
      "        logger.debug(f\"Scheduling worker {worker_type}, {scheduling_configs}\")\n",
      "\n",
      "        nodelist = sch_cfg.scheduling.nodelist\n",
      "        exclude = sch_cfg.scheduling.exclude\n",
      "        container_image = sch_cfg.scheduling.container_image\n",
      "\n",
      "        scheduled_jobs.append(\n",
      "            sched.submit_array(\n",
      "                worker_type=worker_type,\n",
      "                cmd=cmd,\n",
      "                count=sch_cfg.count,\n",
      "                cpu=sch_cfg.scheduling.cpu,\n",
      "                gpu=sch_cfg.scheduling.gpu,\n",
      "                mem=sch_cfg.scheduling.mem,\n",
      "                container_image=container_image,\n",
      "                nodelist=nodelist,\n",
      "                exclude=exclude,\n",
      "                env_vars=job_environs,\n",
      "                hostfile=True,\n",
      "                multiprog=True,\n",
      "                begin=sch_cfg.scheduling.begin,\n",
      "                deadline=sch_cfg.scheduling.deadline,\n",
      "                time_limit=sch_cfg.scheduling.time_limit,\n",
      "            ),\n",
      "        )\n",
      "    return scheduled_jobs\n",
      "\n",
      "\n",
      "def main_start(args, job_group_id: str = \"\", recover_count: int = 0):\n",
      "    if not job_group_id:\n",
      "        job_group_id = str(uuid.uuid4())\n",
      "    logger.info(f\"AReaL Version: {get_full_version_with_dirty_description()}\")\n",
      "    logger.info(f\"AReaL Job Group ID: {job_group_id}\")\n",
      "    logger.info(f\"AReaL Job Group Index (recover count): {recover_count}\")\n",
      "    if recover_count == 0:\n",
      "        constants.set_experiment_trial_names(args.experiment_name, args.trial_name)\n",
      "    experiment = config_package.make_experiment(args.experiment_name)\n",
      "\n",
      "    # Run initial_setup to go through all sanity checks.\n",
      "    try:\n",
      "        exp_cfg = experiment.initial_setup()\n",
      "        assert isinstance(exp_cfg, config_package.ExperimentConfig)\n",
      "        exp_cfg.lazy_init()\n",
      "    except Exception as e:\n",
      "        raise RuntimeError(\"Experiment initial setup failed.\") from e\n",
      "\n",
      "    evaluator = (\n",
      "        AutomaticEvaluator(exp_cfg.evaluator, exp_cfg.wandb)\n",
      "        if exp_cfg.auto_eval\n",
      "        else None\n",
      "    )\n",
      "\n",
      "    if args.mode == \"local\":\n",
      "        assert (\n",
      "            args.recover_mode == \"disabled\"\n",
      "        ), \"Recover mode is not supported for local runs!\"\n",
      "    # handle args\n",
      "    args.ignore_worker_error = (\n",
      "        args.ignore_worker_error and args.recover_mode == \"disabled\"\n",
      "    )\n",
      "    trial_name = args.trial_name or f\"test-{getpass.getuser()}\"\n",
      "    expr_name = args.experiment_name\n",
      "    is_recover_run = False\n",
      "    if args.recover_mode == \"resume\":\n",
      "        is_recover_run = True\n",
      "    if args.recover_mode == \"fault\":\n",
      "        is_recover_run = recover_count > 0\n",
      "    if args.recover_mode == \"auto\":\n",
      "        try:\n",
      "            recover.discover_ckpt(args.experiment_name, args.trial_name)\n",
      "            is_recover_run = True\n",
      "        except recover.InValidRecoverCkpt as e:\n",
      "            logger.warning(\n",
      "                \"Invalid recover checkpoint when recover_mode='auto'. \"\n",
      "                \"Running the experiment from scratch with fault tolerance. \"\n",
      "                f\"Err message: {e}\"\n",
      "            )\n",
      "            is_recover_run = False\n",
      "    if is_recover_run:\n",
      "        recover_ckpt_path, model_ckpt_dirs, recover_info = recover.discover_ckpt(\n",
      "            args.experiment_name, args.trial_name\n",
      "        )\n",
      "        logger.info(f\"Will load recover info from {recover_ckpt_path}.\")\n",
      "        logger.info(f\"Will load model checkpoints from {model_ckpt_dirs}.\")\n",
      "        logger.info(\n",
      "            f\"Training will start from: epoch {recover_info.recover_start.epoch + 1} \"\n",
      "            f\"epoch step {recover_info.recover_start.epoch_step + 1} \"\n",
      "            f\"global step {recover_info.recover_start.global_step + 1}.\"\n",
      "        )\n",
      "    save_recover_states = args.recover_mode != \"disabled\"\n",
      "\n",
      "    cluster_spec_path = os.environ.get(\"CLUSTER_SPEC_PATH\", \"\")\n",
      "    if not cluster_spec_path:\n",
      "        logger.info(\n",
      "            \"Environment variable CLUSTER_SPEC_PATH is not set. \"\n",
      "            \"Will use the fileroot specified in CLI args. \"\n",
      "        )\n",
      "    else:\n",
      "        logger.warning(\n",
      "            \"Environment variable CLUSTER_SPEC_PATH is set. \"\n",
      "            \"Will overwrite the cluster spec in CLI args. \"\n",
      "        )\n",
      "    # set env vars\n",
      "    BASE_ENVIRONS = constants.get_env_vars(\n",
      "        REAL_MODE=args.mode.upper(),\n",
      "        REAL_RECOVER_RUN=\"1\" if is_recover_run else \"0\",\n",
      "        REAL_SAVE_RECOVER_STATES=\"1\" if save_recover_states else \"0\",\n",
      "    )\n",
      "    for k, v in BASE_ENVIRONS.items():\n",
      "        os.environ[k] = v\n",
      "\n",
      "    # setup experiments\n",
      "    sched = sched_client.make(\n",
      "        mode=scheduler_mode(args.mode),\n",
      "        expr_name=expr_name,\n",
      "        trial_name=trial_name,\n",
      "        schedule_strategy=args.schedule_strategy,\n",
      "        evaluator=evaluator,\n",
      "        job_group_id=job_group_id,\n",
      "        job_group_index=recover_count,\n",
      "    )\n",
      "\n",
      "    setup = experiment.scheduling_setup()\n",
      "\n",
      "    logger.info(f\"Resetting name resolving repo...\")\n",
      "\n",
      "    try:\n",
      "        name_resolve.clear_subtree(\n",
      "            names.trial_root(\n",
      "                experiment_name=args.experiment_name, trial_name=args.trial_name\n",
      "            )\n",
      "        )\n",
      "        # NOTE: During fault recovery, the NFS-based name resolve may encounter\n",
      "        # unexpected OS errors when making or removing directories. Sleeping for\n",
      "        # a while to avoid such errors.\n",
      "        time.sleep(5)\n",
      "    except Exception as e:\n",
      "        logger.warning(f\"Resetting name resolving repo failed.\")\n",
      "        raise e\n",
      "    logger.info(f\"Resetting name resolving repo... Done.\")\n",
      "\n",
      "    logger.info(\n",
      "        f\"Running configuration: {experiment.__class__.__name__}. \"\n",
      "        f\"The current recover retry: {recover_count + 1}/{args.recover_retries}\"\n",
      "    )\n",
      "\n",
      "    # Schedule controller\n",
      "    if args.mode == \"ray\":\n",
      "        controller_type = \"ray\"\n",
      "    else:\n",
      "        controller_type = \"zmq\"\n",
      "    # For ray mode, the controller will start all remote workers.\n",
      "    sched.submit_array(\n",
      "        worker_type=\"ctl\",\n",
      "        cmd=sched_client.control_cmd(\n",
      "            expr_name,\n",
      "            trial_name,\n",
      "            args.debug,\n",
      "            args.ignore_worker_error,\n",
      "            controller_type,\n",
      "        ),\n",
      "        count=1,\n",
      "        cpu=1,\n",
      "        gpu=0,\n",
      "        mem=1024,\n",
      "        env_vars=BASE_ENVIRONS,\n",
      "        container_image=setup.controller_image,\n",
      "        time_limit=CONTROLLER_TIME_LIMIT,\n",
      "    )\n",
      "\n",
      "    if args.mode != \"ray\":\n",
      "        workers_configs = ((k, getattr(setup, k)) for k in system.WORKER_TYPES)\n",
      "\n",
      "        for name, scheduling_setup in workers_configs:\n",
      "            if not isinstance(scheduling_setup, list):\n",
      "                scheduling_setup = [scheduling_setup]\n",
      "            # For local or slurm mode, launch all workers.\n",
      "            # For ray mode, nothing to do because workers will be\n",
      "            # started by the controller, rather than the scheduler.\n",
      "            _submit_workers(\n",
      "                sched,\n",
      "                expr_name,\n",
      "                trial_name,\n",
      "                args.debug,\n",
      "                name,\n",
      "                scheduling_setup,\n",
      "                BASE_ENVIRONS,\n",
      "            )\n",
      "\n",
      "    try:\n",
      "        sched.wait(\n",
      "            check_status=(\n",
      "                JobState.CANCELLED,\n",
      "                JobState.FAILED,\n",
      "                JobState.NOT_FOUND,\n",
      "                JobState.COMPLETED,\n",
      "            ),\n",
      "            remove_status=(),\n",
      "        )\n",
      "    except (KeyboardInterrupt, JobException, TimeoutError) as e:\n",
      "        recover_states = [\n",
      "            JobState.CANCELLED,\n",
      "            JobState.FAILED,\n",
      "            JobState.NOT_FOUND,\n",
      "        ]\n",
      "        reason = e.reason if isinstance(e, JobException) else None\n",
      "        recover_this = (\n",
      "            args.recover_mode in [\"auto\", \"fault\"]\n",
      "            and recover_count < args.recover_retries\n",
      "        )\n",
      "        recover_this = recover_this and reason in recover_states\n",
      "\n",
      "        # Check whether this exception is caused by experiment finish.\n",
      "        name = names.experiment_status(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        try:\n",
      "            exp_status = name_resolve.get(name)\n",
      "            recover_this = recover_this and exp_status != str(ExpStatus.COMPLETE)\n",
      "            if exp_status == str(ExpStatus.COMPLETE):\n",
      "                logger.warning(\"*\" * 100)\n",
      "                logger.warning(\n",
      "                    \"*\"\n",
      "                    + f\"Will not recover because the experiment has completed! Congrats!\".center(\n",
      "                        98, \" \"\n",
      "                    )\n",
      "                    + \"*\"\n",
      "                )\n",
      "                logger.warning(\"*\" * 100)\n",
      "        except name_resolve.NameEntryNotFoundError:\n",
      "            raise name_resolve.NameEntryNotFoundError(\n",
      "                f\"Experiment status not found during recover. \"\n",
      "                \"This indicates that the master worker is not running. Exit the recover loop.\"\n",
      "            )\n",
      "\n",
      "        kill_signal = (\n",
      "            \"SIGKILL\" if args.mode == \"slurm\" else \"SIGTERM\"\n",
      "        )  # use sigkill to terminate slurm jobs\n",
      "        # Recovering should use SIGKILL as well, since we\n",
      "        # are not calling exit hook on workers.\n",
      "        # Using SIGINT might cause some workers to be stuck,\n",
      "        # leaving RUNNING state workers in the slurm cluster\n",
      "        sched.stop_all(kill_signal)\n",
      "        if recover_this:\n",
      "            logger.warning(\n",
      "                f\"Recovering from error {e} after {args.recover_after} seconds. Recover count: {recover_count+1}, \"\n",
      "                f\"total recover count {args.recover_retries}\"\n",
      "            )\n",
      "            time.sleep(args.recover_after)\n",
      "            main_start(args, job_group_id=job_group_id, recover_count=recover_count + 1)\n",
      "        else:\n",
      "            raise e\n",
      "\n",
      "\n",
      "def main_stop(args):\n",
      "    sched = sched_client.make(\n",
      "        mode=scheduler_mode(args.mode),\n",
      "        expr_name=args.experiment_name,\n",
      "        trial_name=args.trial_name,\n",
      "    )\n",
      "    sched.find_all()\n",
      "    sched.stop_all()\n",
      "\n",
      "\n",
      "def main_find_config(args):\n",
      "    exp_names = [\n",
      "        x for x in config_package.ALL_EXPERIMENT_CLASSES if re.match(args.regex, x)\n",
      "    ]\n",
      "    if len(exp_names) == 0:\n",
      "        print(\"No matched experiment names.\")\n",
      "    if len(exp_names) > 20:\n",
      "        response = input(f\"Found {len(exp_names)} experiments, list all?(y/n)\")\n",
      "        if response != \"y\":\n",
      "            return\n",
      "    for exp_name in exp_names:\n",
      "        print(exp_name)\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(prog=\"ReaLHF\")\n",
      "    subparsers = parser.add_subparsers(dest=\"cmd\", help=\"sub-command help\")\n",
      "    subparsers.required = True\n",
      "\n",
      "    subparser = subparsers.add_parser(\"start\", help=\"starts an experiment\")\n",
      "    subparser.add_argument(\n",
      "        \"--experiment_name\",\n",
      "        \"-e\",\n",
      "        type=str,\n",
      "        required=True,\n",
      "        help=\"name of the experiment\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--trial_name\",\n",
      "        \"-f\",\n",
      "        type=str,\n",
      "        default=None,\n",
      "        help=\"trial name; by default uses '<USER>-test'\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--mode\",\n",
      "        default=\"slurm\",\n",
      "        choices=[\"local\", \"slurm\", \"ray\"],\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--schedule_strategy\",\n",
      "        default=\"empty_first\",\n",
      "        choices=[\"empty_first\", \"allocated_first\"],\n",
      "        help=\"Schedule strategy for scheduler. Currently only effective in slurm mode. \"\n",
      "        \"In slurm mode, jobs are scheduled in pack to avoid fragmentation. \"\n",
      "        \"Specifically, the scheduler will avoid scheduling master worker/ctl jobs on different nodes from model workers; \"\n",
      "        \"'empty_first': schedule jobs to the node with least allocated resources first; \"\n",
      "        \"'allocated_first': schedule jobs to the node with most allocated resources first. \",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--partition\",\n",
      "        default=\"dev\",\n",
      "        help=\"slurm partition to schedule the trial\",\n",
      "    )\n",
      "    subparser.add_argument(\"--ignore_worker_error\", action=\"store_true\")\n",
      "    subparser.add_argument(\n",
      "        \"--debug\",\n",
      "        action=\"store_true\",\n",
      "        help=\"If True, activate all assertions in the code.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--recover_mode\",\n",
      "        required=False,\n",
      "        default=\"disabled\",\n",
      "        choices=[\"disabled\", \"auto\", \"resume\", \"fault\"],\n",
      "        help=\"Recover mode, 'auto': automatically recover the last failed run, \"\n",
      "        \"otherwise run from sratch with fault tolerance; \"\n",
      "        \"'fault': run from scratch with fault tolerance; \"\n",
      "        \"'resume': force to load the last failed run and run it without fault tolerance; \"\n",
      "        \"'disabled': do nothing when error occurs. \",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--recover_retries\",\n",
      "        type=int,\n",
      "        required=False,\n",
      "        default=1,\n",
      "        help=\"Total number of trials for the system to recover automatically when a worker fails. \"\n",
      "        \"Only effective when recover_mode is 'auto' or 'fault'\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--recover_after\",\n",
      "        type=int,\n",
      "        required=False,\n",
      "        default=10,\n",
      "        help=\"Number of seconds to wait before recovering.\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--allocation_mode\",\n",
      "        type=str,\n",
      "        required=False,\n",
      "        default=\"pipe_model\",\n",
      "        choices=[\"manual\", \"heuristic\", \"pipe_model\", \"pipe_data\"],\n",
      "        help=\"Mode of GPU resource/model parallel strategy allocation.\",\n",
      "    )\n",
      "    subparser.set_defaults(ignore_worker_error=False)\n",
      "    subparser.set_defaults(func=main_start)\n",
      "\n",
      "    subparser = subparsers.add_parser(\n",
      "        \"stop\", help=\"stops an experiment. only slurm experiment is supported.\"\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--experiment_name\",\n",
      "        \"-e\",\n",
      "        type=str,\n",
      "        required=True,\n",
      "        help=\"name of the experiment\",\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--trial_name\", \"-f\", type=str, required=True, help=\"name of the trial\"\n",
      "    )\n",
      "    subparser.add_argument(\n",
      "        \"--mode\",\n",
      "        default=\"slurm\",\n",
      "        choices=[\"local\", \"slurm\", \"ray\"],\n",
      "    )\n",
      "    subparser.set_defaults(func=main_stop)\n",
      "\n",
      "    subparser = subparsers.add_parser(\n",
      "        \"find_config\", help=\"find configuration by matching regular expression.\"\n",
      "    )\n",
      "    subparser.add_argument(\"--regex\", \"-r\", type=str, required=True)\n",
      "    subparser.set_defaults(func=main_find_config)\n",
      "\n",
      "    args = parser.parse_args()\n",
      "    args.func(args)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/gserver_manager.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import asyncio\n",
      "import os\n",
      "import shutil\n",
      "import threading\n",
      "import time\n",
      "from collections import defaultdict\n",
      "from dataclasses import dataclass\n",
      "from typing import List\n",
      "\n",
      "import aiohttp\n",
      "import numpy as np\n",
      "\n",
      "from realhf.api.core.model_api import GenReqMeta, GenRespMeta, ModelVersionReq\n",
      "from realhf.api.core.system_api import ExpStatus\n",
      "from realhf.api.core.system_api import GserverManager as GserverManagerConfig\n",
      "from realhf.base import constants, logging, name_resolve, names, network, recover\n",
      "from realhf.base.monitor import RolloutStat\n",
      "from realhf.system.worker_base import AsyncWorker, PollResult, Worker\n",
      "\n",
      "logger = logging.getLogger(\"Generation Manager\", \"system\")\n",
      "\n",
      "STALENESS_WARNED = defaultdict(lambda: False)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class AllocateRolloutInput:\n",
      "    qid: str\n",
      "\n",
      "\n",
      "class GserverManager(AsyncWorker):\n",
      "    \"\"\"This worker has the following functionalities:\n",
      "    1. As a router, it schedules generation requests and returns the\n",
      "       best server urls to clients for submitting generation requests.\n",
      "    2. It manages the weight update requests of generation servers.\n",
      "       The weight update manager must be unique in each experiment.\n",
      "\n",
      "    This is currently a hack usage of SGLang. We can integrate the\n",
      "    functionalities into sgl-router and srt in the future.\n",
      "    \"\"\"\n",
      "\n",
      "    def _configure(self, config: GserverManagerConfig):\n",
      "        self.config = config\n",
      "        self.model_name = config.model_name\n",
      "\n",
      "        assert self.config.worker_info.worker_count == 1\n",
      "\n",
      "        self.threading_lock = threading.Lock()\n",
      "        self.rollout_stat = RolloutStat()\n",
      "\n",
      "        self.schedule_policy = config.schedule_policy\n",
      "\n",
      "        self._last_param_realloc_step = 0\n",
      "\n",
      "        self._qid_to_server_url = {}\n",
      "\n",
      "        self._server_token_usage = defaultdict(float)\n",
      "        self._server_request_counts = defaultdict(int)\n",
      "\n",
      "        self._last_thpt_output_time = time.time()\n",
      "        self._gen_tokens = 0\n",
      "\n",
      "        self.experiment_name = config.worker_info.experiment_name\n",
      "        self.trial_name = config.worker_info.trial_name\n",
      "\n",
      "        # manager server\n",
      "        self.manager_http_server = None\n",
      "        self.thread = None\n",
      "\n",
      "        self.server_urls = []\n",
      "\n",
      "        # recover info\n",
      "        self.__recover_run, self.__recover_info = recover.load_recover_info()\n",
      "        if self.__recover_run:\n",
      "            # update weights will be automatically triggered upon the first schedule_request\n",
      "            # self._last_param_realloc_step will also be updated\n",
      "            name = names.model_version(\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                self.model_name.role,\n",
      "            )\n",
      "            name_resolve.add(name, self.__recover_info.last_step_info.global_step)\n",
      "\n",
      "            self._loaded_recover_weights = False\n",
      "            hist_rollouts = (\n",
      "                self.config.train_batch_size\n",
      "                * self.__recover_info.last_step_info.global_step\n",
      "            )\n",
      "            self.rollout_stat.submitted = hist_rollouts\n",
      "            self.rollout_stat.accepted = hist_rollouts\n",
      "\n",
      "        return config.worker_info\n",
      "\n",
      "    def _discover_servers(self, n_servers: int, timeout: int = 300) -> List[str]:\n",
      "        logger.info(f\"Waiting for {n_servers} generation servers...\")\n",
      "        name = names.gen_servers(self.experiment_name, self.trial_name)\n",
      "        cnt = 0\n",
      "        while len(name_resolve.find_subtree(name)) < n_servers:\n",
      "            time.sleep(1)\n",
      "            cnt += 1\n",
      "            if cnt >= timeout:\n",
      "                raise TimeoutError(\"Waiting generation servers timeout.\")\n",
      "        urls = name_resolve.get_subtree(name)\n",
      "        assert len(set(urls)) == len(urls), (len(urls), len(set(urls)), urls)\n",
      "        return urls\n",
      "\n",
      "    def _get_recover_ckpt_path(self, role: str):\n",
      "        assert self.__recover_run\n",
      "        epoch = self.__recover_info.last_step_info.epoch + 1\n",
      "        epochstep = self.__recover_info.last_step_info.epoch_step + 1\n",
      "        globalstep = self.__recover_info.last_step_info.global_step + 1\n",
      "        save_root = os.path.join(\n",
      "            constants.MODEL_SAVE_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "        )\n",
      "        role_path = os.path.join(save_root, role)\n",
      "        if not os.path.exists(role_path):\n",
      "            raise RuntimeError(\n",
      "                f\"Guessed checkpoint path {role_path} does not exist. \"\n",
      "                \"Skip loading checkpoints in the recovered run.\"\n",
      "            )\n",
      "        model_path = os.path.join(\n",
      "            role_path,\n",
      "            f\"epoch{epoch}epochstep{epochstep}globalstep{globalstep}\",\n",
      "        )\n",
      "        if not os.path.exists(model_path):\n",
      "            raise RuntimeError(\n",
      "                f\"Guessed checkpoint path {model_path} does not exist. \"\n",
      "                \"Skip loading checkpoints in the recovered run.\"\n",
      "            )\n",
      "        return model_path\n",
      "\n",
      "    def check_new_params(self) -> str | None:\n",
      "        name = names.model_version(\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            self.model_name.role,\n",
      "        )\n",
      "        try:\n",
      "            realloc_version = int(name_resolve.get(name))\n",
      "        except name_resolve.NameEntryNotFoundError:\n",
      "            return None\n",
      "\n",
      "        # Update the model weights after parameter realloction.\n",
      "        if realloc_version > self._last_param_realloc_step:\n",
      "            if self.__recover_run and not self._loaded_recover_weights:\n",
      "                realloc_dir = self._get_recover_ckpt_path(self.model_name.role)\n",
      "                self._loaded_recover_weights = True\n",
      "            else:\n",
      "                realloc_dir = os.path.join(\n",
      "                    constants.PARAM_REALLOC_PATH,\n",
      "                    constants.experiment_name(),\n",
      "                    constants.trial_name(),\n",
      "                    self.model_name.role,\n",
      "                    str(realloc_version),\n",
      "                )\n",
      "            self._last_param_realloc_step = realloc_version\n",
      "            return realloc_dir\n",
      "\n",
      "        return None\n",
      "\n",
      "    async def flush_requests_and_update_weights(\n",
      "        self, server_url, new_param_path, update_weights_retries=5\n",
      "    ):\n",
      "        server_index = self.server_urls.index(server_url)\n",
      "        success = False\n",
      "        for _ in range(update_weights_retries):\n",
      "            async with aiohttp.ClientSession(\n",
      "                server_url,\n",
      "                timeout=aiohttp.ClientTimeout(\n",
      "                    total=self.config.flush_request_timeout,\n",
      "                    sock_connect=self.config.flush_request_timeout,\n",
      "                ),\n",
      "            ) as session:\n",
      "                async with session.post(\n",
      "                    f\"/update_weights_from_disk\",\n",
      "                    json=dict(model_path=new_param_path, allow_interrupt=True),\n",
      "                ) as resp:\n",
      "                    if resp.status == 200:\n",
      "                        res = await resp.json()\n",
      "                        success = res[\"success\"]\n",
      "                        if success:\n",
      "                            if \"num_paused_requests\" in res:\n",
      "                                logger.info(\n",
      "                                    f\"{res['num_paused_requests']} requests are interrupted \"\n",
      "                                    f\"during updating weights for server {server_index}: {server_url}\"\n",
      "                                )\n",
      "                            return\n",
      "                        logger.warning(\n",
      "                            f\"Update weights failed: {res['message']}. Retrying.\"\n",
      "                        )\n",
      "                    logger.warning(f\"Update weights failed: {resp.reason}. Retrying.\")\n",
      "                time.sleep(0.1)\n",
      "        raise RuntimeError(\"Update weights failed.\")\n",
      "\n",
      "    def _round_robin_schedule(self, req_meta: GenReqMeta) -> int:\n",
      "        if not hasattr(self, \"round_robin_idx\"):\n",
      "            self.round_robin_idx = 0\n",
      "        r = self.round_robin_idx\n",
      "        self.round_robin_idx += 1\n",
      "        self.round_robin_idx %= self.config.n_servers\n",
      "        return r\n",
      "\n",
      "    def _least_requests_schedule(self, req_meta: GenReqMeta) -> int:\n",
      "        counts = [\n",
      "            self._server_request_counts[server_url] for server_url in self.server_urls\n",
      "        ]\n",
      "        return int(np.argmin(counts))\n",
      "\n",
      "    def _least_token_usage_schedule(self, req_meta: GenReqMeta) -> int:\n",
      "        url = min(self.server_urls, key=lambda k: self._server_token_usage[k])\n",
      "        return self.server_urls.index(url)\n",
      "\n",
      "    async def _poll_async(self):\n",
      "        if not self.thread:\n",
      "            # Find addresses of generation servers\n",
      "            self.server_urls = self._discover_servers(self.config.n_servers)\n",
      "            self.thread = threading.Thread(\n",
      "                target=self._run_routing_service, daemon=True\n",
      "            )\n",
      "            self.thread.start()\n",
      "            time.sleep(3)  # Wait briefly for server to start\n",
      "            # Write address for clients\n",
      "            name = names.gen_server_manager(self.experiment_name, self.trial_name)\n",
      "            name_resolve.add(name, self.manager_addr)\n",
      "            logger.info(\n",
      "                f\"GserverManager HTTP service started in background thread at {self.manager_addr}\"\n",
      "            )\n",
      "\n",
      "        # Check experiment finish.\n",
      "        name = names.experiment_status(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        try:\n",
      "            exp_status = name_resolve.wait(name, timeout=300)\n",
      "            if exp_status != str(ExpStatus.RUNNING):\n",
      "                self.exit()\n",
      "                return PollResult(0, 0)\n",
      "        except TimeoutError:\n",
      "            raise TimeoutError(\n",
      "                f\"Waiting for experiment status timeout. \"\n",
      "                \"This indicates that the master worker is not running. Exit the worker.\"\n",
      "            )\n",
      "\n",
      "        # Check weights.\n",
      "        with self.threading_lock:\n",
      "            # FIXME: we create a sync point across servers to update weights,\n",
      "            # but we can acutally update them individually\n",
      "            new_param_path = self.check_new_params()\n",
      "            if new_param_path is not None:\n",
      "\n",
      "                def _run_in_thread():\n",
      "                    # Create a new event loop for this thread\n",
      "                    new_loop = asyncio.new_event_loop()\n",
      "                    asyncio.set_event_loop(new_loop)\n",
      "                    tasks = [\n",
      "                        self.flush_requests_and_update_weights(base_url, new_param_path)\n",
      "                        for base_url in self.server_urls\n",
      "                    ]\n",
      "                    try:\n",
      "                        return new_loop.run_until_complete(asyncio.gather(*tasks))\n",
      "                    finally:\n",
      "                        new_loop.close()\n",
      "\n",
      "                from concurrent.futures import ThreadPoolExecutor\n",
      "\n",
      "                with ThreadPoolExecutor() as executor:\n",
      "                    future = executor.submit(_run_in_thread)\n",
      "                    _ = future.result()\n",
      "                logger.info(f\"Generaion server updated weights from: {new_param_path}\")\n",
      "\n",
      "        if self.schedule_policy == \"least_token_usage\":\n",
      "            tasks = [\n",
      "                self._get_server_token_usage(server_url)\n",
      "                for server_url in self.server_urls\n",
      "            ]\n",
      "            loop = asyncio.get_event_loop()\n",
      "            token_usages = loop.run_until_complete(asyncio.gather(*tasks))\n",
      "            with self.threading_lock:\n",
      "                for server_url, token_usage in zip(self.server_urls, token_usages):\n",
      "                    self._server_token_usage[server_url] = token_usage\n",
      "\n",
      "        if time.time() - self._last_thpt_output_time > 30:\n",
      "            interval = time.time() - self._last_thpt_output_time\n",
      "            logger.info(\n",
      "                f\"Generation throughput: {self._gen_tokens / interval:.2f} tokens/s\"\n",
      "            )\n",
      "            self._last_thpt_output_time = time.time()\n",
      "            self._gen_tokens = 0\n",
      "\n",
      "        # clear old weights\n",
      "        realloc_root = os.path.join(\n",
      "            constants.PARAM_REALLOC_PATH,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            self.model_name.role,\n",
      "        )\n",
      "        if os.path.exists(realloc_root):\n",
      "            for realloc_version in os.listdir(realloc_root):\n",
      "                # Lock-free is safe here.\n",
      "                # Remain one checkpoint for recover.\n",
      "                if (\n",
      "                    os.path.isdir(os.path.join(realloc_root, realloc_version))\n",
      "                    and int(realloc_version) < self._last_param_realloc_step - 1\n",
      "                ):\n",
      "                    shutil.rmtree(os.path.join(realloc_root, realloc_version))\n",
      "                    logger.info(\n",
      "                        f\"Removed previous reallocated \"\n",
      "                        f\"checkpoint: {os.path.join(realloc_root, realloc_version)}\"\n",
      "                    )\n",
      "\n",
      "        time.sleep(5)\n",
      "\n",
      "        return PollResult(0, 0)\n",
      "\n",
      "    async def _get_server_token_usage(self, server_url):\n",
      "        async with aiohttp.ClientSession(\n",
      "            server_url,\n",
      "            timeout=aiohttp.ClientTimeout(\n",
      "                total=self.config.flush_request_timeout,\n",
      "                sock_connect=self.config.flush_request_timeout,\n",
      "            ),\n",
      "        ) as session:\n",
      "            async with session.get(\"/metrics\") as resp:\n",
      "                resp.raise_for_status()\n",
      "                text = await resp.text()\n",
      "                for l in text.split(\"\\n\"):\n",
      "                    if l.startswith(\"sglang:num_used_tokens\"):\n",
      "                        return float(l.split(\" \")[1])\n",
      "        raise RuntimeError(f\"Failed to get token usage metrics from {server_url}\")\n",
      "\n",
      "    async def _get_server_num_running_requests(self, server_url):\n",
      "        async with aiohttp.ClientSession(\n",
      "            server_url,\n",
      "            timeout=aiohttp.ClientTimeout(\n",
      "                total=self.config.flush_request_timeout,\n",
      "                sock_connect=self.config.flush_request_timeout,\n",
      "            ),\n",
      "        ) as session:\n",
      "            async with session.get(f\"/metrics\") as resp:\n",
      "                resp.raise_for_status()\n",
      "                text = await resp.text()\n",
      "                for line in text.split(\"\\n\"):\n",
      "                    if line.startswith(\"sglang:num_running_reqs\"):\n",
      "                        return float(line.split(\" \")[1])\n",
      "        raise RuntimeError(\n",
      "            f\"Failed to get num running requests metrics from {server_url}\"\n",
      "        )\n",
      "\n",
      "    def get_training_sample_cnt(self):\n",
      "        name = names.training_samples(self.experiment_name, self.trial_name)\n",
      "        try:\n",
      "            return int(name_resolve.get(name))\n",
      "        except name_resolve.NameEntryNotFoundError:\n",
      "            return 0\n",
      "\n",
      "    def is_staled(self):\n",
      "        # Use counter written by the trainer, local counter is inaccurate\n",
      "        global_sample_cnt = self.get_training_sample_cnt() + self.rollout_stat.running\n",
      "        expected_version = global_sample_cnt // self.config.train_batch_size\n",
      "        version = self._last_param_realloc_step\n",
      "        staled = expected_version > self.config.max_head_offpolicyness + version\n",
      "        global STALENESS_WARNED\n",
      "        if staled and not STALENESS_WARNED[version]:\n",
      "            logger.warning(\n",
      "                f\"expected version ({expected_version}) = \"\n",
      "                f\"global sample cnt ({global_sample_cnt}) // batch size ({self.config.train_batch_size}), \"\n",
      "                f\"current latest version {version}, \"\n",
      "                f\"offpolicyness {self.config.max_head_offpolicyness}. Staled? {staled}\"\n",
      "            )\n",
      "            STALENESS_WARNED[version] = True\n",
      "        return staled\n",
      "\n",
      "    def _run_routing_service(self):\n",
      "        \"\"\"Expose an API for clients to find the destination server.\"\"\"\n",
      "        import uvicorn\n",
      "        from fastapi import FastAPI\n",
      "\n",
      "        self.app = FastAPI()\n",
      "\n",
      "        @self.app.post(\"/schedule_request\")\n",
      "        async def schedule_request(req_meta: GenReqMeta):\n",
      "            with self.threading_lock:\n",
      "                if (\n",
      "                    req_meta.previous_server_url\n",
      "                    and req_meta.previous_version == self._last_param_realloc_step\n",
      "                ):\n",
      "                    return dict(\n",
      "                        url=req_meta.previous_server_url,\n",
      "                        version=req_meta.previous_version,\n",
      "                    )\n",
      "\n",
      "                if self.schedule_policy == \"round_robin\":\n",
      "                    server_idx = self._round_robin_schedule(req_meta)\n",
      "                elif self.schedule_policy == \"least_token_usage\":\n",
      "                    server_idx = self._least_token_usage_schedule(req_meta)\n",
      "                elif self.schedule_policy == \"least_requests\":\n",
      "                    server_idx = self._least_requests_schedule(req_meta)\n",
      "                else:\n",
      "                    raise NotImplementedError(\n",
      "                        f\"Unknown schedule policy {self.schedule_policy}\"\n",
      "                    )\n",
      "\n",
      "                server_url = self.server_urls[server_idx]\n",
      "                # qid prompt (n samples) use the same dst server\n",
      "                self._qid_to_server_url[req_meta.qid] = server_url\n",
      "                self._server_request_counts[server_url] += 1\n",
      "                self._server_token_usage[server_url] += (\n",
      "                    req_meta.prompt_len\n",
      "                    + req_meta.new_token_budget * req_meta.group_size * 0.4\n",
      "                )\n",
      "\n",
      "                version = self._last_param_realloc_step\n",
      "            return dict(url=server_url, version=version)\n",
      "\n",
      "        @self.app.post(\"/get_model_version\")\n",
      "        async def get_model_version(req: ModelVersionReq):\n",
      "            with self.threading_lock:\n",
      "                # FIXME: we may have different versions for different servers\n",
      "                version = self._last_param_realloc_step\n",
      "            return dict(version=version)\n",
      "\n",
      "        @self.app.post(\"/allocate_rollout\")\n",
      "        async def allocate_rollout(req: AllocateRolloutInput):\n",
      "            with self.threading_lock:\n",
      "                has_capacity = (\n",
      "                    self.rollout_stat.running < self.config.max_concurrent_rollouts\n",
      "                )\n",
      "                is_staled = self.is_staled()\n",
      "                reason = \"\"\n",
      "                if has_capacity and not is_staled:\n",
      "                    self.rollout_stat.submitted += 1\n",
      "                    self.rollout_stat.running += 1\n",
      "                    logger.debug(\n",
      "                        f\"Allocate rollout for qid {req.qid}. \"\n",
      "                        f\"Submitted: {self.rollout_stat.submitted}, \"\n",
      "                        f\"running: {self.rollout_stat.running}, \"\n",
      "                        f\"accepted: {self.rollout_stat.accepted}.\"\n",
      "                    )\n",
      "                    return dict(success=True, reason=reason)\n",
      "                else:\n",
      "                    if not has_capacity:\n",
      "                        reason += f\"capacity: {self.rollout_stat.running} >= {self.config.max_concurrent_rollouts}\"\n",
      "                    if is_staled:\n",
      "                        global_sample_cnt = (\n",
      "                            self.get_training_sample_cnt() + self.rollout_stat.running\n",
      "                        )\n",
      "                        expected_version = (\n",
      "                            global_sample_cnt // self.config.train_batch_size\n",
      "                        )\n",
      "                        version = self._last_param_realloc_step\n",
      "                        reason += (\n",
      "                            f\" and staled: expected version ({expected_version}) = \"\n",
      "                            f\"global sample cnt ({global_sample_cnt}) // batch size ({self.config.train_batch_size}), \"\n",
      "                            f\"current latest version {version}, \"\n",
      "                            f\"offpolicyness {self.config.max_head_offpolicyness}.\"\n",
      "                        )\n",
      "                    return dict(success=False, reason=reason)\n",
      "\n",
      "        @self.app.post(\"/finish_rollout\")\n",
      "        async def finish_rollout(resp_meta: GenRespMeta):\n",
      "            with self.threading_lock:\n",
      "                server_url = self._qid_to_server_url[resp_meta.qid]\n",
      "                self._server_request_counts[server_url] -= 1\n",
      "                assert (\n",
      "                    self._server_request_counts[server_url] >= 0\n",
      "                ), \"server request count < 0\"\n",
      "                self._qid_to_server_url.pop(resp_meta.qid)\n",
      "                self._gen_tokens += resp_meta.n_tokens\n",
      "                self.rollout_stat.running -= 1\n",
      "                if resp_meta.accepted:\n",
      "                    self.rollout_stat.accepted += 1\n",
      "                logger.debug(\n",
      "                    f\"Finish rollout for qid {resp_meta.qid}. \"\n",
      "                    f\"Submit: {self.rollout_stat.submitted}, \"\n",
      "                    f\"running: {self.rollout_stat.running}, \"\n",
      "                    f\"accepted: {self.rollout_stat.accepted}\"\n",
      "                )\n",
      "                return dict(success=True)\n",
      "\n",
      "        port = network.find_free_port(\n",
      "            experiment_name=self.experiment_name,\n",
      "            trial_name=self.trial_name,\n",
      "        )\n",
      "        self.manager_addr = f\"{network.gethostip()}:{port}\"\n",
      "\n",
      "        config = uvicorn.Config(\n",
      "            self.app,\n",
      "            host=self.manager_addr.split(\":\")[0],\n",
      "            port=int(self.manager_addr.split(\":\")[1]),\n",
      "            log_level=\"warning\",\n",
      "        )\n",
      "        self.manager_http_server = uvicorn.Server(config)\n",
      "        self.manager_http_server.run()\n",
      "\n",
      "    def _exit_hook(self, exit_status):\n",
      "        if self.manager_http_server:\n",
      "            self.manager_http_server.should_exit = True\n",
      "        if self.thread:\n",
      "            self.thread.join(timeout=3)\n",
      "        logger.info(\"Server stopped\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/rollout_worker.py ====\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "import queue\n",
      "import time\n",
      "from asyncio.queues import QueueEmpty\n",
      "from typing import Dict, Hashable, List\n",
      "\n",
      "import aiohttp\n",
      "import numpy as np\n",
      "import torch.utils.data\n",
      "from aiohttp.client import ClientTimeout\n",
      "\n",
      "from realhf.api.core.agent_api import make_agent\n",
      "from realhf.api.core.data_api import SequenceSample, load_hf_tokenizer, make_dataset\n",
      "from realhf.api.core.env_api import make_env\n",
      "from realhf.api.core.system_api import ExpStatus\n",
      "from realhf.api.core.system_api import RolloutWorker as RolloutWorkerConfig\n",
      "from realhf.base import (\n",
      "    constants,\n",
      "    datapack,\n",
      "    logging,\n",
      "    name_resolve,\n",
      "    names,\n",
      "    recover,\n",
      "    seeding,\n",
      ")\n",
      "from realhf.base.monitor import RolloutStat\n",
      "from realhf.system.partial_rollout import PartialRolloutManager\n",
      "from realhf.system.push_pull_stream import NameResolvingZmqPusher\n",
      "from realhf.system.worker_base import AsyncWorker, PollResult\n",
      "\n",
      "# NOTE: Register all implemented agents\n",
      "import realhf.impl.environment  # isort:skip\n",
      "import realhf.impl.agent  # isort:skip\n",
      "\n",
      "logger = logging.getLogger(\"RolloutWorker\")\n",
      "\n",
      "# Should be equal to the poll time of partial rollout\n",
      "ROLLOUT_POLL_WAIT_TIME = 0.4\n",
      "\n",
      "\n",
      "class RolloutWorker(AsyncWorker):\n",
      "    def _configure(self, config: RolloutWorkerConfig):\n",
      "        self.model_name = config.model_name\n",
      "\n",
      "        self.config = config\n",
      "        self.worker_index = config.worker_info.worker_index\n",
      "        self.worker_count = config.worker_info.worker_count\n",
      "\n",
      "        self.experiment_name = config.worker_info.experiment_name\n",
      "        self.trial_name = config.worker_info.trial_name\n",
      "\n",
      "        self.env = make_env(config.env)\n",
      "        self.agent = make_agent(config.agent)\n",
      "\n",
      "        self.rollout_request_queue = asyncio.Queue(1024)\n",
      "        self.rollout_response_queue = asyncio.Queue(1024)\n",
      "\n",
      "        self.act_queues = {}\n",
      "        self.rollout_tasks = {}\n",
      "\n",
      "        self.inference_maker = PartialRolloutManager(\n",
      "            worker_index=self.worker_index,\n",
      "            request_queue=self.rollout_request_queue,\n",
      "            reply_queue=self.rollout_response_queue,\n",
      "            new_tokens_per_chunk=config.new_tokens_per_chunk,\n",
      "            tokenizer=load_hf_tokenizer(config.tokenizer_path),\n",
      "            timeout=self.config.rollout_request_timeout,\n",
      "        )\n",
      "        self.push_stream = None\n",
      "\n",
      "        seeding.set_random_seed(\n",
      "            config.base_seed, f\"rollout_worker{config.worker_info.worker_index}\"\n",
      "        )\n",
      "\n",
      "        self.data_generator = None\n",
      "        self.is_new_epoch = False\n",
      "\n",
      "        self._cur_data = None\n",
      "\n",
      "        self.gserver_manager_addr = None\n",
      "        self.rollout_tasks: Dict[Hashable, asyncio.Task] = {}\n",
      "\n",
      "        # Since the rollout worker doesn't compute staleness,\n",
      "        # we don't need to recover rollout_stat here.\n",
      "        self.rollout_stat = RolloutStat()\n",
      "\n",
      "        # recover info\n",
      "        self.__recover_run, self.__recover_info = recover.load_recover_info()\n",
      "\n",
      "        return config.worker_info\n",
      "\n",
      "    def make_datasets(self):\n",
      "        # Make datasets.\n",
      "        datasets = [\n",
      "            make_dataset(\n",
      "                d,\n",
      "                # NOTE: we must use the same seed to ensure the same dataset split\n",
      "                self.config.base_seed,\n",
      "                self.worker_index,\n",
      "                self.worker_count,\n",
      "                self.config.tokenizer_path,\n",
      "                self.config.worker_info.experiment_name,\n",
      "                self.config.worker_info.trial_name,\n",
      "                cache_root=(\n",
      "                    None\n",
      "                    if not self.config.use_dataset_cache\n",
      "                    else self.config.dataset_cahce_root\n",
      "                ),\n",
      "            )\n",
      "            for d in self.config.datasets\n",
      "        ]\n",
      "        if len(self.config.datasets) == 1:\n",
      "            self.dataset = datasets[0]\n",
      "        else:\n",
      "            self.dataset = torch.utils.data.ConcatDataset(datasets)\n",
      "        self.dataset_size = len(self.dataset)\n",
      "        g = torch.Generator()\n",
      "        g.manual_seed(seeding.get_seed())\n",
      "        self.dataloader = torch.utils.data.DataLoader(\n",
      "            self.dataset,\n",
      "            batch_size=None,\n",
      "            shuffle=True,\n",
      "            generator=g,\n",
      "        )\n",
      "        self.data_generator = enumerate(self.dataloader)\n",
      "\n",
      "        # Recover indices for dynamic dataset\n",
      "        if hasattr(self.dataset, \"filter\"):\n",
      "            dataset_indices_path = os.path.join(\n",
      "                constants.MODEL_SAVE_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                f\"dataset_indices_{self.worker_index}.npy\",\n",
      "            )\n",
      "            if os.path.exists(dataset_indices_path):\n",
      "                indices = np.load(dataset_indices_path).tolist()\n",
      "                logger.info(\n",
      "                    f\"DP rank {self.worker_index} updating dataset indices upon recover, \"\n",
      "                    f\"size {len(self.dataset.active_indices)} -> {len(indices)}\"\n",
      "                )\n",
      "                self.dataset.active_indices = indices\n",
      "\n",
      "    def load_next_data(self):\n",
      "        # Create an epoch-wise barrier to prevent data over-consumption.\n",
      "        if self.is_new_epoch:\n",
      "            if len(self.rollout_tasks) > 0:\n",
      "                return None\n",
      "            self.is_new_epoch = False\n",
      "\n",
      "        # Fetch.\n",
      "        try:\n",
      "            _, cur_sample = next(self.data_generator)\n",
      "        except StopIteration:\n",
      "            self.is_new_epoch = True\n",
      "            # Upon the first fetch request, filter dataset and create dataloader.\n",
      "            eval_scores_path = os.path.join(\n",
      "                constants.MODEL_SAVE_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"dataset_eval_scores.json\",\n",
      "            )\n",
      "            dataset_indices_path = os.path.join(\n",
      "                constants.MODEL_SAVE_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                f\"dataset_indices_{self.worker_index}.npy\",\n",
      "            )\n",
      "            if hasattr(self.dataset, \"filter\") and os.path.exists(eval_scores_path):\n",
      "                # Don't filter dataset on the first poll after recover.\n",
      "                with open(eval_scores_path, \"r\", encoding=\"utf-8\") as f:\n",
      "                    dataset_eval_scores = json.load(f)\n",
      "                self.dataset.filter(dataset_eval_scores)\n",
      "                # Save the dataset indices after filtering\n",
      "                np.save(\n",
      "                    dataset_indices_path,\n",
      "                    self.dataset.active_indices,\n",
      "                )\n",
      "            g = torch.Generator()\n",
      "            g = g.set_state(self.dataloader.generator.get_state())\n",
      "            self.dataloader = torch.utils.data.DataLoader(\n",
      "                self.dataset,\n",
      "                batch_size=None,\n",
      "                shuffle=True,\n",
      "                generator=g,\n",
      "            )\n",
      "            self.data_generator = enumerate(self.dataloader)\n",
      "            return None\n",
      "\n",
      "        # NOTE: no need to ignore ids during recover, because model workers will do so\n",
      "        data_id = cur_sample.ids[0]\n",
      "        if self.__recover_run and data_id in self.__recover_info.hash_vals_to_ignore:\n",
      "            self.__recover_info.hash_vals_to_ignore.remove(data_id)\n",
      "            return None\n",
      "        assert data_id not in self.rollout_tasks\n",
      "        return cur_sample\n",
      "\n",
      "    async def allocate_new_rollout(self, qid) -> bool:\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            async with session.post(\n",
      "                f\"http://{self.gserver_manager_addr}/allocate_rollout\",\n",
      "                json=dict(qid=qid),\n",
      "                timeout=ClientTimeout(\n",
      "                    total=self.config.rollout_request_timeout,\n",
      "                    sock_connect=self.config.rollout_request_timeout,\n",
      "                ),\n",
      "            ) as resp:\n",
      "                resp.raise_for_status()\n",
      "                res = await resp.json()\n",
      "                if not res[\"success\"]:\n",
      "                    logger.debug(\n",
      "                        f\"Cannot allocate new rollout because: {res['reason']}\"\n",
      "                    )\n",
      "                return res[\"success\"]\n",
      "\n",
      "    async def _poll_async(self):\n",
      "        # Lazily initializing dataset to avoid over long configuration time.\n",
      "        if self.data_generator is None:\n",
      "            tik = time.perf_counter()\n",
      "            logger.info(f\"Rollout worker {self.worker_index} making datasets..\")\n",
      "            self.make_datasets()\n",
      "            logger.info(\n",
      "                f\"Rollout worker {self.worker_index} finishes making datasets. \"\n",
      "                f\"Time consumed: {time.perf_counter() - tik}s\"\n",
      "            )\n",
      "\n",
      "        # Check experiment finish.\n",
      "        name = names.experiment_status(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        try:\n",
      "            exp_status = name_resolve.wait(name, timeout=300)\n",
      "            if exp_status != str(ExpStatus.RUNNING):\n",
      "                self.exit()\n",
      "                return PollResult(0, 0)\n",
      "        except TimeoutError:\n",
      "            raise TimeoutError(\n",
      "                f\"Waiting for experiment status timeout. \"\n",
      "                \"This indicates that the master worker is not running. Exit the worker.\"\n",
      "            )\n",
      "\n",
      "        if self.push_stream is None:\n",
      "            # Initialize stream after configure to ensure that puller names have been written.\n",
      "            self.push_stream = NameResolvingZmqPusher(\n",
      "                self.experiment_name,\n",
      "                self.trial_name,\n",
      "                pusher_index=self.worker_index,\n",
      "                pusher_cnt=self.worker_count,\n",
      "            )\n",
      "\n",
      "        if self.gserver_manager_addr is None:\n",
      "            name = names.gen_server_manager(self.experiment_name, self.trial_name)\n",
      "            self.gserver_manager_addr = name_resolve.wait(name)\n",
      "\n",
      "        # Create new trajectory collection tasks.\n",
      "        # Load only one data in each poll to avoid over consumption.\n",
      "        if self._cur_data is None:\n",
      "            self._cur_data = self.load_next_data()\n",
      "\n",
      "        if self._cur_data is not None:\n",
      "            data = self._cur_data\n",
      "            qid = data.ids[0]\n",
      "            can_rollout = await self.allocate_new_rollout(qid)\n",
      "            if can_rollout:\n",
      "                assert qid not in self.act_queues\n",
      "                self.act_queues[qid] = asyncio.Queue(1024)\n",
      "\n",
      "                task = asyncio.create_task(self.rollout_task(qid, data))\n",
      "                assert qid not in self.rollout_tasks\n",
      "                self.rollout_tasks[qid] = task\n",
      "\n",
      "                self._cur_data = None\n",
      "\n",
      "                self.rollout_stat.submitted += 1\n",
      "                self.rollout_stat.running += 1\n",
      "                logger.debug(\n",
      "                    f\"Submit a new rollout for qid {qid}. \"\n",
      "                    f\"Submit: {self.rollout_stat.submitted}, \"\n",
      "                    f\"running: {self.rollout_stat.running}, \"\n",
      "                    f\"accepted: {self.rollout_stat.accepted}.\"\n",
      "                )\n",
      "\n",
      "        # Run rollouts and wait\n",
      "        done, *_ = await asyncio.gather(\n",
      "            self.poll_rollout_task(),\n",
      "            self.poll_queue_dispatch_task(),\n",
      "            self.poll_inference_task(),\n",
      "        )\n",
      "\n",
      "        # Process done tasks.\n",
      "        batch_count = sample_count = 0\n",
      "        for task in done:\n",
      "            qid, trajs = await task\n",
      "            trajs: List[SequenceSample]\n",
      "            assert len(set(traj.ids[0] for traj in trajs)) == len(trajs), [\n",
      "                traj.ids[0] for traj in trajs\n",
      "            ]\n",
      "            self.rollout_tasks.pop(qid)\n",
      "            self.act_queues.pop(qid)\n",
      "\n",
      "            self.rollout_stat.running -= 1\n",
      "\n",
      "            accepted = False\n",
      "            if len(trajs) > 0:\n",
      "                accepted = True\n",
      "                self.push_stream.push([traj.as_json_compatible() for traj in trajs])\n",
      "                self.rollout_stat.accepted += 1\n",
      "\n",
      "            n_tokens = 0\n",
      "            for traj in trajs:\n",
      "                seqlens = [sum(datapack.flat2d(ss)) for ss in traj.seqlens.values()]\n",
      "                n_tokens += max(seqlens)\n",
      "            info = dict(qid=qid, accepted=accepted, n_tokens=n_tokens)\n",
      "            async with aiohttp.ClientSession(\n",
      "                f\"http://{self.gserver_manager_addr}\"\n",
      "            ) as session:\n",
      "                async with session.post(\n",
      "                    \"/finish_rollout\",\n",
      "                    json=info,\n",
      "                    timeout=ClientTimeout(\n",
      "                        total=self.config.rollout_request_timeout,\n",
      "                        sock_connect=self.config.rollout_request_timeout,\n",
      "                    ),\n",
      "                ) as resp:\n",
      "                    resp.raise_for_status()\n",
      "                    assert (await resp.json())[\"success\"]\n",
      "            logger.debug(\n",
      "                f\"Finish rollout for qid {qid}. \"\n",
      "                f\"Submit: {self.rollout_stat.submitted}, \"\n",
      "                f\"running: {self.rollout_stat.running}, \"\n",
      "                f\"accepted: {self.rollout_stat.accepted}.\"\n",
      "            )\n",
      "\n",
      "            for traj in trajs:\n",
      "                batch_count += traj.bs\n",
      "                sample_count += max(\n",
      "                    [sum(datapack.flat2d(slens)) for slens in traj.seqlens.values()]\n",
      "                )\n",
      "\n",
      "        return PollResult(batch_count, sample_count)\n",
      "\n",
      "    async def rollout_task(self, qid, data):\n",
      "        return qid, await self.agent.collect_trajectory(\n",
      "            env=self.env,\n",
      "            prompt=data,\n",
      "            act_queue=self.act_queues[qid],\n",
      "            obs_queue=self.rollout_request_queue,\n",
      "        )\n",
      "\n",
      "    async def poll_inference_task(self):\n",
      "        await self.inference_maker.run_step()\n",
      "\n",
      "    async def poll_rollout_task(self):\n",
      "        tasks = list(self.rollout_tasks.values())\n",
      "        done = []\n",
      "        if tasks:\n",
      "            done, _ = await asyncio.wait(\n",
      "                tasks,\n",
      "                timeout=ROLLOUT_POLL_WAIT_TIME,\n",
      "                return_when=asyncio.FIRST_COMPLETED,\n",
      "            )\n",
      "        return done\n",
      "\n",
      "    async def poll_queue_dispatch_task(self):\n",
      "        for _ in range(20):\n",
      "            try:\n",
      "                resp = self.rollout_response_queue.get_nowait()\n",
      "                self.act_queues[resp.qid].put_nowait(resp)\n",
      "            except QueueEmpty:\n",
      "                await asyncio.sleep(0.02)\n",
      "\n",
      "    async def _exit_async_tasks(self):\n",
      "        for task in self.rollout_tasks.values():\n",
      "            task.cancel()\n",
      "            try:\n",
      "                await task\n",
      "            except asyncio.CancelledError:\n",
      "                pass\n",
      "\n",
      "    def _exit_hook(self, exit_status):\n",
      "        if self.push_stream is not None:\n",
      "            self.push_stream.close()\n",
      "        loop = asyncio.get_event_loop()\n",
      "        loop.run_until_complete(self._exit_async_tasks())\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/model_function_call.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import itertools\n",
      "import json\n",
      "import os\n",
      "import time\n",
      "import uuid\n",
      "from collections import defaultdict\n",
      "from typing import Dict, Hashable, List, Set, Tuple\n",
      "\n",
      "import wandb\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import realhf.api.core.config as config_api\n",
      "import realhf.api.core.data_api as data_api\n",
      "import realhf.api.core.dfg as dfg\n",
      "import realhf.api.core.system_api as config_pkg\n",
      "import realhf.base.recover as recover\n",
      "import realhf.system.request_reply_stream as request_reply_stream\n",
      "from realhf.api.core.config import ModelName, ModelShardID\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.base import constants, logging, stats_tracker, topology\n",
      "from realhf.system.buffer import AsyncIOSequenceBuffer\n",
      "from realhf.system.flops_counter import FlopsCounter\n",
      "from realhf.system.redistributor import RedistribPlanner, RedistribStep\n",
      "\n",
      "logger = logging.getLogger(__name__, \"system\")\n",
      "blogger = logging.getLogger(\"benchmark\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RPCCorountineControl:\n",
      "    # for counting the number of finished training steps\n",
      "    # one training step corresponds to traversal of the whole DFG\n",
      "    train_count: asyncio.Queue\n",
      "    # For flushing requests\n",
      "    topo_level_count: asyncio.Queue\n",
      "\n",
      "    lock: asyncio.Lock\n",
      "    # for training data management and data cleaning after each step\n",
      "    ids_to_clear: Set[Hashable] = dataclasses.field(default_factory=set)\n",
      "    flops_counter: FlopsCounter = dataclasses.field(default_factory=FlopsCounter)\n",
      "\n",
      "    should_save: bool = False\n",
      "    should_eval: bool = False\n",
      "    should_ckpt: bool = False\n",
      "    step_info: recover.StepInfo = dataclasses.field(default_factory=recover.StepInfo)\n",
      "\n",
      "    # recover information\n",
      "    used_hash_vals_this_epoch: List[int] = dataclasses.field(default_factory=list)\n",
      "\n",
      "\n",
      "class ModelFunctionCall:\n",
      "    def __init__(\n",
      "        self,\n",
      "        rpc: dfg.MFCDef,\n",
      "        src_rpc: dfg.MFCDef,\n",
      "        stream: request_reply_stream.NameResolvingRequestClient,\n",
      "        msid2mwid: Dict[config_pkg.ModelShardID, int],\n",
      "        model_topos: Dict[str, topology.ProcessTopology],\n",
      "        model_configs: Dict[str, None | ReaLModelConfig],\n",
      "        ctrl: RPCCorountineControl,\n",
      "        buffers: List[AsyncIOSequenceBuffer],\n",
      "        redistrib_planner: RedistribPlanner,\n",
      "        summary_writer: SummaryWriter | None,\n",
      "    ):\n",
      "\n",
      "        self.rpc = rpc\n",
      "        self.src_rpc = src_rpc\n",
      "        self.stream = stream\n",
      "\n",
      "        self.n_model_workers = len(set(msid2mwid.values()))\n",
      "\n",
      "        self.msid2mwid = msid2mwid\n",
      "        self.model_topos = model_topos\n",
      "        self.model_configs = model_configs\n",
      "\n",
      "        self.mwid2msids = defaultdict(list)\n",
      "        for msid, mwid in msid2mwid.items():\n",
      "            self.mwid2msids[mwid].append(msid)\n",
      "\n",
      "        self.model_save_root = os.path.join(\n",
      "            constants.MODEL_SAVE_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "        )\n",
      "\n",
      "        self.rpc_ctrl = ctrl\n",
      "        self.buffers = buffers\n",
      "        self.redistrib_planner = redistrib_planner\n",
      "\n",
      "        self.summary_writer = summary_writer\n",
      "\n",
      "    @property\n",
      "    def dp_size(self):\n",
      "        return self.model_topos[self.rpc.model_name].get_dim(\"data\")\n",
      "\n",
      "    @property\n",
      "    def pp_size(self):\n",
      "        return self.model_topos[self.rpc.model_name].get_dim(\"pipe\")\n",
      "\n",
      "    def attach_payloads_with_hooks(\n",
      "        self,\n",
      "        payloads: Dict[config_api.ModelShardID, request_reply_stream.Payload],\n",
      "        mwids: List[int],\n",
      "        main_handlers: List[config_pkg.ModelShardID],\n",
      "        hook_type: str,\n",
      "    ) -> Tuple[Dict[config_api.ModelShardID, request_reply_stream.Payload], List[int]]:\n",
      "        assert hook_type in [\"pre\", \"post\"], hook_type\n",
      "\n",
      "        rpc = self.rpc\n",
      "        model_topos = self.model_topos\n",
      "        model_configs = self.model_configs\n",
      "\n",
      "        main_mwids = set([self.msid2mwid[h] for h in main_handlers])\n",
      "        for hook in getattr(rpc, f\"_{hook_type}_hooks\"):\n",
      "            if isinstance(hook, dfg.ParamReallocHook):\n",
      "                assert (hook.source is None) != (hook.target is None), hook\n",
      "                if hook.source is None:\n",
      "                    src_topo = model_topos[rpc.model_name]\n",
      "                    dst_topo = model_topos[hook.target]\n",
      "                    dst_config = model_configs[hook.target]\n",
      "                    src_model_name, dst_model_name = rpc.model_name, hook.target\n",
      "                    other_model_name = hook.target\n",
      "                    other_topo = dst_topo\n",
      "                else:\n",
      "                    src_topo = model_topos[hook.source]\n",
      "                    dst_topo = model_topos[rpc.model_name]\n",
      "                    dst_config = model_configs[rpc.model_name]\n",
      "                    src_model_name, dst_model_name = hook.source, rpc.model_name\n",
      "                    other_model_name = hook.source\n",
      "                    other_topo = src_topo\n",
      "\n",
      "                ps_data = {\n",
      "                    \"from_model_name\": src_model_name,\n",
      "                    \"to_model_name\": dst_model_name,\n",
      "                    \"from_topo\": src_topo,\n",
      "                    \"to_topo\": dst_topo,\n",
      "                    \"to_model_config\": dst_config,\n",
      "                    \"eta\": hook.eta,\n",
      "                }\n",
      "                for h in main_handlers:\n",
      "                    getattr(payloads[h], f\"{hook_type}_hooks\").append(\"param_realloc\")\n",
      "                    getattr(payloads[h], f\"{hook_type}_hook_data\").append(ps_data)\n",
      "                other_handlers = [\n",
      "                    config_api.ModelShardID.from_parallelism_rank(\n",
      "                        other_model_name, other_topo, j\n",
      "                    )\n",
      "                    for j in range(other_topo.world_size())\n",
      "                ]\n",
      "                for h in other_handlers:\n",
      "                    if self.msid2mwid[h] not in mwids:\n",
      "                        payloads[h] = request_reply_stream.Payload(\n",
      "                            handler=h,\n",
      "                            handle_name=\"empty\",\n",
      "                        )\n",
      "                        setattr(payloads[h], f\"{hook_type}_hooks\", [\"param_realloc\"])\n",
      "                        setattr(payloads[h], f\"{hook_type}_hook_data\", [ps_data])\n",
      "                        mwids.append(self.msid2mwid[h])\n",
      "                    elif self.msid2mwid[h] not in main_mwids:\n",
      "                        hh = next(\n",
      "                            hh\n",
      "                            for hh in payloads\n",
      "                            if self.msid2mwid[hh] == self.msid2mwid[h]\n",
      "                        )\n",
      "                        getattr(payloads[hh], f\"{hook_type}_hooks\").append(\n",
      "                            \"param_realloc\"\n",
      "                        )\n",
      "                        getattr(payloads[hh], f\"{hook_type}_hook_data\").append(ps_data)\n",
      "\n",
      "            elif isinstance(hook, dfg.OffloadHook):\n",
      "                for h in main_handlers:\n",
      "                    getattr(payloads[h], f\"{hook_type}_hooks\").append(\"offload\")\n",
      "                    getattr(payloads[h], f\"{hook_type}_hook_data\").append(\n",
      "                        dict(model_name=h.model_name)\n",
      "                    )\n",
      "            else:\n",
      "                raise NotImplementedError(f\"Unknown hook type: {hook}\")\n",
      "        return payloads, mwids\n",
      "\n",
      "    def request(\n",
      "        self,\n",
      "        data_transfer_plan: List[RedistribStep],\n",
      "        partitioned_ids: List[Hashable],\n",
      "        meta_sample: data_api.SequenceSample,\n",
      "        handlers: List[config_pkg.ModelShardID],\n",
      "    ) -> Tuple[List[uuid.UUID], List[uuid.UUID]]:\n",
      "\n",
      "        rpc = self.rpc\n",
      "        ctrl = self.rpc_ctrl\n",
      "\n",
      "        dt_data = {\n",
      "            \"target\": rpc.model_name,\n",
      "            \"plan\": [json.dumps(dataclasses.asdict(x)) for x in data_transfer_plan],\n",
      "            \"partitioned_ids\": partitioned_ids,\n",
      "            \"handle_name\": rpc.interface_type.value,\n",
      "            \"meta_sample\": meta_sample,\n",
      "            \"partitioned_ids\": partitioned_ids,\n",
      "        }\n",
      "\n",
      "        payloads = {\n",
      "            handler: request_reply_stream.Payload(\n",
      "                handler=handler,\n",
      "                handle_name=rpc.interface_type.value,\n",
      "                pre_hooks=[\"data_transfer\"],\n",
      "                pre_hook_data=[dt_data],\n",
      "                data=rpc.name,\n",
      "            )\n",
      "            for handler in handlers\n",
      "        }\n",
      "        if ctrl.should_eval:\n",
      "            for p in payloads.values():\n",
      "                p.post_hooks.append(\"evaluate\")\n",
      "                p.post_hook_data.append(dict(model_name=rpc.model_name))\n",
      "        if (\n",
      "            ctrl.should_save or ctrl.should_ckpt\n",
      "        ) and rpc.interface_type == dfg.ModelInterfaceType.TRAIN_STEP:\n",
      "            for p in payloads.values():\n",
      "                p.post_hooks.append(\"save\")\n",
      "                save_dir = os.path.join(\n",
      "                    self.model_save_root,\n",
      "                    rpc.model_name.role,\n",
      "                    f\"epoch{ctrl.step_info.epoch + 1}\"\n",
      "                    f\"epochstep{ctrl.step_info.epoch_step + 1}\"\n",
      "                    f\"globalstep{ctrl.step_info.global_step + 1}\",\n",
      "                )\n",
      "                p.post_hook_data.append(\n",
      "                    dict(\n",
      "                        model_name=rpc.model_name,\n",
      "                        save_dir=save_dir,\n",
      "                        recover_only=not ctrl.should_save,\n",
      "                    )\n",
      "                )\n",
      "        mwids = [self.msid2mwid[h] for h in handlers]\n",
      "        assert len(mwids) == len(set(mwids))\n",
      "\n",
      "        for step in data_transfer_plan:\n",
      "            if step.root not in mwids:\n",
      "                handler = self.mwid2msids[step.root][0]\n",
      "                payloads[handler] = request_reply_stream.Payload(\n",
      "                    handler=handler,\n",
      "                    handle_name=\"empty\",\n",
      "                    pre_hooks=[\"data_transfer\"],\n",
      "                    pre_hook_data=[dt_data],\n",
      "                )\n",
      "                mwids.append(step.root)\n",
      "            if step.comm_type == \"gather\":\n",
      "                for src in step.srcs:\n",
      "                    if src not in mwids:\n",
      "                        handler = self.mwid2msids[src][0]\n",
      "                        payloads[handler] = request_reply_stream.Payload(\n",
      "                            handler=handler,\n",
      "                            handle_name=\"empty\",\n",
      "                            pre_hooks=[\"data_transfer\"],\n",
      "                            pre_hook_data=[dt_data],\n",
      "                        )\n",
      "                        mwids.append(src)\n",
      "\n",
      "        payloads, mwids = self.attach_payloads_with_hooks(\n",
      "            payloads,\n",
      "            mwids,\n",
      "            main_handlers=handlers,\n",
      "            hook_type=\"pre\",\n",
      "        )\n",
      "        payloads, mwids = self.attach_payloads_with_hooks(\n",
      "            payloads,\n",
      "            mwids,\n",
      "            main_handlers=handlers,\n",
      "            hook_type=\"post\",\n",
      "        )\n",
      "        main_payloads = [p for h, p in payloads.items() if h in handlers]\n",
      "        other_payloads = [p for h, p in payloads.items() if h not in handlers]\n",
      "        all_req_ids = self.stream.request(\n",
      "            payloads=main_payloads + other_payloads,\n",
      "        )\n",
      "        return all_req_ids[: len(main_payloads)], all_req_ids[len(main_payloads) :]\n",
      "\n",
      "    def data_parallel_dispatch(\n",
      "        self, buf_indices: List[int], sample: data_api.SequenceSample\n",
      "    ) -> Tuple[List[int], data_api.SequenceSample, List[Tuple[int, int]]]:\n",
      "        # Dispatch data to different data parallel ranks.\n",
      "        if self.rpc.is_generate():\n",
      "            # The workload of generation is decided by batch size, instead of the generated length.\n",
      "            lens = [1 for _ in range(sample.bs)]\n",
      "            samples, forward_indices, _ = sample.split_with_lengths(\n",
      "                mb_spec=data_api.MicroBatchSpec(n_mbs=self.dp_size),\n",
      "                lens=lens,\n",
      "            )\n",
      "        else:\n",
      "            samples, forward_indices, _ = sample.split(\n",
      "                data_api.MicroBatchSpec(n_mbs=self.dp_size)\n",
      "            )\n",
      "        blogger.debug(\n",
      "            f\"DP split (DP size {self.dp_size}) for RPC {self.rpc.name}: \"\n",
      "            f\"#seqs: {[s.bs for s in samples]}, \"\n",
      "            f\"#tokens: {[sum([sum(lens) for lens in s.seqlens[s._get_split_key()]]) for s in samples]}\"\n",
      "        )\n",
      "        sample = data_api.SequenceSample.gather(samples)\n",
      "        buf_indices = [buf_indices[i] for i in forward_indices]\n",
      "\n",
      "        partitions = data_api.SequenceSplitSpec(\n",
      "            sizes=[s.bs for s in samples]\n",
      "        ).partitions\n",
      "        return buf_indices, sample, partitions\n",
      "\n",
      "    async def run_step(self, buf_indices, sample, buffer_id: int):\n",
      "        rpc = self.rpc\n",
      "        topo = self.model_topos[rpc.model_name]\n",
      "        ctrl = self.rpc_ctrl\n",
      "\n",
      "        handlers = [\n",
      "            config_pkg.ModelShardID.from_parallelism_rank(rpc.model_name, topo, j)\n",
      "            for j in range(topo.world_size())\n",
      "        ]\n",
      "\n",
      "        dp_head_indices = [\n",
      "            topo.get_rank(data=i, pipe=topo.get_dim(\"pipe\") - 1, tensor=0)\n",
      "            for i in range(self.dp_size)\n",
      "        ]\n",
      "\n",
      "        async with ctrl.lock:\n",
      "            ctrl.flops_counter.add_rpc(rpc, sample, self.model_configs[rpc.model_name])\n",
      "\n",
      "        # logger.info(f\"Model rpc {rpc.name} requesting.\")\n",
      "\n",
      "        # Sample may be reordered here.\n",
      "        buf_indices, sample, partitions = self.data_parallel_dispatch(\n",
      "            buf_indices, sample\n",
      "        )\n",
      "\n",
      "        # Build data destinations: GPU id -> List[data ids]\n",
      "        partitioned_ids = []\n",
      "        dests = {}\n",
      "        for dp_rank, (st, ed) in enumerate(partitions):\n",
      "            ranks = topo.filter_match(data=dp_rank)\n",
      "            for rank in ranks:\n",
      "                h = config_pkg.ModelShardID.from_parallelism_rank(\n",
      "                    model_name=rpc.model_name, topo=topo, parallelism_rank=rank\n",
      "                )\n",
      "                gpu_id = self.msid2mwid[h]\n",
      "                assert gpu_id not in dests\n",
      "                dests[gpu_id] = sample.ids[st:ed]\n",
      "            partitioned_ids.append(sample.ids[st:ed])\n",
      "        for i in range(self.n_model_workers):\n",
      "            if i not in dests:\n",
      "                dests[i] = []\n",
      "\n",
      "        pattern = \"gather-scatter\"\n",
      "        data_transfer_plan = self.redistrib_planner.derive_plan(\n",
      "            dests,\n",
      "            keys=rpc.input_keys,\n",
      "            pattern=pattern,\n",
      "        )\n",
      "        blogger.debug(f\"Data tranfer plan for `{rpc.name}`: {data_transfer_plan}.\")\n",
      "\n",
      "        # Update storage tracker for transferred data.\n",
      "        if pattern == \"bcast\":\n",
      "            # NOTE: since the data we loaded may be unevenly distributed across DP ranks,\n",
      "            # we should change the owner of the data to the src RPC.\n",
      "            for i in range(topo.world_size()):\n",
      "                h = ModelShardID.from_parallelism_rank(\n",
      "                    model_name=rpc.model_name, topo=topo, parallelism_rank=i\n",
      "                )\n",
      "                is_dp_head = h.tp_rank == 0 and h.pp_rank == topo.get_dim(\"pipe\") - 1\n",
      "                gpu_id = self.msid2mwid[h]\n",
      "                for key in rpc.input_keys:\n",
      "                    await self.redistrib_planner.storage_tracker.add_data(\n",
      "                        gpu_id, partitioned_ids[h.dp_rank], key=key, is_owner=is_dp_head\n",
      "                    )\n",
      "        else:\n",
      "            for step in data_transfer_plan:\n",
      "                if step.comm_type == \"scatter\":\n",
      "                    for gpu_id, ids in zip(step.dsts, step.ids):\n",
      "                        for key in step.keys:\n",
      "                            await self.redistrib_planner.storage_tracker.add_data(\n",
      "                                gpu_id, ids, key=key, is_owner=False\n",
      "                            )\n",
      "                elif step.comm_type == \"gather\":\n",
      "                    for key in step.keys:\n",
      "                        await self.redistrib_planner.storage_tracker.add_data(\n",
      "                            step.root,\n",
      "                            list(itertools.chain.from_iterable(step.ids)),\n",
      "                            key=key,\n",
      "                            is_owner=False,\n",
      "                        )\n",
      "\n",
      "        await asyncio.sleep(0)\n",
      "        # send partitioned data to model workers\n",
      "        req_ids, other_req_ids = self.request(\n",
      "            data_transfer_plan=data_transfer_plan,\n",
      "            partitioned_ids=partitioned_ids,\n",
      "            meta_sample=sample,\n",
      "            handlers=handlers,\n",
      "        )\n",
      "        tik = time.perf_counter()\n",
      "\n",
      "        await ctrl.topo_level_count.put(1)\n",
      "        logger.info(f\"Model rpc {rpc.name} requested.\")\n",
      "\n",
      "        # Then, wait for all main requests to finish.\n",
      "        responses = await self.stream.gather_async(request_ids=req_ids)\n",
      "        # logger.info(f\"rpc {rpc.name} received responses {req_ids}\")\n",
      "\n",
      "        # Filter out responses other than DP heads.\n",
      "        # Other repsonses are duplicated or None.\n",
      "        responses, time_records = list(zip(*[responses[i] for i in dp_head_indices]))\n",
      "\n",
      "        # If the returned data is a SequenceSample, it is the data returned by\n",
      "        # model function calls. The data should be amended into buffer.\n",
      "        # Otherwise, it's the train statistics and should be reduced and logged.\n",
      "        if isinstance(responses[-1], data_api.SequenceSample):\n",
      "            # Update storage tracker for generated data.\n",
      "            for dp_rank, x in enumerate(responses):\n",
      "                pp_size = topo.get_dim(\"pipe\")\n",
      "                ranks = topo.filter_match(data=dp_rank, pipe=pp_size - 1, tensor=0)\n",
      "                for rank in ranks:\n",
      "                    h = config_pkg.ModelShardID.from_parallelism_rank(\n",
      "                        model_name=rpc.model_name, topo=topo, parallelism_rank=rank\n",
      "                    )\n",
      "                    gpu_id = self.msid2mwid[h]\n",
      "                    for k in rpc.output_keys:\n",
      "                        await self.redistrib_planner.storage_tracker.add_data(\n",
      "                            gpu_id,\n",
      "                            x.ids,\n",
      "                            key=k,\n",
      "                            is_owner=True,\n",
      "                        )\n",
      "            res = data_api.SequenceSample.gather(responses)\n",
      "        elif isinstance(responses[0], dict):\n",
      "            res = data_api.gather_stat(responses)\n",
      "        else:\n",
      "            assert isinstance(responses[0], list)\n",
      "            res = [\n",
      "                data_api.gather_stat([r[i] for r in responses])\n",
      "                for i in range(len(responses[0]))\n",
      "            ]\n",
      "\n",
      "        if rpc.log_return_value:\n",
      "            if isinstance(res, dict):\n",
      "                logger.info(\n",
      "                    f\"RPC name {rpc.name} returns\\n{data_api.tabulate_stats(res)}\"\n",
      "                )\n",
      "                logging.log_wandb_tensorboard(\n",
      "                    res,\n",
      "                    step=ctrl.step_info.global_step,\n",
      "                    summary_writer=self.summary_writer,\n",
      "                )\n",
      "            elif isinstance(res, list):\n",
      "                for j, r in enumerate(res):\n",
      "                    logger.info(\n",
      "                        f\"RPC name {rpc.name} returns ({j + 1}/{len(res)})\\n{data_api.tabulate_stats(r)}\"\n",
      "                    )\n",
      "                    offset = len(res) * ctrl.step_info.global_step\n",
      "                    logging.log_wandb_tensorboard(\n",
      "                        r,\n",
      "                        step=offset + j,\n",
      "                        summary_writer=self.summary_writer,\n",
      "                    )\n",
      "            else:\n",
      "                logger.info(f\"RPC name {rpc.name} returns\\n{res}\")\n",
      "\n",
      "        # Log rpc execution time.\n",
      "        for time_record in time_records:\n",
      "            stats_tracker.scalar(**time_record)\n",
      "        time_stats = stats_tracker.export()\n",
      "        logging.log_wandb_tensorboard(\n",
      "            time_stats,\n",
      "            summary_writer=self.summary_writer,\n",
      "        )\n",
      "\n",
      "        logger.info(\n",
      "            f\"Model rpc {rpc.name} finished. \"\n",
      "            f\"Request-reply time {time.perf_counter() - tik:.4f}s. \"\n",
      "            f\"Detailed time stats:\\n{data_api.tabulate_stats(time_stats, floatfmt='.2f')}.\"\n",
      "        )\n",
      "\n",
      "        # If this RPC is the final node in the dataflow graph,\n",
      "        # update the train counter.\n",
      "        # Otherwise, amend data in the buffer.\n",
      "        if rpc.is_dst:\n",
      "            async with ctrl.lock:\n",
      "                ctrl.ids_to_clear = ctrl.ids_to_clear.union(sample.ids)\n",
      "            await ctrl.train_count.put(1)\n",
      "        else:\n",
      "            logger.info(f\"Amending RPC {rpc.name} output keys: {res.keys}\")\n",
      "            await self.buffers[buffer_id].amend_batch(buf_indices, res.unpack())\n",
      "\n",
      "        # Wait for all side-effect requests to finish.\n",
      "        # Side-effect or empty requests are required for data transfer\n",
      "        # and parameter synchronization.\n",
      "        # Wait them after the main request to log the oorrect MFC time.\n",
      "        await self.stream.gather_async(other_req_ids)\n",
      "\n",
      "    async def run(self, buffer_id: int):\n",
      "        rpc = self.rpc\n",
      "        topo = self.model_topos[rpc.model_name]\n",
      "\n",
      "        logger.info(\n",
      "            f\"Running Model RPC, interface_type=#{rpc.interface_type}# \"\n",
      "            f\"(dp,tp,pp) = *({topo.get_dim('data')},{topo.get_dim('tensor')},{topo.get_dim('pipe')})*\"\n",
      "        )\n",
      "\n",
      "        consumed = 0\n",
      "        while True:\n",
      "            buf_indices, sample = await self.buffers[buffer_id].get_batch_for_rpc(rpc)\n",
      "\n",
      "            await self.run_step(buf_indices, sample, buffer_id)\n",
      "            consumed += sample.bs\n",
      "\n",
      "            # Ensure that parent RPCs will not be over-consumed.\n",
      "            if all(consumed >= c.n_seqs for c in rpc.all_successors()):\n",
      "                break\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/request_reply_stream.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# Request-reply stream between model workers and the master worker.\n",
      "# The stream is composed of a pair of ZMQ sockets, one PUSH and one PULL, for asynchronous communication,\n",
      "# i.e., the model worker can buffer requests from the master and execute them in any order under the hood.\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import pickle\n",
      "import re\n",
      "import socket\n",
      "import time\n",
      "import uuid\n",
      "from typing import Any, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import zmq\n",
      "\n",
      "import realhf.api.core.system_api as system_api\n",
      "from realhf.base import logging, name_resolve, names\n",
      "\n",
      "logger = logging.getLogger(\"Request-Replay Stream\")\n",
      "ZMQ_IO_THREADS = 8\n",
      "\n",
      "PUBSUB_BARRIER_NAME = \"__pubsub_barrier__\"\n",
      "\n",
      "\n",
      "def create_exact_match_pattern(string_list: List[Union[uuid.UUID, str]]) -> re.Pattern:\n",
      "    \"\"\"Create exact match patterns for filtering out the desired respones.\n",
      "\n",
      "    The pattern is used to filter request IDs.\n",
      "    \"\"\"\n",
      "    escaped_strings = [re.escape(str(s)) for s in string_list]\n",
      "    pattern = f\"({'|'.join(escaped_strings)})$\"\n",
      "    return re.compile(pattern)\n",
      "\n",
      "\n",
      "class NoMessage(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class NoResponse:\n",
      "    pass\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class Payload:\n",
      "    handler: Union[system_api.ModelShardID, str]\n",
      "    handle_name: str\n",
      "\n",
      "    request_id: uuid.UUID = None\n",
      "    syn_reply_id: uuid.UUID = None\n",
      "    ack_reply_id: uuid.UUID = None\n",
      "\n",
      "    no_syn: bool = True\n",
      "\n",
      "    send_time: float = None\n",
      "\n",
      "    # Non-tensor data\n",
      "    data: Any = None\n",
      "\n",
      "    # RPC hooks\n",
      "    pre_hooks: List[str] = dataclasses.field(default_factory=list)\n",
      "    pre_hook_data: List[Any] = dataclasses.field(default_factory=list)\n",
      "\n",
      "    post_hooks: List[str] = dataclasses.field(default_factory=list)\n",
      "    post_hook_data: List[Any] = dataclasses.field(default_factory=list)\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.request_id is None:\n",
      "            self.request_id = uuid.uuid4()\n",
      "        if self.syn_reply_id is None:\n",
      "            self.syn_reply_id = uuid.uuid4()\n",
      "        if self.ack_reply_id is None:\n",
      "            self.ack_reply_id = uuid.uuid4()\n",
      "\n",
      "\n",
      "class NameResolvingRequestClient:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        experiment_name: str,\n",
      "        trial_name: str,\n",
      "        n_subscribers: int,\n",
      "        handler_routing: Dict[str | system_api.ModelShardID, int],\n",
      "    ):\n",
      "\n",
      "        self.context = zmq.Context.instance(io_threads=ZMQ_IO_THREADS)\n",
      "        self.context.set(zmq.MAX_SOCKETS, 65536)\n",
      "\n",
      "        host_ip = socket.gethostbyname(socket.gethostname())\n",
      "\n",
      "        self.send_sockets: List[zmq.Socket] = []\n",
      "        for i in range(n_subscribers):\n",
      "            s: zmq.Socket = self.context.socket(zmq.PUSH)\n",
      "            send_port = s.bind_to_random_port(f\"tcp://{host_ip}\")\n",
      "            s.setsockopt(zmq.LINGER, 0)\n",
      "\n",
      "            master_send_name = names.request_reply_stream(\n",
      "                experiment_name, trial_name, f\"master_send_{i}\"\n",
      "            )\n",
      "            name_resolve.add(name=master_send_name, value=f\"{host_ip}:{send_port}\")\n",
      "            logger.debug(\n",
      "                f\"Add master send address {host_ip}:{send_port} as {master_send_name}\"\n",
      "            )\n",
      "            self.send_sockets.append(s)\n",
      "\n",
      "        self.recv_socket: zmq.Socket = self.context.socket(zmq.PULL)\n",
      "        recv_port = self.recv_socket.bind_to_random_port(f\"tcp://{host_ip}\")\n",
      "        self.recv_socket.setsockopt(zmq.LINGER, 0)\n",
      "        self.recv_address = f\"{host_ip}:{recv_port}\"\n",
      "\n",
      "        master_recv_name = names.request_reply_stream(\n",
      "            experiment_name, trial_name, \"master_recv\"\n",
      "        )\n",
      "        name_resolve.add(name=master_recv_name, value=self.recv_address)\n",
      "        logger.debug(\n",
      "            f\"Add master send address {self.recv_address} as {master_recv_name}\"\n",
      "        )\n",
      "\n",
      "        self._response_buffer: Dict[uuid.UUID, Payload] = {}\n",
      "        self._handler_routing = handler_routing\n",
      "\n",
      "        # master needs to wait all peers (subscribers) to connect\n",
      "        while (\n",
      "            len(\n",
      "                name_resolve.get_subtree(\n",
      "                    names.request_reply_stream(\n",
      "                        experiment_name, trial_name, PUBSUB_BARRIER_NAME\n",
      "                    )\n",
      "                )\n",
      "            )\n",
      "            < n_subscribers\n",
      "        ):\n",
      "            time.sleep(0.1)\n",
      "        logger.debug(\n",
      "            f\"Master discovered all {n_subscribers} \"\n",
      "            f\"subscribers: {name_resolve.get_subtree(names.request_reply_stream(experiment_name, trial_name, PUBSUB_BARRIER_NAME))}.\"\n",
      "        )\n",
      "\n",
      "    def route_to(self, handler) -> int:\n",
      "        return self._handler_routing[handler]\n",
      "\n",
      "    def close(self):\n",
      "        self.recv_socket.close()\n",
      "        for send_socket in self.send_sockets:\n",
      "            send_socket.close()\n",
      "        self.context.destroy()\n",
      "\n",
      "    def __del__(self):\n",
      "        self.close()\n",
      "\n",
      "    def post(self, payload: Payload) -> uuid.UUID:\n",
      "        assert payload.request_id is not None and payload.handle_name is not None\n",
      "        payload.send_time = time.monotonic()\n",
      "        idx = self._handler_routing[payload.handler]\n",
      "        self.send_sockets[idx].send(pickle.dumps(payload))\n",
      "        return payload.request_id\n",
      "\n",
      "    def request(\n",
      "        self,\n",
      "        handlers: List[str | int] | None = None,\n",
      "        handle_type: str | None = None,\n",
      "        datas: List[Any] | None = None,\n",
      "        payloads: List[Payload] | None = None,\n",
      "        verbose: bool = True,\n",
      "        no_syn: bool = True,\n",
      "    ) -> List[uuid.UUID]:\n",
      "        \"\"\"Send requests of type `handle_type` to all `handlers` with\n",
      "        corresponding `data`.\n",
      "\n",
      "        If no_syn is True, only send the requests without\n",
      "        synchronization.\n",
      "\n",
      "        Otherwise, wait for handlers' SYN message and respond with an\n",
      "        ACK. This protocol ensures that all handlers will receive the\n",
      "        request after this function exits.\n",
      "        \"\"\"\n",
      "        if payloads is not None:\n",
      "            if datas is not None:\n",
      "                raise RuntimeError(\"Cannot specify both `datas` and `payloads`.\")\n",
      "            requests = payloads\n",
      "            handle_type = payloads[0].handle_name\n",
      "        else:\n",
      "            if datas is None:\n",
      "                datas = [None] * len(handlers)\n",
      "            requests = [\n",
      "                Payload(\n",
      "                    handler=handler,\n",
      "                    handle_name=handle_type,\n",
      "                    data=data,\n",
      "                    no_syn=no_syn,\n",
      "                )\n",
      "                for handler, data in zip(handlers, datas)\n",
      "            ]\n",
      "        if verbose:\n",
      "            logger.debug(f\"master worker #request_all# *end* time ${time.time_ns()}$\")\n",
      "        tik = time.perf_counter()\n",
      "\n",
      "        # A protocol to ensure that any model worker execute jobs in the same order.\n",
      "        [self.post(r) for r in requests]\n",
      "        if not no_syn:\n",
      "            [\n",
      "                self.poll(\n",
      "                    block=True, pattern=create_exact_match_pattern([p.syn_reply_id])\n",
      "                )\n",
      "                for p in requests\n",
      "            ]\n",
      "            [\n",
      "                self.post(\n",
      "                    Payload(\n",
      "                        handler=r.handler, handle_name=\"ack\", request_id=r.ack_reply_id\n",
      "                    )\n",
      "                )\n",
      "                for r in requests\n",
      "            ]\n",
      "        t = time.perf_counter() - tik\n",
      "\n",
      "        if verbose:\n",
      "            logger.debug(\n",
      "                f'Request \"{handle_type}\" time in total: '\n",
      "                f\"{t:.4f}s, {t / len(requests):.4f}s per request\"\n",
      "            )\n",
      "        return [r.request_id for r in requests]\n",
      "\n",
      "    def call(\n",
      "        self,\n",
      "        handlers: List[str | int] | None = None,\n",
      "        handle_type: str | None = None,\n",
      "        datas: List[Any] | None = None,\n",
      "        payloads: List[Payload] | None = None,\n",
      "        verbose: bool = True,\n",
      "    ):\n",
      "        req_ids = self.request(\n",
      "            handlers=handlers,\n",
      "            handle_type=handle_type,\n",
      "            datas=datas,\n",
      "            payloads=payloads,\n",
      "            verbose=verbose,\n",
      "        )\n",
      "        return self.gather(req_ids, verbose=verbose)\n",
      "\n",
      "    async def call_async(\n",
      "        self,\n",
      "        handlers,\n",
      "        handle_type: str,\n",
      "        datas: List,\n",
      "        verbose: bool = True,\n",
      "    ) -> List:\n",
      "        return await self.gather_async(\n",
      "            self.request(handlers, handle_type, datas, verbose=verbose),\n",
      "        )\n",
      "\n",
      "    def poll(self, pattern: re.Pattern | None = None, block: bool = False) -> Payload:\n",
      "        payloads = self.poll_batch(pattern=pattern, block=block)\n",
      "        for p in payloads[1:]:\n",
      "            self._response_buffer[p.request_id] = p\n",
      "        return payloads[0]\n",
      "\n",
      "    async def poll_async(self, pattern: re.Pattern | None = None) -> Payload:\n",
      "        while True:\n",
      "            try:\n",
      "                return self.poll(pattern=pattern, block=False)\n",
      "            except NoMessage:\n",
      "                await asyncio.sleep(0.01)\n",
      "                continue\n",
      "\n",
      "    def gather(self, request_ids: List[uuid.UUID], verbose: bool = True) -> List[Any]:\n",
      "        responses = [\n",
      "            self.poll(pattern=create_exact_match_pattern([req_id]), block=True)\n",
      "            for req_id in request_ids\n",
      "        ]\n",
      "        if verbose:\n",
      "            logger.debug(f\"master #gather_replies# *end* time ${time.time_ns()}$\")\n",
      "        return [r.data for r in responses]\n",
      "\n",
      "    async def gather_async(\n",
      "        self, request_ids: List[uuid.UUID], verbose: bool = True\n",
      "    ) -> List[Payload]:\n",
      "        responses = await asyncio.gather(\n",
      "            *[\n",
      "                self.poll_async(pattern=create_exact_match_pattern([req_id]))\n",
      "                for req_id in request_ids\n",
      "            ]\n",
      "        )\n",
      "        if verbose:\n",
      "            logger.debug(f\"master #async_gather_replies# *end* time ${time.time_ns()}$\")\n",
      "        return [r.data for r in responses]\n",
      "\n",
      "    def poll_batch(\n",
      "        self, pattern: re.Pattern | None = None, block: bool = False\n",
      "    ) -> List[Payload]:\n",
      "        \"\"\"Collect responses that match some pattern from the stream.\n",
      "\n",
      "        This function may NOT actually pull from the stream. It may fetch something\n",
      "        from the buffer, which records mismatched responses.\n",
      "\n",
      "        Args:\n",
      "            pattern (Optional[re.Pattern], optional): Only responses with this\n",
      "                specific regex pattern will be returned.\n",
      "                None means no pattern specified. Defaults to None.\n",
      "            block (bool, optional): Whether to block to receive a\n",
      "                response (with the given pattern). Defaults to False.\n",
      "        \"\"\"\n",
      "        if not block:\n",
      "            return self._poll_batch_nonblock(pattern)\n",
      "        else:\n",
      "            while True:\n",
      "                try:\n",
      "                    return self._poll_batch_nonblock(pattern)\n",
      "                except NoMessage:\n",
      "                    time.sleep(0.05)\n",
      "\n",
      "    def _poll_batch_nonblock(\n",
      "        self, pattern: Optional[re.Pattern] = None\n",
      "    ) -> List[Payload]:\n",
      "        # Check whether there's response in the buffer.\n",
      "        # If so, return immediately.\n",
      "        if pattern is None:\n",
      "            pattern = re.compile(\".*\")\n",
      "\n",
      "        payloads = []\n",
      "        for req_id, p in self._response_buffer.items():\n",
      "            if pattern.match(str(req_id)):\n",
      "                payloads.append(p)\n",
      "        for p in payloads:\n",
      "            self._response_buffer.pop(p.request_id)\n",
      "        if len(payloads) > 0:\n",
      "            return payloads\n",
      "\n",
      "        # Otherwise, pull from the socket.\n",
      "        try:\n",
      "            p_bytes = self.recv_socket.recv(flags=zmq.NOBLOCK)\n",
      "        except zmq.ZMQError:\n",
      "            raise NoMessage()\n",
      "        payload: Payload = pickle.loads(p_bytes)\n",
      "        # logger.info(f\"Payload transfer time: {time.monotonic() - payload.send_time:.4f}s\")\n",
      "        self._response_buffer[payload.request_id] = payload\n",
      "\n",
      "        payloads = []\n",
      "        for req_id, p in self._response_buffer.items():\n",
      "            if pattern.match(str(req_id)):\n",
      "                payloads.append(p)\n",
      "        for p in payloads:\n",
      "            self._response_buffer.pop(p.request_id)\n",
      "        if len(payloads) > 0:\n",
      "            return payloads\n",
      "        raise NoMessage()\n",
      "\n",
      "\n",
      "class NameResolvingReplyServer:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        experiment_name: str,\n",
      "        trial_name: str,\n",
      "        idx: int,\n",
      "    ):\n",
      "        self.context = zmq.Context.instance(io_threads=ZMQ_IO_THREADS)\n",
      "\n",
      "        send_name = names.request_reply_stream(\n",
      "            experiment_name, trial_name, \"master_recv\"\n",
      "        )\n",
      "        try:\n",
      "            master_recv_addr = name_resolve.wait(send_name, timeout=300)\n",
      "        except TimeoutError as e:\n",
      "            logger.error(f\"Worker timeout waiting for master receive stream.\")\n",
      "            raise e\n",
      "\n",
      "        recv_name = names.request_reply_stream(\n",
      "            experiment_name, trial_name, f\"master_send_{idx}\"\n",
      "        )\n",
      "        try:\n",
      "            master_send_addr = name_resolve.wait(recv_name, timeout=300)\n",
      "        except TimeoutError as e:\n",
      "            logger.error(f\"Worker timeout waiting for master send stream\")\n",
      "            raise e\n",
      "\n",
      "        self.accept(master_send_addr, master_recv_addr)\n",
      "\n",
      "        name_resolve.add_subentry(\n",
      "            name=names.request_reply_stream(\n",
      "                experiment_name, trial_name, PUBSUB_BARRIER_NAME\n",
      "            ),\n",
      "            value=socket.gethostbyname(socket.gethostname()),\n",
      "            keepalive_ttl=1200,\n",
      "        )\n",
      "\n",
      "    def accept(self, server_send_addr: str, server_recv_addr: str):\n",
      "        recv_socket: zmq.Socket = self.context.socket(zmq.PULL)\n",
      "        recv_socket.connect(f\"tcp://{server_send_addr}\")\n",
      "        recv_socket.setsockopt(zmq.LINGER, 0)\n",
      "        self.recv_socket = recv_socket\n",
      "\n",
      "        send_socket: zmq.Socket = self.context.socket(zmq.PUSH)\n",
      "        send_socket.connect(f\"tcp://{server_recv_addr}\")\n",
      "        send_socket.setsockopt(zmq.LINGER, 0)\n",
      "        self.send_socket = send_socket\n",
      "\n",
      "    def post(self, payload: Payload) -> uuid.UUID:\n",
      "        assert payload.request_id is not None and payload.handle_name is not None\n",
      "        payload.send_time = time.monotonic()\n",
      "        self.send_socket.send(pickle.dumps(payload))\n",
      "        return payload.request_id\n",
      "\n",
      "    def poll(self, block: bool = False) -> Payload:\n",
      "        try:\n",
      "            payload_bytes = self.recv_socket.recv(flags=0 if block else zmq.NOBLOCK)\n",
      "        except zmq.ZMQError:\n",
      "            raise NoMessage()\n",
      "\n",
      "        payload: Payload = pickle.loads(payload_bytes)\n",
      "        # logger.debug(f\"Payload transfer time: {time.monotonic() - payload.send_time:.4f}s\")\n",
      "        return payload\n",
      "\n",
      "    def close(self):\n",
      "        self.recv_socket.close()\n",
      "        self.send_socket.close()\n",
      "        self.context.destroy()\n",
      "\n",
      "    def __del__(self):\n",
      "        self.close()\n",
      "\n",
      "\n",
      "def make_master_stream(\n",
      "    worker_info: system_api.WorkerInformation,\n",
      "    n_subscribers: int,\n",
      "    handler_routing: Dict[str | system_api.ModelShardID, int],\n",
      ") -> NameResolvingRequestClient:\n",
      "    return NameResolvingRequestClient(\n",
      "        experiment_name=worker_info.experiment_name,\n",
      "        trial_name=worker_info.trial_name,\n",
      "        n_subscribers=n_subscribers,\n",
      "        handler_routing=handler_routing,\n",
      "    )\n",
      "\n",
      "\n",
      "def make_worker_stream(\n",
      "    worker_info: system_api.WorkerInformation,\n",
      "    idx: int,\n",
      ") -> NameResolvingReplyServer:\n",
      "    return NameResolvingReplyServer(\n",
      "        experiment_name=worker_info.experiment_name,\n",
      "        trial_name=worker_info.trial_name,\n",
      "        idx=idx,\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/controller.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import copy\n",
      "import dataclasses\n",
      "import enum\n",
      "import functools\n",
      "import getpass\n",
      "import json\n",
      "import os\n",
      "import re\n",
      "import socket\n",
      "import sys\n",
      "import threading\n",
      "import time\n",
      "import traceback\n",
      "from dataclasses import asdict\n",
      "from datetime import datetime\n",
      "from typing import Any, Dict, List, Optional, Tuple\n",
      "\n",
      "import colorama\n",
      "import ray\n",
      "import ray.util.queue as rq\n",
      "import torch\n",
      "from omegaconf import OmegaConf\n",
      "\n",
      "import realhf.api.core.system_api as system_api\n",
      "from realhf.base import constants, gpu_utils, logging, name_resolve, names, pkg_version\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.system import WORKER_TYPES, load_worker, worker_base, worker_control\n",
      "from realhf.system.worker_base import WorkerServerStatus as Wss\n",
      "\n",
      "flask_available = False\n",
      "if pkg_version.is_available(\"flask\"):\n",
      "\n",
      "    from flask import Flask, jsonify\n",
      "\n",
      "    app = Flask(__name__)\n",
      "\n",
      "    @app.route(\"/discovery\", methods=[\"GET\"])\n",
      "    def discovery():\n",
      "        key = names.metric_server_root(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        addresses = name_resolve.get_subtree(key)\n",
      "\n",
      "        result = []\n",
      "        if len(addresses) > 0:\n",
      "            result.append(\n",
      "                {\n",
      "                    \"targets\": addresses,\n",
      "                    \"labels\": {\n",
      "                        \"experiment\": constants.experiment_name(),\n",
      "                        \"trial\": constants.trial_name(),\n",
      "                    },\n",
      "                }\n",
      "            )\n",
      "\n",
      "        logger.info(f\"Discover metric servers: {result}\")\n",
      "        return jsonify(result)\n",
      "\n",
      "    def start_metric_discovery_server(port: int):\n",
      "        host_ip = socket.gethostbyname(socket.gethostname())\n",
      "        logger.info(f\"Start metric discovery server: http://{host_ip}:{port}/discovery\")\n",
      "        app.run(debug=False, use_reloader=False, host=\"0.0.0.0\", port=port)\n",
      "\n",
      "    flask_available = True\n",
      "\n",
      "\n",
      "CONNECTION_RETRY_AFTER_SECONDS = 360\n",
      "\n",
      "logger = logging.getLogger(\"controller\", \"colored\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TrialStatus:\n",
      "    experiment_name: str\n",
      "    trial_name: str\n",
      "    running_workers: Dict[str, List[str]] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TrialHistory:\n",
      "    experiment_name: str\n",
      "    trial_name: str\n",
      "    age_days: int\n",
      "\n",
      "\n",
      "class ControllerExitStatus(enum.Enum):\n",
      "    SUCCESS = 0\n",
      "    TIMEOUT = 1\n",
      "    INTERRUPTED = 9\n",
      "    FAIL = 101\n",
      "    LOST = 102\n",
      "    UNKNOWN = 404\n",
      "\n",
      "\n",
      "class Controller:\n",
      "\n",
      "    def __init__(\n",
      "        self, experiment_name, trial_name, panel: worker_base.WorkerControlPanel\n",
      "    ):\n",
      "        assert \"_\" not in experiment_name, (\n",
      "            f\"_ not allowed in experiment_name (args: -e) \"\n",
      "            f\"{experiment_name}, use '-' instead.\"\n",
      "        )\n",
      "        assert (\n",
      "            \"_\" not in trial_name\n",
      "        ), f\"_ not allowed in trial_name (args: -f) {trial_name}, use '-' instead.\"\n",
      "        self.experiment_name = experiment_name\n",
      "        self.trial_name = trial_name\n",
      "\n",
      "        logger.info(\"Experiment: %s %s\", self.experiment_name, self.trial_name)\n",
      "\n",
      "        self.__control = panel\n",
      "\n",
      "    def reconnect(self):\n",
      "        \"\"\"Automatically reconnect to workers.\n",
      "\n",
      "        And list all jobs to scheduler.\n",
      "        \"\"\"\n",
      "        self.__control.auto_connect()\n",
      "\n",
      "    def __check_consistent_scheduling(\n",
      "        self,\n",
      "        scheduling: system_api.ExperimentScheduling,\n",
      "        setup: system_api.ExperimentConfig,\n",
      "        verbose=False,\n",
      "    ):\n",
      "        # Scheduling and connecting to workers.\n",
      "        workers_configs = [\n",
      "            (k, getattr(setup, k), getattr(scheduling, k))\n",
      "            for k in WORKER_TYPES\n",
      "            if len(getattr(setup, k)) > 0\n",
      "        ]\n",
      "\n",
      "        # Sanity check for scheduling and configuration.\n",
      "        for _, worker_setups, schedules in workers_configs:\n",
      "            if not isinstance(schedules, List):\n",
      "                schedules = [schedules]\n",
      "            if len(worker_setups) != sum(s.count for s in schedules):\n",
      "                raise ValueError(\n",
      "                    f\"Configuration and scheduling mismatch. \"\n",
      "                    f\"Number of worker configurations: {len(worker_setups)}, \"\n",
      "                    f\"Scheduling configs: {schedules}.\"\n",
      "                )\n",
      "\n",
      "        for name, config, schedule in workers_configs:\n",
      "            count = (\n",
      "                sum([s.count for s in schedule])\n",
      "                if isinstance(schedule, list)\n",
      "                else schedule.count\n",
      "            )\n",
      "            if len(config) != count:\n",
      "                logger.error(\n",
      "                    \"Scheduling and config mismatch, interrupting all workers.\"\n",
      "                )\n",
      "                self.interrupt()\n",
      "                raise IndexError(\n",
      "                    f\"Configuration has {len(config)} {name}, {count} scheduled.\"\n",
      "                )\n",
      "            if verbose:\n",
      "                logger.info(f\"Configuration has {len(config)} {name}.\")\n",
      "\n",
      "    def start(self, experiment: system_api.Experiment, ignore_worker_error=False):\n",
      "        if flask_available and experiment.metric_discovery_port > 0:\n",
      "            server_thread = threading.Thread(\n",
      "                target=start_metric_discovery_server,\n",
      "                args=(experiment.metric_discovery_port,),\n",
      "            )\n",
      "            server_thread.start()\n",
      "\n",
      "        if ignore_worker_error:\n",
      "            check_worker_status = ()\n",
      "            remove_worker_status = (\n",
      "                Wss.COMPLETED,\n",
      "                Wss.ERROR,\n",
      "                Wss.LOST,\n",
      "                Wss.UNKNOWN,\n",
      "                Wss.PAUSED,\n",
      "            )\n",
      "        else:\n",
      "            check_worker_status = (Wss.ERROR, Wss.LOST, Wss.UNKNOWN)\n",
      "            remove_worker_status = (Wss.COMPLETED, Wss.PAUSED)\n",
      "\n",
      "        scheduling = experiment.scheduling_setup()\n",
      "        raw_experiment = copy.deepcopy(experiment)\n",
      "        setups = experiment.initial_setup()\n",
      "        if not isinstance(setups, list):\n",
      "            setups = [setups]\n",
      "\n",
      "        # Sanity check before launching workers.\n",
      "        for i, setup in enumerate(setups):\n",
      "            self.__check_consistent_scheduling(scheduling, setup, verbose=(i == 0))\n",
      "\n",
      "        worker_counts = [\n",
      "            (k, len(getattr(setups[0], k)))\n",
      "            for k in WORKER_TYPES\n",
      "            if len(getattr(setups[0], k)) > 0\n",
      "        ]\n",
      "\n",
      "        name_resolve.add(\n",
      "            names.trial_registry(self.experiment_name, self.trial_name),\n",
      "            value=datetime.now().strftime(\"%Y%m%d\"),\n",
      "            delete_on_exit=False,\n",
      "            replace=True,\n",
      "        )\n",
      "        name_resolve.add(\n",
      "            names.worker_status(\n",
      "                experiment_name=self.experiment_name,\n",
      "                trial_name=self.trial_name,\n",
      "                worker_name=\"ctl\",\n",
      "            ),\n",
      "            value=\"READY\",\n",
      "            delete_on_exit=True,\n",
      "        )\n",
      "\n",
      "        while True:\n",
      "            try:\n",
      "                logger.info(\"Connecting to workers...\")\n",
      "                self.__control.connect(\n",
      "                    [\n",
      "                        self.__control.name(name, i)\n",
      "                        for name, count in worker_counts\n",
      "                        for i in range(count)\n",
      "                    ],\n",
      "                    progress=True,\n",
      "                    timeout=CONNECTION_RETRY_AFTER_SECONDS,\n",
      "                    raises_timeout_error=True,\n",
      "                )\n",
      "                break\n",
      "\n",
      "            except TimeoutError:\n",
      "                logger.info(\"Connecting to workers timeout. Retrying...\")\n",
      "            except KeyboardInterrupt as e:\n",
      "                logger.info(\"Interrupted by user. Stopping all and exiting...\")\n",
      "                raise e\n",
      "\n",
      "        name_resolve.delete(\n",
      "            names.worker_status(\n",
      "                experiment_name=self.experiment_name,\n",
      "                trial_name=self.trial_name,\n",
      "                worker_name=\"ctl\",\n",
      "            )\n",
      "        )\n",
      "\n",
      "        # If a log exists, find the last failed setup and run it.\n",
      "        start_idx = 0\n",
      "        prev_logfile = os.path.join(\n",
      "            constants.LOG_ROOT, self.experiment_name, self.trial_name, \"ctl-0\"\n",
      "        )\n",
      "        if os.path.exists(prev_logfile):\n",
      "            with open(prev_logfile, \"r\") as f:\n",
      "                for l in f.readlines():\n",
      "                    match = re.search(r\"Entering setup (\\d+)/(\\d+)\", l)\n",
      "                    if match and int(match.group(2)) == len(setups):\n",
      "                        last_end_idx = int(match.group(1)) - 1\n",
      "                        if last_end_idx < len(setups) - 1:\n",
      "                            start_idx = last_end_idx\n",
      "\n",
      "        # NOTE: Since worker processes are created and killed by the scheduler,\n",
      "        # the controller cannot restart a dead worker when error occurs,\n",
      "        # and it's impossible to continue the experiment when any of the multiple setups fails.\n",
      "        # We can only relaunch the entire experiment in this case.\n",
      "        # In particular, while it seems to be possible to continue the experiment if\n",
      "        # the OOM error occurs, OOM will cause NCCL communication getting stuck (e.g, send/recv),\n",
      "        # which will finally throw out a C++ exception in the watchdog thread after reaching timeout.\n",
      "        # We cannot catch this exception, so OOM is irrecoverable.\n",
      "        for offset, setup in enumerate(setups[start_idx:]):\n",
      "            i = offset + start_idx\n",
      "\n",
      "            s = f\" Entering setup {i+1}/{len(setups)}... \".center(80, \"#\")\n",
      "            logger.info(colorama.Fore.RED + \"#\" * len(s) + colorama.Style.RESET_ALL)\n",
      "            logger.info(colorama.Fore.RED + s + colorama.Style.RESET_ALL)\n",
      "            logger.info(colorama.Fore.RED + \"#\" * len(s) + colorama.Style.RESET_ALL)\n",
      "\n",
      "            # Configure workers.\n",
      "            setup.set_worker_information(\n",
      "                experiment_name=self.experiment_name, trial_name=self.trial_name\n",
      "            )\n",
      "            try:\n",
      "                for name in WORKER_TYPES:\n",
      "                    if len(getattr(setup, name)) == 0:\n",
      "                        continue\n",
      "                    worker_infos = [x.worker_info for x in getattr(setup, name)]\n",
      "                    logger.info(f\"Configuring Workers: {name}...\")\n",
      "\n",
      "                    self.__control.group_request(\n",
      "                        \"configure\",\n",
      "                        worker_names=[\n",
      "                            self.__control.name(name, i)\n",
      "                            for i in range(len(worker_infos))\n",
      "                        ],\n",
      "                        worker_kwargs=[\n",
      "                            dict(worker_info=wi, setup_id=i) for wi in worker_infos\n",
      "                        ],\n",
      "                        progress=True,\n",
      "                    )\n",
      "            except Exception as e:\n",
      "                logger.error(f\"Configuring Failed: {e}. Exiting Workers.\")\n",
      "                logger.error(traceback.format_exc())\n",
      "                self.interrupt(wait_timeout=120)\n",
      "                raise e\n",
      "\n",
      "            logger.info(\"Start workers...\")\n",
      "            self.__control.group_request(\"start\")\n",
      "            logger.info(\"Started.\")\n",
      "            try:\n",
      "                self.wait(\n",
      "                    timeout=None,\n",
      "                    check_status=check_worker_status,\n",
      "                    remove_status=remove_worker_status,\n",
      "                )\n",
      "            except worker_base.WorkerException as e:\n",
      "                logger.error(e)\n",
      "                self.interrupt(wait_timeout=30)\n",
      "            except KeyboardInterrupt:\n",
      "                logger.info(\"Interrupted.\")\n",
      "                self.interrupt(wait_timeout=30)\n",
      "\n",
      "            s = f\" Finishing setup {i+1}/{len(setups)}, pausing workers... \".center(\n",
      "                80, \"#\"\n",
      "            )\n",
      "            logger.info(colorama.Fore.RED + s + colorama.Style.RESET_ALL)\n",
      "\n",
      "        logger.info(\n",
      "            colorama.Fore.YELLOW\n",
      "            + colorama.Style.BRIGHT\n",
      "            + \"\\033[1m\"\n",
      "            + \"=\" * 80\n",
      "            + colorama.Style.RESET_ALL\n",
      "        )\n",
      "        logger.info(\n",
      "            colorama.Fore.YELLOW\n",
      "            + colorama.Style.BRIGHT\n",
      "            + \"\\033[1m\"\n",
      "            + (\n",
      "                f\" All {len(setups)} setups are done. \"\n",
      "                \"You've done an excellent job! Congrats! \"\n",
      "            ).center(80, \"=\")\n",
      "            + colorama.Style.RESET_ALL\n",
      "        )\n",
      "        logger.info(\n",
      "            colorama.Fore.YELLOW\n",
      "            + colorama.Style.BRIGHT\n",
      "            + \"\\033[1m\"\n",
      "            + \"=\" * 80\n",
      "            + colorama.Style.RESET_ALL\n",
      "        )\n",
      "        logger.info(f\"Existing all workers...\")\n",
      "        self.__control.group_request(\"exit\")\n",
      "\n",
      "    def wait(\n",
      "        self,\n",
      "        timeout: Optional[int],\n",
      "        check_status: Tuple[Wss, ...],\n",
      "        remove_status: Tuple[Wss, ...],\n",
      "    ):\n",
      "        deadline = None if timeout is None else time.time() + timeout\n",
      "        left = set(self.__control.worker_names)\n",
      "        num_jobs_left = len(left)\n",
      "        logger.info(f\"Waiting for {num_jobs_left} jobs.\")\n",
      "        current_status = {name: Wss.UNKNOWN for name in self.__control.worker_names}\n",
      "        while len(left) > 0:\n",
      "            logger.debug(\n",
      "                f\"JOBS LEFT: {[str(len([l for l in left if job_type in l])) + ' ' + job_type for job_type in set([job_id.split('/')[0] for job_id in left])]}\"\n",
      "            )\n",
      "            if len(left) < num_jobs_left:\n",
      "                num_jobs_left = len(left)\n",
      "                logger.info(f\"Waiting for {num_jobs_left} jobs.\")\n",
      "            if deadline is not None and time.time() > deadline:\n",
      "                raise TimeoutError(\n",
      "                    f\"Timeout waiting for {self.experiment_name, self.trial_name}: {', '.join(sorted(left))}\"\n",
      "                )\n",
      "            for worker_name, worker_status in self.__control.pulse().items():\n",
      "                if worker_status in check_status:\n",
      "                    raise worker_base.WorkerException(\n",
      "                        worker_name, worker_status, \"experiment is running.\"\n",
      "                    )\n",
      "                if worker_status in remove_status:\n",
      "                    if worker_name in current_status:\n",
      "                        logger.debug(\n",
      "                            f\"Worker {worker_name} is {worker_status}. Removed from waiting list.\"\n",
      "                        )\n",
      "                        current_status.pop(worker_name)\n",
      "                    else:\n",
      "                        pass\n",
      "                else:\n",
      "                    if current_status.get(worker_name, None) != worker_status:\n",
      "                        current_status.update({worker_name: worker_status})\n",
      "                        logger.debug(\n",
      "                            f\"Update worker status: {worker_name} -> {worker_status}\"\n",
      "                        )\n",
      "\n",
      "            left = set(current_status.keys())\n",
      "            time.sleep(10)\n",
      "\n",
      "    def stop(self):\n",
      "        \"\"\"Stop the experiment.\n",
      "\n",
      "        Note:\n",
      "            This method assumes that the controller and scheduler is connected to the correct workers. To ensure this,\n",
      "            call controller.reconnect before your call controller.stop.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def interrupt(self, wait_timeout=120):\n",
      "        \"\"\"Interrupt the experiment.\"\"\"\n",
      "        logger.info(\"Interrupting experiment\")\n",
      "        self.__control.group_request(\"interrupt\", wait_response=False)\n",
      "        try:\n",
      "            self.wait(\n",
      "                timeout=wait_timeout,\n",
      "                check_status=(),\n",
      "                remove_status=(\n",
      "                    Wss.ERROR,\n",
      "                    Wss.LOST,\n",
      "                    Wss.COMPLETED,\n",
      "                    Wss.INTERRUPTED,\n",
      "                ),\n",
      "            )\n",
      "        except TimeoutError:\n",
      "            raise RuntimeError(f\"Fail to interrupt workers, timeout={wait_timeout}.\")\n",
      "\n",
      "\n",
      "def run_ray_worker(\n",
      "    worker_type,\n",
      "    idx,\n",
      "    world_size,\n",
      "    experiment_name,\n",
      "    trial_name,\n",
      "    comm: Tuple[rq.Queue, rq.Queue],\n",
      "):\n",
      "\n",
      "    constants.set_experiment_trial_names(experiment_name, trial_name)\n",
      "\n",
      "    import realhf.api.core.system_api as system_api\n",
      "    from realhf.api.quickstart.entrypoint import QUICKSTART_CONFIG_CLASSES\n",
      "    from realhf.base import importing\n",
      "    from realhf.base.constants import QUICKSTART_EXPR_CACHE_PATH\n",
      "\n",
      "    if os.path.exists(QUICKSTART_EXPR_CACHE_PATH):\n",
      "        for exp_cache in os.listdir(QUICKSTART_EXPR_CACHE_PATH):\n",
      "            target_cache_name = f\"{experiment_name}_{trial_name}.json\"\n",
      "            if exp_cache != target_cache_name:\n",
      "                continue\n",
      "            cache_file = os.path.join(QUICKSTART_EXPR_CACHE_PATH, target_cache_name)\n",
      "            with open(cache_file, \"r\") as f:\n",
      "                cache = json.load(f)\n",
      "            usercode_path = cache[\"usercode_path\"]\n",
      "            exp_cls_args = OmegaConf.create(cache[\"args\"])\n",
      "            config_name = cache[\"config_name\"]\n",
      "            # Import user code to register quickstart experiments.\n",
      "            importing.import_usercode(usercode_path, \"_realhf_user_code\")\n",
      "            # Register the internal experiment.\n",
      "            exp_cls = QUICKSTART_CONFIG_CLASSES[config_name]\n",
      "            system_api.register_experiment(\n",
      "                experiment_name, functools.partial(exp_cls, **exp_cls_args)\n",
      "            )\n",
      "\n",
      "    # Isolate within the same slurm job, among different jobsteps.\n",
      "    if torch.cuda.is_initialized():\n",
      "        raise RuntimeError(\n",
      "            \"CUDA already initialized before isolating CUDA devices. This should not happen.\"\n",
      "        )\n",
      "    gpu_utils.isolate_cuda_device(\n",
      "        worker_type,\n",
      "        idx,\n",
      "        world_size,\n",
      "        experiment_name,\n",
      "        trial_name,\n",
      "    )\n",
      "    if os.environ.get(\"CUDA_VISIBLE_DEVICES\", None):\n",
      "        logger.debug(\"CUDA_VISIBLE_DEVICES: %s\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
      "\n",
      "    # NOTE: Importing these will initialize DeepSpeed/CUDA devices.\n",
      "    # profiler.import_profiler_registers()\n",
      "    if worker_type != \"master_worker\":\n",
      "        # For master_worker, there could be errors while importing and it is not necessary.\n",
      "        import realhf.impl.dataset\n",
      "        import realhf.impl.model\n",
      "        import realhf.system\n",
      "\n",
      "    worker_name = f\"{worker_type}/{idx}\"\n",
      "    server = worker_control.make_server(\n",
      "        \"ray\",\n",
      "        worker_name=worker_name,\n",
      "        experiment_name=experiment_name,\n",
      "        trial_name=trial_name,\n",
      "        comm=comm,\n",
      "    )\n",
      "    worker = load_worker(worker_type)(server=server)\n",
      "    try:\n",
      "        worker.run()\n",
      "    except Exception as e:\n",
      "        logging.error(\"Worker %s failed with exception: %s\", worker_name, e)\n",
      "        logging.error(traceback.format_exc())\n",
      "        raise e\n",
      "\n",
      "\n",
      "class RayController:\n",
      "    \"\"\"A controller that uses Ray to manage workers.\n",
      "\n",
      "    It uses the basic Controller to configure workers. Besides, it\n",
      "    launchs all remote workers using Ray, instead of submitting them to\n",
      "    the scheduler.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, experiment_name, trial_name):\n",
      "        # base controller will be lazier initialized when launching workers.\n",
      "        self.__experiment_name = experiment_name\n",
      "        self.__trial_name = trial_name\n",
      "        self.__base_controller = None\n",
      "\n",
      "        self.__workers_reply_comm = None\n",
      "        self.__workers_request_comm = None\n",
      "        self.__workers_ref = None\n",
      "\n",
      "    def _launch_workers(\n",
      "        self, worker_counts: List[Tuple[str, int, system_api.TasksGroup]]\n",
      "    ):\n",
      "        # Launch remote workers.\n",
      "        logger.info(\"Launching remote workers using Ray...\")\n",
      "        self.__workers_ref: Dict[str, ray.ObjectRef] = {}\n",
      "        self.__workers_request_comm: Dict[str, rq.Queue] = dict()\n",
      "        self.__workers_reply_comm: Dict[str, rq.Queue] = dict()\n",
      "\n",
      "        # Count the total required resources and check whether Ray currently has enough of them.\n",
      "        cpu = gpu = mem = 0.0\n",
      "        for worker_type, _, schedule in worker_counts:\n",
      "            if not isinstance(schedule, List):\n",
      "                schedule = [schedule]\n",
      "            for s in schedule:\n",
      "                cpu += s.scheduling.cpu * s.count\n",
      "                gpu += s.scheduling.gpu * s.count\n",
      "                mem += s.scheduling.mem * s.count / 1024  # in GB\n",
      "        available_resources = ray.available_resources()\n",
      "        acpu = available_resources.get(\"CPU\", 0)\n",
      "        agpu = available_resources.get(\"GPU\", 0)\n",
      "        amem = available_resources.get(\"memory\", 0) / 1024**3\n",
      "        if acpu < cpu or agpu < gpu or amem < mem:\n",
      "            logger.critical(\n",
      "                f\"Ray does not have enough resources to launch workers. \"\n",
      "                f\"Required: {cpu} CPU, {gpu} GPU, {mem:.2f} GB memory. \"\n",
      "                f\"Available: {acpu} CPU, {agpu} GPU, {amem:.2f} GB memory. \"\n",
      "                f\"Please launch more Ray nodes otherwise the experiment will get stuck.\"\n",
      "            )\n",
      "\n",
      "        # Launch ray jobs.\n",
      "        for worker_type, count, schedule in worker_counts:\n",
      "            all_schedules: List[system_api.TasksGroup] = []\n",
      "            if isinstance(schedule, List):\n",
      "                for s in schedule:\n",
      "                    for _ in range(s.count):\n",
      "                        s_ = copy.deepcopy(s)\n",
      "                        s_.count = 1\n",
      "                        all_schedules.append(s_)\n",
      "            else:\n",
      "                for _ in range(schedule.count):\n",
      "                    s_ = copy.deepcopy(schedule)\n",
      "                    s_.count = 1\n",
      "                    all_schedules.append(s_)\n",
      "            assert len(all_schedules) == count\n",
      "            comms = [(rq.Queue(maxsize=8), rq.Queue(maxsize=8)) for _ in all_schedules]\n",
      "            world_size = len(all_schedules)\n",
      "            if any(sch.scheduling.gpu > 0 for sch in all_schedules):\n",
      "                # For GPU jobs, use a customized packed scheduling method\n",
      "                # that sequentially allocates nodes.\n",
      "                if not all(\n",
      "                    sch.scheduling.gpu == all_schedules[0].scheduling.gpu == 1\n",
      "                    for sch in all_schedules\n",
      "                ):\n",
      "                    raise ValueError(\n",
      "                        \"Ray scheduler only supports resource requirements where #GPU=1 or #GPU=0.\"\n",
      "                    )\n",
      "                available_nodes = [\n",
      "                    k\n",
      "                    for k in available_resources\n",
      "                    if re.match(r\"node:(\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b)\", k)\n",
      "                ]\n",
      "                total_gpus = available_resources[\"GPU\"]\n",
      "                if total_gpus % len(available_nodes) != 0:\n",
      "                    raise ValueError(\n",
      "                        \"Cannot schedule Ray jobs to nodes with heterogeneous numbers of GPUs.\"\n",
      "                    )\n",
      "                n_gpus_per_node = int(total_gpus // len(available_nodes))\n",
      "                if total_gpus < count:\n",
      "                    raise RuntimeError(\n",
      "                        \"Available GPUs is smaller than the number of scheduled GPU workers.\"\n",
      "                    )\n",
      "\n",
      "                jobs = []\n",
      "                for node_idx, i in enumerate(range(0, count, n_gpus_per_node)):\n",
      "                    _schedules = all_schedules[i : i + n_gpus_per_node]\n",
      "                    _comms = comms[i : i + n_gpus_per_node]\n",
      "                    for _idx, (comm, sch) in enumerate(zip(_comms, _schedules)):\n",
      "                        # Schedule jobs one-by-one to maintain the order on remote nodes.\n",
      "                        job = ray.remote(\n",
      "                            num_cpus=sch.scheduling.cpu,\n",
      "                            num_gpus=sch.scheduling.gpu,\n",
      "                            memory=sch.scheduling.mem * 1024**2,\n",
      "                            name=f\"{worker_type}/{_idx + i}\",\n",
      "                            resources={available_nodes[node_idx]: 1 / n_gpus_per_node},\n",
      "                        )(run_ray_worker).remote(\n",
      "                            worker_type,\n",
      "                            _idx + i,\n",
      "                            world_size,\n",
      "                            self.__experiment_name,\n",
      "                            self.__trial_name,\n",
      "                            comm,\n",
      "                        )\n",
      "                        try:\n",
      "                            ray.get(job, timeout=0.1)\n",
      "                        except ray.exceptions.GetTimeoutError:\n",
      "                            pass\n",
      "                        jobs.append(job)\n",
      "            else:\n",
      "                # Use the default Ray scheduler, which may have some randomness.\n",
      "                jobs = [\n",
      "                    ray.remote(\n",
      "                        num_cpus=sch.scheduling.cpu,\n",
      "                        num_gpus=sch.scheduling.gpu,\n",
      "                        memory=sch.scheduling.mem * 1024**2,\n",
      "                        name=f\"{worker_type}/{idx}\",\n",
      "                    )(run_ray_worker).remote(\n",
      "                        worker_type,\n",
      "                        idx,\n",
      "                        world_size,\n",
      "                        self.__experiment_name,\n",
      "                        self.__trial_name,\n",
      "                        comm,\n",
      "                    )\n",
      "                    for idx, (comm, sch) in enumerate(zip(comms, all_schedules))\n",
      "                ]\n",
      "            for idx, (job, c) in enumerate(zip(jobs, comms)):\n",
      "                name = f\"{worker_type}/{idx}\"\n",
      "                self.__workers_ref[name] = job\n",
      "                self.__workers_request_comm[name] = c[0]\n",
      "                self.__workers_reply_comm[name] = c[1]\n",
      "            # Perform a poll step on remote jobs to let them raise setup errors,\n",
      "            # e.g., ImportError, ModuleNotFoundError, etc.\n",
      "            try:\n",
      "                ray.get(jobs, timeout=1)\n",
      "            except ray.exceptions.GetTimeoutError:\n",
      "                pass\n",
      "            logger.info(f\"Launched {count} {worker_type}.\")\n",
      "\n",
      "        panel = worker_control.make_control(\n",
      "            \"ray\",\n",
      "            self.__experiment_name,\n",
      "            self.__trial_name,\n",
      "            request_comms=self.__workers_request_comm,\n",
      "            reply_comms=self.__workers_reply_comm,\n",
      "        )\n",
      "        self.__base_controller = Controller(\n",
      "            self.__experiment_name, self.__trial_name, panel\n",
      "        )\n",
      "        logger.info(\"All Ray workers are lauched.\")\n",
      "\n",
      "    def start(self, experiment: system_api.Experiment, ignore_worker_error=False):\n",
      "        scheduling: system_api.ExperimentScheduling = experiment.scheduling_setup()\n",
      "        setup = experiment.initial_setup()\n",
      "        if not isinstance(setup, list):\n",
      "            setup = [setup]\n",
      "        worker_counts = [\n",
      "            (k, len(getattr(setup[0], k)), getattr(scheduling, k))\n",
      "            for k in WORKER_TYPES\n",
      "            if len(getattr(setup[0], k)) > 0\n",
      "        ]\n",
      "\n",
      "        env_vars = constants.get_env_vars(\n",
      "            REAL_MODE=os.environ.get(\"REAL_MODE\", \"\"),\n",
      "            REAL_RECOVER_RUN=os.environ.get(\"REAL_RECOVER_RUN\", \"\"),\n",
      "            REAL_SAVE_RECOVER_STATES=os.environ.get(\"REAL_SAVE_RECOVER_STATES\", \"\"),\n",
      "        )\n",
      "        runtime_env = {\n",
      "            \"env_vars\": env_vars,\n",
      "            \"working_dir\": os.getcwd(),\n",
      "        }\n",
      "        logger.info(f\"Ray workers runtime env: {runtime_env}\")\n",
      "        ray.init(runtime_env=runtime_env)\n",
      "\n",
      "        logger.info(\"Ray initialized! Ready to run workers.\")\n",
      "\n",
      "        try:\n",
      "            self._launch_workers(worker_counts)\n",
      "            self.__base_controller.start(experiment, ignore_worker_error)\n",
      "        except Exception as e:\n",
      "            ray.shutdown()\n",
      "            raise e\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/buffer.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import copy\n",
      "import time\n",
      "from dataclasses import dataclass, field\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import realhf.api.core.dfg as dfg\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "\n",
      "logger = logging.getLogger(\"buffer\")\n",
      "\n",
      "\n",
      "class BufferFull(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class _ReplayEntry:\n",
      "    reuses_left: int\n",
      "    receive_time: float\n",
      "    sample: SequenceSample\n",
      "\n",
      "\n",
      "BUFFER_KEY_WARN_CACHE = set()\n",
      "\n",
      "\n",
      "class _TensorDictSequenceBuffer:\n",
      "    \"\"\"An thread-unsafe buffer implementation based on list.\n",
      "\n",
      "    Used as an internal buffer object in asyncio-based SequenceBuffer.\n",
      "    Can be replaced with a more efficient C++ implementation based on\n",
      "    std vector.\n",
      "\n",
      "    Methods starting with _ should be called in a locked context.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, keys: List[str], max_size: int, reuses: int):\n",
      "        # Fixed-size storage, storing pointers, but sequeces in dict have variable lengths.\n",
      "        self.__storage: List[_ReplayEntry] = [None for _ in range(max_size)]\n",
      "\n",
      "        # Some states of the storage. Read/Write applied to them should be locked.\n",
      "        self.__has_keys = np.zeros((max_size, len(keys)), dtype=bool)\n",
      "\n",
      "        self.__keys = keys\n",
      "        self.__reuses = reuses\n",
      "\n",
      "    def _update_has_keys(self, indices: List[int]):\n",
      "        for idx in indices:\n",
      "            self.__has_keys[idx] = [\n",
      "                k in self.__storage[idx].sample.keys for k in self.__keys\n",
      "            ]\n",
      "            if any(k not in self.__keys for k in self.__storage[idx].sample.keys):\n",
      "                global BUFFER_KEY_WARN_CACHE\n",
      "                ck = (\n",
      "                    tuple(sorted(self.__keys)),\n",
      "                    tuple(sorted(self.__storage[idx].sample.keys)),\n",
      "                )\n",
      "                if ck not in BUFFER_KEY_WARN_CACHE:\n",
      "                    logger.debug(\n",
      "                        f\"Unexpected keys in the sample. Expected keys from all MFCDef: {self.__keys}. \"\n",
      "                        f\"Keys in the current sample: {self.__storage[idx].sample.keys}\"\n",
      "                    )\n",
      "                    BUFFER_KEY_WARN_CACHE.add(ck)\n",
      "\n",
      "    def _get_has_keys(self, indices):\n",
      "        return self.__has_keys[indices, :]\n",
      "\n",
      "    def put_batch(self, indices: List[int], xs: List[SequenceSample]):\n",
      "        assert len(indices) == len(xs)\n",
      "        # Can be parallelized.\n",
      "        for idx, x in zip(indices, xs):\n",
      "            self.__storage[idx] = _ReplayEntry(\n",
      "                reuses_left=self.__reuses,\n",
      "                receive_time=time.time(),\n",
      "                sample=x,\n",
      "            )\n",
      "\n",
      "    def amend_batch(self, indices: List[int], xs: List[SequenceSample]):\n",
      "        assert len(indices) == len(xs)\n",
      "        # Can be parallelized.\n",
      "        for idx, x in zip(indices, xs):\n",
      "            self.__storage[idx].sample.update_(x)\n",
      "\n",
      "    def get_batch(self, indices: List[int]) -> List[_ReplayEntry]:\n",
      "        # Can be parallelized.\n",
      "        res = []\n",
      "        for idx in indices:\n",
      "            r = self.__storage[idx]\n",
      "            r.reuses_left -= 1\n",
      "            res.append(r)\n",
      "        return res\n",
      "\n",
      "    def inspect_batch(self, indices: List[int]) -> List[_ReplayEntry]:\n",
      "        res = []\n",
      "        for idx in indices:\n",
      "            r = self.__storage[idx]\n",
      "            res.append(r)\n",
      "        return res\n",
      "\n",
      "    def pop_batch(self, indices: List[int]):\n",
      "        res = []\n",
      "        for idx in indices:\n",
      "            r = self.__storage[idx]\n",
      "            self.__storage[idx] = None\n",
      "            self.__has_keys[idx] = False\n",
      "            res.append(r)\n",
      "        return res\n",
      "\n",
      "\n",
      "class AsyncIOSequenceBuffer:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        rpcs: List[dfg.MFCDef],\n",
      "        max_size: int,\n",
      "    ):\n",
      "        self.rpcs = rpcs\n",
      "        self._lock = asyncio.Condition(asyncio.Lock())\n",
      "\n",
      "        # Buffer indicators, should be locked by self._lock.\n",
      "        # Put, amend, ready, idle, and empty are mutually exclusive.\n",
      "        self._is_being_put = np.zeros(max_size, dtype=bool)\n",
      "        self._is_being_amended = np.zeros(max_size, dtype=bool)\n",
      "        self._is_being_read = np.zeros(max_size, dtype=bool)\n",
      "        self._is_idle = np.zeros(max_size, dtype=bool)\n",
      "        self._is_empty = np.ones(max_size, dtype=bool)\n",
      "\n",
      "        self._buf_size = 0\n",
      "\n",
      "        self._birth_time = np.zeros(max_size, dtype=np.int64)\n",
      "\n",
      "        self._buf_size = 0\n",
      "\n",
      "        # We allow concurrent amenders and readers.\n",
      "        self._n_amenders = np.zeros(max_size, dtype=int)\n",
      "        self._n_readers = np.zeros(max_size, dtype=int)\n",
      "\n",
      "        self._ready_for_rpcs = np.zeros((max_size, len(rpcs)), dtype=bool)\n",
      "        self._completed_rpc = np.zeros((max_size, len(rpcs)), dtype=bool)\n",
      "\n",
      "        self._rpc_data_keys = rpc_data_keys = list(\n",
      "            set().union(*[rpc.input_keys for rpc in rpcs])\n",
      "        )\n",
      "        # We can efficiently compute whether an RPC is ready using this mask\n",
      "        self._rpc_key_mask = np.stack(\n",
      "            [\n",
      "                np.array([k in rpc.input_keys for k in rpc_data_keys], dtype=bool)\n",
      "                for rpc in rpcs\n",
      "            ],\n",
      "            axis=1,\n",
      "        )\n",
      "        self._rpc_names = [rpc.name for rpc in rpcs]\n",
      "\n",
      "        # The internal buffer implementation.\n",
      "        self.__max_size = max_size\n",
      "        self.__buffer = _TensorDictSequenceBuffer(\n",
      "            keys=rpc_data_keys, max_size=max_size, reuses=len(rpcs)\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def max_size(self) -> int:\n",
      "        return self.__max_size\n",
      "\n",
      "    @property\n",
      "    def size(self) -> int:\n",
      "        return self._buf_size\n",
      "\n",
      "    @property\n",
      "    def lock(self):\n",
      "        return self._lock\n",
      "\n",
      "    @property\n",
      "    def n_rpcs(self):\n",
      "        return len(self._rpc_names)\n",
      "\n",
      "    def _assert_valid_indicator(self):\n",
      "        assert (\n",
      "            self._is_being_put\n",
      "            + self._is_being_amended\n",
      "            + self._is_being_read\n",
      "            + self._is_idle\n",
      "        ).sum() == self._buf_size\n",
      "        assert (self._is_empty.sum() + self._buf_size) == self.__max_size\n",
      "        assert ((self._n_amenders > 0) == self._is_being_amended).all()\n",
      "        assert (self._n_amenders >= 0).all()\n",
      "        assert ((self._n_readers > 0) == self._is_being_read).all()\n",
      "        assert (self._n_readers >= 0).all()\n",
      "        assert (self._is_empty[:, None] * self._ready_for_rpcs).sum() == 0\n",
      "        assert (self._is_empty[:, None] * self._completed_rpc).sum() == 0\n",
      "\n",
      "    def put_batch_synced(self, samples: List[SequenceSample]):\n",
      "        self._assert_valid_indicator()\n",
      "\n",
      "        n = len(samples)\n",
      "\n",
      "        if n == 0:\n",
      "            return np.array([], dtype=np.int64)\n",
      "\n",
      "        indices = np.where(self._is_empty)[0][:n]\n",
      "\n",
      "        if len(indices) < n:\n",
      "            raise BufferFull(\n",
      "                \"You are probably using a large dataset. \"\n",
      "                \"The default buffer size 1M is not large enough. \"\n",
      "                \"Please set a larger buffer size by setting \"\n",
      "                \"the environment variable, e.g., REAL_MASTER_BUFFER_SIZE=3000000.\"\n",
      "            )\n",
      "        self._is_empty[indices] = False\n",
      "        self._is_being_put[indices] = True\n",
      "\n",
      "        self.__buffer.put_batch(indices, samples)\n",
      "\n",
      "        self.__buffer._update_has_keys(indices)\n",
      "\n",
      "        # Set a slight difference in birth time to let the order\n",
      "        # be deterministic.\n",
      "        self._birth_time[indices] = time.monotonic_ns() + np.arange(\n",
      "            len(indices), dtype=np.int64\n",
      "        )\n",
      "\n",
      "        has_keys = self.__buffer._get_has_keys(indices)  # [bs, #keys]\n",
      "        rpc_key_mask = self._rpc_key_mask  # [#keys, #rpcs]\n",
      "        self._ready_for_rpcs[indices] = (\n",
      "            has_keys[:, :, None] >= rpc_key_mask[None, :, :]\n",
      "        ).all(axis=1)\n",
      "\n",
      "        self._is_being_put[indices] = False\n",
      "        self._is_idle[indices] = True\n",
      "\n",
      "        self._buf_size += len(samples)\n",
      "        if self._buf_size >= 0.95 * self.__max_size:\n",
      "            logger.warning(\n",
      "                f\"Buffer is 95% full. The current buffer size is {self._buf_size} \"\n",
      "                f\"while the maximum size is {self.__max_size}. \"\n",
      "                f\"If your dataset has more than 1M sequences, consider enlarge \"\n",
      "                f\"the default batch size in the master worker.\"\n",
      "            )\n",
      "        return indices\n",
      "\n",
      "    async def put_batch(\n",
      "        self, samples: List[SequenceSample], birth_times: List[int] | None = None\n",
      "    ):\n",
      "        n = len(samples)\n",
      "\n",
      "        if n == 0:\n",
      "            return np.array([], dtype=np.int64)\n",
      "\n",
      "        async with self._lock:\n",
      "            self._assert_valid_indicator()\n",
      "\n",
      "            indices = np.where(self._is_empty)[0][:n]\n",
      "\n",
      "            if len(indices) < n:\n",
      "                raise BufferFull(\n",
      "                    \"You are probably using a large dataset. \"\n",
      "                    \"The default buffer size 1M is not large enough. \"\n",
      "                    \"Please set a larger buffer size by setting \"\n",
      "                    \"the environment variable, e.g., REAL_MASTER_BUFFER_SIZE=3000000.\"\n",
      "                )\n",
      "            self._is_empty[indices] = False\n",
      "            self._is_being_put[indices] = True\n",
      "\n",
      "        self.__buffer.put_batch(indices, samples)\n",
      "\n",
      "        # Set a slight difference in birth time to let the order\n",
      "        # be deterministic.\n",
      "        if birth_times is None:\n",
      "            self._birth_time[indices] = time.monotonic_ns() + np.arange(\n",
      "                len(indices), dtype=np.int64\n",
      "            )\n",
      "        else:\n",
      "            self._birth_time[indices] = birth_times\n",
      "\n",
      "        async with self._lock:\n",
      "            self.__buffer._update_has_keys(indices)\n",
      "\n",
      "            has_keys = self.__buffer._get_has_keys(indices)  # [bs, #keys]\n",
      "            rpc_key_mask = self._rpc_key_mask  # [#keys, #rpcs]\n",
      "            self._ready_for_rpcs[indices] = (\n",
      "                has_keys[:, :, None] >= rpc_key_mask[None, :, :]\n",
      "            ).all(axis=1)\n",
      "\n",
      "            self._is_being_put[indices] = False\n",
      "            self._is_idle[indices] = True\n",
      "\n",
      "            self._buf_size += len(samples)\n",
      "            if self._buf_size >= 0.95 * self.__max_size:\n",
      "                logger.warning(\n",
      "                    f\"Buffer is 95% full. The current buffer size is {self._buf_size} \"\n",
      "                    f\"while the maximum size is {self.__max_size}. \"\n",
      "                    f\"If your dataset has more than 1M sequences, consider enlarge \"\n",
      "                    f\"the default batch size in the master worker.\"\n",
      "                )\n",
      "\n",
      "            can_do_rpcs = {rpc.name: self._can_do_rpc(rpc) for rpc in self.rpcs}\n",
      "            logger.debug(f\"After putting batch, can do RPCs? {can_do_rpcs}.\")\n",
      "\n",
      "            self._lock.notify(len(self._rpc_names))\n",
      "        return indices\n",
      "\n",
      "    async def amend_batch(self, indices: List[int], samples: List[SequenceSample]):\n",
      "        async with self._lock:\n",
      "            await self._lock.wait_for(\n",
      "                lambda: (\n",
      "                    self._is_idle[indices] | self._is_being_amended[indices]\n",
      "                ).all(),\n",
      "            )\n",
      "            self._assert_valid_indicator()\n",
      "            self._is_idle[indices] = False\n",
      "            self._is_being_amended[indices] = True\n",
      "            self._n_amenders[indices] += 1\n",
      "\n",
      "        self.__buffer.amend_batch(indices, samples)\n",
      "\n",
      "        async with self._lock:\n",
      "            self.__buffer._update_has_keys(indices)\n",
      "\n",
      "            has_keys = self.__buffer._get_has_keys(indices)  # [bs, #keys]\n",
      "            rpc_key_mask = self._rpc_key_mask  # [#keys, #rpcs]\n",
      "            self._ready_for_rpcs[indices] = (\n",
      "                has_keys[:, :, None] >= rpc_key_mask[None, :, :]\n",
      "            ).all(axis=1)\n",
      "\n",
      "            self._n_amenders[indices] -= 1\n",
      "            self._is_being_amended[indices] = self._n_amenders[indices] > 0\n",
      "            self._is_idle[indices] = np.logical_not(self._is_being_amended[indices])\n",
      "            if self._is_idle[indices].any():\n",
      "                self._lock.notify(len(self._rpc_names))\n",
      "\n",
      "    def _can_do_rpc(self, rpc: dfg.MFCDef) -> bool:\n",
      "        rpc_idx = self._rpc_names.index(rpc.name)\n",
      "        ready_indices = np.nonzero(\n",
      "            (self._is_idle | self._is_being_read)\n",
      "            & self._ready_for_rpcs[:, rpc_idx]\n",
      "            & ~self._completed_rpc[:, rpc_idx]\n",
      "        )[0]\n",
      "        if len(ready_indices) < rpc.n_seqs:\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    async def get_batch_for_rpc(\n",
      "        self, rpc: dfg.MFCDef\n",
      "    ) -> Tuple[List[int], SequenceSample]:\n",
      "        logger.debug(\n",
      "            f\"MFC {rpc.name} is waiting for its input keys: {rpc.input_keys}...\"\n",
      "        )\n",
      "        rpc_idx = self._rpc_names.index(rpc.name)\n",
      "\n",
      "        async with self._lock:\n",
      "            # await self._lock.wait_for(_can_do_rpc)\n",
      "\n",
      "            while not self._can_do_rpc(rpc):\n",
      "                await self._lock.wait()\n",
      "\n",
      "            logger.debug(f\"Input keys ({rpc.input_keys}) for MFC {rpc.name} are ready!\")\n",
      "            self._assert_valid_indicator()\n",
      "\n",
      "            ready_indices = np.nonzero(\n",
      "                (self._is_idle | self._is_being_read)\n",
      "                & self._ready_for_rpcs[:, rpc_idx]\n",
      "                & ~self._completed_rpc[:, rpc_idx]\n",
      "            )[0]\n",
      "\n",
      "            # Prioritize old data.\n",
      "            assert np.all(self._birth_time[ready_indices] > 0)\n",
      "            indices = ready_indices[\n",
      "                np.argsort(self._birth_time[ready_indices])[: rpc.n_seqs]\n",
      "            ]\n",
      "\n",
      "            self._is_idle[indices] = False\n",
      "            self._is_being_read[indices] = True\n",
      "            self._n_readers[indices] += 1\n",
      "\n",
      "        entries = self.__buffer.get_batch(indices)\n",
      "        assert all([entry.reuses_left >= 0 for entry in entries])\n",
      "        pop_indices = [\n",
      "            idx for idx, entry in zip(indices, entries) if entry.reuses_left == 0\n",
      "        ]\n",
      "        # The following call is safe because no more RPC will write to popped data.\n",
      "        if len(pop_indices) > 0:\n",
      "            self.__buffer.pop_batch(pop_indices)\n",
      "\n",
      "        async with self._lock:\n",
      "            self._n_readers[indices] -= 1\n",
      "            self._is_being_read[indices] = self._n_readers[indices] > 0\n",
      "            self._is_idle[indices] = self._n_readers[indices] == 0\n",
      "            self._completed_rpc[indices, rpc_idx] = True\n",
      "\n",
      "            assert (self._n_readers[pop_indices] == 0).all()\n",
      "            assert (self._n_amenders[pop_indices] == 0).all()\n",
      "            self._is_empty[pop_indices] = True\n",
      "            self._is_idle[pop_indices] = False\n",
      "            self._completed_rpc[pop_indices] = False\n",
      "            self._ready_for_rpcs[pop_indices] = False\n",
      "            self._buf_size -= len(pop_indices)\n",
      "\n",
      "            if self._is_idle[indices].any():\n",
      "                self._lock.notify(len(self._rpc_names))\n",
      "        return indices, SequenceSample.gather(\n",
      "            [e.sample for e in entries], keys=rpc.input_keys\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/redistributor.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import itertools\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "\n",
      "\n",
      "class GlobalStorageTracker:\n",
      "    def __init__(self, world_size: int):\n",
      "        self.lock = asyncio.Lock()\n",
      "        self.storages: List[Dict[Hashable, List[str]]]\n",
      "        self.storages = [{} for _ in range(world_size)]\n",
      "        self.data_owner: Dict[Tuple[Hashable, str], int]\n",
      "        self.data_owner = {}\n",
      "\n",
      "    async def add_data(self, rank: int, ids: List[Hashable], key: str, is_owner: bool):\n",
      "        async with self.lock:\n",
      "            for data_id in ids:\n",
      "                if data_id not in self.storages[rank]:\n",
      "                    self.storages[rank][data_id] = [key]\n",
      "                else:\n",
      "                    if key not in self.storages[rank][data_id]:\n",
      "                        self.storages[rank][data_id].append(key)\n",
      "                if is_owner:\n",
      "                    self.data_owner[(data_id, key)] = rank\n",
      "\n",
      "    def add_data_synced(self, rank: int, ids: List[Hashable], key: str, is_owner: bool):\n",
      "        for data_id in ids:\n",
      "            if data_id not in self.storages[rank]:\n",
      "                self.storages[rank][data_id] = [key]\n",
      "            else:\n",
      "                if key not in self.storages[rank][data_id]:\n",
      "                    self.storages[rank][data_id].append(key)\n",
      "            if is_owner:\n",
      "                self.data_owner[(data_id, key)] = rank\n",
      "\n",
      "    async def clear_data(self, ids: List[Hashable]):\n",
      "        async with self.lock:\n",
      "            for storage in self.storages:\n",
      "                for i in ids:\n",
      "                    if i in storage:\n",
      "                        storage.pop(i)\n",
      "            keys = list(self.data_owner.keys())\n",
      "            for i, k in keys:\n",
      "                if i in ids:\n",
      "                    self.data_owner.pop((i, k))\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RedistribStep:\n",
      "    comm_type: str\n",
      "    root: int | None\n",
      "    srcs: List[int] | None\n",
      "    dsts: List[int] | None\n",
      "    ids: List[List[Hashable]]\n",
      "    keys: List[str]\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        if self.comm_type == \"gather\":\n",
      "            return f\"Gather {self.keys} to {self.root} from {self.srcs}.\"\n",
      "        if self.comm_type == \"scatter\":\n",
      "            return f\"Scatter {self.keys} from {self.root} to {self.dsts}.\"\n",
      "        if self.comm_type == \"bcast\":\n",
      "            return f\"Bcast {self.keys} from {self.root} to {self.dsts}.\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "class RedistribPlanner:\n",
      "    def __init__(self, storage_tracker: GlobalStorageTracker):\n",
      "        self.storage_tracker = storage_tracker\n",
      "\n",
      "    def derive_plan(\n",
      "        self,\n",
      "        dests: Dict[int, List[Hashable]],\n",
      "        keys: List[str],\n",
      "        pattern: str = \"gather-scatter\",\n",
      "    ) -> List[RedistribStep]:\n",
      "        if pattern == \"gather-scatter\":\n",
      "            return self.derive_plan_gather_scatter(dests, keys)\n",
      "        elif pattern == \"bcast\":\n",
      "            return self.derive_plan_bcast(dests, keys)\n",
      "        raise NotImplementedError(f\"Unknown data redistribution pattern: {pattern}\")\n",
      "\n",
      "    def derive_plan_gather_scatter(\n",
      "        self, dests: Dict[int, List[Hashable]], keys: List[str]\n",
      "    ) -> List[RedistribStep]:\n",
      "        self.dests = dests\n",
      "\n",
      "        all_data_ids = set()\n",
      "        for all_samples in dests.values():\n",
      "            for data_id in all_samples:\n",
      "                all_data_ids.add(data_id)\n",
      "\n",
      "        transfer_plan = []\n",
      "        for key in keys:\n",
      "            owners = sorted(\n",
      "                list(\n",
      "                    set(\n",
      "                        [\n",
      "                            self.storage_tracker.data_owner[(i, key)]\n",
      "                            for i in all_data_ids\n",
      "                        ]\n",
      "                    )\n",
      "                )\n",
      "            )\n",
      "            gather_ids = []\n",
      "            for owner in owners:\n",
      "                this_owner_ids = []\n",
      "                for i in all_data_ids:\n",
      "                    if (\n",
      "                        i in self.storage_tracker.storages[owner]\n",
      "                        and key in self.storage_tracker.storages[owner][i]\n",
      "                    ):\n",
      "                        this_owner_ids.append(i)\n",
      "                gather_ids.append(sorted(this_owner_ids))\n",
      "            gather_step = RedistribStep(\n",
      "                comm_type=\"gather\",\n",
      "                root=owners[0],\n",
      "                srcs=owners,\n",
      "                dsts=None,\n",
      "                ids=gather_ids,\n",
      "                keys=[key],\n",
      "            )\n",
      "\n",
      "            scatter_dsts = sorted([i for i in dests if dests[i]])\n",
      "            scatter_ids = [sorted(dests[i]) for i in scatter_dsts]\n",
      "            scatter_step = RedistribStep(\n",
      "                comm_type=\"scatter\",\n",
      "                root=owners[0],\n",
      "                dsts=scatter_dsts,\n",
      "                srcs=None,\n",
      "                ids=scatter_ids,\n",
      "                keys=[key],\n",
      "            )\n",
      "            transfer_plan += [gather_step, scatter_step]\n",
      "\n",
      "        # Prune the plan.\n",
      "        pop_indices = []\n",
      "        for idx, step in enumerate(transfer_plan):\n",
      "            # 1. Omit the gather step if data has already been gathered before.\n",
      "            if step.comm_type == \"gather\":\n",
      "                all_gather_ids = list(itertools.chain.from_iterable(step.ids))\n",
      "                key = step.keys[0]\n",
      "                if any(\n",
      "                    i not in self.storage_tracker.storages[step.root]\n",
      "                    for i in all_gather_ids\n",
      "                ):\n",
      "                    continue\n",
      "                if any(\n",
      "                    key not in self.storage_tracker.storages[step.root][i]\n",
      "                    for i in all_gather_ids\n",
      "                ):\n",
      "                    continue\n",
      "                pop_indices.append(idx)\n",
      "            # 2. Omit the gather + scatter step if data has already exists in all dst GPUs.\n",
      "            if step.comm_type == \"scatter\":\n",
      "                key = step.keys[0]\n",
      "                all_exists = True\n",
      "                for dst, ids in zip(step.dsts, step.ids):\n",
      "                    if any(i not in self.storage_tracker.storages[dst] for i in ids):\n",
      "                        all_exists = False\n",
      "                        break\n",
      "                    if any(\n",
      "                        key not in self.storage_tracker.storages[dst][i] for i in ids\n",
      "                    ):\n",
      "                        all_exists = False\n",
      "                        break\n",
      "                if all_exists:\n",
      "                    pop_indices.append(idx)\n",
      "                    pop_indices.append(idx - 1)\n",
      "        for pop_idx in reversed(sorted(set(pop_indices))):\n",
      "            transfer_plan.pop(pop_idx)\n",
      "\n",
      "        # Merging the gather/scatter of different keys\n",
      "        gather_plan = {}\n",
      "        scatter_plan = {}\n",
      "        for step in transfer_plan:\n",
      "            if step.comm_type == \"gather\":\n",
      "                plan_key = (\n",
      "                    step.root,\n",
      "                    tuple(sorted(step.srcs)),\n",
      "                    tuple([tuple(sorted(ids)) for ids in step.ids]),\n",
      "                )\n",
      "                if plan_key not in gather_plan:\n",
      "                    gather_plan[plan_key] = step\n",
      "                else:\n",
      "                    assert all(\n",
      "                        k not in gather_plan[plan_key].keys for k in step.keys\n",
      "                    ), (\n",
      "                        gather_plan[plan_key],\n",
      "                        step,\n",
      "                        plan_key,\n",
      "                    )\n",
      "                    gather_plan[plan_key].keys += step.keys\n",
      "            if step.comm_type == \"scatter\":\n",
      "                plan_key = (\n",
      "                    step.root,\n",
      "                    tuple(sorted(step.dsts)),\n",
      "                    tuple([tuple(sorted(ids)) for ids in step.ids]),\n",
      "                )\n",
      "                if plan_key not in scatter_plan:\n",
      "                    scatter_plan[plan_key] = step\n",
      "                else:\n",
      "                    assert all(\n",
      "                        k not in scatter_plan[plan_key].keys for k in step.keys\n",
      "                    ), (\n",
      "                        scatter_plan[plan_key],\n",
      "                        step,\n",
      "                        plan_key,\n",
      "                    )\n",
      "                    scatter_plan[plan_key].keys += step.keys\n",
      "\n",
      "        # Prioritize gather over scatter\n",
      "        return list(gather_plan.values()) + list(scatter_plan.values())\n",
      "\n",
      "    def derive_plan_bcast(\n",
      "        self, dests: Dict[int, List[Hashable]], keys: List[str] | Tuple[str]\n",
      "    ) -> List[RedistribStep]:\n",
      "        assert isinstance(keys, (list, tuple)), type(keys)\n",
      "        keys = list(keys)\n",
      "        self.dests = dests\n",
      "\n",
      "        # Get all requried data IDs.\n",
      "        all_data_ids = set()\n",
      "        for all_samples in self.dests.values():\n",
      "            for data_id in all_samples:\n",
      "                all_data_ids.add(data_id)\n",
      "\n",
      "        # The producers for each required data.\n",
      "        id2gpu_src = {}\n",
      "        for data_id in all_data_ids:\n",
      "            for key in keys:\n",
      "                id2gpu_src[(data_id, key)] = []\n",
      "                for gpu_id, ids2keys in enumerate(self.storage_tracker.storages):\n",
      "                    if data_id in ids2keys and key in ids2keys[data_id]:\n",
      "                        id2gpu_src[(data_id, key)].append(gpu_id)\n",
      "\n",
      "        # The consumers for each requried data.\n",
      "        id2gpu_dst = {}\n",
      "        for data_id in all_data_ids:\n",
      "            for key in keys:\n",
      "                id2gpu_dst[(data_id, key)] = []\n",
      "                for gpu_id, ids in self.dests.items():\n",
      "                    if data_id in ids:\n",
      "                        id2gpu_dst[(data_id, key)].append(gpu_id)\n",
      "\n",
      "        self.transfer_plan = {}\n",
      "\n",
      "        for data_id, key in itertools.product(all_data_ids, keys):\n",
      "            source_gpus = id2gpu_src[(data_id, key)]\n",
      "            target_gpus = id2gpu_dst[(data_id, key)]\n",
      "\n",
      "            assert len(source_gpus) > 0, (data_id, key, id2gpu_src, id2gpu_dst)\n",
      "\n",
      "            # Omit data transfer if it exists in the target GPU\n",
      "            target_gpus = [gpu for gpu in target_gpus if gpu not in source_gpus]\n",
      "            if not target_gpus:\n",
      "                continue\n",
      "\n",
      "            # Find the \"nearest\" GPU for data transfer.\n",
      "            best_src = self._select_best_bcast_source(source_gpus, target_gpus)\n",
      "\n",
      "            self.transfer_plan[(data_id, key)] = {\"src\": best_src, \"dsts\": target_gpus}\n",
      "\n",
      "        return self._group_bcast_transfers()\n",
      "\n",
      "    def _on_same_node(self, i, j) -> bool:\n",
      "        return (i // cluster_spec.n_gpus_per_node) == (\n",
      "            j // cluster_spec.n_gpus_per_node\n",
      "        )\n",
      "\n",
      "    def _select_best_bcast_source(self, source_gpus, target_gpus):\n",
      "        same_node_counts = {}\n",
      "        for src in source_gpus:\n",
      "            same_node_count = sum(\n",
      "                1 for dst in target_gpus if self._on_same_node(src, dst)\n",
      "            )\n",
      "            same_node_counts[src] = same_node_count\n",
      "\n",
      "        # Find the source that maximizes locality.\n",
      "        max_same_node = max(same_node_counts.values())\n",
      "        best_sources = [\n",
      "            src for src, count in same_node_counts.items() if count == max_same_node\n",
      "        ]\n",
      "\n",
      "        # Find the source with the smallest workload.\n",
      "        src_load = defaultdict(int)\n",
      "        for plan in self.transfer_plan.values():\n",
      "            src_gpu = plan[\"src\"]\n",
      "            src_load[src_gpu] += len(plan[\"dsts\"])\n",
      "        return min(best_sources, key=lambda src: src_load[src])\n",
      "\n",
      "    def _group_bcast_transfers(self) -> List[RedistribStep]:\n",
      "        # Group data ids that should be transferred from \"src\" to \"dsts\"\n",
      "        src_to_transfers = defaultdict(lambda: defaultdict(list))\n",
      "        for (data_id, key), plan in self.transfer_plan.items():\n",
      "            src_to_transfers[(plan[\"src\"], key)][tuple(sorted(plan[\"dsts\"]))].append(\n",
      "                data_id\n",
      "            )\n",
      "\n",
      "        stages = []\n",
      "        while any(src_to_transfers.values()):\n",
      "            stage = []\n",
      "            used_dsts = set()\n",
      "            used_srcs = set()\n",
      "\n",
      "            for (src, key), transfers in list(src_to_transfers.items()):\n",
      "                if src in used_srcs:\n",
      "                    continue\n",
      "                if not transfers:\n",
      "                    continue\n",
      "\n",
      "                # Find a transfer that can be concurrent executed.\n",
      "                pop_key = None\n",
      "                for i, dsts in enumerate(transfers):\n",
      "                    if not any(dst in used_dsts for dst in dsts):\n",
      "                        pop_key = dsts\n",
      "                        break\n",
      "\n",
      "                if pop_key is not None:\n",
      "                    data_ids = transfers.pop(pop_key)\n",
      "                    stage.append(\n",
      "                        RedistribStep(\n",
      "                            comm_type=\"bcast\",\n",
      "                            root=src,\n",
      "                            srcs=[src],\n",
      "                            keys=[key],\n",
      "                            dsts=pop_key,\n",
      "                            ids=data_ids,\n",
      "                        )\n",
      "                    )\n",
      "                    used_dsts.update(dsts)\n",
      "                    used_srcs.add(src)\n",
      "\n",
      "            if stage:\n",
      "                stages += stage\n",
      "            else:\n",
      "                for (src, key), transfers in list(src_to_transfers.items()):\n",
      "                    if transfers:\n",
      "                        dsts, data_ids = transfers.pop(0)\n",
      "                        stages.append(\n",
      "                            RedistribStep(\n",
      "                                comm_type=\"bcast\",\n",
      "                                srcs=[src],\n",
      "                                root=src,\n",
      "                                dsts=dsts,\n",
      "                                ids=data_ids,\n",
      "                                keys=[key],\n",
      "                            )\n",
      "                        )\n",
      "                        break\n",
      "\n",
      "        return stages\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/data_manager.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import bisect\n",
      "import dataclasses\n",
      "import itertools\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "from realhf.api.core.config import ModelName, ModelShardID\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "from realhf.base import constants, logging\n",
      "from realhf.base.topology import ProcessTopology, new_or_get_group\n",
      "from realhf.impl.model.comm.global_comm import filter_match_mwids\n",
      "from realhf.system.redistributor import RedistribStep\n",
      "\n",
      "BCAST_GROUPS = {}\n",
      "GATHER_GROUPS = {}\n",
      "SCATTER_GROUPS = {}\n",
      "\n",
      "logger = logging.getLogger(\"data_manager\", \"system\")\n",
      "\n",
      "\n",
      "def find_minimal_superset(A: List[Set[int]], B: Set[int]) -> Set[int] | None:\n",
      "    min_size = float(\"inf\")\n",
      "    result = None\n",
      "    for S in A:\n",
      "        if B.issubset(S):\n",
      "            if len(S) < min_size:\n",
      "                min_size = len(S)\n",
      "                result = S\n",
      "    return result\n",
      "\n",
      "\n",
      "class DataManager:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model_topos: Dict[ModelName, ProcessTopology],\n",
      "        msid2mwid: Optional[Dict[ModelShardID, int]] = None,\n",
      "        data_transfer_pairs: Optional[List[Tuple[ModelName, ModelName]]] = None,\n",
      "    ):\n",
      "        self.model_topos = model_topos\n",
      "        self.msid2mwid = msid2mwid\n",
      "        self.data_transfer_pairs = data_transfer_pairs\n",
      "\n",
      "        self.storage: Dict[Hashable, SequenceSample] = {}\n",
      "\n",
      "    def setup_process_groups(self):\n",
      "        if self.msid2mwid is None or self.data_transfer_pairs is None:\n",
      "            return\n",
      "\n",
      "        model_topos = self.model_topos\n",
      "        msid2mwid = self.msid2mwid\n",
      "        data_transfer_pairs = self.data_transfer_pairs\n",
      "\n",
      "        # Stores the ranks given a (model_name, dp_rank) pair.\n",
      "        # These workers correspond to a complete set of model parameters sharded by TP+PP.\n",
      "        mw_dp_ranks: Dict[Tuple[ModelName, int], List[int]] = {}\n",
      "\n",
      "        mw_ranks: Dict[ModelName, List[int]] = {}\n",
      "\n",
      "        # Stores the dp_head (i.e., tp_rank=0, pp_rank=-1) ranks given a model_name.\n",
      "        mw_dp_head_ranks: Dict[ModelName, List[int]] = defaultdict(list)\n",
      "\n",
      "        assert msid2mwid is not None\n",
      "        for model_name, topo in model_topos.items():\n",
      "            mw_ranks[model_name] = filter_match_mwids(\n",
      "                model_name,\n",
      "                topo,\n",
      "                msid2mwid,\n",
      "            )\n",
      "            mw_dp_head_ranks[model_name] = filter_match_mwids(\n",
      "                model_name,\n",
      "                topo,\n",
      "                msid2mwid,\n",
      "                pipe=topo.get_dim(\"pipe\") - 1,\n",
      "                tensor=0,\n",
      "            )\n",
      "            dp_size = topo.get_dim(\"data\")\n",
      "            for dp_i in range(dp_size):\n",
      "                mw_dp_ranks[model_name, dp_i] = filter_match_mwids(\n",
      "                    model_name,\n",
      "                    topo,\n",
      "                    msid2mwid,\n",
      "                    data=dp_i,\n",
      "                )\n",
      "\n",
      "        for src, dst in data_transfer_pairs:\n",
      "            src_topo = model_topos[src]\n",
      "            dst_topo = model_topos[dst]\n",
      "\n",
      "            ranks = tuple(sorted(mw_dp_head_ranks[src]))\n",
      "            GATHER_GROUPS[ranks] = new_or_get_group(\n",
      "                list(ranks), backend=\"nccl\" if constants.use_cuda() else \"gloo\"\n",
      "            )\n",
      "\n",
      "            for rank in ranks:\n",
      "                scatter_ranks = tuple(sorted(set([rank] + mw_ranks[dst])))\n",
      "                SCATTER_GROUPS[scatter_ranks] = new_or_get_group(\n",
      "                    list(scatter_ranks),\n",
      "                    backend=\"nccl\" if constants.use_cuda() else \"gloo\",\n",
      "                )\n",
      "\n",
      "            # Construct all src-dst pairs, from any src dp rank to any dst dp rank.\n",
      "            # Note that a dp rank corresponds to multiple parameter shards (TP+PP),\n",
      "            # so each pair is a group-to-group communication.\n",
      "            # Since the models in the source group have duplicate data (TP+PP),\n",
      "            # we just use its \"head\" as the broadcast source,\n",
      "            # and broadcast to all the ranks in the destination group.\n",
      "            for src_dp, dst_dp in itertools.product(\n",
      "                range(src_topo.get_dim(\"data\")), range(dst_topo.get_dim(\"data\"))\n",
      "            ):\n",
      "                src_mw_rank = mw_dp_head_ranks[src][src_dp]\n",
      "                dst_mw_ranks = mw_dp_ranks[dst, dst_dp]\n",
      "                # The src and dst groups can be disjoint or overlapped.\n",
      "                # If they are disjoint, we need to include the src_mw_rank in the group.\n",
      "                # Otherwise, we only need to include the dst_mw_ranks.\n",
      "                if src_mw_rank not in dst_mw_ranks:\n",
      "                    _ranks = [src_mw_rank] + dst_mw_ranks\n",
      "                else:\n",
      "                    _ranks = dst_mw_ranks\n",
      "                key = tuple(sorted(_ranks))\n",
      "                BCAST_GROUPS[key] = new_or_get_group(\n",
      "                    _ranks, backend=\"nccl\" if constants.use_cuda() else \"gloo\"\n",
      "                )\n",
      "\n",
      "    def storage_size(self):\n",
      "        return len(self.storage)\n",
      "\n",
      "    def store(self, x: SequenceSample):\n",
      "        assert len(x.ids) == 1\n",
      "        assert x.ids[0] not in self.storage\n",
      "        self.storage[x.ids[0]] = x\n",
      "\n",
      "    def update(self, x: SequenceSample):\n",
      "        self.storage[x.ids[0]].update_(x)\n",
      "\n",
      "    def get(self, data_id: Hashable):\n",
      "        return self.storage[data_id]\n",
      "\n",
      "    def has_data(self, data_id: Hashable):\n",
      "        return data_id in self.storage\n",
      "\n",
      "    def remove(self, ids: List[Hashable]):\n",
      "        for data_id in ids:\n",
      "            if data_id in self.storage:\n",
      "                del self.storage[data_id]\n",
      "\n",
      "    def clear_data(self):\n",
      "        self.storage.clear()\n",
      "\n",
      "    def _bcast_recv(\n",
      "        self,\n",
      "        step: RedistribStep,\n",
      "        data_infos: Dict[Hashable, SequenceSample],\n",
      "    ):\n",
      "        assert len(step.keys) == 1\n",
      "        ids = step.ids\n",
      "        key = step.keys[0]\n",
      "        dtype = data_infos[ids[0]].dtypes[key]\n",
      "        total_len = sum(sum(data_infos[_id].seqlens[key][0]) for _id in ids)\n",
      "        trailing_shape = data_infos[ids[0]].trailing_shapes[key]\n",
      "\n",
      "        buf = torch.zeros(\n",
      "            (total_len, *trailing_shape),\n",
      "            dtype=dtype,\n",
      "            device=constants.current_device(),\n",
      "        )\n",
      "\n",
      "        if len(step.dsts) == 1:\n",
      "            dist.recv(buf, src=step.root)\n",
      "        else:\n",
      "            global BCAST_GROUPS\n",
      "            group = BCAST_GROUPS[tuple(sorted([step.root] + list(step.dsts)))]\n",
      "            dist.broadcast(buf, src=step.root, group=group)\n",
      "\n",
      "        # Split the received data and put it into the storage.\n",
      "        offset = 0\n",
      "        for _id in ids:\n",
      "            seqlens = data_infos[_id].seqlens[key]\n",
      "            assert len(seqlens) == 1\n",
      "            seqlen = sum(seqlens[0])\n",
      "            if buf is not None:\n",
      "                vs = buf[offset : offset + seqlen]\n",
      "            else:\n",
      "                vs = None\n",
      "            offset = offset + seqlen\n",
      "            with SequenceSample.disable_validation():\n",
      "                s = SequenceSample(\n",
      "                    keys=[key],\n",
      "                    dtypes={key: vs.dtype if vs is not None else None},\n",
      "                    trailing_shapes={key: vs.shape[1:] if vs is not None else None},\n",
      "                    ids=[_id],\n",
      "                    seqlens={key: seqlens},\n",
      "                    data={key: vs},\n",
      "                    metadata={},\n",
      "                )\n",
      "            if _id in self.storage:\n",
      "                self.storage[_id].update_(s)\n",
      "            else:\n",
      "                self.storage[_id] = s\n",
      "\n",
      "    def _bcast_send(self, step: RedistribStep):\n",
      "        ids = step.ids\n",
      "        for _id in ids:\n",
      "            self.storage[_id].to_device(constants.current_device())\n",
      "        assert len(step.keys) == 1\n",
      "        key = step.keys[0]\n",
      "        vs = torch.cat(\n",
      "            [self.storage[_id].data[key] for _id in ids],\n",
      "            dim=0,\n",
      "        )\n",
      "        if len(step.dsts) == 1:\n",
      "            dist.send(vs, dst=step.dsts[0])\n",
      "        else:\n",
      "            global BCAST_GROUPS\n",
      "            group = BCAST_GROUPS[tuple(sorted([step.root] + list(step.dsts)))]\n",
      "            dist.broadcast(vs, src=step.root, group=group)\n",
      "\n",
      "    def _run_bcast(\n",
      "        self, step: RedistribStep, data_infos: Dict[Hashable, SequenceSample]\n",
      "    ):\n",
      "        if dist.get_rank() in step.dsts:\n",
      "            self._bcast_recv(step, data_infos=data_infos)\n",
      "\n",
      "        if dist.get_rank() == step.root:\n",
      "            self._bcast_send(step)\n",
      "\n",
      "    def _pad_data(self, x: torch.Tensor, maxlen: int):\n",
      "        assert x.dtype == torch.float32\n",
      "        assert len(x.shape) == 1\n",
      "        if maxlen > x.numel():\n",
      "            return torch.nn.functional.pad(x, (0, maxlen - x.numel()), value=0.0)\n",
      "        return x\n",
      "\n",
      "    def _run_gather(\n",
      "        self, step: RedistribStep, data_infos: Dict[Hashable, SequenceSample]\n",
      "    ):\n",
      "        # It's possible that some DP rank is not involved.\n",
      "        # Create dummpy data to make the gather happy.\n",
      "        gather_ranks = find_minimal_superset(\n",
      "            [set(k) for k in GATHER_GROUPS.keys()], set(step.srcs)\n",
      "        )\n",
      "        assert gather_ranks is not None, (\n",
      "            set(step.srcs),\n",
      "            [set(k) for k in GATHER_GROUPS.keys()],\n",
      "        )\n",
      "        gather_ranks = sorted(list(gather_ranks))\n",
      "\n",
      "        pgroup = GATHER_GROUPS[tuple(gather_ranks)]\n",
      "\n",
      "        if dist.get_rank() not in gather_ranks:\n",
      "            return\n",
      "\n",
      "        maxlen = 0\n",
      "        for ids in step.ids:\n",
      "            infos = [data_infos[i] for i in ids]\n",
      "            maxlen = max(\n",
      "                maxlen,\n",
      "                sum(\n",
      "                    [\n",
      "                        sum([sum(info.seqlens[key][0]) for info in infos])\n",
      "                        for key in step.keys\n",
      "                    ]\n",
      "                ),\n",
      "            )\n",
      "\n",
      "        if dist.get_rank() == step.root:\n",
      "            gather_list = [\n",
      "                torch.empty(\n",
      "                    maxlen, device=constants.current_device(), dtype=torch.float32\n",
      "                )\n",
      "                for _ in gather_ranks\n",
      "            ]\n",
      "            is_valid_gather = [i in step.srcs for i in gather_ranks]\n",
      "        else:\n",
      "            gather_list = None\n",
      "\n",
      "        if dist.get_rank() in step.srcs:\n",
      "            local_gather_idx = step.srcs.index(dist.get_rank())\n",
      "            ids = step.ids[local_gather_idx]\n",
      "            for i in ids:\n",
      "                self.storage[i].to_device(constants.current_device())\n",
      "            samples = [self.storage[i] for i in ids]\n",
      "            data = torch.cat(\n",
      "                [\n",
      "                    sample.data[key].float().flatten()\n",
      "                    for sample in samples\n",
      "                    for key in step.keys\n",
      "                ]\n",
      "            )\n",
      "            data = self._pad_data(data, maxlen)\n",
      "        else:\n",
      "            data = torch.empty(\n",
      "                maxlen, device=constants.current_device(), dtype=torch.float32\n",
      "            )\n",
      "\n",
      "        dist.gather(\n",
      "            data,\n",
      "            gather_list,\n",
      "            dst=step.root,\n",
      "            group=pgroup,\n",
      "        )\n",
      "\n",
      "        if dist.get_rank() != step.root:\n",
      "            del data\n",
      "            return\n",
      "\n",
      "        cnt = 0\n",
      "        for is_valid, buf in zip(is_valid_gather, gather_list):\n",
      "            if not is_valid:\n",
      "                continue\n",
      "            ids = step.ids[cnt]\n",
      "            offset = 0\n",
      "            for i in ids:\n",
      "                for key in step.keys:\n",
      "                    seqlen = data_infos[i].seqlens[key][0]\n",
      "                    dtype = data_infos[i].dtypes[key]\n",
      "                    trailing_shape = data_infos[i].trailing_shapes[key]\n",
      "                    size = int(np.prod(trailing_shape) * sum(seqlen))\n",
      "                    data = buf[offset : offset + size].to(dtype)\n",
      "                    offset += size\n",
      "                    # with SequenceSample.disable_validation():\n",
      "                    s = SequenceSample(\n",
      "                        keys=[key],\n",
      "                        dtypes={key: dtype},\n",
      "                        trailing_shapes={key: trailing_shape},\n",
      "                        ids=[i],\n",
      "                        seqlens={key: [seqlen]},\n",
      "                        data={key: data},\n",
      "                        metadata={},\n",
      "                    )\n",
      "                    if i in self.storage:\n",
      "                        self.storage[i].update_(s)\n",
      "                    else:\n",
      "                        self.storage[i] = s\n",
      "            cnt += 1\n",
      "        assert cnt == len(step.srcs) == len(step.ids)\n",
      "        del data\n",
      "\n",
      "    def _run_scatter(\n",
      "        self, step: RedistribStep, data_infos: Dict[Hashable, SequenceSample]\n",
      "    ):\n",
      "        if dist.get_rank() != step.root and dist.get_rank() not in step.dsts:\n",
      "            return\n",
      "\n",
      "        maxlen = 0\n",
      "        for ids in step.ids:\n",
      "            infos = [data_infos[i] for i in ids]\n",
      "            maxlen = max(\n",
      "                maxlen,\n",
      "                sum(\n",
      "                    [\n",
      "                        sum([sum(info.seqlens[key][0]) for info in infos])\n",
      "                        for key in step.keys\n",
      "                    ]\n",
      "                ),\n",
      "            )\n",
      "\n",
      "        buf = torch.empty(\n",
      "            maxlen, device=constants.current_device(), dtype=torch.float32\n",
      "        )\n",
      "\n",
      "        if dist.get_rank() == step.root:\n",
      "            # Scatter destinations include all DP, TP, and PP ranks\n",
      "            # and data is duplicated among TP/PP groups\n",
      "            # We allocate new memory for DP ranks, but use the same pointer\n",
      "            # for all TP and PP ranks to save memory.\n",
      "            scatter_clusters = []\n",
      "            for idx, ids in enumerate(step.ids):\n",
      "                for _ids, idx_list in scatter_clusters:\n",
      "                    if set(ids) == set(_ids):\n",
      "                        idx_list.append(idx)\n",
      "                        break\n",
      "                else:\n",
      "                    scatter_clusters.append((ids, [idx]))\n",
      "            scatter_list = [None for _ in range(len(step.ids))]\n",
      "            before_pad = []\n",
      "            for ids, idx_list in scatter_clusters:\n",
      "                for i in ids:\n",
      "                    self.storage[i].to_device(constants.current_device())\n",
      "                samples = [self.storage[i] for i in ids]\n",
      "                data = torch.cat(\n",
      "                    [\n",
      "                        sample.data[key].float().flatten()\n",
      "                        for sample in samples\n",
      "                        for key in step.keys\n",
      "                    ]\n",
      "                )\n",
      "                before_pad.append(data)\n",
      "\n",
      "            maxlen = max([x.shape[0] for x in before_pad])\n",
      "            after_pad = [self._pad_data(x, maxlen) for x in before_pad]\n",
      "            for (ids, idx_list), data in zip(scatter_clusters, after_pad):\n",
      "                for idx in idx_list:\n",
      "                    scatter_list[idx] = data\n",
      "\n",
      "            assert all([torch.is_tensor(t) for t in scatter_list])\n",
      "\n",
      "            if step.root not in step.dsts:\n",
      "                idx = bisect.bisect(step.dsts, step.root)\n",
      "                scatter_list.insert(idx, buf)\n",
      "        else:\n",
      "            scatter_list = None\n",
      "\n",
      "        key = tuple(sorted(set([step.root] + step.dsts)))\n",
      "        dist.scatter(buf, scatter_list, src=step.root, group=SCATTER_GROUPS[key])\n",
      "\n",
      "        if dist.get_rank() not in step.dsts:\n",
      "            return\n",
      "\n",
      "        local_dst_idx = step.dsts.index(dist.get_rank())\n",
      "        ids = step.ids[local_dst_idx]\n",
      "        offset = 0\n",
      "        for i in ids:\n",
      "            for key in step.keys:\n",
      "                seqlen = data_infos[i].seqlens[key][0]\n",
      "                dtype = data_infos[i].dtypes[key]\n",
      "                trailing_shape = data_infos[i].trailing_shapes[key]\n",
      "                size = int(np.prod(trailing_shape) * sum(seqlen))\n",
      "                data = buf[offset : offset + size].to(dtype)\n",
      "                offset += size\n",
      "                # with SequenceSample.disable_validation():\n",
      "                s = SequenceSample(\n",
      "                    keys=[key],\n",
      "                    dtypes={key: dtype},\n",
      "                    trailing_shapes={key: trailing_shape},\n",
      "                    ids=[i],\n",
      "                    seqlens={key: [seqlen]},\n",
      "                    data={key: data},\n",
      "                    metadata={},\n",
      "                )\n",
      "                if i in self.storage:\n",
      "                    self.storage[i].update_(s)\n",
      "                else:\n",
      "                    self.storage[i] = s\n",
      "\n",
      "    def redistribute(\n",
      "        self,\n",
      "        data_info: SequenceSample,\n",
      "        plan: List[RedistribStep],\n",
      "    ):\n",
      "        data_infos = {x.ids[0]: x for x in data_info.unpack()}\n",
      "\n",
      "        for step in plan:\n",
      "            if step.comm_type == \"bcast\":\n",
      "                self._run_bcast(step, data_infos)\n",
      "            elif step.comm_type == \"gather\":\n",
      "                self._run_gather(step, data_infos)\n",
      "            elif step.comm_type == \"scatter\":\n",
      "                self._run_scatter(step, data_infos)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/generation_server.py ====\n",
      "\n",
      "import os\n",
      "import subprocess\n",
      "import sys\n",
      "import time\n",
      "from pathlib import Path\n",
      "\n",
      "import ray\n",
      "import requests\n",
      "\n",
      "from realhf.api.cli_args import SGLangConfig\n",
      "from realhf.api.core.system_api import ExpStatus\n",
      "from realhf.api.core.system_api import GenerationServer as GenerationServerConfig\n",
      "from realhf.base import (\n",
      "    constants,\n",
      "    gpu_utils,\n",
      "    logging,\n",
      "    name_resolve,\n",
      "    names,\n",
      "    network,\n",
      "    pkg_version,\n",
      "    seeding,\n",
      ")\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.system.worker_base import PollResult, Worker\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def execute_shell_command(command: str) -> subprocess.Popen:\n",
      "    \"\"\"\n",
      "    Execute a shell command and return its process handle.\n",
      "    \"\"\"\n",
      "    # Replace newline continuations and split the command string.\n",
      "    command = command.replace(\"\\\\\\n\", \" \").replace(\"\\\\\", \" \")\n",
      "    parts = command.split()\n",
      "    return subprocess.Popen(\n",
      "        parts,\n",
      "        text=True,\n",
      "        stdout=sys.stdout,\n",
      "        stderr=subprocess.STDOUT,\n",
      "    )\n",
      "\n",
      "\n",
      "def apply_sglang_path():\n",
      "    p = Path(os.path.dirname(__file__))\n",
      "    patch_path = str(\n",
      "        p.parent.parent\n",
      "        / \"patch\"\n",
      "        / \"sglang\"\n",
      "        / f\"v{pkg_version.get_version('sglang')}.patch\"\n",
      "    )\n",
      "\n",
      "    target_path = \"\"\n",
      "    sglang_meta = subprocess.check_output(\n",
      "        \"python3 -m pip show sglang\", shell=True\n",
      "    ).decode(\"ascii\")\n",
      "    for line in sglang_meta.split(\"\\n\"):\n",
      "        line = line.strip()\n",
      "        if line.startswith(\"Editable project location: \"):\n",
      "            target_path = str(Path(line.split(\": \")[1]).parent)\n",
      "\n",
      "    if target_path:\n",
      "        proc = subprocess.Popen(\n",
      "            [\"git\", \"apply\", patch_path],\n",
      "            cwd=target_path,\n",
      "            stderr=sys.stdout,\n",
      "            stdout=sys.stdout,\n",
      "        )\n",
      "        proc.wait()\n",
      "        logger.info(f\"Applied SGLang patch at {target_path}\")\n",
      "\n",
      "\n",
      "def launch_server_cmd(command: str, port: int = 30000):\n",
      "    \"\"\"\n",
      "    Launch the server using the given command.\n",
      "    If no port is specified, a free port is reserved.\n",
      "    \"\"\"\n",
      "    if not ray.is_initialized():\n",
      "        apply_sglang_path()\n",
      "    assert port is not None\n",
      "    full_command = f\"{command} --port {port}\"\n",
      "    process = execute_shell_command(full_command)\n",
      "    return process, port\n",
      "\n",
      "\n",
      "def terminate_process(process, port=None):\n",
      "    \"\"\"\n",
      "    Terminate the process and, if a port was reserved, release it.\n",
      "    \"\"\"\n",
      "    from sglang.srt.utils import kill_process_tree\n",
      "\n",
      "    kill_process_tree(process.pid)\n",
      "\n",
      "\n",
      "def wait_for_server(base_url: str, timeout: int = None) -> None:\n",
      "    \"\"\"Wait for the server to be ready by polling the /v1/models endpoint.\n",
      "\n",
      "    Args:\n",
      "        base_url: The base URL of the server\n",
      "        timeout: Maximum time to wait in seconds. None means wait forever.\n",
      "    \"\"\"\n",
      "    start_time = time.time()\n",
      "    while True:\n",
      "        try:\n",
      "            response = requests.get(\n",
      "                f\"{base_url}/v1/models\",\n",
      "                headers={\"Authorization\": \"Bearer None\"},\n",
      "            )\n",
      "            if response.status_code == 200:\n",
      "                time.sleep(5)\n",
      "                break\n",
      "\n",
      "            if timeout and time.time() - start_time > timeout:\n",
      "                raise TimeoutError(\"Server did not become ready within timeout period\")\n",
      "        except requests.exceptions.RequestException:\n",
      "            time.sleep(1)\n",
      "\n",
      "\n",
      "PORT_CLEARANCE_PERIOD = 90\n",
      "\n",
      "\n",
      "class GenerationServer(Worker):\n",
      "    def _configure(self, config: GenerationServerConfig):\n",
      "        self.config = config\n",
      "        self.worker_index = config.worker_info.worker_index\n",
      "        self.worker_count = config.worker_info.worker_count\n",
      "        self.experiment_name = config.worker_info.experiment_name\n",
      "        self.trial_name = config.worker_info.trial_name\n",
      "        seeding.set_random_seed(\n",
      "            config.base_seed, f\"generation_server{self.worker_index}\"\n",
      "        )\n",
      "\n",
      "        # Cancel the effect of CUDA device isolation\n",
      "        if ray.is_initialized():\n",
      "            self.base_gpu_id = 0\n",
      "        elif \"CUDA_VISIBLE_DEVICES\" in os.environ:\n",
      "            self.base_gpu_id = int(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
      "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(\n",
      "                map(str, range(gpu_utils.gpu_count()))\n",
      "            )\n",
      "        else:\n",
      "            servers_per_node = cluster_spec.n_gpus_per_node // self.config.tp_size\n",
      "            idx_on_this_node = self.worker_index % servers_per_node\n",
      "            self.base_gpu_id = idx_on_this_node * self.config.tp_size\n",
      "\n",
      "        self.server_process = None\n",
      "        self.server_addr = None\n",
      "\n",
      "        return config.worker_info\n",
      "\n",
      "    def launch_server_subprocess(self):\n",
      "        config = self.config\n",
      "\n",
      "        assert config.backend_type == \"sglang\"\n",
      "\n",
      "        host_ip = network.gethostip()\n",
      "        host = \"localhost\" if not config.backend_args.enable_metrics else host_ip\n",
      "\n",
      "        # NOTE: Ports returned by `find_multiple_free_ports` are unique,\n",
      "        # but SGLang servers still encounter conflicts.\n",
      "        # Use a clearance period to hack over this issue.\n",
      "        servers_per_node = cluster_spec.n_gpus_per_node // self.config.tp_size\n",
      "        idx_on_this_node = self.worker_index % servers_per_node\n",
      "        time.sleep(idx_on_this_node * PORT_CLEARANCE_PERIOD / servers_per_node)\n",
      "\n",
      "        ports = network.find_multiple_free_ports(\n",
      "            2,\n",
      "            low=10000,\n",
      "            high=60000,\n",
      "            experiment_name=self.experiment_name,\n",
      "            trial_name=self.trial_name,\n",
      "        )\n",
      "        server_port = ports[0]\n",
      "        nccl_port = ports[1]\n",
      "\n",
      "        cmd = SGLangConfig.build_cmd(\n",
      "            config.backend_args,\n",
      "            config.model_path,\n",
      "            tp_size=config.tp_size,\n",
      "            server_index=self.worker_index,\n",
      "            base_gpu_id=self.base_gpu_id,\n",
      "            dist_init_addr=f\"{host}:{nccl_port}\",\n",
      "        )\n",
      "\n",
      "        self.server_process, self.server_port = launch_server_cmd(cmd, port=server_port)\n",
      "        self.server_addr = f\"http://{host}:{self.server_port}\"\n",
      "\n",
      "        wait_for_server(self.server_addr)\n",
      "\n",
      "        name = names.gen_servers(self.experiment_name, self.trial_name)\n",
      "        name_resolve.add_subentry(name, self.server_addr)\n",
      "\n",
      "        key = names.metric_server(\n",
      "            self.experiment_name,\n",
      "            self.trial_name,\n",
      "            \"sglang\",\n",
      "            f\"server{self.worker_index}\",\n",
      "        )\n",
      "        name_resolve.add(\n",
      "            key, f\"{host}:{self.server_port}\", keepalive_ttl=None, delete_on_exit=True\n",
      "        )\n",
      "\n",
      "        logger.info(f\"SGLang server launched at: {self.server_addr}\")\n",
      "\n",
      "    def _poll(self):\n",
      "        if self.server_process is None:\n",
      "            self.launch_server_subprocess()\n",
      "\n",
      "        # Check experiment finish.\n",
      "        name = names.experiment_status(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        try:\n",
      "            exp_status = name_resolve.wait(name, timeout=300)\n",
      "            if exp_status != str(ExpStatus.RUNNING):\n",
      "                self.exit()\n",
      "                return PollResult(0, 0)\n",
      "        except TimeoutError:\n",
      "            raise TimeoutError(\n",
      "                f\"Waiting for experiment status timeout. \"\n",
      "                \"This indicates that the master worker is not running. Exit the worker.\"\n",
      "            )\n",
      "\n",
      "        time.sleep(5)\n",
      "\n",
      "        return PollResult(0, 0)\n",
      "\n",
      "    def _exit_hook(self, exit_status):\n",
      "        if self.server_process is not None and self.config.backend_type == \"sglang\":\n",
      "\n",
      "            terminate_process(self.server_process)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/stream_dataset.py ====\n",
      "\n",
      "import queue\n",
      "import sys\n",
      "import threading\n",
      "import time\n",
      "from typing import Any, List, Optional\n",
      "\n",
      "from torch.utils.data import ConcatDataset, Dataset\n",
      "\n",
      "from realhf.api.core.config import DatasetAbstraction\n",
      "from realhf.api.core.data_api import (\n",
      "    DatasetUtility,\n",
      "    SequenceSample,\n",
      "    make_dataset,\n",
      "    register_dataset,\n",
      ")\n",
      "from realhf.base import constants, logging\n",
      "from realhf.system.push_pull_stream import NameResolvingZmqPuller\n",
      "\n",
      "logger = logging.getLogger(\"StreamDataset\")\n",
      "\n",
      "\n",
      "class PullerStreamDataset(Dataset):\n",
      "    def __init__(\n",
      "        self,\n",
      "        util: DatasetUtility,\n",
      "        dataset_cfgs: List[DatasetAbstraction],\n",
      "        pull_timeout_ms=100,\n",
      "    ):\n",
      "        # This dataset is just used for computing the dataset size,\n",
      "        # and the number of steps per epoch.\n",
      "        datasets = [\n",
      "            make_dataset(\n",
      "                dataset_cfg,\n",
      "                seed=util.seed,\n",
      "                dp_rank=util.dp_rank,\n",
      "                world_size=util.world_size,\n",
      "                tokenizer_or_tokenizer_name=util.tokenizer,\n",
      "                experiment_name=constants.experiment_name(),\n",
      "                trial_name=constants.trial_name(),\n",
      "            )\n",
      "            for dataset_cfg in dataset_cfgs\n",
      "        ]\n",
      "        if len(datasets) == 1:\n",
      "            dataset = datasets[0]\n",
      "        else:\n",
      "            dataset = ConcatDataset(datasets)\n",
      "        self.dataset_size = len(dataset)\n",
      "        del dataset, datasets\n",
      "\n",
      "        self.pull_timeout_ms = pull_timeout_ms\n",
      "        self.data_queue = queue.Queue(maxsize=self.dataset_size * util.world_size)\n",
      "        self._stop_event = threading.Event()\n",
      "\n",
      "        # Pass ZMQ context (thread-safe) and let worker create the socket\n",
      "        self.util = util\n",
      "        self.worker_thread = threading.Thread(target=self._pull_data_worker)\n",
      "        self.worker_thread.start()\n",
      "\n",
      "    def _pull_data_worker(self):\n",
      "        \"\"\"Worker thread that creates its own ZMQ puller and streams data.\"\"\"\n",
      "        # Initialize the puller inside the worker thread\n",
      "        stream = NameResolvingZmqPuller(\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            puller_index=self.util.dp_rank,\n",
      "        )\n",
      "        try:\n",
      "            while not self._stop_event.is_set():\n",
      "                try:\n",
      "                    data = stream.pull(timeout_ms=self.pull_timeout_ms)\n",
      "                    processed_data = [\n",
      "                        SequenceSample.from_json_compatible(x) for x in data\n",
      "                    ]\n",
      "                    logger.debug(\n",
      "                        f\"Get data {[x.ids[0] for x in processed_data]} from puller stream.\"\n",
      "                    )\n",
      "                    self.data_queue.put(processed_data)\n",
      "                except queue.Empty:\n",
      "                    logger.debug(f\"No data from puller stream.\")\n",
      "                    time.sleep(0.1)\n",
      "                    continue\n",
      "        finally:\n",
      "            # Ensure socket is closed in the same thread\n",
      "            del stream\n",
      "            # Exit if this thread has an error\n",
      "            sys.exit(1)\n",
      "\n",
      "    def __getitem__(self, idx: int) -> Optional[Any]:\n",
      "        samples = []\n",
      "        while True:\n",
      "            try:\n",
      "                samples += self.data_queue.get_nowait()\n",
      "            except queue.Empty:\n",
      "                break\n",
      "        return samples\n",
      "\n",
      "    def __len__(self) -> int:\n",
      "        return self.dataset_size\n",
      "\n",
      "    def __del__(self):\n",
      "        self._stop_event.set()\n",
      "        if self.worker_thread.is_alive():\n",
      "            self.worker_thread.join(timeout=1.0)\n",
      "\n",
      "\n",
      "register_dataset(\"puller_stream\", PullerStreamDataset)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/model_worker.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import contextlib\n",
      "import copy\n",
      "import gc\n",
      "import itertools\n",
      "import json\n",
      "import multiprocessing as mp\n",
      "import os\n",
      "import pickle\n",
      "import queue\n",
      "import re\n",
      "import shutil\n",
      "import socket\n",
      "import time\n",
      "import uuid\n",
      "from collections import defaultdict\n",
      "from pathlib import Path\n",
      "from typing import Any, Dict, Hashable, List, Optional, Set, Tuple\n",
      "\n",
      "import numpy as np\n",
      "import pynvml\n",
      "import tabulate\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.utils.data\n",
      "\n",
      "import realhf.impl.model.comm.global_comm as global_comm\n",
      "import realhf.impl.model.comm.param_realloc as param_realloc_comm\n",
      "from realhf.api.core import data_api, dfg, model_api, system_api\n",
      "from realhf.api.core.config import ModelName\n",
      "from realhf.base import (\n",
      "    constants,\n",
      "    gpu_utils,\n",
      "    logging,\n",
      "    name_resolve,\n",
      "    names,\n",
      "    network,\n",
      "    recover,\n",
      "    seeding,\n",
      "    timeutil,\n",
      "    topology,\n",
      ")\n",
      "from realhf.base.monitor import (\n",
      "    CUDATimeMarkType,\n",
      "    cuda_tmark,\n",
      "    cuda_tmarked,\n",
      "    dump_tmark_db,\n",
      ")\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.utils import cuda_graph\n",
      "from realhf.system import request_reply_stream, worker_base\n",
      "from realhf.system.data_manager import DataManager\n",
      "from realhf.system.redistributor import RedistribStep\n",
      "from realhf.system.stream_dataset import PullerStreamDataset\n",
      "\n",
      "# NOTE: Register all implemented datasets and models.\n",
      "import realhf.impl.dataset  # isort:skip\n",
      "import realhf.impl.model  # isort:skip\n",
      "\n",
      "logger = logging.getLogger(\"Model Worker\", \"colored\")\n",
      "blogger = logging.getLogger(\"benchmark\")\n",
      "\n",
      "TIME_RECORD_RPCS = [\n",
      "    \"generate\",\n",
      "    \"inference\",\n",
      "    \"train_step\",\n",
      "    \"initialize\",\n",
      "]\n",
      "NON_BLOCKING_RPCS = [\n",
      "    \"model_config\",\n",
      "    \"fetch\",\n",
      "    \"spec\",\n",
      "    \"clear_data_cache\",\n",
      "]\n",
      "\n",
      "# The model worker will poll requests from the master worker for this many seconds.\n",
      "# Increase the value if the model worker cannot receive concurrent requests in time.\n",
      "_MODEL_WORKER_POLL_REQUESTS_SECS = 0.1\n",
      "_MODEL_WORKER_POLL_REQUESTS_INTERVAL_SECS = 0.01\n",
      "\n",
      "\n",
      "def get_pytorch_profiler(kernel_only: bool, enabled: bool = True):\n",
      "    if enabled and kernel_only:\n",
      "        return torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CUDA])\n",
      "    elif enabled:\n",
      "        return torch.profiler.profile(\n",
      "            activities=[\n",
      "                torch.profiler.ProfilerActivity.CPU,\n",
      "                torch.profiler.ProfilerActivity.CUDA,\n",
      "            ],\n",
      "            record_shapes=True,\n",
      "            profile_memory=True,\n",
      "            with_stack=True,\n",
      "            with_flops=True,\n",
      "        )\n",
      "    else:\n",
      "        return contextlib.nullcontext()\n",
      "\n",
      "\n",
      "class NoRequestToHandle(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class ModelWorker(worker_base.Worker):\n",
      "    _setup_counter = -1\n",
      "\n",
      "    def _configure(self, cfg: system_api.ModelWorker):\n",
      "        self._setup_counter += 1\n",
      "\n",
      "        self.config = cfg\n",
      "        self.model_names = [s.id.model_name for s in cfg.shards]\n",
      "\n",
      "        self.__experiment_name = self.config.worker_info.experiment_name\n",
      "        self.__trial_name = self.config.worker_info.trial_name\n",
      "\n",
      "        self.data_consumers = self.config.model_rpcs[0].data_consumers\n",
      "\n",
      "        self.__worker_index = cfg.worker_info.worker_index\n",
      "\n",
      "        seeding.set_random_seed(cfg.base_seed, f\"model_worker{self.__worker_index}\")\n",
      "\n",
      "        # Reveal process group identity of this worker to world.\n",
      "        gpu_utils.reveal_pg_identity(\n",
      "            self.__experiment_name, self.__trial_name, self.__worker_index\n",
      "        )\n",
      "        self.__dist_env_resolved = False\n",
      "\n",
      "        self.__clear_cache_frequency = timeutil.FrequencyControl(\n",
      "            frequency_steps=self.config.cuda_cache_clear_freq\n",
      "        )\n",
      "        self.torch_cache_mysophobia = cfg.torch_cache_mysophobia\n",
      "\n",
      "        r = self.config.worker_info\n",
      "\n",
      "        # recover info\n",
      "        self.__recover_run, self.__recover_info = recover.load_recover_info()\n",
      "\n",
      "        # Whether to enable profiling is controlled by the following environment variables.\n",
      "        self.__enable_profiler = os.getenv(\"REAL_DUMP_TRACE\", \"0\") == \"1\"\n",
      "        self.__record_performance = os.getenv(\"REAL_RECORD_PERFORMANCE\", \"0\") == \"1\"\n",
      "        self.__enable_memory_dump = os.getenv(\"REAL_DUMP_MEMORY\", \"0\") == \"1\"\n",
      "        self.__performance_recorder = dict()\n",
      "\n",
      "        # Add an additional subscript pattern for source RPCs.\n",
      "        self.__has_dataset = False\n",
      "        self.__dataset_dp_size = self.__dataset_dp_rank = 0\n",
      "        sub_patterns = [s.id for s in self.config.shards]\n",
      "        self.src_rpc = src_rpc = [rpc for rpc in self.config.model_rpcs if rpc.is_src][\n",
      "            0\n",
      "        ]\n",
      "        for s in self.config.shards:\n",
      "            _pp_size = s.id.topo.get_dim(\"pipe\")\n",
      "            if not (s.id.tp_rank == 0 and s.id.pp_rank == _pp_size - 1):\n",
      "                continue\n",
      "            if src_rpc.model_name == s.id.model_name:\n",
      "                self.__has_dataset = True\n",
      "                self.__dataset_dp_size = s.id.topo.get_dim(\"data\")\n",
      "                self.__dataset_dp_rank = s.id.dp_rank\n",
      "                sub_patterns.append(f\"__data{self.__dataset_dp_rank}__\")\n",
      "                break\n",
      "\n",
      "        if self.__has_dataset:\n",
      "            name = names.stream_pullers(self.__experiment_name, self.__trial_name)\n",
      "            name_resolve.add_subentry(name, str(self.__dataset_dp_rank))\n",
      "\n",
      "        return r\n",
      "\n",
      "    def _get_recover_ckpt_path(self, role: str):\n",
      "        if not self.__recover_run:\n",
      "            return None\n",
      "        epoch = self.__recover_info.last_step_info.epoch + 1\n",
      "        epochstep = self.__recover_info.last_step_info.epoch_step + 1\n",
      "        globalstep = self.__recover_info.last_step_info.global_step + 1\n",
      "        save_root = os.path.join(\n",
      "            constants.MODEL_SAVE_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "        )\n",
      "        if epoch > 0:\n",
      "            role_path = os.path.join(save_root, role)\n",
      "            if os.path.exists(role_path):\n",
      "                model_path = os.path.join(\n",
      "                    role_path,\n",
      "                    f\"epoch{epoch}epochstep{epochstep}globalstep{globalstep}\",\n",
      "                )\n",
      "                if not os.path.exists(model_path):\n",
      "                    raise RuntimeError(\n",
      "                        f\"Guessed checkpoint path {model_path} does not exist. \"\n",
      "                        \"Skip loading checkpoints in the recovered run.\"\n",
      "                    )\n",
      "                return model_path\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def _tp_rank(self) -> int:\n",
      "        return constants.tensor_parallel_rank()\n",
      "\n",
      "    @property\n",
      "    def _pp_rank(self) -> int:\n",
      "        return constants.pipe_parallel_rank()\n",
      "\n",
      "    @property\n",
      "    def _dp_rank(self) -> int:\n",
      "        return constants.data_parallel_rank()\n",
      "\n",
      "    @property\n",
      "    def _pp_size(self) -> int:\n",
      "        return constants.pipe_parallel_world_size()\n",
      "\n",
      "    @property\n",
      "    def _tp_size(self) -> int:\n",
      "        return constants.tensor_parallel_world_size()\n",
      "\n",
      "    @property\n",
      "    def _dp_size(self) -> int:\n",
      "        return constants.data_parallel_world_size()\n",
      "\n",
      "    @property\n",
      "    def _is_dp_head(self) -> bool:\n",
      "        return self._tp_rank == 0 and self._pp_rank == self._pp_size - 1\n",
      "\n",
      "    @property\n",
      "    def _model(self) -> model_api.Model:\n",
      "        return self.__models[constants.model_name()]\n",
      "\n",
      "    @property\n",
      "    def _interface(self) -> model_api.ModelInterface:\n",
      "        return self.__interfaces[constants.model_name()]\n",
      "\n",
      "    @property\n",
      "    def _eval_dataloader(self) -> torch.utils.data.DataLoader:\n",
      "        return self.__eval_dataloaders[constants.model_name()]\n",
      "\n",
      "    @property\n",
      "    def _module(self) -> torch.nn.Module | ReaLModel:\n",
      "        return self.__unwrapped_models[constants.model_name()]\n",
      "\n",
      "    @property\n",
      "    def _backend(self) -> model_api.ModelBackend:\n",
      "        return self.__backends[constants.model_name()]\n",
      "\n",
      "    def __lazy_setup(self):\n",
      "\n",
      "        # Build stream connecting with master workers.\n",
      "        self.__stream = request_reply_stream.make_worker_stream(\n",
      "            self.config.worker_info,\n",
      "            idx=self.__worker_index,\n",
      "        )\n",
      "\n",
      "        self.__pg_info = global_comm.setup_global_comm(\n",
      "            expr_name=self.__experiment_name,\n",
      "            trial_name=self.__trial_name,\n",
      "            worker_index=self.__worker_index,\n",
      "            model_topos=self.config.model_topos,\n",
      "            msid2mwid=self.config.msid2mwid,\n",
      "        )\n",
      "\n",
      "        self.data_manager = DataManager(\n",
      "            model_topos=self.config.model_topos,\n",
      "            msid2mwid=self.config.msid2mwid,\n",
      "            data_transfer_pairs=self.config.data_transfer_pairs,\n",
      "        )\n",
      "        self.data_manager.setup_process_groups()\n",
      "\n",
      "        self.__param_realloc_info = param_realloc_comm.setup_param_realloc(\n",
      "            model_topos=self.config.model_topos,\n",
      "            msid2mwid=self.config.msid2mwid,\n",
      "            param_realloc_pairs=self.config.sync_param_pairs,\n",
      "        )\n",
      "\n",
      "        logger.info(\n",
      "            f\"SetUp Information - Model worker {self.__worker_index} runs on \"\n",
      "            f\"node {network.gethostname()} (IP {network.gethostip()}) \"\n",
      "            f\"device index {self.__pg_info.local_gpu_id}.\"\n",
      "        )\n",
      "\n",
      "        self.__device = (\n",
      "            torch.device(\"cuda:0\") if constants.use_cuda() else torch.device(\"cpu\")\n",
      "        )\n",
      "\n",
      "        for model_name_, topo_ in self.config.model_topos.items():\n",
      "            rpcs = [\n",
      "                rpc for rpc in self.config.model_rpcs if rpc.model_name == model_name_\n",
      "            ]\n",
      "            assert len(rpcs) >= 1\n",
      "            is_trainable_model = any(\n",
      "                [\n",
      "                    rpc.interface_type == dfg.ModelInterfaceType.TRAIN_STEP\n",
      "                    for rpc in rpcs\n",
      "                ]\n",
      "            )\n",
      "            param_realloc_comm.set_trainable(model_name_, is_trainable_model)\n",
      "            constants.set_rank_mapping(model_name_, topo_, self.config.msid2mwid)\n",
      "            grid = topology.ParallelGrid(\n",
      "                topology=topo_,\n",
      "                rank_mapping=constants.rank_mapping_of_model(model_name_),\n",
      "                process_group=self.__pg_info.model_groups[model_name_],\n",
      "            )\n",
      "            constants.set_grid(model_name_, grid)\n",
      "\n",
      "        # Set up training dataset for source RPCs.\n",
      "        self.__datasets = []\n",
      "        if self.__has_dataset:\n",
      "            datasets = [\n",
      "                data_api.make_dataset(\n",
      "                    d,\n",
      "                    # NOTE: we must use the same seed to ensure the same dataset split\n",
      "                    self.config.base_seed,\n",
      "                    self.__dataset_dp_rank,\n",
      "                    self.__dataset_dp_size,\n",
      "                    self.config.tokenizer_name_or_path,\n",
      "                    self.config.worker_info.experiment_name,\n",
      "                    self.config.worker_info.trial_name,\n",
      "                    cache_root=(\n",
      "                        None\n",
      "                        if not self.config.use_dataset_cache\n",
      "                        else self.config.dataset_cahce_root\n",
      "                    ),\n",
      "                )\n",
      "                for d in self.config.datasets\n",
      "            ]\n",
      "            self.__datasets = datasets\n",
      "\n",
      "            self.__dataloaders: List[\n",
      "                torch.utils.data.DataLoader[data_api.SequenceSample]\n",
      "            ] = []\n",
      "            for i, d in enumerate(self.__datasets):\n",
      "                g = torch.Generator()\n",
      "                g.manual_seed(\n",
      "                    self.config.base_seed + seeding._seed_from_key(f\"__dataloader{i}__\")\n",
      "                )\n",
      "                dataloader_kwargs = dict(\n",
      "                    shuffle=self.config.shuffle_dataset,\n",
      "                    generator=g,\n",
      "                )\n",
      "                if not isinstance(d, PullerStreamDataset):\n",
      "                    dataloader_kwargs[\"collate_fn\"] = data_api.SequenceSample.gather\n",
      "                    # NOTE: This is *NOT* the actual batch size for training.\n",
      "                    # It is just a proper size to load data to workers.\n",
      "                    dataloader_kwargs[\"batch_size\"] = 10240\n",
      "                else:\n",
      "                    dataloader_kwargs[\"batch_size\"] = None\n",
      "                self.__dataloaders.append(\n",
      "                    torch.utils.data.DataLoader(d, **dataloader_kwargs)\n",
      "                )\n",
      "\n",
      "            self.dataset_size = sum(len(d) for d in self.__datasets)\n",
      "\n",
      "            self.__data_generators = [enumerate(d) for d in self.__dataloaders]\n",
      "\n",
      "        self.__models: Dict[ModelName, model_api.Model] = dict()\n",
      "        self.__model_is_handle: Dict[ModelName, bool] = dict()\n",
      "        self.__interfaces: Dict[ModelName, model_api.ModelInterface] = dict()\n",
      "        self.__eval_dataloaders: Dict[ModelName, torch.utils.data.DataLoader] = dict()\n",
      "\n",
      "        self.__backends: Dict[ModelName, model_api.ModelBackend] = dict()\n",
      "        self.__unwrapped_models: Dict[ModelName, torch.nn.Module | ReaLModel] = dict()\n",
      "\n",
      "        self.__backend_initialized: Dict[ModelName, bool] = dict()\n",
      "\n",
      "        recover_ckpt_paths = []\n",
      "        for s in self.config.shards:\n",
      "            with constants.model_scope(s.id.model_name):\n",
      "                self.__backend_initialized[s.id.model_name] = False\n",
      "                tik = time.perf_counter()\n",
      "                if self.__recover_run:\n",
      "                    model_path = self._get_recover_ckpt_path(s.id.model_name.role)\n",
      "                    if model_path is not None:\n",
      "                        logger.info(f\"Loading checkpoint during recover: {model_path}\")\n",
      "                        recover_ckpt_paths.append(model_path)\n",
      "                        if s.model.type_ == \"real_model\":\n",
      "                            s.model.args[\"model_path\"] = model_path\n",
      "                            s.model.args[\"init_critic_from_actor\"] = False\n",
      "                            s.model.args[\"init_from_scratch\"] = False\n",
      "                        elif constants.parallelism_rank() == 0:\n",
      "                            logger.warning(\n",
      "                                f\"Unknown how to recover model type {s.model.type_}\"\n",
      "                            )\n",
      "\n",
      "                        # Recover indices for dynamic dataset\n",
      "                        for i, d in enumerate(self.__datasets):\n",
      "                            if (\n",
      "                                s.id.model_name == self.src_rpc.model_name\n",
      "                                and self.__has_dataset\n",
      "                                and hasattr(d, \"filter\")\n",
      "                            ):\n",
      "                                dataset_indices_path = os.path.join(\n",
      "                                    constants.MODEL_SAVE_ROOT,\n",
      "                                    constants.experiment_name(),\n",
      "                                    constants.trial_name(),\n",
      "                                    \"dataset_indices\",\n",
      "                                    f\"{self._dp_rank}_{i}.npy\",\n",
      "                                )\n",
      "                                if os.path.exists(dataset_indices_path):\n",
      "                                    indices = np.load(dataset_indices_path).tolist()\n",
      "                                    logger.info(\n",
      "                                        f\"DP rank {self._dp_rank} updating dataset indices upon recover, \"\n",
      "                                        f\"size {len(d.active_indices)} -> {len(indices)}\"\n",
      "                                    )\n",
      "                                    d.active_indices = indices\n",
      "\n",
      "                if constants.parallelism_rank() == 0:\n",
      "                    self.logger.info(\n",
      "                        f\"Making model {s.id.model_name}, configuration {s.model}...\"\n",
      "                    )\n",
      "\n",
      "                self.__models[s.id.model_name] = model = model_api.make_model(\n",
      "                    s.model, name=s.id.model_name, device=self.__device\n",
      "                )\n",
      "                if self.__recover_run:\n",
      "                    model.version = copy.deepcopy(self.__recover_info.last_step_info)\n",
      "                self.__unwrapped_models[s.id.model_name] = model.module\n",
      "                if s.should_instantiate:\n",
      "                    if isinstance(model.module, ReaLModel):\n",
      "                        model.module.instantiate()\n",
      "                    self.__model_is_handle[s.id.model_name] = False\n",
      "                else:\n",
      "                    self.__model_is_handle[s.id.model_name] = True\n",
      "                self.__backends[s.id.model_name] = model_api.make_backend(s.backend)\n",
      "                interface_impl = [\n",
      "                    rpc.interface_impl\n",
      "                    for rpc in self.config.model_rpcs\n",
      "                    if rpc.model_name == s.id.model_name\n",
      "                ]\n",
      "                assert all(x == interface_impl[0] for x in interface_impl)\n",
      "                self.__interfaces[s.id.model_name] = model_api.make_interface(\n",
      "                    interface_impl[0]\n",
      "                )\n",
      "\n",
      "                if s.eval_dataset is not None:\n",
      "                    eval_dataset = data_api.make_dataset(\n",
      "                        s.eval_dataset,\n",
      "                        # NOTE: we must use the same seed to ensure the same dataset split\n",
      "                        self.config.base_seed,\n",
      "                        s.id.dp_rank,\n",
      "                        s.id.topo.get_dim(\"data\"),\n",
      "                        self.__models[s.id.model_name].tokenizer,\n",
      "                        self.config.worker_info.experiment_name,\n",
      "                        self.config.worker_info.trial_name,\n",
      "                        cache_root=(\n",
      "                            None\n",
      "                            if not self.config.use_dataset_cache\n",
      "                            else self.config.dataset_cahce_root\n",
      "                        ),\n",
      "                    )\n",
      "                    eval_dataloader = torch.utils.data.DataLoader(\n",
      "                        eval_dataset,\n",
      "                        batch_size=s.eval_bs,\n",
      "                        collate_fn=data_api.SequenceSample.gather,\n",
      "                        shuffle=False,\n",
      "                    )\n",
      "                else:\n",
      "                    eval_dataloader = None\n",
      "                self.__eval_dataloaders[s.id.model_name] = eval_dataloader\n",
      "\n",
      "        all_recover_ckpt_paths = [None for _ in range(dist.get_world_size())]\n",
      "        dist.all_gather_object(all_recover_ckpt_paths, recover_ckpt_paths)\n",
      "        recover_ckpt_paths = set(itertools.chain.from_iterable(all_recover_ckpt_paths))\n",
      "        for model_path in recover_ckpt_paths:\n",
      "            if dist.get_rank() == 0 and os.path.islink(model_path):\n",
      "                # Make the base model path persistent if it is a symlink to the recover checkpoint,\n",
      "                # because we may want to copy huggingface configurations from it, and\n",
      "                # th next recover save will remove this symlink.\n",
      "                dst_path = Path(model_path).parent / \"_tmp_ckpt\"\n",
      "                shutil.rmtree(dst_path, ignore_errors=True)\n",
      "                shutil.copytree(model_path, dst_path)\n",
      "                os.unlink(model_path)\n",
      "                os.system(f\"mv {str(dst_path)} {model_path}\")\n",
      "        dist.barrier()\n",
      "\n",
      "        self.__request_cache = {}\n",
      "        self.__ack_cache = {}\n",
      "\n",
      "        self.__request_queue = queue.Queue(maxsize=10240)\n",
      "        self.__reply_queue = queue.Queue(maxsize=10240)\n",
      "        self.__request_sample_size = dict()\n",
      "\n",
      "        self.__compute_input_queues = {\n",
      "            model_name: dict(\n",
      "                train_step=queue.Queue(4),\n",
      "                inference=queue.Queue(4),\n",
      "                generate=queue.Queue(4),\n",
      "            )\n",
      "            for model_name in self.__models.keys()\n",
      "        }\n",
      "\n",
      "    def __handle_one_rpc_hook(self, hook: str, hook_data: Any):\n",
      "        ret = None\n",
      "\n",
      "        tik = time.perf_counter()\n",
      "        if hook == \"data_transfer\":\n",
      "            self.__data_transfer_among_workers(hook_data)\n",
      "        elif hook == \"param_realloc\":\n",
      "            self.__param_realloc(hook_data)\n",
      "        elif hook == \"offload\":\n",
      "            # NOTE: Profiling (or cuda synchronization) will cause an overhead ~0.5s.\n",
      "            with cuda_tmarked(\"offload\", CUDATimeMarkType.mem_layout):\n",
      "                m = self.__unwrapped_models[hook_data[\"model_name\"]]\n",
      "                if not isinstance(m, ReaLModel):\n",
      "                    logger.warning(\n",
      "                        f\"Model {hook_data['model_name']} (type={type(m)}) is not a ReaLModel, \"\n",
      "                        f\"so it can't use offload.\"\n",
      "                    )\n",
      "                    return\n",
      "                if not m._offloaded:\n",
      "                    m.async_offload()\n",
      "        elif hook == \"save\":\n",
      "            self.__save_model(hook_data)\n",
      "        elif hook == \"evaluate\":\n",
      "            logger.debug(f\"hook_data: {hook_data}\")\n",
      "            with constants.model_scope(hook_data[\"model_name\"]):\n",
      "                ret = self._interface.evaluate(self._model, self._eval_dataloader)\n",
      "            if ret:\n",
      "                logger.info(\n",
      "                    f\"Model {hook_data['model_name']} evaluation done. \"\n",
      "                    f\"Statistics: {ret}. Time consumption: {time.perf_counter() - tik:.4f}s.\"\n",
      "                )\n",
      "        else:\n",
      "            raise NotImplementedError(f\"Unknown hook {hook}.\")\n",
      "\n",
      "        self._clear_memory()\n",
      "        blogger.debug(\n",
      "            f\"Model worker {self.__worker_index} handle \"\n",
      "            f\"RPC hook {hook} CPU time {time.perf_counter() - tik:.4f}s.\"\n",
      "        )\n",
      "        if constants.use_cuda():\n",
      "            torch.cuda.synchronize()\n",
      "        return ret\n",
      "\n",
      "    def _clear_memory(self, force=False):\n",
      "        # empty cache to remove large cache blocks, ~0.1s overhead\n",
      "        if force or self.torch_cache_mysophobia:\n",
      "            gc.collect()\n",
      "            if torch.cuda.is_available():\n",
      "                torch.cuda.empty_cache()\n",
      "                gc.collect()\n",
      "\n",
      "    def handle_all_pre_hooks(self):\n",
      "        # drain request queues, handle all pending hooks, then recover the queue\n",
      "        cache = []\n",
      "        while True:\n",
      "            try:\n",
      "                (\n",
      "                    request,\n",
      "                    data,\n",
      "                    handled,\n",
      "                    res,\n",
      "                    time_record,\n",
      "                ) = self.__request_queue.get_nowait()\n",
      "                request: request_reply_stream.Payload\n",
      "                if not handled:\n",
      "                    while len(request.pre_hooks) > 0:\n",
      "                        assert len(request.pre_hooks) == len(request.pre_hook_data)\n",
      "                        assert not handled and res is None\n",
      "                        with constants.model_scope(request.handler.model_name):\n",
      "                            if constants.parallelism_rank() == 0:\n",
      "                                logger.debug(\n",
      "                                    f\"Model `{request.handler.model_name}` handling \"\n",
      "                                    f\"{len(request.pre_hooks)} pre-hook for request `{request.handle_name}`. \"\n",
      "                                    f\"The current hook is `{request.pre_hooks[0]}`. \"\n",
      "                                    f\"{self.__request_queue.qsize()} requests left to handle their potential pre-hooks.\"\n",
      "                                )\n",
      "                        tik = time.perf_counter()\n",
      "                        hook = request.pre_hooks.pop(0)\n",
      "                        hook_data = request.pre_hook_data.pop(0)\n",
      "                        self.__handle_one_rpc_hook(hook, hook_data)\n",
      "                        time_record[\n",
      "                            f\"timeperf/{request.handler.model_name.role}_{request.handle_name}/pre-{hook}\"\n",
      "                        ] += (time.perf_counter() - tik)\n",
      "                cache.append((request, data, handled, res, time_record))\n",
      "            except queue.Empty:\n",
      "                break\n",
      "\n",
      "        for c in cache:\n",
      "            self.__request_queue.put_nowait(c)\n",
      "\n",
      "    def handle_non_blocking_request(self, request: request_reply_stream.Payload):\n",
      "        assert len(request.pre_hooks) == 0, request\n",
      "        assert len(request.post_hooks) == 0, request\n",
      "\n",
      "        if request.handle_name == \"model_config\":\n",
      "            if isinstance(\n",
      "                self.__unwrapped_models[request.handler.model_name],\n",
      "                ReaLModel,\n",
      "            ):\n",
      "                res = self.__unwrapped_models[request.handler.model_name].config\n",
      "            else:\n",
      "                res = None\n",
      "        elif request.handle_name == \"fetch\":\n",
      "            dp_rank = int(re.search(r\"__data(\\d+)__\", request.handler).group(1))\n",
      "            assert self.__has_dataset\n",
      "            assert isinstance(request.data, int), request.data\n",
      "            dataset_id = request.data\n",
      "            # Fetch.\n",
      "            try:\n",
      "                self.__dataset_batch_counter, cur_sample = next(\n",
      "                    self.__data_generators[dataset_id]\n",
      "                )\n",
      "            except StopIteration:\n",
      "                # Upon the first fetch request, filter dataset and create dataloader.\n",
      "                eval_scores_path = os.path.join(\n",
      "                    constants.MODEL_SAVE_ROOT,\n",
      "                    constants.experiment_name(),\n",
      "                    constants.trial_name(),\n",
      "                    \"dataset_eval_scores.json\",\n",
      "                )\n",
      "                dataset_indices_path = os.path.join(\n",
      "                    constants.MODEL_SAVE_ROOT,\n",
      "                    constants.experiment_name(),\n",
      "                    constants.trial_name(),\n",
      "                    \"dataset_indices\",\n",
      "                    f\"{dp_rank}_{dataset_id}.npy\",\n",
      "                )\n",
      "                os.makedirs(os.path.dirname(dataset_indices_path), exist_ok=True)\n",
      "                if hasattr(self.__datasets[dataset_id], \"filter\") and os.path.exists(\n",
      "                    eval_scores_path\n",
      "                ):\n",
      "                    # Don't filter dataset on the first poll after recover.\n",
      "                    with open(eval_scores_path, \"r\", encoding=\"utf-8\") as f:\n",
      "                        dataset_eval_scores = json.load(f)\n",
      "                    self.__datasets[dataset_id].filter(dataset_eval_scores)\n",
      "                    # Save the dataset indices after filtering\n",
      "                    np.save(\n",
      "                        dataset_indices_path,\n",
      "                        self.__datasets[dataset_id].active_indices,\n",
      "                    )\n",
      "                g = torch.Generator()\n",
      "                g = g.set_state(self.__dataloaders[dataset_id].generator.get_state())\n",
      "                dataloader_kwargs = dict(\n",
      "                    shuffle=self.config.shuffle_dataset,\n",
      "                    generator=g,\n",
      "                )\n",
      "                if not isinstance(self.__datasets[dataset_id], PullerStreamDataset):\n",
      "                    dataloader_kwargs[\"collate_fn\"] = data_api.SequenceSample.gather\n",
      "                    # NOTE: This is *NOT* the actual batch size for training.\n",
      "                    # It is just a proper size to load data to workers.\n",
      "                    dataloader_kwargs[\"batch_size\"] = 10240\n",
      "                else:\n",
      "                    dataloader_kwargs[\"batch_size\"] = None\n",
      "                self.__dataloaders[dataset_id] = torch.utils.data.DataLoader(\n",
      "                    self.__datasets[dataset_id], **dataloader_kwargs\n",
      "                )\n",
      "                self.__data_generators[dataset_id] = enumerate(\n",
      "                    self.__dataloaders[dataset_id]\n",
      "                )\n",
      "                self.__dataset_batch_counter, cur_sample = next(\n",
      "                    self.__data_generators[dataset_id]\n",
      "                )\n",
      "\n",
      "            if isinstance(cur_sample, data_api.SequenceSample):\n",
      "                samples = cur_sample.unpack()\n",
      "            else:\n",
      "                assert isinstance(cur_sample, list), type(cur_sample)\n",
      "                samples = cur_sample\n",
      "\n",
      "            data_loaded = []\n",
      "            for x in samples:\n",
      "                if (\n",
      "                    self.__recover_run\n",
      "                    and x.ids[0] in self.__recover_info.hash_vals_to_ignore\n",
      "                    # Rollout worker has filered once\n",
      "                    and not isinstance(self.__datasets[dataset_id], PullerStreamDataset)\n",
      "                ):\n",
      "                    self.__recover_info.hash_vals_to_ignore.remove(x.ids[0])\n",
      "                    continue\n",
      "                if self.data_manager.has_data(x.ids[0]):\n",
      "                    continue\n",
      "                data_loaded.append(x)\n",
      "                self.data_manager.store(x)\n",
      "            assert len(set([x.ids[0] for x in data_loaded])) == len(data_loaded)\n",
      "\n",
      "            meta_sample = None\n",
      "            birth_times = []\n",
      "            if len(data_loaded) > 0:\n",
      "                sample = data_api.SequenceSample.gather(data_loaded)\n",
      "                meta_sample = sample.meta()\n",
      "                if \"birth_time\" in sample.keys:\n",
      "                    birth_times = (\n",
      "                        sample.data[\"birth_time\"].flatten().cpu().numpy().tolist()\n",
      "                    )\n",
      "                    assert len(birth_times) == meta_sample.bs\n",
      "                else:\n",
      "                    birth_times = (\n",
      "                        time.monotonic_ns()\n",
      "                        + np.arange(len(data_loaded), dtype=np.int64)\n",
      "                    ).tolist()\n",
      "\n",
      "            res = data_api.DataBatchMeta(\n",
      "                dp_rank=dp_rank,\n",
      "                meta_sample=meta_sample,\n",
      "                birth_times=birth_times,\n",
      "            )\n",
      "        elif request.handle_name == \"spec\":\n",
      "            # Raw dataset without filtering.\n",
      "            res = {\n",
      "                \"n_datasets\": len(self.__datasets),\n",
      "                \"dataset_size\": self.dataset_size,\n",
      "            }\n",
      "        elif request.handle_name == \"clear_data_cache\":\n",
      "            with cuda_tmarked(\"clear_data_cache\", CUDATimeMarkType.misc):\n",
      "                ids = request.data\n",
      "                self.data_manager.remove(ids)\n",
      "                gc.collect()\n",
      "                if (\n",
      "                    self.config.cuda_cache_cleanliness\n",
      "                    and self.__clear_cache_frequency.check()\n",
      "                ):\n",
      "                    st = time.monotonic()\n",
      "                    self._clear_memory(force=True)\n",
      "                    et = time.monotonic()\n",
      "                    blogger.debug(\n",
      "                        f\"Model worker {self.__worker_index} cleared cache in {et-st:.4f}s. \"\n",
      "                    )\n",
      "            logger.debug(\n",
      "                \"Get clear_data_cache, dump cuda tmark. \"\n",
      "                f\"Remaining data in local storage: {self.data_manager.storage_size()}. \"\n",
      "            )\n",
      "            dump_tmark_db(self.__worker_index)\n",
      "            res = request_reply_stream.NoResponse()\n",
      "        self.__reply_queue.put_nowait((request, res))\n",
      "        self.__request_sample_size[request.request_id] = 1\n",
      "\n",
      "    def handle_blocking_request(\n",
      "        self,\n",
      "        request: request_reply_stream.Payload,\n",
      "        data: Any,\n",
      "        handled: bool,\n",
      "        res: Optional[Any],\n",
      "        time_record: Dict,\n",
      "    ) -> worker_base.PollResult:\n",
      "        tik = time.perf_counter()\n",
      "\n",
      "        assert not handled and res is None, (\n",
      "            handled,\n",
      "            res,\n",
      "            len(request.post_hooks),\n",
      "        )\n",
      "\n",
      "        model_name = request.handler.model_name\n",
      "        with constants.model_scope(model_name):\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                blogger.debug(\n",
      "                    f\"Model #{request.handler.model_name}# \"\n",
      "                    f\"starts handling request *{request.handle_name}*.\"\n",
      "                )\n",
      "            res = None\n",
      "            if request.handle_name == \"empty\":\n",
      "                # Empty request is used for executing hooks,\n",
      "                # e.g., data transfer, parameter syncrhonization.\n",
      "                pass\n",
      "            elif request.handle_name == \"initialize\":\n",
      "                self.__models[request.handler.model_name] = self._backend.initialize(\n",
      "                    self._model, data\n",
      "                )\n",
      "                if self.__recover_run:\n",
      "                    model_path = self._get_recover_ckpt_path(model_name.role)\n",
      "                    if model_path is not None:\n",
      "                        self._backend.load(\n",
      "                            self.__models[request.handler.model_name], model_path\n",
      "                        )\n",
      "                        logger.info(\n",
      "                            f\"Loaded backend states during recover: {model_path}\"\n",
      "                        )\n",
      "                self.__backend_initialized[request.handler.model_name] = True\n",
      "                # Offload this model after initialization if any MFC requires offloading.\n",
      "                for rpc in self.config.model_rpcs:\n",
      "                    if rpc.model_name != request.handler.model_name:\n",
      "                        continue\n",
      "                    if all(\n",
      "                        not isinstance(hook, dfg.OffloadHook)\n",
      "                        for hook in rpc._post_hooks\n",
      "                    ):\n",
      "                        continue\n",
      "                    self.__unwrapped_models[request.handler.model_name].async_offload()\n",
      "                    break\n",
      "            ############## computation function calls ##############\n",
      "            elif request.handle_name in [\"inference\", \"generate\", \"train_step\"]:\n",
      "                res = self.__handle_model_function_calls(request, data)\n",
      "            else:\n",
      "                raise NotImplementedError(\n",
      "                    f\"Unknown request type: {request.handle_name}.\"\n",
      "                )\n",
      "\n",
      "            if (\n",
      "                request.handle_name in TIME_RECORD_RPCS\n",
      "                and self._is_dp_head\n",
      "                and self._dp_rank == 0\n",
      "            ):\n",
      "                blogger.debug(\n",
      "                    f\"Model #{request.handler.model_name}# handle \"\n",
      "                    f\"request *{request.handle_name}*\"\n",
      "                    f\" in ${time.perf_counter() - tik:.4f}$s\"\n",
      "                )\n",
      "        time_record[\n",
      "            f\"timeperf/{request.handler.model_name.role}_{request.handle_name}/main\"\n",
      "        ] += (time.perf_counter() - tik)\n",
      "\n",
      "        # Handle all post hooks right after the main computation\n",
      "        if len(request.post_hooks) > 0:\n",
      "            assert len(request.post_hooks) == len(request.post_hook_data)\n",
      "            for hook, hook_data in zip(request.post_hooks, request.post_hook_data):\n",
      "                tik = time.perf_counter()\n",
      "                ret = self.__handle_one_rpc_hook(hook, hook_data)\n",
      "                if hook == \"evaluate\":\n",
      "                    assert request.handle_name == \"train_step\", request.handle_name\n",
      "                    assert isinstance(ret, dict), ret\n",
      "                    if isinstance(res, dict):\n",
      "                        res.update(ret)\n",
      "                    else:\n",
      "                        res[0].update(ret)\n",
      "                time_record[\n",
      "                    f\"timeperf/{request.handler.model_name.role}_{request.handle_name}/post-{hook}\"\n",
      "                ] += (time.perf_counter() - tik)\n",
      "\n",
      "        # update param realloc step after handling post hooks\n",
      "        if request.handle_name == \"train_step\":\n",
      "            tik = time.perf_counter()\n",
      "            global_step = self.__models[model_name].version.global_step\n",
      "            realloc_dir = os.path.join(\n",
      "                constants.PARAM_REALLOC_PATH,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                model_name.role,\n",
      "                str(global_step),\n",
      "            )\n",
      "            save_meta = dict(\n",
      "                model_name=model_name,\n",
      "                save_backend=False,\n",
      "                save_dir=realloc_dir,\n",
      "            )\n",
      "            self.__save_model(save_meta)\n",
      "            name = names.model_version(\n",
      "                self.__experiment_name,\n",
      "                self.__trial_name,\n",
      "                model_name.role,\n",
      "            )\n",
      "            with constants.model_scope(model_name):\n",
      "                dist.barrier(group=constants.cpu_parallelism_group())\n",
      "                if constants.parallelism_rank() == 0:\n",
      "                    name_resolve.add(name, str(global_step), replace=True)\n",
      "            time_record[\n",
      "                f\"timeperf/{request.handler.model_name.role}_{request.handle_name}/param-sync-save\"\n",
      "            ] += (time.perf_counter() - tik)\n",
      "\n",
      "        res = (res, time_record)\n",
      "        self.__reply_queue.put_nowait((request, res))\n",
      "        sample_count = data.bs if isinstance(data, data_api.SequenceSample) else 1\n",
      "        self.__request_sample_size[request.request_id] = sample_count\n",
      "\n",
      "    def _get_setup_logdir(self, name):\n",
      "        subdir = os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            name,\n",
      "            f\"setup{self._setup_counter}\",\n",
      "        )\n",
      "        os.makedirs(subdir, exist_ok=True)\n",
      "        return subdir\n",
      "\n",
      "    @contextlib.contextmanager\n",
      "    def __maybe_profile_rpc(self, rpc: dfg.MFCDef):\n",
      "        # barrier within this model group before and after profiled RPC\n",
      "        if (\n",
      "            self.__record_performance\n",
      "            or self.__enable_profiler\n",
      "            or self.__enable_memory_dump\n",
      "        ):\n",
      "            torch.cuda.synchronize()\n",
      "            dist.barrier(group=constants.cpu_parallelism_group())\n",
      "            # pfer can be a null context if enable_profiler is False\n",
      "            pfer = get_pytorch_profiler(\n",
      "                kernel_only=False, enabled=self.__enable_profiler\n",
      "            )\n",
      "            pfer.__enter__()\n",
      "            # The pytorch profiler will call cuda synchronize for us.\n",
      "            tik = time.perf_counter()\n",
      "\n",
      "        try:\n",
      "            yield self\n",
      "        finally:\n",
      "            # Dump profiler results.\n",
      "            if (\n",
      "                self.__record_performance\n",
      "                or self.__enable_profiler\n",
      "                or self.__enable_memory_dump\n",
      "            ):\n",
      "                pfer.__exit__(None, None, None)\n",
      "                dist.barrier(group=constants.cpu_parallelism_group())\n",
      "                torch.cuda.synchronize()\n",
      "                tok = time.perf_counter()\n",
      "                rpc_time = tok - tik\n",
      "\n",
      "            if self.__record_performance:\n",
      "                if len(self.__performance_recorder) == 0:\n",
      "                    self.__performance_recorder[\"info\"] = {\n",
      "                        \"pipeline_size\": self._pp_size,\n",
      "                        \"model_size\": self._tp_size,\n",
      "                        \"data_size\": self._dp_size,\n",
      "                        \"rank\": constants.parallelism_rank(),\n",
      "                        \"sequence_parallel_enabled\": constants.sequence_parallel(),\n",
      "                        \"gradient_checkpointing_enabled\": constants.gradient_checkpointing(),\n",
      "                        \"interface_type\": str(rpc.interface_type),\n",
      "                    }\n",
      "                    self.__performance_recorder[\"time\"] = [rpc_time]\n",
      "                else:\n",
      "                    self.__performance_recorder[\"time\"].append(rpc_time)\n",
      "\n",
      "                with open(\n",
      "                    os.path.join(\n",
      "                        self._get_setup_logdir(\"performance\"),\n",
      "                        f\"rpc-mw{self.__worker_index}.txt\",\n",
      "                    ),\n",
      "                    \"a\",\n",
      "                ) as f:\n",
      "                    f.write(\n",
      "                        f\"rpc: {rpc.name} rank: {dist.get_rank()} time: {rpc_time}\\n\"\n",
      "                    )\n",
      "\n",
      "            if self.__enable_profiler:\n",
      "                if self._dp_rank == 0 and self._is_dp_head:\n",
      "                    blogger.info(\n",
      "                        f\"RPC {rpc.name} execution time \"\n",
      "                        f\"w/o external data processing: {rpc_time:.2f} secs.\"\n",
      "                    )\n",
      "                    collect_tik = time.perf_counter()\n",
      "                    blogger.info(\n",
      "                        f\"Collecting system metrics from the profiler. \"\n",
      "                        \"This may take for a while...\"\n",
      "                    )\n",
      "\n",
      "                pfer.export_chrome_trace(\n",
      "                    os.path.join(\n",
      "                        self._get_setup_logdir(\"trace\"),\n",
      "                        f\"{rpc.name}_r{dist.get_rank()}.json\",\n",
      "                    )\n",
      "                )\n",
      "                if self._dp_rank == 0 and self._is_dp_head:\n",
      "                    blogger.info(\n",
      "                        f\"System metrics collected. Time consumption:\"\n",
      "                        f\" {time.perf_counter() - collect_tik:.2f} secs.\"\n",
      "                    )\n",
      "\n",
      "    def __handle_model_function_calls(\n",
      "        self, request: request_reply_stream.Payload, data: Any\n",
      "    ):\n",
      "        # Check that the model is instantiated and is not empty.\n",
      "        assert not self.__model_is_handle[\n",
      "            request.handler.model_name\n",
      "        ], request.handler.model_name\n",
      "\n",
      "        input_queue = self.__compute_input_queues[request.handler.model_name][\n",
      "            request.handle_name\n",
      "        ]\n",
      "        rpc: dfg.MFCDef = next(\n",
      "            rpc for rpc in self.config.model_rpcs if rpc.name == request.data\n",
      "        )\n",
      "\n",
      "        data: data_api.SequenceSample = input_queue.get_nowait()\n",
      "\n",
      "        if self.config.profile_mode:\n",
      "            data = self._interface.mock(request.handle_name, self._model, data)\n",
      "\n",
      "        if rpc.input_key_remap:\n",
      "            data.remap_keys_(rpc.input_key_remap)\n",
      "\n",
      "        with self.__maybe_profile_rpc(rpc):\n",
      "            if request.handle_name == \"inference\":\n",
      "                res = self._interface.inference(\n",
      "                    self._model,\n",
      "                    data,\n",
      "                    mb_spec=rpc.mb_spec,\n",
      "                )  # -> SequenceSample\n",
      "            elif request.handle_name == \"train_step\":\n",
      "                res = self._interface.train_step(\n",
      "                    self._model,\n",
      "                    data,\n",
      "                    mb_spec=rpc.mb_spec,\n",
      "                )  # -> Dict\n",
      "            elif request.handle_name == \"generate\":\n",
      "                res = self._interface.generate(\n",
      "                    self._model,\n",
      "                    data,\n",
      "                    mb_spec=rpc.mb_spec,\n",
      "                )  # -> SequenceSample\n",
      "            else:\n",
      "                raise NotImplementedError(f\"Unknown MFC type: {request.handle_name}.\")\n",
      "\n",
      "        eval_scores_path = os.path.join(\n",
      "            constants.MODEL_SAVE_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"dataset_eval_scores.json\",\n",
      "        )\n",
      "        eval_scores = {}\n",
      "        if isinstance(res, data_api.SequenceSample) and constants.is_dp_head():\n",
      "            if rpc.output_key_remap:\n",
      "                res.remap_keys_(rpc.output_key_remap)\n",
      "            res = res.select(rpc.output_keys)\n",
      "\n",
      "            # Update scores to update data sample distribution.\n",
      "            if \"scores\" in res.metadata:\n",
      "                # All-gather across the DP rank\n",
      "                all_scores = [None for _ in range(self._dp_size)]\n",
      "                local_scores = {i: s for i, s in zip(res.ids, res.metadata[\"scores\"])}\n",
      "                dist.all_gather_object(\n",
      "                    all_scores,\n",
      "                    local_scores,\n",
      "                    group=constants.data_parallel_group(),\n",
      "                )\n",
      "                # Since the device mesh generating \"scores\" may not overlap\n",
      "                # with the device mesh of dataset, write all scores into the disk\n",
      "                # for later usage.\n",
      "\n",
      "                if os.path.exists(eval_scores_path):\n",
      "                    with open(eval_scores_path, \"r\", encoding=\"utf-8\") as f:\n",
      "                        eval_scores.update(json.load(f))\n",
      "                for scores in all_scores:\n",
      "                    eval_scores.update(scores)\n",
      "\n",
      "                res.metadata.pop(\"scores\")\n",
      "        dist.barrier(group=constants.cpu_parallelism_group())\n",
      "        if len(eval_scores) > 0 and self._dp_rank == 0 and self._is_dp_head:\n",
      "            with open(\n",
      "                eval_scores_path,\n",
      "                \"w\",\n",
      "                encoding=\"utf-8\",\n",
      "            ) as f:\n",
      "                json.dump(eval_scores, f, ensure_ascii=False, indent=4)\n",
      "\n",
      "        # Store data into storage.\n",
      "        if self._is_dp_head and isinstance(res, data_api.SequenceSample):\n",
      "            for x in res.unpack():\n",
      "                # The input data must exist in the storage, otherwise\n",
      "                # the model function call will not run.\n",
      "                self.data_manager.update(x)\n",
      "\n",
      "        # Only return meta data back to the master worker.\n",
      "        if isinstance(res, data_api.SequenceSample):\n",
      "            res = res.meta()\n",
      "\n",
      "        if constants.use_cuda():\n",
      "            # Monitoring info. There's an all-gather and an all-reduce\n",
      "            # over the parallelism group in this function.\n",
      "            torch.cuda.synchronize()\n",
      "            if (\n",
      "                self._model.backend_name != \"vllm\"\n",
      "                and self._model.backend_name != \"sglang\"\n",
      "            ):\n",
      "                # Since vLLM/SGLang allocates GPU memory in advance, it is very\n",
      "                # easy to exceed the 0.95 threshold that triggers a kill.\n",
      "                # We omit GPU stats logging for vLLM/SGLang.\n",
      "                self.__log_gpu_stats(request)\n",
      "\n",
      "        self._clear_memory()\n",
      "        if constants.use_cuda():\n",
      "            torch.cuda.synchronize()\n",
      "        dist.barrier(group=constants.cpu_parallelism_group())\n",
      "        return res\n",
      "\n",
      "    @cuda_tmark(\"data_transfer\", CUDATimeMarkType.comm)\n",
      "    def __data_transfer_among_workers(self, hook_data: Dict[str, Any]):\n",
      "        meta_sample = hook_data[\"meta_sample\"]\n",
      "\n",
      "        plan = [RedistribStep(**json.loads(x)) for x in hook_data[\"plan\"]]\n",
      "        self.data_manager.redistribute(meta_sample, plan=plan)\n",
      "\n",
      "        if hook_data[\"target\"] in self.__models:\n",
      "            with constants.model_scope(hook_data[\"target\"]):\n",
      "                local_ids = hook_data[\"partitioned_ids\"][self._dp_rank]\n",
      "            r = data_api.SequenceSample.gather(\n",
      "                [\n",
      "                    self.data_manager.get(_id).to_device(constants.current_device())\n",
      "                    for _id in local_ids\n",
      "                ],\n",
      "                keys=meta_sample.keys,\n",
      "            )\n",
      "            self.__compute_input_queues[hook_data[\"target\"]][\n",
      "                hook_data[\"handle_name\"]\n",
      "            ].put_nowait(r)\n",
      "\n",
      "    def __param_realloc(self, hook_data: Dict):\n",
      "        from_model_name: ModelName = hook_data[\"from_model_name\"]\n",
      "        to_model_name: ModelName = hook_data[\"to_model_name\"]\n",
      "\n",
      "        from_topo: topology.ProcessTopology = hook_data[\"from_topo\"]\n",
      "        to_topo: topology.ProcessTopology = hook_data[\"to_topo\"]\n",
      "\n",
      "        # NOTE: For the convenience of future developement, we\n",
      "        # run parameter reallocation with disk save-load by default.\n",
      "        if os.getenv(\"REAL_PARAM_REALLOC_IMPL\", \"DISK\") == \"DISK\":\n",
      "            if hook_data[\"eta\"] != 1.0:\n",
      "                raise NotImplementedError(\"eta != 1.0 is not supported yet.\")\n",
      "\n",
      "            # If the source is not a trainable model, it will not own\n",
      "            # parameters, so we just release its GPU memory.\n",
      "            with constants.model_scope(from_model_name):\n",
      "                from_model_ranks = sorted(constants.parallelism_group_ranks())\n",
      "            if not param_realloc_comm.is_trainable(from_model_name):\n",
      "                if dist.get_rank() not in from_model_ranks:\n",
      "                    return\n",
      "                if not isinstance(self.__unwrapped_models[from_model_name], ReaLModel):\n",
      "                    # We can only release the memory of ReaLModel,\n",
      "                    # because we don't know how to rebuild the parameters otherwise.\n",
      "                    return\n",
      "                m = self.__unwrapped_models[from_model_name]\n",
      "                dummy_tensor = torch.tensor((), dtype=m.dtype, device=m.device)\n",
      "                for p in m.layers.parameters():\n",
      "                    p.data = dummy_tensor\n",
      "                m.contiguous_param = dummy_tensor\n",
      "                return\n",
      "\n",
      "            # Get global_step from source model via broadcast,\n",
      "            # since there are no global_step information on model workers for generation.\n",
      "            if (\n",
      "                from_model_name in self.__models\n",
      "                and dist.get_rank() == from_model_ranks[0]\n",
      "            ):\n",
      "                global_step = self.__models[from_model_name].version.global_step\n",
      "            else:\n",
      "                global_step = 0\n",
      "            g = self.__param_realloc_info.param_realloc_model_cpu_group[\n",
      "                param_realloc_comm.ParamReallocModelPair(from_model_name, to_model_name)\n",
      "            ]\n",
      "            global_step = torch.tensor(global_step, device=\"cpu\")\n",
      "            dist.broadcast(global_step, src=from_model_ranks[0], group=g)\n",
      "            global_step = int(global_step.item())\n",
      "\n",
      "            realloc_dir = os.path.join(\n",
      "                constants.PARAM_REALLOC_PATH,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                from_model_name.role,\n",
      "                str(global_step),\n",
      "            )\n",
      "            if from_model_name in self.__unwrapped_models:\n",
      "                save_meta = dict(\n",
      "                    model_name=from_model_name,\n",
      "                    save_backend=False,\n",
      "                    save_dir=realloc_dir,\n",
      "                )\n",
      "                self.__save_model(save_meta)\n",
      "            dist.barrier(group=g)\n",
      "            if to_model_name in self.__unwrapped_models:\n",
      "                load_meta = dict(\n",
      "                    model_name=to_model_name,\n",
      "                    load_dir=realloc_dir,\n",
      "                )\n",
      "                self.__load_model(load_meta)\n",
      "                # Remove the reallocated checkpoint.\n",
      "                with constants.model_scope(to_model_name):\n",
      "                    dist.barrier(constants.cpu_parallelism_group())\n",
      "                    if constants.parallelism_rank() == 0:\n",
      "                        shutil.rmtree(realloc_dir, ignore_errors=True)\n",
      "                        os.makedirs(realloc_dir, exist_ok=True)\n",
      "        else:\n",
      "            logger.warning(\n",
      "                \"[Depreated Warning] Parameter reallocation through \"\n",
      "                \"NCCL will be disabled in future versions.\"\n",
      "            )\n",
      "            to_model_config = hook_data[\"to_model_config\"]\n",
      "            if from_model_name in self.__unwrapped_models:\n",
      "                m = self.__unwrapped_models[from_model_name]\n",
      "            else:\n",
      "                m = self.__unwrapped_models[to_model_name]\n",
      "            try:\n",
      "                new_layers, new_param, _ = m.build_reparallelized_layers_async(\n",
      "                    from_model_name=from_model_name,\n",
      "                    to_model_name=to_model_name,\n",
      "                    from_topo=from_topo,\n",
      "                    to_topo=to_topo,\n",
      "                    to_model_config=to_model_config,\n",
      "                    pg_info=self.__param_realloc_info,\n",
      "                )\n",
      "            except RuntimeError as e:\n",
      "                if from_model_name in self.__unwrapped_models:\n",
      "                    logger.error(f\"from model error: {from_model_name}\")\n",
      "                if to_model_name in self.__unwrapped_models:\n",
      "                    logger.info(f\"to model error: {to_model_name}\")\n",
      "                raise e\n",
      "            if to_model_name in self.__models and param_realloc_comm.is_trainable(\n",
      "                from_model_name\n",
      "            ):\n",
      "                self.__unwrapped_models[to_model_name].patch_reparallelization(\n",
      "                    (new_layers, new_param), eta=hook_data[\"eta\"]\n",
      "                )\n",
      "\n",
      "        if from_model_name in self.__models and not param_realloc_comm.is_trainable(\n",
      "            from_model_name\n",
      "        ):\n",
      "            self.__model_is_handle[from_model_name] = True\n",
      "        if to_model_name in self.__models and param_realloc_comm.is_trainable(\n",
      "            from_model_name\n",
      "        ):\n",
      "            self.__model_is_handle[to_model_name] = False\n",
      "\n",
      "    def __save_model(self, hook_data: Dict):\n",
      "        # NOTE: we should not create the `save_dir` here,\n",
      "        # because it will be automatically created by our save function.\n",
      "        # As such, if the checkpoint dir exists, we know that the checkpoint\n",
      "        # must have been properly saved.\n",
      "        tik = time.perf_counter()\n",
      "        # When `recover_only` is True, the model should save an overwrittable checkpoint for recover.\n",
      "        recover_only = hook_data.get(\"recover_only\", False)\n",
      "        with constants.model_scope(hook_data[\"model_name\"]):\n",
      "            if not recover_only:\n",
      "                save_dir = hook_data[\"save_dir\"]\n",
      "            else:\n",
      "                # Remove all previous symlinks.\n",
      "                save_root = Path(hook_data[\"save_dir\"]).parent\n",
      "                save_dir = str(save_root / \"recover_checkpoint\")\n",
      "                if constants.parallelism_rank() == 0:\n",
      "                    if os.path.exists(save_root):\n",
      "                        for fn in os.listdir(save_root):\n",
      "                            if (save_root / fn).is_dir() and (\n",
      "                                save_root / fn\n",
      "                            ).is_symlink():\n",
      "                                os.unlink(save_root / fn)\n",
      "                    shutil.rmtree(save_dir, ignore_errors=True)\n",
      "            dist.barrier(constants.cpu_parallelism_group())\n",
      "            self._interface.save(self._model, save_dir)\n",
      "            # The `save` method of the interface may be empty.\n",
      "            # We only save the backend state if the parameters have been indeed saved.\n",
      "            if os.path.exists(save_dir) and hook_data.get(\"save_backend\", True):\n",
      "                self._backend.save(self._model, save_dir)\n",
      "\n",
      "            t = torch.tensor(\n",
      "                float(time.perf_counter() - tik),\n",
      "                dtype=torch.float64,\n",
      "                device=constants.current_device(),\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                t, op=dist.ReduceOp.MAX, group=constants.parallelism_group()\n",
      "            )\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                if recover_only and os.path.exists(save_dir):\n",
      "                    # Create a symlink from \"recover_checkpoint\" to a directory with step counter,\n",
      "                    # such that we can directly load it as a persistent checkpoint.\n",
      "                    os.symlink(save_dir, hook_data[\"save_dir\"])\n",
      "                logger.info(\n",
      "                    f\"Model {hook_data['model_name']} saved at {hook_data['save_dir']}. \"\n",
      "                    f\"Time consumption: {float(t):.4f}s.\"\n",
      "                )\n",
      "\n",
      "    def __load_model(self, hook_data: Dict):\n",
      "        tik = time.perf_counter()\n",
      "        with constants.model_scope(hook_data[\"model_name\"]):\n",
      "            if isinstance(self._model.module, torch.nn.Identity) and isinstance(\n",
      "                self._backend,\n",
      "                (\n",
      "                    model_api.ALL_BACKEND_CLASSES[\"sglang\"],\n",
      "                    model_api.ALL_BACKEND_CLASSES[\"vllm\"],\n",
      "                ),\n",
      "            ):\n",
      "                # The uninitialized vLLM/SGLang model. Since we create the model\n",
      "                # inside the vLLM/SGLang backend, the initial param realloc before\n",
      "                # backend initialization can be ignored.\n",
      "                return\n",
      "            if self._model.backend_name in [\"vllm\", \"sglang\"]:\n",
      "                if constants.parallelism_rank() == 0:\n",
      "                    logger.info(f\"Updating {self._model.backend_name} model from disk.\")\n",
      "                module = self._model.module\n",
      "                module.update_weights_from_disk(hook_data[\"load_dir\"])\n",
      "            else:\n",
      "                module: ReaLModel = self.__unwrapped_models[hook_data[\"model_name\"]]\n",
      "                assert isinstance(module, ReaLModel), type(module)\n",
      "                module.instantiate()\n",
      "                module.load_from_hf(hook_data[\"load_dir\"], init_critic_from_actor=False)\n",
      "\n",
      "            t = torch.tensor(\n",
      "                float(time.perf_counter() - tik),\n",
      "                dtype=torch.float64,\n",
      "                device=constants.current_device(),\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                t, op=dist.ReduceOp.MAX, group=constants.parallelism_group()\n",
      "            )\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                logger.info(\n",
      "                    f\"Model {hook_data['model_name']} loaded from {hook_data['load_dir']}. \"\n",
      "                    f\"Time consumption: {float(t):.4f}s.\"\n",
      "                )\n",
      "\n",
      "    @cuda_tmark(\"post_response\", CUDATimeMarkType.misc)\n",
      "    def maybe_post_responses(self):\n",
      "        ready_to_post = []\n",
      "        while True:\n",
      "            try:\n",
      "                request, res = self.__reply_queue.get_nowait()\n",
      "                ready_to_post.append((request, res))\n",
      "            except queue.Empty:\n",
      "                break\n",
      "\n",
      "        batch_size = sample_size = 0\n",
      "        for request, res in ready_to_post:\n",
      "            # For some requests, do not respond to the master worker.\n",
      "            if isinstance(res, request_reply_stream.NoResponse):\n",
      "                continue\n",
      "            request: request_reply_stream.Payload\n",
      "            reply = request_reply_stream.Payload(\n",
      "                handler=\"master\",\n",
      "                request_id=request.request_id,\n",
      "                handle_name=request.handle_name,\n",
      "                data=res,\n",
      "            )\n",
      "            self.__stream.post(reply)\n",
      "            # logger.info(f\"handle_name {request.handle_name} Posted req id = {request.request_id}\")\n",
      "            sample_size += self.__request_sample_size.pop(request.request_id)\n",
      "            batch_size += 1\n",
      "        return worker_base.PollResult(sample_count=sample_size, batch_count=batch_size)\n",
      "\n",
      "    def __maybe_receive_one_request(self):\n",
      "        try:\n",
      "            r: request_reply_stream.Payload = self.__stream.poll()\n",
      "            if r.handle_name == \"ack\":\n",
      "                self.__ack_cache[r.request_id] = r\n",
      "            else:\n",
      "                if r.no_syn:\n",
      "                    self.__request_queue.put_nowait(\n",
      "                        (r, r.data, False, None, defaultdict(int))\n",
      "                    )\n",
      "                else:\n",
      "                    self.__stream.post(\n",
      "                        request_reply_stream.Payload(\n",
      "                            handler=\"master\",\n",
      "                            request_id=r.syn_reply_id,\n",
      "                            handle_name=\"syn\",\n",
      "                        ),\n",
      "                    )\n",
      "                    self.__request_cache[r.ack_reply_id] = r\n",
      "        except request_reply_stream.NoMessage:\n",
      "            time.sleep(_MODEL_WORKER_POLL_REQUESTS_INTERVAL_SECS)\n",
      "            pass\n",
      "\n",
      "    @cuda_tmark(\"receive_request\", CUDATimeMarkType.misc)\n",
      "    def maybe_receive_requests(self):\n",
      "        tik = time.perf_counter()\n",
      "        while time.perf_counter() - tik < _MODEL_WORKER_POLL_REQUESTS_SECS:\n",
      "            self.__maybe_receive_one_request()\n",
      "            cur_ack_ids = list(self.__ack_cache.keys())\n",
      "            for ack_id in cur_ack_ids:\n",
      "                if ack_id in self.__request_cache:\n",
      "                    self.__ack_cache.pop(ack_id)\n",
      "                    req = self.__request_cache.pop(ack_id)\n",
      "                    self.__request_queue.put_nowait(\n",
      "                        (req, req.data, False, None, defaultdict(int))\n",
      "                    )\n",
      "\n",
      "    def _poll(self):\n",
      "        if not self.__dist_env_resolved:\n",
      "            self.__lazy_setup()\n",
      "            if constants.use_cuda():\n",
      "                self._clear_memory(force=True)\n",
      "                pynvml.nvmlInit()\n",
      "                self.__nvml_handle = pynvml.nvmlDeviceGetHandleByIndex(\n",
      "                    self.__pg_info.local_gpu_id\n",
      "                )\n",
      "            else:\n",
      "                self.__nvml_handle = None\n",
      "            self.__dist_env_resolved = True\n",
      "\n",
      "        self.maybe_receive_requests()\n",
      "\n",
      "        r = worker_base.PollResult(0, 0)\n",
      "\n",
      "        # Prioritize the `reset` and `flush` request.\n",
      "        # If `flush`, run all the remaining blocking requests.\n",
      "        # These requested tasks typically involve NCCL communication\n",
      "        # or GPU computation. We need to ensure that all these tasks\n",
      "        # are executed in the same order across all model workers.\n",
      "        flush = False\n",
      "        for _ in range(self.__request_queue.qsize()):\n",
      "            request, data, handled, res, time_record = self.__request_queue.get_nowait()\n",
      "            if request.handle_name == \"reset\":\n",
      "                # Pause the worker and wait for the next `configure`\n",
      "                # command from the controller.\n",
      "                return self.__experiment_complete_exit()\n",
      "            elif request.handle_name == \"flush\":\n",
      "                flush = True\n",
      "            elif request.handle_name in NON_BLOCKING_RPCS:\n",
      "                self.handle_non_blocking_request(request)\n",
      "            else:\n",
      "                self.__request_queue.put_nowait(\n",
      "                    (request, data, handled, res, time_record)\n",
      "                )\n",
      "\n",
      "        # Non-blocking requests are usually fast, so we can\n",
      "        # respond them in a batch without affecting the accuracy\n",
      "        # of time logging in the master worker.\n",
      "        r += self.maybe_post_responses()\n",
      "\n",
      "        if flush:\n",
      "            # NOTE: We ensure that all model workers have the same set of requests\n",
      "            # at any time through a TCP-like protocol, i.e., req -> ack -> syn -> resp.\n",
      "            # Each request is composed of pre-hooks, the main request, and post-hooks.\n",
      "            # We execute all pre-hooks first because they involve data transfer\n",
      "            # among workers. Executing them first avoids blocking MFCs that require\n",
      "            # data from the same set of GPUs but are executed on disjoint GPUs.\n",
      "            self.handle_all_pre_hooks()\n",
      "\n",
      "            # Prioritize requests that requires a smaller device mesh.\n",
      "            rescheduled_requests = []\n",
      "            other_requests = []\n",
      "            for _ in range(self.__request_queue.qsize()):\n",
      "                (\n",
      "                    request,\n",
      "                    data,\n",
      "                    handled,\n",
      "                    res,\n",
      "                    time_record,\n",
      "                ) = self.__request_queue.get_nowait()\n",
      "                if request.handle_name not in [\"inference\", \"generate\", \"train_step\"]:\n",
      "                    other_requests.append((request, data, handled, res, time_record))\n",
      "                else:\n",
      "                    with constants.model_scope(request.handler.model_name):\n",
      "                        w = dist.get_world_size(constants.parallelism_group())\n",
      "                    rescheduled_requests.append(\n",
      "                        (request, data, handled, res, time_record, w)\n",
      "                    )\n",
      "            rescheduled_requests.sort(key=lambda x: x[-1])\n",
      "            for request, data, handled, res, time_record, _ in rescheduled_requests:\n",
      "                self.__request_queue.put_nowait(\n",
      "                    (request, data, handled, res, time_record)\n",
      "                )\n",
      "            for request, data, handled, res, time_record in other_requests:\n",
      "                self.__request_queue.put_nowait(\n",
      "                    (request, data, handled, res, time_record)\n",
      "                )\n",
      "\n",
      "            # Execute one MFC them immediately return the result, such that\n",
      "            # we can correctly log the time consumption in the master worker.\n",
      "            while True:\n",
      "                try:\n",
      "                    (\n",
      "                        request,\n",
      "                        data,\n",
      "                        handled,\n",
      "                        res,\n",
      "                        time_record,\n",
      "                    ) = self.__request_queue.get_nowait()\n",
      "                    self.handle_blocking_request(\n",
      "                        request, data, handled, res, time_record\n",
      "                    )\n",
      "                    r += self.maybe_post_responses()\n",
      "                except queue.Empty:\n",
      "                    break\n",
      "        return r\n",
      "\n",
      "    def __experiment_complete_exit(self):\n",
      "        # maybe dump profile recorder\n",
      "        if self.__record_performance:\n",
      "            with open(\n",
      "                os.path.join(\n",
      "                    self._get_setup_logdir(\"performance\"),\n",
      "                    f\"mw{self.__worker_index}.json\",\n",
      "                ),\n",
      "                \"w\",\n",
      "            ) as f:\n",
      "                json.dump(self.__performance_recorder, f, indent=4)\n",
      "\n",
      "        self.__stream.close()\n",
      "\n",
      "        self.__unwrapped_models.clear()\n",
      "\n",
      "        # Calling backend.destroy removes all hooks and releases the memory.\n",
      "        for model_name, backend in self.__backends.items():\n",
      "            backend.destroy(self.__models[model_name])\n",
      "\n",
      "        self.__models.clear()\n",
      "        self.__backends.clear()\n",
      "        self.__interfaces.clear()\n",
      "\n",
      "        # Reset model worker states.\n",
      "        self.__dist_env_resolved = False\n",
      "\n",
      "        if constants.use_cuda():\n",
      "            before_mem = pynvml.nvmlDeviceGetMemoryInfo(self.__nvml_handle).used\n",
      "\n",
      "        constants.reset_run()\n",
      "        topology.destroy_all_comm_groups()\n",
      "        cuda_graph.destroy_all()\n",
      "\n",
      "        self._clear_memory(force=True)\n",
      "\n",
      "        if constants.use_cuda():\n",
      "            # Record memory.\n",
      "            after_mem = pynvml.nvmlDeviceGetMemoryInfo(self.__nvml_handle).used\n",
      "            blogger.debug(\n",
      "                f\"GPU memory used upon experiment complete: \"\n",
      "                f\"{before_mem/1024**2:.2f}MB -> {after_mem / 1024**2:.2f}MB\"\n",
      "            )\n",
      "\n",
      "            self.__nvml_handle = None\n",
      "            try:\n",
      "                pynvml.nvmlShutdown()\n",
      "            except pynvml.nvml.NVMLError_Uninitialized:\n",
      "                pass\n",
      "        self.pause()\n",
      "        return worker_base.PollResult(sample_count=0, batch_count=0)\n",
      "\n",
      "    # def __recover_save(self):\n",
      "    #     # store model and dataset states for recover\n",
      "    #     if self.__dist_env_resolved:\n",
      "\n",
      "    #         for model_name, model in self.__models.items():\n",
      "    #             if self.__model_is_handle[model_name]:\n",
      "    #                 continue\n",
      "    #             constants._model_name = None  # force quit model_scope\n",
      "    #             with constants.model_scope(model_name):\n",
      "    #                 ckpt_save_dir = os.path.join(\n",
      "    #                     self.__recover_states_root, \"ckpt\", model_name.role\n",
      "    #                 )\n",
      "    #                 # replace old recover ckpt\n",
      "    #                 logger.info(\n",
      "    #                     f\"saving model {model_name} ckpt for recover at {ckpt_save_dir}. \"\n",
      "    #                     f\"epoch {model.version.epoch}, epoch_step {model.version.epoch_step}, \"\n",
      "    #                     f\"global step {model.version.global_step}\"\n",
      "    #                 )\n",
      "    #                 if self.__has_dataset:\n",
      "    #                     logger.info(\n",
      "    #                         f\"Dataset info: \" f\"dataset epoch {self.__dataset_epoch}\"\n",
      "    #                     )\n",
      "    #                 self._interface.save(model, ckpt_save_dir)\n",
      "    #                 logger.info(f\"saving done.\")\n",
      "\n",
      "    # def _exit_hook(self, exit_status: worker_base.WorkerServerStatus):\n",
      "    #     logger.info(\n",
      "    #         f\"Model worker {self.__worker_index} exit with status {exit_status}.\"\n",
      "    #     )\n",
      "    #     if os.getenv(\"REAL_SAVE_RECOVER_STATES\", \"0\") != \"1\":\n",
      "    #         return\n",
      "    #     if exit_status == worker_base.WorkerServerStatus.ERROR:\n",
      "    #         try:\n",
      "    #             sleep_time = 600\n",
      "    #             current_sleep_time = 0\n",
      "    #             while current_sleep_time < sleep_time:\n",
      "    #                 logger.info(\n",
      "    #                     f\"ERROR exit, waited {current_sleep_time} s for interruption ...\"\n",
      "    #                 )\n",
      "    #                 time.sleep(10)\n",
      "    #                 current_sleep_time += 10\n",
      "    #         except KeyboardInterrupt:\n",
      "    #             logger.info(\"Received SIGINT, starting recover save\")\n",
      "\n",
      "    #     self.__recover_save()\n",
      "\n",
      "    def __log_gpu_stats(self, request: request_reply_stream.Payload):\n",
      "        # Log GPU utilization and memory statistics.\n",
      "        utilization = pynvml.nvmlDeviceGetUtilizationRates(self.__nvml_handle)  # bytes\n",
      "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(self.__nvml_handle)  # bytes\n",
      "        kill_threshold = float(os.environ.get(\"REAL_GPU_MEMORY_KILL_THRESHOLD\", \"1.0\"))\n",
      "        if memory_info.used / memory_info.total > kill_threshold:\n",
      "            raise RuntimeError(\n",
      "                f\"GPU memory excceeds kill threshold {kill_threshold:.2f}. \"\n",
      "                \"This threshold could be adjusted by changing environment \"\n",
      "                'variable \"REAL_GPU_MEMORY_KILL_THRESHOLD\".'\n",
      "            )\n",
      "\n",
      "        torch_mem_stats = torch.cuda.memory_stats(0)\n",
      "\n",
      "        # All-gather hostname, gpu ID, and stats.\n",
      "        hostname = socket.gethostname()\n",
      "        hostname_len = len(hostname)\n",
      "        assert hostname_len < 64, \"hostname should not have more than 64 chars\"\n",
      "        # Encode hostnames into long.\n",
      "        hostname_np = np.fromstring(\n",
      "            hostname + \"x\" * (64 - len(hostname)), dtype=np.int64\n",
      "        )\n",
      "        local_mem_stats = torch.tensor(\n",
      "            [hostname_len, self.__pg_info.local_gpu_id]\n",
      "            + hostname_np.tolist()\n",
      "            + [\n",
      "                torch_mem_stats[\"allocated_bytes.all.peak\"],\n",
      "                torch_mem_stats[\"reserved_bytes.all.peak\"],\n",
      "                memory_info.used,\n",
      "            ],\n",
      "            dtype=torch.long,\n",
      "            device=\"cuda\",\n",
      "        )  # length 2 + 8 + 3 = 13\n",
      "        mem_stats = local_mem_stats.new_zeros(\n",
      "            size=(\n",
      "                dist.get_world_size(constants.parallelism_group()),\n",
      "                local_mem_stats.shape[0],\n",
      "            )\n",
      "        )\n",
      "        # All-gather memory stats.\n",
      "        dist.all_gather_into_tensor(\n",
      "            mem_stats, local_mem_stats, group=constants.parallelism_group()\n",
      "        )\n",
      "        mem_stats = mem_stats.cpu().numpy()\n",
      "\n",
      "        # All-reduce utilization.\n",
      "        gpu_compute_util = torch.tensor(\n",
      "            utilization.gpu, dtype=torch.float32, device=\"cuda\"\n",
      "        )\n",
      "        dist.all_reduce(gpu_compute_util, group=constants.parallelism_group())\n",
      "        gpu_compute_util = gpu_compute_util.item() / dist.get_world_size(\n",
      "            constants.parallelism_group()\n",
      "        )\n",
      "\n",
      "        def _decode_hostname(idx):\n",
      "            hn_np = mem_stats[idx, 2 : 2 + 8]\n",
      "            l = mem_stats[idx, 0]\n",
      "            return hn_np.tobytes().decode(\"utf-8\")[:l]\n",
      "\n",
      "        def _decode_gpu_id(idx):\n",
      "            return f\"{_decode_hostname(idx)}:{mem_stats[idx, 1]}\"\n",
      "\n",
      "        max_used_gpu_id = _decode_gpu_id(np.argmax(mem_stats[:, -1]))\n",
      "        max_reserved_gpu_id = _decode_gpu_id(np.argmax(mem_stats[:, -2]))\n",
      "        max_tensor_gpu_id = _decode_gpu_id(np.argmax(mem_stats[:, -3]))\n",
      "\n",
      "        # NOTE: We only log the peak memory because it's\n",
      "        # the most important for detecting OOM issues.\n",
      "        headers = [\n",
      "            \" \",\n",
      "            \"TotalMem\",\n",
      "            \"PeakUsedMem\",\n",
      "            \"PeakTensorMem\",\n",
      "            \"PeakReservedMem\",\n",
      "            \"MaxMemUtil\",\n",
      "            \"AvgComputeUtil\",\n",
      "        ]\n",
      "        line1 = [\n",
      "            \"Value\",\n",
      "            f\"{memory_info.total / 1024**2:.2f}MB\",\n",
      "            f\"{max(mem_stats[:, -1]) / 1024**2:.2f}MB\",\n",
      "            f\"{max(mem_stats[:, -3]) / 1024**2:.2f}MB\",\n",
      "            f\"{max(mem_stats[:, -2]) / 1024**2:.2f}MB\",\n",
      "            f\"{max(mem_stats[:, -1]) / memory_info.total * 100:.2f}%\",\n",
      "            f\"{gpu_compute_util:.2f}%\",\n",
      "        ]\n",
      "        line2 = [\n",
      "            \"GPU ID\",\n",
      "            \"-\",\n",
      "            max_used_gpu_id,\n",
      "            max_tensor_gpu_id,\n",
      "            max_reserved_gpu_id,\n",
      "            max_used_gpu_id,\n",
      "            \"-\",\n",
      "        ]\n",
      "\n",
      "        if self._dp_rank == 0 and self._is_dp_head:\n",
      "            logger.info(\n",
      "                f\"Aggregated GPU memory stats after MFC `{request.handle_name}`\"\n",
      "                f\" within model `{request.handler.model_name}`:\\n\"\n",
      "                + tabulate.tabulate(\n",
      "                    [headers, line1, line2], headers=\"firstrow\", tablefmt=\"fancy_grid\"\n",
      "                )\n",
      "            )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/push_pull_stream.py ====\n",
      "\n",
      "import logging\n",
      "from queue import Empty as QueueEmpty\n",
      "from typing import Any, Dict, List, Optional, Union\n",
      "\n",
      "import orjson\n",
      "import zmq\n",
      "from zmq.utils.strtypes import asbytes\n",
      "\n",
      "from realhf.base import logging, name_resolve, names, network\n",
      "\n",
      "logger = logging.getLogger(\"ZMQ Push-Pull Stream\")\n",
      "\n",
      "# Type alias for JSON-compatible objects\n",
      "JSONType = Union[Dict[str, Any], List[Any], str, int, float, bool, None]\n",
      "\n",
      "\n",
      "class ZMQJsonPusher:\n",
      "    \"\"\"\n",
      "    JSON pusher using ZeroMQ.\n",
      "\n",
      "    Args:\n",
      "        host: Host address (default: 'localhost')\n",
      "        port: Port number (default: 5555)\n",
      "        hwm: High-water mark for outgoing messages (default: 1000)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, host: str = \"localhost\", port: int = 5555, hwm: int = 1000):\n",
      "        self.host = host\n",
      "        self.port = port\n",
      "\n",
      "        self.ctx = zmq.Context.instance()\n",
      "        self.socket = self.ctx.socket(zmq.PUSH)\n",
      "        self.socket.setsockopt(zmq.SNDHWM, hwm)\n",
      "        self.socket.connect(f\"tcp://{self.host}:{self.port}\")\n",
      "\n",
      "    def push(self, data: JSONType) -> None:\n",
      "        \"\"\"\n",
      "        Push JSON-compatible data efficiently.\n",
      "\n",
      "        Args:\n",
      "            data: JSON-serializable Python object\n",
      "\n",
      "        Raises:\n",
      "            TypeError: If data is not JSON-serializable\n",
      "            zmq.ZMQError: If ZeroMQ operation fails\n",
      "        \"\"\"\n",
      "        # Directly encode to bytes without intermediate string\n",
      "        json_bytes = asbytes(orjson.dumps(data))\n",
      "        self.socket.send(json_bytes, copy=False)\n",
      "\n",
      "    def close(self) -> None:\n",
      "        \"\"\"Clean up resources.\"\"\"\n",
      "        self.socket.close(linger=0)\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "        self.close()\n",
      "\n",
      "\n",
      "class ZMQJsonPuller:\n",
      "    \"\"\"\n",
      "    JSON puller using ZeroMQ with per-call timeout support in pull() method.\n",
      "\n",
      "    Args:\n",
      "        host: Host address (default: 'localhost')\n",
      "        port: Port number (default: 5555)\n",
      "        default_timeout_ms: Default receive timeout in milliseconds (default: 1000)\n",
      "        hwm: High-water mark for incoming messages (default: 1000)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        host: str = \"localhost\",\n",
      "        port: int = 5555,\n",
      "        default_timeout_ms: int = 1000,\n",
      "        hwm: int = 1000,\n",
      "    ):\n",
      "        self.host = host\n",
      "        self.port = port\n",
      "        self.default_timeout_ms = default_timeout_ms\n",
      "\n",
      "        self.ctx = zmq.Context.instance()\n",
      "        self.socket = self.ctx.socket(zmq.PULL)\n",
      "        self.socket.setsockopt(zmq.RCVHWM, hwm)\n",
      "        self.socket.setsockopt(zmq.RCVTIMEO, self.default_timeout_ms)\n",
      "        self.socket.bind(f\"tcp://{self.host}:{self.port}\")\n",
      "\n",
      "        self.poller = zmq.Poller()\n",
      "        self.poller.register(self.socket, zmq.POLLIN)\n",
      "\n",
      "    def pull(self, timeout_ms: Optional[int] = None):\n",
      "        \"\"\"\n",
      "        Pull and decode JSON data with configurable timeout.\n",
      "\n",
      "        Args:\n",
      "            timeout_ms: Optional timeout in seconds. If None, uses default_timeout_ms.\n",
      "\n",
      "        Returns:\n",
      "            Deserialized JSON-compatible Python object\n",
      "\n",
      "        Raises:\n",
      "            queue.Empty: If no message available within timeout\n",
      "        \"\"\"\n",
      "        current_timeout = self.default_timeout_ms if timeout_ms is None else timeout_ms\n",
      "        events = dict(self.poller.poll(current_timeout))\n",
      "        if self.socket in events:\n",
      "            msg = self.socket.recv(flags=zmq.NOBLOCK, copy=False)\n",
      "            return orjson.loads(msg.bytes.decode(\"utf-8\"))\n",
      "        raise QueueEmpty(f\"No data available after {current_timeout}ms timeout\")\n",
      "\n",
      "    def close(self) -> None:\n",
      "        \"\"\"Clean up resources.\"\"\"\n",
      "        self.socket.close(linger=0)\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "        self.close()\n",
      "\n",
      "\n",
      "def grouping(num_senders, num_receivers):\n",
      "    groups = {}\n",
      "    assert num_senders >= num_receivers\n",
      "    # Each PULL gets multiple PUSH\n",
      "    senders_per_receiver = num_senders // num_receivers\n",
      "    for receiver_id in range(num_receivers):\n",
      "        start = receiver_id * senders_per_receiver\n",
      "        end = (receiver_id + 1) * senders_per_receiver\n",
      "        groups[receiver_id] = list(range(start, end))\n",
      "    # Distribute remaining senders\n",
      "    remaining = num_senders % num_receivers\n",
      "    for i in range(remaining):\n",
      "        groups[i].append(num_receivers * senders_per_receiver + i)\n",
      "    return groups\n",
      "\n",
      "\n",
      "class NameResolvingZmqPusher(ZMQJsonPusher):\n",
      "    def __init__(self, experiment_name, trial_name, pusher_index, pusher_cnt, **kwargs):\n",
      "        pullers = name_resolve.get_subtree(\n",
      "            names.stream_pullers(experiment_name, trial_name)\n",
      "        )\n",
      "        pullers = list(map(int, pullers))\n",
      "        puller_cnt = len(pullers)\n",
      "        assert sorted(pullers) == list(range(puller_cnt))\n",
      "        groups = grouping(pusher_cnt, puller_cnt)\n",
      "        puller_index = None\n",
      "        for puller_index, pusher_indices in groups.items():\n",
      "            if pusher_index in pusher_indices:\n",
      "                break\n",
      "        assert puller_index is not None\n",
      "        name = names.push_pull_stream(\n",
      "            experiment_name, trial_name, stream_name=f\"puller{puller_index}\"\n",
      "        )\n",
      "        addr = name_resolve.wait(name)\n",
      "        host, port = addr.split(\":\")\n",
      "        super().__init__(host, int(port), **kwargs)\n",
      "\n",
      "\n",
      "class NameResolvingZmqPuller(ZMQJsonPuller):\n",
      "    def __init__(self, experiment_name, trial_name, puller_index, **kwargs):\n",
      "        name = names.push_pull_stream(\n",
      "            experiment_name, trial_name, stream_name=f\"puller{puller_index}\"\n",
      "        )\n",
      "        host, port = network.gethostip(), network.find_free_port(\n",
      "            experiment_name=experiment_name, trial_name=trial_name\n",
      "        )\n",
      "        addr = f\"{host}:{port}\"\n",
      "        name_resolve.add(name, addr)\n",
      "        super().__init__(host, port, **kwargs)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/master_worker.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import copy\n",
      "import gc\n",
      "import os\n",
      "import time\n",
      "from typing import Dict\n",
      "\n",
      "import colorama\n",
      "import networkx as nx\n",
      "import numpy as np\n",
      "import wandb\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import realhf.api.core.dfg as dfg\n",
      "import realhf.api.core.model_api as model_api\n",
      "import realhf.api.core.system_api as config_pkg\n",
      "import realhf.base.recover as recover\n",
      "import realhf.system.request_reply_stream as request_reply_stream\n",
      "import realhf.system.worker_base as worker_base\n",
      "from realhf.api.core.config import ModelName\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.api.core.system_api import ExpStatus\n",
      "from realhf.base import (\n",
      "    constants,\n",
      "    logging,\n",
      "    name_resolve,\n",
      "    names,\n",
      "    seeding,\n",
      "    timeutil,\n",
      "    topology,\n",
      ")\n",
      "from realhf.system.buffer import AsyncIOSequenceBuffer\n",
      "from realhf.system.function_executor import FunctionExecutor\n",
      "from realhf.system.model_function_call import RPCCorountineControl\n",
      "\n",
      "logger = logging.getLogger(\"master worker\", \"system\")\n",
      "blogger = logging.getLogger(\"benchmark\")\n",
      "\n",
      "\n",
      "class MasterWorker(worker_base.AsyncWorker):\n",
      "    global_exp_tik = time.perf_counter()\n",
      "\n",
      "    def _configure(self, config: config_pkg.MasterWorker):\n",
      "        self.config = config\n",
      "\n",
      "        seeding.set_random_seed(self.config.base_seed, \"master_worker\")\n",
      "\n",
      "        self.__model_topos: Dict[ModelName, topology.ProcessTopology] = (\n",
      "            config.model_topos\n",
      "        )\n",
      "\n",
      "        # Build execution graph and initialize concurrency utilities.\n",
      "        self.__model_rpcs = config.model_rpcs\n",
      "\n",
      "        # Sort all MFCs in the topological order and\n",
      "        # calculate the width of each level.\n",
      "        # These numbers will determine when to flush MFC requests.\n",
      "        self.__topo_widths = []\n",
      "        for generation in nx.topological_generations(self.__model_rpcs[0]._G):\n",
      "            self.__topo_widths.append(len(generation))\n",
      "        logger.debug(\"Topological widths: \" + str(self.__topo_widths))\n",
      "\n",
      "        self.__rpc_srcs = list(filter(lambda rpc: rpc.is_src, self.__model_rpcs))\n",
      "        self.__rpc_dsts = list(filter(lambda rpc: rpc.is_dst, self.__model_rpcs))\n",
      "\n",
      "        # Save and eval control.\n",
      "        self.__total_train_epochs = config.exp_ctrl.total_train_epochs\n",
      "        self.__save_ctl = timeutil.EpochStepTimeFreqCtl(\n",
      "            freq_epoch=config.exp_ctrl.save_freq_epochs,\n",
      "            freq_step=config.exp_ctrl.save_freq_steps,\n",
      "            freq_sec=config.exp_ctrl.save_freq_secs,\n",
      "        )\n",
      "        if (\n",
      "            config.exp_ctrl.ckpt_freq_epochs is None\n",
      "            and config.exp_ctrl.ckpt_freq_steps is None\n",
      "            and config.exp_ctrl.ckpt_freq_secs is None\n",
      "        ):\n",
      "            self.__ckpt_ctl = timeutil.EpochStepTimeFreqCtl(\n",
      "                freq_epoch=config.exp_ctrl.save_freq_epochs,\n",
      "                freq_step=config.exp_ctrl.save_freq_steps,\n",
      "                freq_sec=config.exp_ctrl.save_freq_secs,\n",
      "            )\n",
      "        else:\n",
      "            self.__ckpt_ctl = timeutil.EpochStepTimeFreqCtl(\n",
      "                freq_epoch=config.exp_ctrl.ckpt_freq_epochs,\n",
      "                freq_step=config.exp_ctrl.ckpt_freq_steps,\n",
      "                freq_sec=config.exp_ctrl.ckpt_freq_secs,\n",
      "            )\n",
      "        self.__eval_ctl = timeutil.EpochStepTimeFreqCtl(\n",
      "            freq_epoch=config.exp_ctrl.eval_freq_epochs,\n",
      "            freq_step=config.exp_ctrl.eval_freq_steps,\n",
      "            freq_sec=config.exp_ctrl.eval_freq_secs,\n",
      "        )\n",
      "\n",
      "        self.MODEL_SAVE_ROOT = os.path.join(\n",
      "            constants.MODEL_SAVE_ROOT,\n",
      "            config.worker_info.experiment_name,\n",
      "            config.worker_info.trial_name,\n",
      "        )\n",
      "        os.makedirs(self.MODEL_SAVE_ROOT, exist_ok=True)\n",
      "\n",
      "        self.__initialized = False\n",
      "        self.__recover_run, self.__recover_info = recover.load_recover_info()\n",
      "        if self.__recover_info is not None:\n",
      "            logger.info(\n",
      "                f\"Loaded recover info: recover_start={self.__recover_info.recover_start}, \"\n",
      "                f\"last_step_info={self.__recover_info.last_step_info}.\"\n",
      "            )\n",
      "            logger.info(\n",
      "                f\"Number of used data in recover info: {len(self.__recover_info.hash_vals_to_ignore)}. \"\n",
      "                f\"The previous experiment probably ran for {len(self.__recover_info.hash_vals_to_ignore) // self.__rpc_srcs[0].n_seqs} steps in the epoch.\"\n",
      "            )\n",
      "\n",
      "        # Create corountine control objects for running the dataflow graph.\n",
      "        self.__rpc_ctrl = RPCCorountineControl(\n",
      "            train_count=asyncio.Queue(maxsize=len(self.__rpc_dsts)),\n",
      "            topo_level_count=asyncio.Queue(maxsize=sum(self.__topo_widths)),\n",
      "            lock=asyncio.Lock(),\n",
      "            # NOTE: We should accumulate the used data hashes in the same epoch\n",
      "            # to prevent loading data used before.\n",
      "            used_hash_vals_this_epoch=(\n",
      "                copy.deepcopy(self.__recover_info.hash_vals_to_ignore)\n",
      "                if self.__recover_run\n",
      "                else list()\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        if self.__recover_run:\n",
      "            self.__rpc_ctrl.step_info = copy.deepcopy(self.__recover_info.recover_start)\n",
      "\n",
      "            self.__eval_ctl.load_state_dict(self.__recover_info.eval_ctl_info)\n",
      "            self.__save_ctl.load_state_dict(self.__recover_info.save_ctl_info)\n",
      "            self.__ckpt_ctl.load_state_dict(self.__recover_info.ckpt_ctl_info)\n",
      "\n",
      "            logger.info(\n",
      "                f\"Recovering from previous run. \"\n",
      "                f\"Epoch: {self.__rpc_ctrl.step_info.epoch + 1}, \"\n",
      "                f\"Epoch Step: {self.__rpc_ctrl.step_info.epoch_step + 1} \"\n",
      "                f\"Global Step: {self.__rpc_ctrl.step_info.global_step + 1}.\"\n",
      "            )\n",
      "\n",
      "            # Recover the previous number of training samples\n",
      "            train_rpcs = list(\n",
      "                filter(\n",
      "                    lambda rpc: rpc.interface_type == dfg.ModelInterfaceType.TRAIN_STEP,\n",
      "                    self.__model_rpcs,\n",
      "                )\n",
      "            )\n",
      "            train_batch_size = train_rpcs[0].n_seqs\n",
      "            hist_samples = (\n",
      "                train_batch_size * self.__recover_info.last_step_info.global_step\n",
      "            )\n",
      "            training_sample_name = names.training_samples(\n",
      "                constants.experiment_name(), constants.trial_name()\n",
      "            )\n",
      "            name_resolve.add(training_sample_name, str(hist_samples), replace=True)\n",
      "\n",
      "        # for benchmark\n",
      "        self.e2e_time_history = []\n",
      "        self.__benchmark_steps = config.exp_ctrl.benchmark_steps\n",
      "        self.__benchmark_n_seqs = config.exp_ctrl.benchmark_n_seqs\n",
      "\n",
      "        return config.worker_info\n",
      "\n",
      "    def initialize_models(self):\n",
      "        # Initialize model backends.\n",
      "        model_names = list(self.__model_topos.keys())\n",
      "        self.logger.debug(f\"Initialize model backends with order: {model_names}.\")\n",
      "        train_rpcs = list(\n",
      "            filter(\n",
      "                lambda rpc: rpc.interface_type == dfg.ModelInterfaceType.TRAIN_STEP,\n",
      "                self.__model_rpcs,\n",
      "            )\n",
      "        )\n",
      "        assert all(rpc.n_seqs == train_rpcs[0].n_seqs for rpc in train_rpcs)\n",
      "        if len(train_rpcs) > 0:\n",
      "            ft_spec = model_api.FinetuneSpec(\n",
      "                total_train_epochs=self.config.exp_ctrl.total_train_epochs,\n",
      "                dataset_size=self._dataset_size,\n",
      "                train_batch_size=train_rpcs[0].n_seqs,\n",
      "            )\n",
      "        else:\n",
      "            ft_spec = model_api.FinetuneSpec(\n",
      "                total_train_epochs=self.config.exp_ctrl.total_train_epochs,\n",
      "                dataset_size=self._dataset_size,\n",
      "                train_batch_size=self.__src_rpc.n_seqs,\n",
      "            )\n",
      "        _initialized_roles = []\n",
      "        for model_name in model_names:\n",
      "            topo = self.config.model_topos[model_name]\n",
      "            # Build FinetuneSpec, which is required to initialize backends.\n",
      "            _handlers = [\n",
      "                config_pkg.ModelShardID.from_parallelism_rank(model_name, topo, j)\n",
      "                for j in range(topo.world_size())\n",
      "            ]\n",
      "\n",
      "            init_payloads = [\n",
      "                request_reply_stream.Payload(\n",
      "                    handler=_h,\n",
      "                    handle_name=\"initialize\",\n",
      "                    data=ft_spec,\n",
      "                )\n",
      "                for _h in _handlers\n",
      "            ]\n",
      "\n",
      "            # Send initialization requests then immediately flush them.\n",
      "            self.__stream.request(\n",
      "                payloads=init_payloads,\n",
      "            )\n",
      "            self.__stream.request(\n",
      "                handlers=_handlers,\n",
      "                handle_type=\"flush\",\n",
      "                no_syn=True,\n",
      "            )\n",
      "\n",
      "            _initialized_roles.append(model_name.role)\n",
      "\n",
      "        self._ft_spec = ft_spec\n",
      "        logger.info(\"Initializations of models and backends complete.\")\n",
      "\n",
      "    def get_dataset_model_info(self):\n",
      "        src_rpc = self.__rpc_srcs[0]\n",
      "        src_rpc_topo = self.config.model_topos[src_rpc.model_name]\n",
      "        src_rpc_dp_size = src_rpc_topo.get_dim(\"data\")\n",
      "\n",
      "        # Request training specification from data workers.\n",
      "        specs = self.__stream.call(\n",
      "            handlers=[f\"__data{i}__\" for i in range(src_rpc_dp_size)],\n",
      "            datas=[None for i in range(src_rpc_dp_size)],\n",
      "            handle_type=\"spec\",\n",
      "        )\n",
      "        assert all(x[\"n_datasets\"] == specs[0][\"n_datasets\"] for x in specs), specs\n",
      "        self._dataset_size = sum(x[\"dataset_size\"] for x in specs)\n",
      "        self._n_datasets = specs[0][\"n_datasets\"]\n",
      "\n",
      "        self._steps_per_epoch = self._dataset_size // src_rpc.n_seqs\n",
      "\n",
      "        # Request model configs from model workers.\n",
      "        # Return None if the model is not a ReaLModel.\n",
      "        self.__model_configs: Dict[ModelName, None | ReaLModelConfig] = {}\n",
      "        for model_name, topo in self.config.model_topos.items():\n",
      "            h = config_pkg.ModelShardID.from_parallelism_rank(model_name, topo, 0)\n",
      "            self.__model_configs[model_name] = self.__stream.call(\n",
      "                handlers=[h],\n",
      "                datas=[None],\n",
      "                handle_type=\"model_config\",\n",
      "            )[0]\n",
      "\n",
      "    def __lazy_init(self):\n",
      "        # Set up streams.\n",
      "        handler_routing = copy.deepcopy(self.config.msid2mwid)\n",
      "        src_rpc = self.__rpc_srcs[0]\n",
      "        src_rpc_topo = self.config.model_topos[src_rpc.model_name]\n",
      "        src_rpc_dp_size = src_rpc_topo.get_dim(\"data\")\n",
      "        src_rpc_pp_size = src_rpc_topo.get_dim(\"pipe\")\n",
      "        for i in range(src_rpc_dp_size):\n",
      "            rank = src_rpc_topo.get_rank(data=i, pipe=src_rpc_pp_size - 1, tensor=0)\n",
      "            handler_routing[f\"__data{i}__\"] = self.config.msid2mwid[\n",
      "                config_pkg.ModelShardID.from_parallelism_rank(\n",
      "                    model_name=src_rpc.model_name,\n",
      "                    topo=src_rpc_topo,\n",
      "                    parallelism_rank=rank,\n",
      "                )\n",
      "            ]\n",
      "        handler_routing.update({i: i for i in range(self.config.n_model_workers)})\n",
      "        self.__stream = request_reply_stream.make_master_stream(\n",
      "            self.config.worker_info,\n",
      "            n_subscribers=self.config.n_model_workers,\n",
      "            handler_routing=handler_routing,\n",
      "        )\n",
      "        self.__stream: request_reply_stream.NameResolvingRequestClient\n",
      "\n",
      "        self.__src_rpc = src_rpc = [\n",
      "            rpc for rpc in self.config.model_rpcs if rpc.is_src\n",
      "        ][0]\n",
      "\n",
      "        self.get_dataset_model_info()\n",
      "\n",
      "        self.initialize_models()\n",
      "\n",
      "        self.__seqbuffers = [\n",
      "            AsyncIOSequenceBuffer(\n",
      "                self.__model_rpcs,\n",
      "                max_size=int(os.getenv(\"REAL_MASTER_BUFFER_SIZE\", str(int(1e7)))),\n",
      "            )\n",
      "            for _ in range(self._n_datasets)\n",
      "        ]\n",
      "\n",
      "        # wandb init, connect to remote wandb host\n",
      "        if self.wandb_config.mode != \"disabled\":\n",
      "            wandb.login()\n",
      "        wandb.init(\n",
      "            mode=self.wandb_config.mode,\n",
      "            entity=self.wandb_config.entity,\n",
      "            project=self.wandb_config.project or constants.experiment_name(),\n",
      "            name=self.wandb_config.name or f\"{constants.trial_name()}_train\",\n",
      "            job_type=self.wandb_config.job_type,\n",
      "            group=self.wandb_config.group\n",
      "            or f\"{constants.experiment_name()}_{constants.trial_name()}\",\n",
      "            notes=self.wandb_config.notes,\n",
      "            tags=self.wandb_config.tags,\n",
      "            config=self.wandb_config.config,\n",
      "            dir=os.path.join(\n",
      "                constants.LOG_ROOT, constants.experiment_name(), constants.trial_name()\n",
      "            ),\n",
      "            force=True,\n",
      "            id=f\"{constants.experiment_name()}_{constants.trial_name()}_train\",\n",
      "            resume=\"allow\",\n",
      "            settings=wandb.Settings(start_method=\"fork\"),\n",
      "        )\n",
      "        # tensorboard logging\n",
      "        self.__summary_writer = None\n",
      "        if self.tensorboard_config.path is not None:\n",
      "            self.__summary_writer = SummaryWriter(log_dir=self.tensorboard_config.path)\n",
      "\n",
      "        # Create coroutines for model RPCs.\n",
      "        logger.debug(f\"Creating asyncio coroutines...\")\n",
      "        self.func_executor = FunctionExecutor(\n",
      "            rpcs=self.__model_rpcs,\n",
      "            msid2mwid=self.config.msid2mwid,\n",
      "            stream=self.__stream,\n",
      "            buffers=self.__seqbuffers,\n",
      "            model_topos=self.__model_topos,\n",
      "            model_configs=self.__model_configs,\n",
      "            ctrl=self.__rpc_ctrl,\n",
      "            summary_writer=self.__summary_writer,\n",
      "            shuffle_dataset=self.config.shuffle_dataset,\n",
      "        )\n",
      "        if self.__recover_run:\n",
      "            self.func_executor.data_loading_dp_idx = (\n",
      "                self.__recover_info.data_loading_dp_idx\n",
      "            )\n",
      "        logger.debug(f\"Coroutines created. The master worker is ready to run.\")\n",
      "\n",
      "        self.__initialized = True\n",
      "        self._train_start_time = time.perf_counter()\n",
      "\n",
      "        self.__last_step_info = recover.StepInfo(\n",
      "            epoch=-1,\n",
      "            epoch_step=-1,\n",
      "            global_step=-1,\n",
      "        )\n",
      "\n",
      "    async def __poll_async(self):\n",
      "        is_new_epoch = False\n",
      "\n",
      "        if not self.__initialized:\n",
      "            self.__lazy_init()\n",
      "\n",
      "        # Main execution steps. The graph runs under-the-hood in RPC & stream threads.\n",
      "        # Wait for the finish of the traversal of the execution graph.\n",
      "        execution_start = time.perf_counter()\n",
      "\n",
      "        is_new_epoch = self._ft_spec.is_new_epoch(self.__rpc_ctrl.step_info)\n",
      "        is_epoch_last_step = self._ft_spec.is_epoch_last_step(self.__rpc_ctrl.step_info)\n",
      "\n",
      "        # Check whether we should evaluate or save models.\n",
      "        self.__rpc_ctrl.should_eval = self.__eval_ctl.check(\n",
      "            epochs=int(is_epoch_last_step), steps=1\n",
      "        )\n",
      "        self.__rpc_ctrl.should_save = self.__save_ctl.check(\n",
      "            epochs=int(is_epoch_last_step), steps=1\n",
      "        )\n",
      "        self.__rpc_ctrl.should_ckpt = self.__ckpt_ctl.check(\n",
      "            epochs=int(is_epoch_last_step), steps=1\n",
      "        )\n",
      "\n",
      "        # Log eval/save info.\n",
      "        epoch = self.__rpc_ctrl.step_info.epoch + 1\n",
      "        epoch_step = self.__rpc_ctrl.step_info.epoch_step + 1\n",
      "        global_step = self.__rpc_ctrl.step_info.global_step + 1\n",
      "        if is_new_epoch:\n",
      "            epoch += 1\n",
      "            epoch_step = 1\n",
      "        s = f\"The next step is epoch {epoch}/{self.config.exp_ctrl.total_train_epochs} \"\n",
      "        s += f\"step {epoch_step}/{self._steps_per_epoch} \"\n",
      "        s += f\"(global step {global_step}). \"\n",
      "        s += f\"Should checkpoint? {self.__rpc_ctrl.should_ckpt}. \"\n",
      "        s += f\"Should save? {self.__rpc_ctrl.should_save}. \"\n",
      "        s += f\"Should run evaluation? {self.__rpc_ctrl.should_eval}. \"\n",
      "        s += f\"Is the first step in epoch? {is_new_epoch}. \"\n",
      "        s += f\"Is the last step in epoch? {is_epoch_last_step}. \"\n",
      "        self.logger.info(s)\n",
      "\n",
      "        # Traverse over the dataflow graph for once.\n",
      "        await self.func_executor.execute_step()\n",
      "\n",
      "        # Post-process.\n",
      "        if self.__rpc_ctrl.should_save or self.__rpc_ctrl.should_ckpt:\n",
      "            self.__last_step_info = copy.deepcopy(self.__rpc_ctrl.step_info)\n",
      "\n",
      "        if is_epoch_last_step:\n",
      "            self.__rpc_ctrl.used_hash_vals_this_epoch = (\n",
      "                self.__rpc_ctrl.used_hash_vals_this_epoch[self._dataset_size :]\n",
      "            )\n",
      "\n",
      "        if is_new_epoch:\n",
      "            self.__rpc_ctrl.step_info.epoch += 1\n",
      "            self.__rpc_ctrl.step_info.epoch_step = 0\n",
      "\n",
      "        # Logging.\n",
      "        time_since_configure = time.perf_counter() - self._train_start_time\n",
      "        e2e_time = time.perf_counter() - execution_start\n",
      "        self.e2e_time_history.append(e2e_time)\n",
      "\n",
      "        self._log_training_stats(e2e_time, time_since_configure)\n",
      "\n",
      "        # Updata counters.\n",
      "        self.__rpc_ctrl.step_info.epoch_step += 1\n",
      "        self.__rpc_ctrl.step_info.global_step += 1\n",
      "\n",
      "        if self.__rpc_ctrl.should_save or self.__rpc_ctrl.should_ckpt:\n",
      "            self.__recover_save()\n",
      "\n",
      "        # Pause the worker if experiment or system-wise benchmark completes.\n",
      "        if (\n",
      "            (\n",
      "                self.__benchmark_steps is not None\n",
      "                and self.__rpc_ctrl.step_info.global_step >= self.__benchmark_steps\n",
      "            )\n",
      "            or (\n",
      "                self.__rpc_ctrl.step_info.global_step * self.__src_rpc.n_seqs\n",
      "                >= self.__total_train_epochs * self._dataset_size\n",
      "            )\n",
      "            or (\n",
      "                self.__benchmark_n_seqs is not None\n",
      "                and self.__rpc_ctrl.step_info.global_step\n",
      "                * self._ft_spec.train_batch_size\n",
      "                >= self.__benchmark_n_seqs\n",
      "            )\n",
      "        ):\n",
      "            # We don't know whether it is the last step of the current epoch,\n",
      "            # so we exit at the first step of the next epoch.\n",
      "            if (\n",
      "                self.__benchmark_steps is not None\n",
      "                or self.__benchmark_n_seqs is not None\n",
      "            ):\n",
      "                logger.info(\n",
      "                    f\"Finished benchmark {self.__benchmark_steps}. \"\n",
      "                    f\"Time consumption of this setup: {time_since_configure:.3f}\"\n",
      "                )\n",
      "                logger.info(f\"avg #e2e# time *{np.mean(self.e2e_time_history):.3f}*\")\n",
      "            # TODO: inform generation workers to exit\n",
      "            return self.experiment_complete_exit()\n",
      "\n",
      "        return worker_base.PollResult(sample_count=1, batch_count=1)\n",
      "\n",
      "    async def _poll_async(self):\n",
      "        name = names.experiment_status(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        name_resolve.add(name, ExpStatus.RUNNING, replace=True)\n",
      "        try:\n",
      "            r = await self.__poll_async()\n",
      "        except Exception as e:\n",
      "            name_resolve.add(name, ExpStatus.ABORTED, replace=True)\n",
      "            raise e\n",
      "        return r\n",
      "\n",
      "    def _log_training_stats(self, e2e_time: float, time_since_configure: float):\n",
      "        # calculate flops\n",
      "        #########################################\n",
      "        if not all(\n",
      "            isinstance(v, ReaLModelConfig) for v in self.__model_configs.values()\n",
      "        ):\n",
      "            logger.warning(\n",
      "                f\"Not all models are ReaLModels. Unable to calculate FLOP/s.\"\n",
      "            )\n",
      "            flops = None\n",
      "            tflops_per_gpu = float(\"inf\")\n",
      "        else:\n",
      "            flops = self.__rpc_ctrl.flops_counter.get_flops()\n",
      "            tflops = flops / (e2e_time * (10**12))\n",
      "            tflops_per_gpu = flops / (e2e_time * self.config.n_model_workers * (10**12))\n",
      "        self.__rpc_ctrl.flops_counter.clear()\n",
      "        #########################################\n",
      "\n",
      "        epoch = self.__rpc_ctrl.step_info.epoch + 1\n",
      "        epoch_step = self.__rpc_ctrl.step_info.epoch_step + 1\n",
      "        global_step = self.__rpc_ctrl.step_info.global_step + 1\n",
      "        s = f\"Epoch {epoch}/{self.config.exp_ctrl.total_train_epochs} \"\n",
      "        s += f\"step {epoch_step}/{self._steps_per_epoch} \"\n",
      "        s += f\"(global step {global_step}) finishes. \"\n",
      "        s += f\"#End to end# execution time: *{e2e_time:.3f}*s. \"\n",
      "        s += f\"Total time consumption: {time_since_configure:.3f}s. \"\n",
      "        logging.log_wandb_tensorboard({\"timeperf/e2e\": e2e_time})\n",
      "        if len(self.e2e_time_history) > 2:\n",
      "            remaining_steps = self._steps_per_epoch - epoch_step\n",
      "            remaining_epochs = self.__total_train_epochs - epoch\n",
      "            avg_t = np.mean(self.e2e_time_history[2:])\n",
      "            remain_t = avg_t * remaining_steps\n",
      "            remain_t += avg_t * self._steps_per_epoch * remaining_epochs\n",
      "            s += f\"Estimated remaining time: {remain_t:.3f}s. \"\n",
      "        if flops is not None:\n",
      "            s += f\"TFLOP/s per GPU: {tflops_per_gpu:.2f}, total TFLOP/s: {tflops:.2f}.\"\n",
      "        self.logger.info(s)\n",
      "        self.logger.info(\n",
      "            f\"Time taken so far across all configurations: {time.perf_counter() - self.global_exp_tik:.2f}s\"\n",
      "        )\n",
      "\n",
      "    def experiment_complete_exit(self):\n",
      "        logger.info(\n",
      "            colorama.Style.RESET_ALL\n",
      "            + colorama.Fore.YELLOW\n",
      "            + colorama.Style.BRIGHT\n",
      "            + \"\\033[1m\"\n",
      "            + \"Experiment Completes! Yeah!!!!!!!!\"\n",
      "            + colorama.Style.RESET_ALL\n",
      "        )\n",
      "\n",
      "        # Update experiment status to inform other workers\n",
      "        name = names.experiment_status(\n",
      "            constants.experiment_name(), constants.trial_name()\n",
      "        )\n",
      "        name_resolve.add(name, ExpStatus.COMPLETE, replace=True)\n",
      "\n",
      "        # Send requests to pause model workers.\n",
      "        # Model workers will not respond to this message.\n",
      "        # FIXME: request to model workers is unnecessary\n",
      "        self.__stream.request(\n",
      "            handlers=list(range(self.config.n_model_workers)),\n",
      "            handle_type=\"reset\",\n",
      "            datas=[None for _ in list(range(self.config.n_model_workers))],\n",
      "        )\n",
      "        self.__stream.close()\n",
      "        constants.reset_run()\n",
      "        # Reset names used for distributed training.\n",
      "        # The next round of training will set up a new distributed environment.\n",
      "        name_resolve.clear_subtree(\n",
      "            names.distributed_root(constants.experiment_name(), constants.trial_name())\n",
      "        )\n",
      "        name_resolve.clear_subtree(\n",
      "            names.request_reply_stream_root(\n",
      "                constants.experiment_name(), constants.trial_name()\n",
      "            )\n",
      "        )\n",
      "\n",
      "        wandb.finish()\n",
      "        if self.__summary_writer is not None:\n",
      "            self.__summary_writer.close()\n",
      "        gc.collect()\n",
      "        self.__initialized = False\n",
      "        self.pause()\n",
      "        return worker_base.PollResult(0, 0)\n",
      "\n",
      "    def __recover_save(self):\n",
      "        # save step info for recover\n",
      "        if os.getenv(\"REAL_SAVE_RECOVER_STATES\", \"0\") != \"1\":\n",
      "            return\n",
      "        # save step info for recover\n",
      "        this_step_info = copy.deepcopy(self.__rpc_ctrl.step_info)\n",
      "        recover_info = recover.RecoverInfo(\n",
      "            recover_start=this_step_info,\n",
      "            last_step_info=self.__last_step_info,\n",
      "            save_ctl_info=self.__save_ctl.state_dict(),\n",
      "            ckpt_ctl_info=self.__ckpt_ctl.state_dict(),\n",
      "            eval_ctl_info=self.__eval_ctl.state_dict(),\n",
      "            data_loading_dp_idx=self.func_executor.data_loading_dp_idx,\n",
      "            hash_vals_to_ignore=self.__rpc_ctrl.used_hash_vals_this_epoch,\n",
      "        )\n",
      "\n",
      "        recover.dump_recover_info(recover_info)\n",
      "        logger.info(\"Dumped recover info to file.\")\n",
      "        logger.info(f\"Will recover from: {recover_info.recover_start}\")\n",
      "        logger.info(\n",
      "            f\"Number of data used in this epoch: {len(recover_info.hash_vals_to_ignore)}\"\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/function_executor.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "import asyncio\n",
      "import random\n",
      "from typing import *\n",
      "\n",
      "import networkx as nx\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "from realhf.api.core.config import ModelShardID\n",
      "from realhf.api.core.data_api import DataBatchMeta, get_shuffle_indices\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.base import constants, logging, name_resolve, names, seeding\n",
      "from realhf.base.topology import ProcessTopology\n",
      "from realhf.system.buffer import AsyncIOSequenceBuffer\n",
      "from realhf.system.model_function_call import ModelFunctionCall, RPCCorountineControl\n",
      "from realhf.system.redistributor import GlobalStorageTracker, RedistribPlanner\n",
      "from realhf.system.request_reply_stream import NameResolvingRequestClient\n",
      "\n",
      "logger = logging.getLogger(__name__, \"system\")\n",
      "blogger = logging.getLogger(\"benchmark\")\n",
      "\n",
      "\n",
      "class FunctionExecutor:\n",
      "    def __init__(\n",
      "        self,\n",
      "        rpcs: List[MFCDef],\n",
      "        msid2mwid: Dict[ModelShardID, int],\n",
      "        stream: NameResolvingRequestClient,\n",
      "        buffers: List[AsyncIOSequenceBuffer],\n",
      "        model_topos: Dict[str, ProcessTopology],\n",
      "        model_configs: Dict[str, None | ReaLModelConfig],\n",
      "        ctrl: RPCCorountineControl,\n",
      "        summary_writer: SummaryWriter | None,\n",
      "        shuffle_dataset: bool,\n",
      "    ):\n",
      "\n",
      "        self.func_calls: Dict[str, ModelFunctionCall] = {}\n",
      "        self.ctrl = ctrl\n",
      "\n",
      "        self.n_model_workers = len(set(msid2mwid.values()))\n",
      "        self.msid2mwid = msid2mwid\n",
      "\n",
      "        self.storage_tracker = GlobalStorageTracker(self.n_model_workers)\n",
      "        self.redistrib_planner = RedistribPlanner(self.storage_tracker)\n",
      "\n",
      "        self.rpcs = rpcs\n",
      "        self.src_rpc = list(filter(lambda rpc: rpc.is_src, rpcs))[0]\n",
      "        self.src_dp_size = model_topos[self.src_rpc.model_name].get_dim(\"data\")\n",
      "\n",
      "        # Create model function calls.\n",
      "        for rpc in self.rpcs:\n",
      "            func_call = ModelFunctionCall(\n",
      "                rpc=rpc,\n",
      "                src_rpc=self.src_rpc,\n",
      "                stream=stream,\n",
      "                msid2mwid=msid2mwid,\n",
      "                model_topos=model_topos,\n",
      "                model_configs=model_configs,\n",
      "                ctrl=ctrl,\n",
      "                buffers=buffers,\n",
      "                redistrib_planner=self.redistrib_planner,\n",
      "                summary_writer=summary_writer,\n",
      "            )\n",
      "            self.func_calls[rpc.name] = func_call\n",
      "\n",
      "        self.stream = stream\n",
      "        self.buffers = buffers\n",
      "        self.buffer_id = 0\n",
      "\n",
      "        self.data_loading_dp_idx = -1\n",
      "        self.shuffle_dataset = shuffle_dataset\n",
      "\n",
      "        # Sort all MFCs in the topological order and\n",
      "        # calculate the width of each level.\n",
      "        # These numbers will determine when to flush MFC requests.\n",
      "        self.topo_widths = []\n",
      "        for generation in nx.topological_generations(rpcs[0]._G):\n",
      "            self.topo_widths.append(len(generation))\n",
      "\n",
      "    def get_leaf_tasks(self) -> List[str]:\n",
      "        dst_rpcs = list(filter(lambda rpc: rpc.is_dst, self.rpcs))\n",
      "        return [rpc.name for rpc in dst_rpcs]\n",
      "\n",
      "    async def flush_calls(self):\n",
      "        for level, w in enumerate(self.topo_widths):\n",
      "            for _ in range(w):\n",
      "                await self.ctrl.topo_level_count.get()\n",
      "            logger.debug(f\"DFG level {level}. Flushing {w} function calls.\")\n",
      "            self.stream.request(\n",
      "                handlers=list(range(self.n_model_workers)), handle_type=\"flush\"\n",
      "            )\n",
      "\n",
      "    async def finish_traverse(self):\n",
      "        for _ in range(len(self.get_leaf_tasks())):\n",
      "            await self.ctrl.train_count.get()\n",
      "        await self.clear_gpu_cache()\n",
      "\n",
      "    async def clear_gpu_cache(self):\n",
      "        async with self.ctrl.lock:\n",
      "            self.ctrl.used_hash_vals_this_epoch += list(self.ctrl.ids_to_clear)\n",
      "            self.stream.request(\n",
      "                handlers=list(range(self.n_model_workers)),\n",
      "                handle_type=\"clear_data_cache\",\n",
      "                datas=[\n",
      "                    self.ctrl.ids_to_clear for _ in list(range(self.n_model_workers))\n",
      "                ],\n",
      "                no_syn=True,\n",
      "            )\n",
      "            # Clear resource tracker as well.\n",
      "            await self.storage_tracker.clear_data(self.ctrl.ids_to_clear)\n",
      "\n",
      "            self.ctrl.ids_to_clear.clear()\n",
      "\n",
      "    async def load_data(self, buffer_id: int):\n",
      "        buffer = self.buffers[buffer_id]\n",
      "        ctrl = self.ctrl\n",
      "\n",
      "        received_ids = set()\n",
      "\n",
      "        load_data_iter = 0\n",
      "\n",
      "        while buffer.size < max(rpc.n_seqs for rpc in self.rpcs):\n",
      "            load_data_iter += 1\n",
      "            resps = await self.stream.call_async(\n",
      "                handlers=[f\"__data{dp_idx}__\" for dp_idx in range(self.src_dp_size)],\n",
      "                handle_type=\"fetch\",\n",
      "                datas=[buffer_id for _ in range(self.src_dp_size)],\n",
      "                verbose=False,\n",
      "            )\n",
      "\n",
      "            all_data = []\n",
      "            all_birth_time = []\n",
      "            data_cnt = []\n",
      "            gpu_id_data = {}\n",
      "            for dp_rank, x in enumerate(resps):\n",
      "                x: DataBatchMeta | None\n",
      "\n",
      "                if x is None:\n",
      "                    data_cnt.append(0)\n",
      "                    continue\n",
      "                if x.meta_sample is None:\n",
      "                    data_cnt.append(0)\n",
      "                    continue\n",
      "\n",
      "                for xx in x.meta_sample.unpack():\n",
      "                    async with ctrl.lock:\n",
      "                        if xx.ids[0] in received_ids:\n",
      "                            raise ValueError(f\"Duplicate data id {xx.ids[0]}.\")\n",
      "                        received_ids.add(xx.ids[0])\n",
      "\n",
      "                gpu_id = self.stream.route_to(f\"__data{dp_rank}__\")\n",
      "                all_data += x.meta_sample.unpack()\n",
      "                all_birth_time += x.birth_times\n",
      "                gpu_id_data[gpu_id] = x.meta_sample.unpack()\n",
      "                data_cnt.append(x.meta_sample.bs)\n",
      "\n",
      "            if self.shuffle_dataset:\n",
      "                # We load data in a round-robin manner across different DP ranks,\n",
      "                # so we also need to shuffle the data to fuse different dataset splits.\n",
      "                shuffle_indices = get_shuffle_indices(\n",
      "                    seeding.get_seed()\n",
      "                    + 47 * self.ctrl.step_info.global_step\n",
      "                    + 97 * load_data_iter,\n",
      "                    len(all_data),\n",
      "                )\n",
      "                all_data = [all_data[i] for i in shuffle_indices]\n",
      "                all_birth_time = [all_birth_time[i] for i in shuffle_indices]\n",
      "\n",
      "            if len(all_data) > 0:\n",
      "                # Update resource tracker for planning data redistribution.\n",
      "                for gpu_id, data in gpu_id_data.items():\n",
      "                    for k in data[0].keys:\n",
      "                        await self.storage_tracker.add_data(\n",
      "                            gpu_id,\n",
      "                            [d.ids[0] for d in data],\n",
      "                            k,\n",
      "                            is_owner=True,\n",
      "                        )\n",
      "\n",
      "                # Store into buffer!\n",
      "                assert len(all_data) == len(all_birth_time)\n",
      "                buffer_indices = await buffer.put_batch(all_data, all_birth_time)\n",
      "                assert len(buffer_indices) == len(all_data)\n",
      "\n",
      "                training_sample_name = names.training_samples(\n",
      "                    constants.experiment_name(), constants.trial_name()\n",
      "                )\n",
      "                try:\n",
      "                    n_samples = int(name_resolve.get(training_sample_name))\n",
      "                except name_resolve.NameEntryNotFoundError:\n",
      "                    n_samples = 0\n",
      "                name_resolve.add(\n",
      "                    training_sample_name, str(n_samples + len(all_data)), replace=True\n",
      "                )\n",
      "\n",
      "                blogger.info(\n",
      "                    f\"Master worker loaded {len(all_data)} pieces of data from all dp ranks: \"\n",
      "                    f\"{data_cnt} from each rank. \"\n",
      "                    f\"Current buffer size: {buffer.size}/{buffer.max_size}. \"\n",
      "                )\n",
      "            else:\n",
      "                await asyncio.sleep(1)\n",
      "\n",
      "    async def execute_step(self):\n",
      "        logger.debug(\"Waiting for the finish of the execution graph.\")\n",
      "        loop = asyncio.get_event_loop()\n",
      "\n",
      "        tasks = [\n",
      "            loop.create_task(fc.run(self.buffer_id)) for fc in self.func_calls.values()\n",
      "        ] + [\n",
      "            loop.create_task(self.flush_calls()),\n",
      "            loop.create_task(self.load_data(self.buffer_id)),\n",
      "            loop.create_task(self.finish_traverse()),\n",
      "        ]\n",
      "\n",
      "        await asyncio.gather(*tasks)\n",
      "        self.buffer_id = (self.buffer_id + 1) % len(self.buffers)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/worker_control.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import pickle\n",
      "import socket\n",
      "import time\n",
      "from typing import Any, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import ray.util.queue as rq\n",
      "import zmq\n",
      "\n",
      "import realhf.system.worker_base as worker_base\n",
      "from realhf.base import logging\n",
      "from realhf.system.worker_base import WorkerServerStatus\n",
      "\n",
      "logger = logging.getLogger(\"worker-control\")\n",
      "WORKER_WAIT_FOR_CONTROLLER_SECONDS = 3600\n",
      "WORKER_JOB_STATUS_LINGER_SECONDS = 60\n",
      "\n",
      "\n",
      "class ZmqTaskQueue(worker_base.WorkerServerTaskQueue):\n",
      "\n",
      "    def __init__(self, port=0):\n",
      "        self.__context = zmq.Context()\n",
      "        self.__socket = self.__context.socket(zmq.REP)\n",
      "        host_ip = socket.gethostbyname(socket.gethostname())\n",
      "        if port == 0:\n",
      "            self.__port = self.__socket.bind_to_random_port(f\"tcp://{host_ip}\")\n",
      "        else:\n",
      "            self.__socket.bind(f\"tcp://{host_ip}:{port}\")\n",
      "            self.__port = port\n",
      "\n",
      "    def __del__(self):\n",
      "        self.__socket.close()\n",
      "\n",
      "    @property\n",
      "    def port(self):\n",
      "        return self.__port\n",
      "\n",
      "    def try_get_request(self) -> Tuple[str, Dict[str, Any]]:\n",
      "        try:\n",
      "            data = self.__socket.recv(zmq.NOBLOCK)\n",
      "        except zmq.ZMQError:\n",
      "            # Currently no request in the queue.\n",
      "            raise worker_base.NoRequstForWorker()\n",
      "        return pickle.loads(data)\n",
      "\n",
      "    def respond(self, response):\n",
      "        self.__socket.send(pickle.dumps(response))\n",
      "\n",
      "\n",
      "class RayTaskQueue(worker_base.WorkerServerTaskQueue):\n",
      "\n",
      "    def __init__(self, comm: Tuple[rq.Queue, rq.Queue]):\n",
      "        recv_queue, send_queue = comm\n",
      "        self.__recv_queue = recv_queue\n",
      "        self.__send_queue = send_queue\n",
      "\n",
      "    def try_get_request(self) -> Tuple[str, Dict[str, Any]]:\n",
      "        try:\n",
      "            command, kwargs = self.__recv_queue.get_nowait()\n",
      "        except rq.Empty:\n",
      "            # Currently no request in the queue.\n",
      "            raise worker_base.NoRequstForWorker()\n",
      "        return command, kwargs\n",
      "\n",
      "    def respond(self, response):\n",
      "        self.__send_queue.put(response)\n",
      "\n",
      "\n",
      "class ZmqRequester(worker_base.WorkerControlPanelRequester):\n",
      "\n",
      "    class ZmqFuture(worker_base.WorkerControlPanelRequester.Future):\n",
      "        # Every ZmqFuture connect one socket, close after returning results.\n",
      "        def __init__(\n",
      "            self,\n",
      "            payload,\n",
      "            context: zmq.Context,\n",
      "            address,\n",
      "            worker_name,\n",
      "            wait_response=True,\n",
      "        ):\n",
      "            self.__worker_name = worker_name\n",
      "            self.__socket = context.socket(zmq.REQ)\n",
      "            self.__socket.setsockopt(zmq.LINGER, 0)\n",
      "            self.__socket.connect(f\"tcp://{address}\")\n",
      "            self.__socket.send(payload, flags=zmq.NOBLOCK)\n",
      "            if not wait_response:\n",
      "                self.__socket.close()\n",
      "\n",
      "        def result(self, timeout=None):\n",
      "            if timeout is not None:\n",
      "                self.__socket.RCVTIMEO = int(timeout * 1000)\n",
      "            else:\n",
      "                self.__socket.RCVTIMEO = int(1e9)\n",
      "            try:\n",
      "                r = pickle.loads(self.__socket.recv())\n",
      "            except zmq.error.Again as e:\n",
      "                raise TimeoutError(f\"Waiting for RPC server response timeout: {e}\")\n",
      "            if isinstance(r, Exception):\n",
      "                logger.error(f\"Error configuring worker {self.__worker_name}\")\n",
      "                raise r\n",
      "            self.__socket.close()\n",
      "            return r\n",
      "\n",
      "    def __init__(self):\n",
      "        self.__context = zmq.Context()\n",
      "        self.__context.set(zmq.MAX_SOCKETS, 20480)\n",
      "\n",
      "    def async_request(\n",
      "        self, worker_name, address, command, wait_response=True, **kwargs\n",
      "    ):\n",
      "        payload = pickle.dumps((command, kwargs))\n",
      "        r = self.ZmqFuture(\n",
      "            payload,\n",
      "            self.__context,\n",
      "            address,\n",
      "            worker_name,\n",
      "            wait_response=wait_response,\n",
      "        )\n",
      "        return r\n",
      "\n",
      "\n",
      "class RayRequester(worker_base.WorkerControlPanelRequester):\n",
      "\n",
      "    class RayQueueFuture(worker_base.WorkerControlPanelRequester.Future):\n",
      "\n",
      "        def __init__(self, worker_name: str, queue: rq.Queue):\n",
      "            self.__queue = queue\n",
      "            self.__worker_name = worker_name\n",
      "\n",
      "        def result(self, timeout=None):\n",
      "            try:\n",
      "                return self.__queue.get(timeout=timeout)\n",
      "            except rq.Empty:\n",
      "                raise TimeoutError(\n",
      "                    f\"Waiting for Ray worker {self.__worker_name} response timeout.\"\n",
      "                )\n",
      "            except Exception as e:\n",
      "                raise RuntimeError(\n",
      "                    f\"Error waiting for Ray queue future {self.__worker_name}.\"\n",
      "                ) from e\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        request_comms: Dict[str, rq.Queue],\n",
      "        reply_comms: Dict[str, rq.Queue],\n",
      "    ):\n",
      "        self.__request_comms: Dict[str, rq.Queue] = request_comms\n",
      "        self.__reply_comms: Dict[str, rq.Queue] = reply_comms\n",
      "\n",
      "    def async_request(self, worker_name, _, command, __, **kwargs):\n",
      "        request_queue = self.__request_comms[worker_name]\n",
      "        request_queue.put((command, kwargs))\n",
      "        reply_queue = self.__reply_comms[worker_name]\n",
      "        return self.RayQueueFuture(worker_name, reply_queue)\n",
      "\n",
      "\n",
      "def make_server(type_, worker_name, experiment_name, trial_name, **kwargs):\n",
      "    if type_ == \"zmq\":\n",
      "        q = ZmqTaskQueue(**kwargs)\n",
      "    elif type_ == \"ray\":\n",
      "        q = RayTaskQueue(**kwargs)\n",
      "    else:\n",
      "        raise NotImplementedError(type_)\n",
      "    return worker_base.WorkerServer(worker_name, experiment_name, trial_name, q)\n",
      "\n",
      "\n",
      "def make_control(type_, experiment_name, trial_name, **kwargs):\n",
      "    if type_ == \"zmq\":\n",
      "        requester = ZmqRequester(**kwargs)\n",
      "    elif type_ == \"ray\":\n",
      "        requester = RayRequester(**kwargs)\n",
      "    else:\n",
      "        raise NotImplementedError(type_)\n",
      "    return worker_base.WorkerControlPanel(experiment_name, trial_name, requester)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/partial_rollout.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import time\n",
      "from asyncio.queues import QueueEmpty\n",
      "from collections import defaultdict\n",
      "from dataclasses import asdict\n",
      "from typing import Dict, Hashable, List\n",
      "\n",
      "import aiohttp\n",
      "from aiohttp.client import ClientTimeout\n",
      "from transformers import PreTrainedTokenizerFast\n",
      "\n",
      "from realhf.api.cli_args import GenerationHyperparameters\n",
      "from realhf.api.core.model_api import (\n",
      "    APIGenerateInput,\n",
      "    APIGenerateOutput,\n",
      "    BundledGenerationOutputs,\n",
      "    GenReqMeta,\n",
      ")\n",
      "from realhf.base import constants, logging, name_resolve, names\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "GENERATION_POLL_WAIT_TIME = 0.05\n",
      "\n",
      "\n",
      "class PartialRolloutManager:\n",
      "    \"\"\"Manages the partial rollout for a client.\n",
      "\n",
      "    It will submit generation requests in chunks, i.e.,\n",
      "    generating at most `new_tokens_per_chunk` tokens each time.\n",
      "    In this way, we can reduce the overhead of flushing all requests\n",
      "    upon model weights update.\n",
      "\n",
      "    This is a hack usage. We don't need it if the server can pause\n",
      "    requests, update weights, and recompute kv caches at any time.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        worker_index: int,\n",
      "        request_queue: asyncio.Queue,\n",
      "        reply_queue: asyncio.Queue,\n",
      "        new_tokens_per_chunk: int,\n",
      "        tokenizer: PreTrainedTokenizerFast,\n",
      "        timeout: int,\n",
      "    ):\n",
      "        self.worker_index = worker_index\n",
      "\n",
      "        # qid -> {group_idx -> aiohttp Task}\n",
      "        self.gen_requests: Dict[Hashable, Dict[int, asyncio.Task]]\n",
      "        self.gen_requests = defaultdict(dict)\n",
      "\n",
      "        # NOTE: Grouped generations are managed separately. Store early returned\n",
      "        # answers in this cache and pop the result when the whole group is done.\n",
      "        self.gen_cache: Dict[Hashable, Dict[int, APIGenerateOutput]]\n",
      "        self.gen_cache = defaultdict(dict)\n",
      "\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "        self.request_queue = request_queue\n",
      "        self.reply_queue = reply_queue\n",
      "\n",
      "        self.new_tokens_per_chunk = new_tokens_per_chunk\n",
      "\n",
      "        self.gserver_manager_addr = None\n",
      "        self.timeout = timeout\n",
      "\n",
      "    async def _schedule_request(self, req_meta: GenReqMeta):\n",
      "        if self.gserver_manager_addr is None:\n",
      "            # Get the address of gserver manager to schedule requests\n",
      "            name = names.gen_server_manager(\n",
      "                constants.experiment_name(), constants.trial_name()\n",
      "            )\n",
      "            self.gserver_manager_addr = name_resolve.wait(name, timeout=300)\n",
      "            time.sleep(1)  # Wait for the server to start\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            async with session.post(\n",
      "                f\"http://{self.gserver_manager_addr}/schedule_request\",\n",
      "                json=asdict(req_meta),\n",
      "                timeout=ClientTimeout(total=self.timeout, sock_connect=self.timeout),\n",
      "            ) as response:\n",
      "                response.raise_for_status()\n",
      "                res = await response.json()\n",
      "                return res\n",
      "\n",
      "    def get_num_gen_requests(self):\n",
      "        return len(self.gen_requests)\n",
      "\n",
      "    async def _run_gen(\n",
      "        self,\n",
      "        url,\n",
      "        qid,\n",
      "        group_idx,\n",
      "        prompt_ids,\n",
      "        input_ids,\n",
      "        prev_logprobs,\n",
      "        version_start,\n",
      "        cur_server_version,\n",
      "        raw_gconfig,\n",
      "    ):\n",
      "        from realhf.impl.model.backend.sglang import SGLangAPIClient\n",
      "\n",
      "        max_new_tokens = min(raw_gconfig.max_new_tokens, self.new_tokens_per_chunk)\n",
      "        max_new_tokens = min(\n",
      "            max_new_tokens,\n",
      "            raw_gconfig.max_new_tokens - len(input_ids) + len(prompt_ids),\n",
      "        )\n",
      "        gconfig = raw_gconfig.new(\n",
      "            n=1,\n",
      "            max_new_tokens=max_new_tokens,\n",
      "        )\n",
      "        assert self.tokenizer.pad_token_id is not None\n",
      "        assert self.tokenizer.eos_token_id is not None\n",
      "        # Don't need to request updating weights\n",
      "        async with SGLangAPIClient(\n",
      "            generate_url=f\"{url}/generate\", update_weights_url=\"\"\n",
      "        ) as api_client:\n",
      "            res = await api_client.async_add_generate_request(\n",
      "                APIGenerateInput(\n",
      "                    qid=qid,\n",
      "                    prompt_ids=prompt_ids,\n",
      "                    input_ids=input_ids,\n",
      "                    gconfig=gconfig,\n",
      "                    stop_token_ids=[\n",
      "                        self.tokenizer.pad_token_id,\n",
      "                        self.tokenizer.eos_token_id,\n",
      "                    ],\n",
      "                    return_logprob=True,\n",
      "                    version_start=version_start,\n",
      "                    prev_logprobs=prev_logprobs,\n",
      "                    metadata=dict(\n",
      "                        group_idx=group_idx,\n",
      "                        raw_gconfig=raw_gconfig,\n",
      "                        server_url=url,\n",
      "                        version=cur_server_version,\n",
      "                    ),\n",
      "                ),\n",
      "                stream=False,\n",
      "            )\n",
      "            res.version_end = [cur_server_version for _ in range(res.group_size)]\n",
      "            return res\n",
      "\n",
      "    async def _issue_generation(\n",
      "        self,\n",
      "        url: str,\n",
      "        qid: Hashable,\n",
      "        group_idx: int,\n",
      "        prompt_ids: List[int],\n",
      "        input_ids: List[int],\n",
      "        prev_logprobs: List[float],\n",
      "        version_start: int,\n",
      "        raw_gconfig: GenerationHyperparameters,\n",
      "        cur_server_version: int,\n",
      "    ):\n",
      "        \"\"\"Issue a generation request.\n",
      "\n",
      "        `input_ids` can be a partial prefix and longer than `prompt_ids`.\n",
      "        If model weights are updated, the KV cache will be refreshed,\n",
      "        otherwise the server will reuse the radix cache with no additional overhead.\n",
      "        \"\"\"\n",
      "\n",
      "        task = asyncio.create_task(\n",
      "            self._run_gen(\n",
      "                url,\n",
      "                qid,\n",
      "                group_idx,\n",
      "                prompt_ids,\n",
      "                input_ids,\n",
      "                prev_logprobs,\n",
      "                version_start=version_start,\n",
      "                cur_server_version=cur_server_version,\n",
      "                raw_gconfig=raw_gconfig,\n",
      "            )\n",
      "        )\n",
      "        self.gen_requests[qid][group_idx] = task\n",
      "        await asyncio.sleep(0)\n",
      "\n",
      "    async def refresh_generation(self):\n",
      "        tasks = []\n",
      "        for group_requests in self.gen_requests.values():\n",
      "            tasks += list(group_requests.values())\n",
      "\n",
      "        done = []\n",
      "        if tasks:\n",
      "            # No new checkpoint available, try to wait for the next complete sequence\n",
      "            done, _ = await asyncio.wait(\n",
      "                tasks,\n",
      "                timeout=GENERATION_POLL_WAIT_TIME,\n",
      "                return_when=asyncio.FIRST_COMPLETED,\n",
      "            )\n",
      "\n",
      "        for task in done:\n",
      "            s: APIGenerateOutput = await task\n",
      "            group_idx = s.metadata[\"group_idx\"]\n",
      "            raw_gconfig = s.metadata[\"raw_gconfig\"]\n",
      "            previous_version = s.metadata[\"version\"]\n",
      "\n",
      "            assert s.group_size == 1\n",
      "            no_eos = s.no_eos[0]\n",
      "            gen_len = s.gen_lens[0]\n",
      "\n",
      "            self.gen_requests[s.qid].pop(group_idx)\n",
      "            if len(self.gen_requests[s.qid]) == 0:\n",
      "                self.gen_requests.pop(s.qid)\n",
      "\n",
      "            if no_eos and gen_len < raw_gconfig.max_new_tokens:\n",
      "                # Unfinished request due to chunked generation.\n",
      "                # Send it back to continue.\n",
      "                req_meta = GenReqMeta(\n",
      "                    qid=s.qid,\n",
      "                    prompt_len=s.prompt_len,\n",
      "                    group_size=raw_gconfig.n,\n",
      "                    new_token_budget=raw_gconfig.max_new_tokens,\n",
      "                    predicted_new_tokens=None,\n",
      "                    previous_server_url=s.metadata[\"server_url\"],\n",
      "                    previous_version=previous_version,\n",
      "                )\n",
      "                info = await self._schedule_request(req_meta)\n",
      "                cur_version = info[\"version\"]\n",
      "                server_url = info[\"url\"]\n",
      "\n",
      "                if len(s.output_logprobs) > 0:\n",
      "                    prev_logprobs = s.prev_logprobs + s.output_logprobs[0]\n",
      "                else:\n",
      "                    prev_logprobs = s.prev_logprobs\n",
      "                    if prev_logprobs is None:\n",
      "                        prev_logprobs = []\n",
      "                await self._issue_generation(\n",
      "                    server_url,\n",
      "                    s.qid,\n",
      "                    group_idx,\n",
      "                    s.prompt_ids,\n",
      "                    s.input_ids + s.output_ids[0],\n",
      "                    version_start=s.version_start,\n",
      "                    prev_logprobs=prev_logprobs,\n",
      "                    raw_gconfig=raw_gconfig,\n",
      "                    cur_server_version=cur_version,\n",
      "                )\n",
      "            else:\n",
      "                # Generation finishes. Save to cache for later fetching.\n",
      "                self.gen_cache[s.qid][group_idx] = s\n",
      "                if len(self.gen_cache[s.qid]) >= raw_gconfig.n:\n",
      "                    gen_results = self.gen_cache.pop(s.qid)\n",
      "                    output = BundledGenerationOutputs.from_api_outputs(\n",
      "                        list(gen_results.values())\n",
      "                    )\n",
      "                    self.reply_queue.put_nowait(output)\n",
      "\n",
      "    async def poll_fresh_requests_task(self):\n",
      "        for _ in range(8):\n",
      "            try:\n",
      "                qid, prompt_token_ids, gconfig = self.request_queue.get_nowait()\n",
      "                req_meta = GenReqMeta(\n",
      "                    qid=qid,\n",
      "                    prompt_len=len(prompt_token_ids),\n",
      "                    group_size=gconfig.n,\n",
      "                    new_token_budget=gconfig.max_new_tokens,\n",
      "                    predicted_new_tokens=None,\n",
      "                )\n",
      "                dst_server_info = await self._schedule_request(req_meta)\n",
      "\n",
      "                for group_idx in range(gconfig.n):\n",
      "                    await self._issue_generation(\n",
      "                        dst_server_info[\"url\"],\n",
      "                        qid,\n",
      "                        group_idx,\n",
      "                        prompt_token_ids,\n",
      "                        prompt_token_ids,\n",
      "                        version_start=dst_server_info[\"version\"],\n",
      "                        prev_logprobs=[],\n",
      "                        raw_gconfig=gconfig,\n",
      "                        cur_server_version=dst_server_info[\"version\"],\n",
      "                    )\n",
      "            except QueueEmpty:\n",
      "                break\n",
      "\n",
      "    async def poll_old_requests_task(self):\n",
      "        for _ in range(8):\n",
      "            await self.refresh_generation()\n",
      "\n",
      "    async def run_step(self):\n",
      "\n",
      "        await asyncio.gather(\n",
      "            self.poll_fresh_requests_task(),\n",
      "            self.poll_old_requests_task(),\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import importlib\n",
      "import os\n",
      "import traceback\n",
      "from typing import Type\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "\n",
      "logger = logging.getLogger(\"system\")\n",
      "\n",
      "# NOTE: Workers are configured in the following order.\n",
      "# Take special care when adding a new worker type.\n",
      "WORKER_TYPES = [\n",
      "    \"generation_server\",\n",
      "    \"gserver_manager\",\n",
      "    \"model_worker\",\n",
      "    \"master_worker\",\n",
      "    \"rollout_worker\",\n",
      "]\n",
      "\n",
      "\n",
      "def load_worker(worker_type: str) -> Type:\n",
      "    assert worker_type in WORKER_TYPES, f\"Invalid worker type {worker_type}\"\n",
      "    module = importlib.import_module(worker_type_to_module(worker_type))\n",
      "    class_name = worker_type_to_class_name(worker_type)\n",
      "    return getattr(module, class_name)\n",
      "\n",
      "\n",
      "def worker_type_to_module(worker_type: str):\n",
      "    return \"realhf.system.\" + worker_type\n",
      "\n",
      "\n",
      "def worker_type_to_class_name(worker_type: str):\n",
      "    return \"\".join([w.capitalize() for w in worker_type.split(\"_\")])\n",
      "\n",
      "\n",
      "def run_worker(\n",
      "    worker_type, experiment_name, trial_name, worker_name, worker_server_type\n",
      "):\n",
      "    \"\"\"Run one worker\n",
      "    Args:\n",
      "        worker_type: string, one of the worker types listed above,\n",
      "        experiment_name: string, the experiment this worker belongs to,\n",
      "        trial_name: string, the specific trial this worker belongs to,\n",
      "        worker_name: name given to the worker, typically \"<worker_type>/<worker_index>\"\n",
      "        worker_server_type: string, either 'zmq' or 'ray'.\n",
      "    \"\"\"\n",
      "    worker_class = load_worker(worker_type)\n",
      "    make_server_fn = getattr(\n",
      "        importlib.import_module(\"realhf.system.worker_control\"), \"make_server\"\n",
      "    )\n",
      "    server = make_server_fn(\n",
      "        type_=worker_server_type,\n",
      "        experiment_name=experiment_name,\n",
      "        trial_name=trial_name,\n",
      "        worker_name=worker_name,\n",
      "    )\n",
      "    worker = worker_class(server=server)\n",
      "    try:\n",
      "        if worker_type in [\"rollout_worker\", \"master_worker\", \"gserver_manager\"]:\n",
      "            asyncio.run(worker.run_async())\n",
      "        else:\n",
      "            worker.run()\n",
      "    except Exception as e:\n",
      "        logger.error(\"Worker %s failed with exception: %s\", worker_name, e)\n",
      "        logger.error(traceback.format_exc())\n",
      "        raise e\n",
      "\n",
      "\n",
      "def make_controller(type_, experiment_name, trial_name):\n",
      "    module = importlib.import_module(\"realhf.system.controller\")\n",
      "    if type_ == \"zmq\":\n",
      "        control_module = importlib.import_module(\"realhf.system.worker_control\")\n",
      "        panel = getattr(control_module, \"make_control\")(\n",
      "            \"zmq\", experiment_name, trial_name\n",
      "        )\n",
      "        return getattr(module, \"Controller\")(experiment_name, trial_name, panel)\n",
      "    elif type_ == \"ray\":\n",
      "        return getattr(module, \"RayController\")(experiment_name, trial_name)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Unknown controller type {type_}.\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/flops_counter.py ====\n",
      "\n",
      "import dataclasses\n",
      "from typing import *\n",
      "\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "from realhf.api.core.dfg import MFCDef, ModelInterfaceType\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.base.monitor import (\n",
      "    caculuate_llama_forward_flops,\n",
      "    calculate_llama_gen_flops,\n",
      "    calculate_llama_train_flops,\n",
      ")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class FlopsCounter:\n",
      "    train_configs: List[ReaLModelConfig] = dataclasses.field(default_factory=list)\n",
      "    train_bs: List[int] = dataclasses.field(default_factory=list)\n",
      "    train_seqlens: List[List[int]] = dataclasses.field(default_factory=list)\n",
      "\n",
      "    inf_configs: List[ReaLModelConfig] = dataclasses.field(default_factory=list)\n",
      "    inf_bs: List[int] = dataclasses.field(default_factory=list)\n",
      "    inf_seqlens: List[List[int]] = dataclasses.field(default_factory=list)\n",
      "\n",
      "    gen_configs: List[ReaLModelConfig] = dataclasses.field(default_factory=list)\n",
      "    gen_bs: List[int] = dataclasses.field(default_factory=list)\n",
      "    prompt_lens: List[List[int]] = dataclasses.field(default_factory=list)\n",
      "    gen_len: List[int] = dataclasses.field(default_factory=list)\n",
      "\n",
      "    def clear(self):\n",
      "        self.train_bs.clear()\n",
      "        self.train_seqlens.clear()\n",
      "\n",
      "        self.inf_bs.clear()\n",
      "        self.inf_seqlens.clear()\n",
      "\n",
      "        self.gen_bs.clear()\n",
      "        self.prompt_lens.clear()\n",
      "        self.gen_len.clear()\n",
      "\n",
      "        self.train_configs.clear()\n",
      "        self.inf_configs.clear()\n",
      "        self.gen_configs.clear()\n",
      "\n",
      "    def add_rpc(\n",
      "        self, rpc: MFCDef, sample: SequenceSample, model_config: ReaLModelConfig\n",
      "    ):\n",
      "        # Record the data amount for each interface to compute FLOPs.\n",
      "        # Since the user may arbitrarily specify input/output keys,\n",
      "        # we can only try to find the most probable key name for computing FLOPs.\n",
      "        # If such keys do not exist, we will use the key with the longest\n",
      "        # sequence length in this model function call.\n",
      "        acc_seqlens = {\n",
      "            k: sum(sum(x) for x in slens) for k, slens in sample.seqlens.items()\n",
      "        }\n",
      "        seqlen_key = max(sample.seqlens, key=acc_seqlens.get)\n",
      "        flops_seqlens = [sum(x) for x in sample.seqlens[seqlen_key]]\n",
      "        if rpc.interface_type == ModelInterfaceType.GENERATE:\n",
      "            self.gen_configs.append(model_config)\n",
      "            self.gen_bs.append(sample.bs)\n",
      "            self.gen_len.append(\n",
      "                rpc.interface_impl.args[\"generation_config\"][\"min_new_tokens\"]\n",
      "            )\n",
      "            self.prompt_lens.append(flops_seqlens)\n",
      "        elif rpc.interface_type == ModelInterfaceType.TRAIN_STEP:\n",
      "            self.train_configs.append(model_config)\n",
      "            self.train_bs.append(sample.bs)\n",
      "            self.train_seqlens.append(flops_seqlens)\n",
      "        elif rpc.interface_type == ModelInterfaceType.INFERENCE:\n",
      "            self.inf_configs.append(model_config)\n",
      "            self.inf_bs.append(sample.bs)\n",
      "            self.inf_seqlens.append(flops_seqlens)\n",
      "\n",
      "    def get_flops(self) -> int:\n",
      "        flops = 0\n",
      "        for train_bs, train_seqlens, real_config in zip(\n",
      "            self.train_bs,\n",
      "            self.train_seqlens,\n",
      "            self.train_configs,\n",
      "        ):\n",
      "            flops += calculate_llama_train_flops(\n",
      "                checkpoint_activations_factor=4,\n",
      "                batch_size=train_bs,\n",
      "                seqlens=train_seqlens,\n",
      "                num_layers=real_config.n_layers,\n",
      "                hidden_size=real_config.hidden_dim,\n",
      "                intermediate_size=real_config.intermediate_dim,\n",
      "                vocab_size=real_config.vocab_size,\n",
      "            )\n",
      "        for inf_bs, inf_seqlens, real_config in zip(\n",
      "            self.inf_bs,\n",
      "            self.inf_seqlens,\n",
      "            self.inf_configs,\n",
      "        ):\n",
      "            flops += caculuate_llama_forward_flops(\n",
      "                batch_size=inf_bs,\n",
      "                seqlens=inf_seqlens,\n",
      "                num_layers=real_config.n_layers,\n",
      "                hidden_size=real_config.hidden_dim,\n",
      "                intermediate_size=real_config.intermediate_dim,\n",
      "                vocab_size=real_config.vocab_size,\n",
      "            )\n",
      "        for gen_bs, prompt_lens, gen_len, real_config in zip(\n",
      "            self.gen_bs,\n",
      "            self.prompt_lens,\n",
      "            self.gen_len,\n",
      "            self.gen_configs,\n",
      "        ):\n",
      "            flops += calculate_llama_gen_flops(\n",
      "                batch_size=gen_bs,\n",
      "                prompt_lens=prompt_lens,\n",
      "                gen_len=gen_len,\n",
      "                num_layers=real_config.n_layers,\n",
      "                hidden_size=real_config.hidden_dim,\n",
      "                intermediate_size=real_config.intermediate_dim,\n",
      "                vocab_size=real_config.vocab_size,\n",
      "            )\n",
      "        return flops\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/system/worker_base.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import enum\n",
      "import os\n",
      "import queue\n",
      "import re\n",
      "import socket\n",
      "import threading\n",
      "import time\n",
      "from typing import Any, Dict, List, Optional, Tuple\n",
      "\n",
      "import realhf.api.core.system_api as system_api\n",
      "from realhf.base import logging, name_resolve, names\n",
      "from realhf.base.gpu_utils import set_cuda_device\n",
      "\n",
      "logger = logging.getLogger(\"worker\")\n",
      "\n",
      "_MAX_SOCKET_CONCURRENCY = 1000\n",
      "WORKER_WAIT_FOR_CONTROLLER_SECONDS = 3600\n",
      "\n",
      "\n",
      "class WorkerException(Exception):\n",
      "\n",
      "    def __init__(self, worker_name, worker_status, scenario):\n",
      "        super(WorkerException, self).__init__(\n",
      "            f\"Worker {worker_name} is {worker_status} while {scenario}\"\n",
      "        )\n",
      "        self.worker_name = worker_name\n",
      "        self.worker_status = worker_status\n",
      "        self.scenario = scenario\n",
      "\n",
      "\n",
      "class WorkerServerStatus(str, enum.Enum):\n",
      "    \"\"\"List of all possible Server status.\n",
      "\n",
      "    This is typically set by workers hosting the server, and read by the\n",
      "    controller.\n",
      "    \"\"\"\n",
      "\n",
      "    READY = \"READY\"\n",
      "    RUNNING = \"RUNNING\"\n",
      "    PAUSED = \"PAUSED\"\n",
      "    COMPLETED = \"COMPLETED\"\n",
      "\n",
      "    UNKNOWN = \"UNKNOWN\"  # CANNOT be set.\n",
      "    INTERRUPTED = \"INTERRUPTED\"\n",
      "    ERROR = \"ERROR\"\n",
      "    LOST = \"LOST\"  # CANNOT be set\n",
      "\n",
      "\n",
      "class NoRequstForWorker(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "class WorkerServerTaskQueue:\n",
      "\n",
      "    def try_get_request(self) -> Tuple[str, Dict[str, Any]]:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def respond(self, response):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def port(self) -> int:\n",
      "        return -1\n",
      "\n",
      "\n",
      "class WorkerServer:\n",
      "    \"\"\"A light-weight implementation of an RPC server.\n",
      "\n",
      "    Note that this server only allows a single client connection for now, as that is sufficient for\n",
      "    workers which respond to the controller only.\n",
      "\n",
      "    Example:\n",
      "        # Server side.\n",
      "        server = RpcServer(port)\n",
      "        server.register_handler('foo', foo)\n",
      "        while True:\n",
      "            server.handle_requests()\n",
      "\n",
      "        # Client side.\n",
      "        client = RpcClient(host, port)\n",
      "        client.request('foo', x=42, y='str') # foo(x=42, y='str') will be called on the server side.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        worker_name,\n",
      "        experiment_name,\n",
      "        trial_name,\n",
      "        task_queue: WorkerServerTaskQueue,\n",
      "    ):\n",
      "        \"\"\"Specifies the name of the worker that WorkerControlPanel can used to\n",
      "        find and manage.\n",
      "\n",
      "        Args:\n",
      "            worker_name: Typically \"<worker_type>/<worker_index>\".\n",
      "        \"\"\"\n",
      "        self.__worker_name = worker_name\n",
      "        self.__experiment_name = experiment_name\n",
      "        self.__trial_name = trial_name\n",
      "\n",
      "        self.__task_queue = task_queue\n",
      "\n",
      "        self.__handlers = {}\n",
      "        host_ip = socket.gethostbyname(socket.gethostname())\n",
      "\n",
      "        try:\n",
      "            controller_status = name_resolve.wait(\n",
      "                names.worker_status(experiment_name, trial_name, \"ctl\"),\n",
      "                timeout=WORKER_WAIT_FOR_CONTROLLER_SECONDS,\n",
      "            )\n",
      "        except TimeoutError:\n",
      "            raise TimeoutError(\n",
      "                f\"Worker ({experiment_name, trial_name, worker_name}) connect to controller timeout from host {socket.gethostname()}.\"\n",
      "            )\n",
      "\n",
      "        if controller_status != \"READY\":\n",
      "            raise RuntimeError(\n",
      "                f\"Abnormal controller state on experiment launch {controller_status}.\"\n",
      "            )\n",
      "\n",
      "        if experiment_name is not None and trial_name is not None:\n",
      "            key = names.worker(experiment_name, trial_name, worker_name)\n",
      "            address = f\"{host_ip}:{self.__task_queue.port}\"\n",
      "            name_resolve.add(key, address, keepalive_ttl=1200, delete_on_exit=True)\n",
      "            logger.debug(\n",
      "                \"Added name_resolve entry %s for worker server at %s\",\n",
      "                key,\n",
      "                address,\n",
      "            )\n",
      "\n",
      "    def register_handler(self, command, fn):\n",
      "        \"\"\"Registers an RPC command.\n",
      "\n",
      "        The handler `fn` shall be called when `self.handle_requests()` sees an\n",
      "        incoming command of the registered type.\n",
      "        \"\"\"\n",
      "        if command in self.__handlers:\n",
      "            raise KeyError(f\"Command '{command}' exists\")\n",
      "        self.__handlers[command] = fn\n",
      "\n",
      "    def handle_requests(self, max_count=None):\n",
      "        \"\"\"Handles queued requests in order, optionally limited by `max_count`.\n",
      "\n",
      "        Returns:\n",
      "            The count of requests handled.\n",
      "        \"\"\"\n",
      "        count = 0\n",
      "        while max_count is None or count < max_count:\n",
      "            try:\n",
      "                command, kwargs = self.__task_queue.try_get_request()\n",
      "            except NoRequstForWorker:\n",
      "                # Currently no request in the queue.\n",
      "                break\n",
      "            logger.debug(\"Handle request %s with kwargs %s\", command, kwargs)\n",
      "            if command in self.__handlers:\n",
      "                try:\n",
      "                    response = self.__handlers[command](**kwargs)\n",
      "                    logger.debug(\"Handle request: %s, ok\", command)\n",
      "                except WorkerException:\n",
      "                    raise\n",
      "                except Exception as e:\n",
      "                    logger.error(\"Handle request: %s, error\", command)\n",
      "                    logger.error(e, exc_info=True)\n",
      "                    response = e\n",
      "            else:\n",
      "                logger.error(\"Handle request: %s, no such command\", command)\n",
      "                response = KeyError(f\"No such command: {command}\")\n",
      "            self.__task_queue.respond(response)\n",
      "            logger.debug(\"Handle request: %s, sent reply\", command)\n",
      "            count += 1\n",
      "        return count\n",
      "\n",
      "    def set_status(self, status: WorkerServerStatus):\n",
      "        \"\"\"On graceful exit, worker status is cleared.\"\"\"\n",
      "        name_resolve.add(\n",
      "            names.worker_status(\n",
      "                experiment_name=self.__experiment_name,\n",
      "                trial_name=self.__trial_name,\n",
      "                worker_name=self.__worker_name,\n",
      "            ),\n",
      "            value=status.value,\n",
      "            replace=True,\n",
      "            delete_on_exit=False,\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def experiment_name(self):\n",
      "        return self.__experiment_name\n",
      "\n",
      "    @property\n",
      "    def trial_name(self):\n",
      "        return self.__trial_name\n",
      "\n",
      "\n",
      "class WorkerControlPanelRequester:\n",
      "\n",
      "    class Future:\n",
      "\n",
      "        def result(self, timeout=None):\n",
      "            raise NotImplementedError()\n",
      "\n",
      "    def async_request(\n",
      "        self,\n",
      "        worker_name: str,\n",
      "        address: str,\n",
      "        command: str,\n",
      "        wait_for_response: bool = True,\n",
      "        **kwargs,\n",
      "    ) -> Future:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "class WorkerControlPanel:\n",
      "    \"\"\"A class that defines the management utilities to all the workers of an\n",
      "    experiment trial.\"\"\"\n",
      "\n",
      "    @dataclasses.dataclass\n",
      "    class Response:\n",
      "        worker_name: str\n",
      "        result: WorkerControlPanelRequester.Future\n",
      "        timed_out: bool = False\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        experiment_name,\n",
      "        trial_name,\n",
      "        requester: WorkerControlPanelRequester,\n",
      "    ):\n",
      "        self.__closed = False\n",
      "\n",
      "        self.__experiment_name = experiment_name\n",
      "        self.__trial_name = trial_name\n",
      "        self.__worker_addresses = {}\n",
      "\n",
      "        self.__requester = requester\n",
      "\n",
      "        self.__logger = logging.getLogger(\"worker control panel\")\n",
      "\n",
      "    def __del__(self):\n",
      "        if not self.__closed:\n",
      "            self.close()\n",
      "            self.__closed = True\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "        if not self.__closed:\n",
      "            self.close()\n",
      "            self.__closed = True\n",
      "\n",
      "    def close(self):\n",
      "        self.__logger.info(\"Closing worker control panel.\")\n",
      "\n",
      "    @staticmethod\n",
      "    def name(worker_type, worker_index):\n",
      "        return f\"{worker_type}/{worker_index}\"\n",
      "\n",
      "    @staticmethod\n",
      "    def parse_name(worker_name):\n",
      "        type_, index = worker_name.split(\"/\")\n",
      "        return type_, index\n",
      "\n",
      "    @property\n",
      "    def worker_names(self) -> List[str]:\n",
      "        \"\"\"Returns current connected workers.\n",
      "\n",
      "        A WorkerControlPanel initializes with no connected workers.\n",
      "        Workers are connected via either self.connect(names), or\n",
      "        self.auto_connect().\n",
      "        \"\"\"\n",
      "        return list(self.__worker_addresses.keys())\n",
      "\n",
      "    def connect(\n",
      "        self,\n",
      "        worker_names: List[str],\n",
      "        timeout=None,\n",
      "        raises_timeout_error=False,\n",
      "        reconnect=False,\n",
      "        progress=False,\n",
      "    ) -> List[str]:\n",
      "        \"\"\"Waits until all the workers specified by the given names are ready\n",
      "        for receiving commands.\n",
      "\n",
      "        Args:\n",
      "            worker_names: A list of workers to connect.\n",
      "            timeout: The maximum waiting time in seconds, or None if infinite.\n",
      "            raises_timeout_error: If True, any connection failure will result in the raise of an TimeoutError.\n",
      "                If False, such exception can be detected via the returned succeeded list.\n",
      "            reconnect: If True, this will reconnect to the workers that has already connected. If False, any\n",
      "                worker in `worker_names` that has already connected will be ignored.\n",
      "            progress: Whether to show a progress bar.\n",
      "\n",
      "        Returns:\n",
      "            A list of successfully connected or reconnected workers. If a specified worker is missing, it can\n",
      "            either be that it is already connected and `reconnect` is False, or that the connection timed out.\n",
      "        \"\"\"\n",
      "        rs = []\n",
      "        deadline = time.monotonic() + (timeout or 0)\n",
      "        if progress:\n",
      "            try:\n",
      "                import tqdm\n",
      "\n",
      "                worker_names = tqdm.tqdm(worker_names, leave=False)\n",
      "            except ModuleNotFoundError:\n",
      "                pass\n",
      "        for name in worker_names:\n",
      "            if name in self.__worker_addresses:\n",
      "                if reconnect:\n",
      "                    del self.__worker_addresses[name]\n",
      "                else:\n",
      "                    continue\n",
      "            try:\n",
      "                if timeout is not None:\n",
      "                    timeout = max(0, deadline - time.monotonic())\n",
      "                self.__logger.debug(f\"Connecting to worker {name}, timeout {timeout}\")\n",
      "                server_address = name_resolve.wait(\n",
      "                    names.worker(self.__experiment_name, self.__trial_name, name),\n",
      "                    timeout=timeout,\n",
      "                )\n",
      "                self.__logger.debug(f\"Connecting to worker {name} done\")\n",
      "            except TimeoutError as e:\n",
      "                if raises_timeout_error:\n",
      "                    raise e\n",
      "                continue\n",
      "            # self.__worker_addresses[name] stores address\n",
      "            self.__worker_addresses[name] = server_address\n",
      "            rs.append(name)\n",
      "        return rs\n",
      "\n",
      "    def auto_connect(self) -> List[str]:\n",
      "        \"\"\"Auto-detects available workers belonging to the experiment trial,\n",
      "        and connects to them.\n",
      "\n",
      "        Returns:\n",
      "            Names of successfully connected workers.\n",
      "        \"\"\"\n",
      "        name_root = names.worker_root(self.__experiment_name, self.__trial_name)\n",
      "        worker_names = [\n",
      "            r[len(name_root) :] for r in name_resolve.find_subtree(name_root)\n",
      "        ]\n",
      "        return self.connect(worker_names, timeout=0, raises_timeout_error=True)\n",
      "\n",
      "    def request(self, worker_name: str, command, **kwargs) -> Any:\n",
      "        \"\"\"Sends an request to the specified worker.\"\"\"\n",
      "        address = self.__worker_addresses[worker_name]\n",
      "        return self.__requester.async_request(\n",
      "            worker_name, address, command, **kwargs\n",
      "        ).result()\n",
      "\n",
      "    def group_request(\n",
      "        self,\n",
      "        command,\n",
      "        worker_names: Optional[List[str]] = None,\n",
      "        worker_regex: Optional[str] = None,\n",
      "        timeout=None,\n",
      "        progress=False,\n",
      "        worker_kwargs: Optional[List[Dict[str, Any]]] = None,\n",
      "        wait_response=True,\n",
      "        **kwargs,\n",
      "    ) -> List[Response]:\n",
      "        \"\"\"Requests selected workers, or all connected workers if not\n",
      "        specified.\n",
      "\n",
      "        Args:\n",
      "            command: RPC command.\n",
      "            worker_names: Optional selection of workers.\n",
      "            worker_regex: Optional regex selector of workers.\n",
      "            timeout: Optional timeout.\n",
      "            progress: Whether to show a progress bar.\n",
      "            worker_kwargs: RPC arguments, but one for each worker instead of `kwargs` where every worker share\n",
      "                the arguments. If this is specified, worker_names must be specified, and worker_regex, kwargs\n",
      "                must be None.\n",
      "            wait_response: Whether to wait for server response.\n",
      "            kwargs: RPC arguments.\n",
      "        \"\"\"\n",
      "        selected = self.worker_names\n",
      "        if worker_names is not None:\n",
      "            assert len(set(worker_names).difference(selected)) == 0\n",
      "            selected = worker_names\n",
      "        if worker_regex is not None:\n",
      "            selected = [x for x in selected if re.fullmatch(worker_regex, x)]\n",
      "        if worker_kwargs is not None:\n",
      "            assert worker_names is not None\n",
      "            assert worker_regex is None\n",
      "            assert len(kwargs) == 0\n",
      "            assert len(worker_names) == len(\n",
      "                worker_kwargs\n",
      "            ), f\"{len(worker_names)} != {len(worker_kwargs)}\"\n",
      "        else:\n",
      "            worker_kwargs = [kwargs for _ in selected]\n",
      "\n",
      "        # connect _MAX_SOCKET_CONCURRENCY sockets at most\n",
      "        rs = []\n",
      "        deadline = time.monotonic() + (timeout or 0)\n",
      "        for j in range(0, len(selected), _MAX_SOCKET_CONCURRENCY):\n",
      "            sub_rs: List[WorkerControlPanel.Response] = []\n",
      "            sub_selected = selected[j : j + _MAX_SOCKET_CONCURRENCY]\n",
      "            sub_worker_kwargs = worker_kwargs[j : j + _MAX_SOCKET_CONCURRENCY]\n",
      "            for name, kwargs in zip(sub_selected, sub_worker_kwargs):\n",
      "                address = self.__worker_addresses[name]\n",
      "                result_fut = self.__requester.async_request(\n",
      "                    name, address, command, wait_response, **kwargs\n",
      "                )\n",
      "                sub_rs.append(\n",
      "                    WorkerControlPanel.Response(worker_name=name, result=result_fut)\n",
      "                )\n",
      "\n",
      "            if not wait_response:\n",
      "                continue\n",
      "\n",
      "            bar = range(len(sub_rs))\n",
      "            if progress:\n",
      "                try:\n",
      "                    import tqdm\n",
      "\n",
      "                    bar = tqdm.tqdm(bar, leave=False)\n",
      "                except ModuleNotFoundError:\n",
      "                    pass\n",
      "            for r, _ in zip(sub_rs, bar):\n",
      "                if timeout is not None:\n",
      "                    timeout = max(0, deadline - time.monotonic())\n",
      "                try:\n",
      "                    r.result = r.result.result(timeout=timeout)\n",
      "                except TimeoutError:\n",
      "                    r.timed_out = True\n",
      "            rs.extend(sub_rs)\n",
      "        return rs\n",
      "\n",
      "    def get_worker_status(self, worker_name) -> WorkerServerStatus:\n",
      "        \"\"\"Get status of a connected worker.\n",
      "\n",
      "        Raises:\n",
      "            ValueError if worker is not connected.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            status_str = name_resolve.wait(\n",
      "                names.worker_status(\n",
      "                    experiment_name=self.__experiment_name,\n",
      "                    trial_name=self.__trial_name,\n",
      "                    worker_name=worker_name,\n",
      "                ),\n",
      "                timeout=60,\n",
      "            )\n",
      "            status = WorkerServerStatus(status_str)\n",
      "        except name_resolve.NameEntryNotFoundError:\n",
      "            status = WorkerServerStatus.LOST\n",
      "        return status\n",
      "\n",
      "    def pulse(self):\n",
      "        return {name: self.get_worker_status(name) for name in self.worker_names}\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PollResult:\n",
      "    # Number of total samples and batches processed by the worker. Specifically:\n",
      "    # - For an actor worker, sample_count = batch_count = number of env.step()-s being executed.\n",
      "    # - For a policy worker, number of inference requests being handled, versus how many batches were made.\n",
      "    # - For a trainer worker, number of samples & batches fed into the trainer (typically GPU).\n",
      "    sample_count: int\n",
      "    batch_count: int\n",
      "\n",
      "    def __iadd__(self, p: \"PollResult\"):\n",
      "        return PollResult(\n",
      "            self.sample_count + p.sample_count, self.batch_count + p.batch_count\n",
      "        )\n",
      "\n",
      "\n",
      "class Worker:\n",
      "    \"\"\"The worker base class that provides general methods and entry point.\n",
      "\n",
      "    For simplicity, we use a single-threaded pattern in implementing the worker RPC server. Logic\n",
      "    of every worker are executed via periodical calls to the poll() method, instead of inside\n",
      "    another thread or process (e.g. the gRPC implementation). A subclass only needs to implement\n",
      "    poll() without duplicating the main loop.\n",
      "\n",
      "    The typical code on the worker side is:\n",
      "        worker = make_worker()  # Returns instance of Worker.\n",
      "        worker.run()\n",
      "    and the later is standardized here as:\n",
      "        while exit command is not received:\n",
      "            if worker is started:\n",
      "                worker.poll()\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, server: Optional[WorkerServer] = None):\n",
      "        \"\"\"Initializes a worker server.\n",
      "\n",
      "        Args:\n",
      "            server: The RPC server API for the worker to register handlers and poll requests.\n",
      "        \"\"\"\n",
      "        self.__running = False\n",
      "        self.__exiting = False\n",
      "        self.config = None\n",
      "        self.__is_configured = False\n",
      "\n",
      "        self._server = server\n",
      "        if server is not None:\n",
      "            server.register_handler(\"configure\", self.configure)\n",
      "            server.register_handler(\"reconfigure\", self.reconfigure)\n",
      "            server.register_handler(\"start\", self.start)\n",
      "            server.register_handler(\"pause\", self.pause)\n",
      "            server.register_handler(\"exit\", self.exit)\n",
      "            server.register_handler(\"interrupt\", self.interrupt)\n",
      "            server.register_handler(\"ping\", lambda: \"pong\")\n",
      "\n",
      "        self.logger = logging.getLogger(\"worker\")\n",
      "        self.__worker_type = None\n",
      "        self.__worker_index = None\n",
      "        self.__last_successful_poll_time = None\n",
      "        self.__worker_info = None\n",
      "\n",
      "        self._start_time_ns = None\n",
      "\n",
      "        self.__set_status(WorkerServerStatus.READY)\n",
      "\n",
      "    def __set_status(self, status: WorkerServerStatus):\n",
      "        self.__status = status\n",
      "        if self._server is not None:\n",
      "            self.logger.debug(f\"Setting worker server status to {status}\")\n",
      "            self._server.set_status(status)\n",
      "\n",
      "    @property\n",
      "    def status(self) -> WorkerServerStatus:\n",
      "        return self.__status\n",
      "\n",
      "    @property\n",
      "    def is_configured(self):\n",
      "        return self.__is_configured\n",
      "\n",
      "    def _reconfigure(self, **kwargs) -> system_api.WorkerInformation:\n",
      "        \"\"\"Implemented by sub-classes.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def _configure(self, config) -> system_api.WorkerInformation:\n",
      "        \"\"\"Implemented by sub-classes.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def _poll(self) -> PollResult:\n",
      "        \"\"\"Implemented by sub-classes.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def running(self):\n",
      "        return self.__running\n",
      "\n",
      "    @property\n",
      "    def exiting(self):\n",
      "        return self.__exiting\n",
      "\n",
      "    @property\n",
      "    def is_configured(self):\n",
      "        return self.__is_configured\n",
      "\n",
      "    def configure(\n",
      "        self,\n",
      "        worker_info: system_api.WorkerInformation,\n",
      "        setup_id: int,\n",
      "    ):\n",
      "        assert not self.__running\n",
      "        self.__worker_info = worker_info\n",
      "        self.__worker_type = worker_info.worker_type\n",
      "        self.__worker_index = worker_info.worker_index\n",
      "\n",
      "        experiment = system_api.make_experiment(name=worker_info.experiment_name)\n",
      "\n",
      "        expr_config = experiment.initial_setup()\n",
      "        if isinstance(expr_config, list):\n",
      "            expr_config = expr_config[setup_id]\n",
      "        else:\n",
      "            assert setup_id == 0\n",
      "        expr_config.set_worker_information(\n",
      "            experiment_name=worker_info.experiment_name,\n",
      "            trial_name=worker_info.trial_name,\n",
      "        )\n",
      "        expr_config.lazy_init()\n",
      "        self.wandb_config = expr_config.wandb\n",
      "        os.environ[\"WANDB_MODE\"] = self.wandb_config.mode\n",
      "        self.tensorboard_config = expr_config.tensorboard\n",
      "        config = expr_config.resolve_worker_config(\n",
      "            self.__worker_type, self.__worker_index\n",
      "        )\n",
      "        self.logger.debug(\"Configuring with: %s\", config)\n",
      "\n",
      "        r = self._configure(config)\n",
      "        self.logger = logging.getLogger(r.worker_type + \"-worker\", \"colored\")\n",
      "        if r.host_key is not None:\n",
      "            self.__host_key(\n",
      "                names.worker_key(\n",
      "                    experiment_name=r.experiment_name,\n",
      "                    trial_name=r.trial_name,\n",
      "                    key=r.host_key,\n",
      "                )\n",
      "            )\n",
      "        if r.watch_keys is not None:\n",
      "            keys = [r.watch_keys] if isinstance(r.watch_keys, str) else r.watch_keys\n",
      "            self.__watch_keys(\n",
      "                [\n",
      "                    names.worker_key(\n",
      "                        experiment_name=r.experiment_name,\n",
      "                        trial_name=r.trial_name,\n",
      "                        key=k,\n",
      "                    )\n",
      "                    for k in keys\n",
      "                ]\n",
      "            )\n",
      "\n",
      "        self.__is_configured = True\n",
      "\n",
      "    def reconfigure(self, **kwargs):\n",
      "        assert not self.__running\n",
      "        self.__is_configured = False\n",
      "        self.logger.info(f\"Reconfiguring with: {kwargs}\")\n",
      "        self._reconfigure(**kwargs)\n",
      "        self.__is_configured = True\n",
      "        self.logger.info(\"Reconfigured successfully\")\n",
      "\n",
      "    def start(self):\n",
      "        self.logger.debug(\"Starting worker\")\n",
      "        self.__running = True\n",
      "        self.__set_status(WorkerServerStatus.RUNNING)\n",
      "\n",
      "    def pause(self):\n",
      "        self.logger.info(\n",
      "            f\"Pausing worker index {self.__worker_info.worker_index + 1} out of {self.__worker_info.worker_count}\"\n",
      "        )\n",
      "        self.__running = False\n",
      "        self.__set_status(WorkerServerStatus.PAUSED)\n",
      "\n",
      "    def _exit_hook(self, exit_status: WorkerServerStatus):\n",
      "        logger.warning(f\"Exit with {exit_status}, hook not implemented, pass.\")\n",
      "\n",
      "    def exit(self, err: bool = False):\n",
      "        self.logger.info(\"Exiting worker\")\n",
      "        status = WorkerServerStatus.ERROR if err else WorkerServerStatus.COMPLETED\n",
      "        self._exit_hook(status)\n",
      "        self.__set_status(status)\n",
      "        self.__exiting = True\n",
      "\n",
      "    def interrupt(self):\n",
      "        self.logger.info(\"Worker interrupted by remote control.\")\n",
      "        self._exit_hook(WorkerServerStatus.INTERRUPTED)\n",
      "        self.__set_status(WorkerServerStatus.INTERRUPTED)\n",
      "        raise WorkerException(\n",
      "            worker_name=\"worker\",\n",
      "            worker_status=WorkerServerStatus.INTERRUPTED,\n",
      "            scenario=\"running\",\n",
      "        )\n",
      "\n",
      "    def run(self):\n",
      "        self._start_time_ns = time.monotonic_ns()\n",
      "        self.__last_update_ns = None\n",
      "        self.logger.debug(\"Running worker now\")\n",
      "        try:\n",
      "            while not self.__exiting:\n",
      "                self._server.handle_requests()\n",
      "                if not self.__running:\n",
      "                    time.sleep(0.05)\n",
      "                    continue\n",
      "                if not self.__is_configured:\n",
      "                    raise RuntimeError(\"Worker is not configured\")\n",
      "                start_time = time.monotonic_ns()\n",
      "                r = self._poll()\n",
      "                poll_time = (time.monotonic_ns() - start_time) / 1e9\n",
      "                wait_seconds = 0.0\n",
      "                if self.__last_successful_poll_time is not None:\n",
      "                    # Account the waiting time since the last successful step.\n",
      "                    wait_seconds = (start_time - self.__last_successful_poll_time) / 1e9\n",
      "                self.__last_successful_poll_time = time.monotonic_ns()\n",
      "\n",
      "                if r.sample_count == r.batch_count == 0:\n",
      "                    # time.sleep(0.002)\n",
      "                    pass\n",
      "                else:\n",
      "                    now = time.monotonic_ns()\n",
      "                    if (\n",
      "                        self.__last_update_ns is not None\n",
      "                    ):  # Update new stats with 10 seconds frequency.\n",
      "                        if (now - self.__last_update_ns) / 1e9 >= 10:\n",
      "                            duration = (time.monotonic_ns() - self._start_time_ns) / 1e9\n",
      "                            self.__last_update_ns = now\n",
      "                    else:\n",
      "                        self.__last_update_ns = now\n",
      "        except KeyboardInterrupt:\n",
      "            self.exit()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Worker encountered error {e}\", exc_info=True)\n",
      "            if isinstance(e, WorkerException):\n",
      "                raise e\n",
      "            self.exit(err=True)\n",
      "            raise e\n",
      "\n",
      "    def __host_key(self, key: str):\n",
      "        self.logger.info(f\"Hosting key: {key}\")\n",
      "        name_resolve.add(key, \"up\", keepalive_ttl=15, replace=True, delete_on_exit=True)\n",
      "\n",
      "    def __watch_keys(self, keys: List[str]):\n",
      "        self.logger.info(f\"Watching keys: {keys}\")\n",
      "        name_resolve.watch_names(keys, call_back=self.exit)\n",
      "\n",
      "\n",
      "class AsyncWorker(Worker):\n",
      "    async def _poll_async(self) -> PollResult:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    async def run_async(self):\n",
      "        self.logger.debug(\"Running worker now\")\n",
      "        try:\n",
      "            while not self.exiting:\n",
      "                await asyncio.sleep(0.0)\n",
      "                self._server.handle_requests()\n",
      "                if not self.running:\n",
      "                    await asyncio.sleep(0.05)\n",
      "                    continue\n",
      "                if not self.is_configured:\n",
      "                    raise RuntimeError(\"Worker is not configured\")\n",
      "                r = await self._poll_async()\n",
      "        except KeyboardInterrupt:\n",
      "            self.exit()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Worker encountered error {e}\", exc_info=True)\n",
      "            if isinstance(e, WorkerException):\n",
      "                raise e\n",
      "            self.exit(err=True)\n",
      "            raise e\n",
      "\n",
      "\n",
      "class MappingThread:\n",
      "    \"\"\"Wrapped of a mapping thread.\n",
      "\n",
      "    A mapping thread gets from up_stream_queue, process data, and puts\n",
      "    to down_stream_queue.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        map_fn,\n",
      "        interrupt_flag,\n",
      "        upstream_queue,\n",
      "        downstream_queue: queue.Queue = None,\n",
      "        cuda_device=None,\n",
      "    ):\n",
      "        \"\"\"Init method of MappingThread for Policy Workers.\n",
      "\n",
      "        Args:\n",
      "            map_fn: mapping function.\n",
      "            interrupt_flag: main thread sets this value to True to interrupt the thread.\n",
      "            upstream_queue: the queue to get data from.\n",
      "            downstream_queue: the queue to put data after processing. If None, data will be discarded after processing.\n",
      "        \"\"\"\n",
      "        self.__map_fn = map_fn\n",
      "        self.__interrupt = interrupt_flag\n",
      "        self.__upstream_queue = upstream_queue\n",
      "        self.__downstream_queue = downstream_queue\n",
      "        self.__thread = threading.Thread(target=self._run, daemon=True)\n",
      "        self.__cuda_device = cuda_device\n",
      "\n",
      "    def is_alive(self) -> bool:\n",
      "        \"\"\"Check whether the thread is alive.\n",
      "\n",
      "        Returns:\n",
      "            alive: True if the wrapped thread is alive, False otherwise.\n",
      "        \"\"\"\n",
      "        return self.__interrupt or self.__thread.is_alive()\n",
      "\n",
      "    def start(self):\n",
      "        \"\"\"Start the wrapped thread.\"\"\"\n",
      "        self.__thread.start()\n",
      "\n",
      "    def join(self):\n",
      "        \"\"\"Join the wrapped thread.\"\"\"\n",
      "        self.__thread.join()\n",
      "\n",
      "    def _run(self):\n",
      "        if self.__cuda_device is not None:\n",
      "            set_cuda_device(self.__cuda_device)\n",
      "        while not self.__interrupt:\n",
      "            self._run_step()\n",
      "\n",
      "    def _run_step(self):\n",
      "        try:\n",
      "            data = self.__upstream_queue.get(timeout=1)\n",
      "            data = self.__map_fn(data)\n",
      "            if self.__downstream_queue is not None:\n",
      "                self.__downstream_queue.put(data)\n",
      "        except queue.Empty:\n",
      "            pass\n",
      "\n",
      "    def stop(self):\n",
      "        \"\"\"Stop the wrapped thread.\"\"\"\n",
      "        self.__interrupt = True\n",
      "        if self.__thread.is_alive():\n",
      "            self.__thread.join()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/cli_args.py ====\n",
      "\n",
      "import getpass\n",
      "import os\n",
      "from dataclasses import asdict, dataclass, field, fields, is_dataclass\n",
      "from typing import Dict, List, Optional, Tuple, Type, Union\n",
      "\n",
      "from omegaconf import MISSING\n",
      "\n",
      "from realhf.base import pkg_version\n",
      "\n",
      "## Data and datasets. ##\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class MicroBatchSpec:\n",
      "    \"\"\"Specification for splitting micro-batches during training.\"\"\"\n",
      "\n",
      "    n_mbs: int = field(\n",
      "        default=1,\n",
      "        metadata={\n",
      "            \"help\": \"Number of micro-batches (or minimum number if max_tokens_per_mb is set). Used when max_tokens_per_mb is None or as minimum count\",\n",
      "        },\n",
      "    )\n",
      "    max_tokens_per_mb: int = field(\n",
      "        default=int(1e12),\n",
      "        metadata={\n",
      "            \"help\": \"Maximum tokens per micro-batch. When set, n_mbs becomes the minimum number of micro-batches\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "    @classmethod\n",
      "    def new(cls, mb_spec: \"MicroBatchSpec\", **kwargs):\n",
      "        \"\"\"Create new spec with updated fields while maintaining Omegaconf compatibility.\"\"\"\n",
      "        fields = dict(\n",
      "            n_mbs=mb_spec.n_mbs,\n",
      "            max_tokens_per_mb=mb_spec.max_tokens_per_mb,\n",
      "        )\n",
      "        fields.update(kwargs)\n",
      "        return cls(**fields)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class PromptAnswerDatasetConfig:\n",
      "    \"\"\"Configuration for Supervised Fine-Tuning (SFT) datasets.\n",
      "\n",
      "    Dataset format requirements:\n",
      "    - JSON/JSONL files\n",
      "    - Each entry: {\"prompt\": str, \"answer\": str}\n",
      "    \"\"\"\n",
      "\n",
      "    train_path: str = field(default=\"\", metadata={\"help\": \"Path to training dataset\"})\n",
      "    valid_path: str = field(default=\"\", metadata={\"help\": \"Path to validation dataset\"})\n",
      "    max_seqlen: int = field(\n",
      "        default=1024, metadata={\"help\": \"Maximum sequence length (prompt + answer)\"}\n",
      "    )\n",
      "    train_bs_n_seqs: int = field(\n",
      "        default=256, metadata={\"help\": \"Training batch size in number of sequences\"}\n",
      "    )\n",
      "    valid_bs_n_seqs: int = field(\n",
      "        default=256, metadata={\"help\": \"Validation batch size in number of sequences\"}\n",
      "    )\n",
      "    fill_to_max_length: bool = field(\n",
      "        default=False,\n",
      "        metadata={\n",
      "            \"help\": \"Pad sequences to max length. For testing only - left-fills with non-pad tokens\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class PromptOnlyDatasetConfig:\n",
      "    \"\"\"Configuration for PPO RLHF datasets.\n",
      "\n",
      "    Dataset format requirements:\n",
      "    - JSON/JSONL files\n",
      "    - Each entry: {\"prompt\": str}\n",
      "    \"\"\"\n",
      "\n",
      "    path: str = field(default=\"\", metadata={\"help\": \"Path to dataset\"})\n",
      "    max_prompt_len: int = field(\n",
      "        default=256, metadata={\"help\": \"Maximum prompt length (truncated if longer)\"}\n",
      "    )\n",
      "    train_bs_n_seqs: int = field(\n",
      "        default=256, metadata={\"help\": \"Batch size in number of prompts\"}\n",
      "    )\n",
      "    fill_to_max_length: bool = field(\n",
      "        default=False,\n",
      "        metadata={\n",
      "            \"help\": \"Pad sequences to max length. For testing only - left-fills with non-pad tokens\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "\n",
      "## Model, optimizer, and backends. ##\n",
      "\n",
      "\n",
      "@dataclass(unsafe_hash=True)\n",
      "class ModelFamily:\n",
      "    \"\"\"Identifier for HuggingFace model types (e.g., llama, gpt2).\n",
      "\n",
      "    Used for model registration and allocation.\n",
      "    \"\"\"\n",
      "\n",
      "    _class: str = field(\n",
      "        metadata={\n",
      "            \"help\": \"Model class name (e.g., 'llama'). Must be registered in `register_hf_family`. See \"\n",
      "            \"`realhf/api/from_hf` for supported models.\",\n",
      "        }\n",
      "    )\n",
      "    is_critic: bool = field(\n",
      "        default=False,\n",
      "        metadata={\n",
      "            \"help\": \"Whether this is a critic/reward model. False indicates a standard LLM\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "    def __repr__(self):\n",
      "        \"\"\"Returns formatted string representation: '{class}[-critic]'.\"\"\"\n",
      "        s = f\"{self._class}\"\n",
      "        if self.is_critic:\n",
      "            s += \"-critic\"\n",
      "        return s\n",
      "\n",
      "\n",
      "@dataclass(unsafe_hash=True)\n",
      "class ParallelismConfig:\n",
      "    \"\"\"Configuration for 3D parallelism (tensor, pipeline, and data parallelism).\n",
      "\n",
      "    Note:\n",
      "        Sequence parallelism is only used in combination with tensor-model parallelism.\n",
      "    \"\"\"\n",
      "\n",
      "    tensor_parallel_size: int = field(\n",
      "        default=1, metadata={\"help\": \"Size of tensor-model parallelism\"}\n",
      "    )\n",
      "    pipeline_parallel_size: int = field(\n",
      "        default=1, metadata={\"help\": \"Number of pipeline parallel stages\"}\n",
      "    )\n",
      "    data_parallel_size: int = field(\n",
      "        default=1, metadata={\"help\": \"Data parallelism size for ZeRO optimization\"}\n",
      "    )\n",
      "    use_sequence_parallel: bool = field(\n",
      "        default=False,\n",
      "        metadata={\n",
      "            \"help\": \"Enable sequence parallelism. Only used with tensor-model parallelism in Megatron\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"Returns compact string representation: 'Parallel(mp=X,pp=Y,dp=Z)'.\"\"\"\n",
      "        return (\n",
      "            f\"Parallel(mp={self.tensor_parallel_size},\"\n",
      "            f\"pp={self.pipeline_parallel_size},\"\n",
      "            f\"dp={self.data_parallel_size})\"\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def parallelism_eq(this, other):\n",
      "        \"\"\"Compare parallelism configurations (excluding sequence parallelism).\n",
      "\n",
      "        Note:\n",
      "            Implemented as static method to avoid OmegaConf compatibility issues.\n",
      "        \"\"\"\n",
      "        return (\n",
      "            (this.tensor_parallel_size == other.tensor_parallel_size)\n",
      "            and (this.pipeline_parallel_size == other.pipeline_parallel_size)\n",
      "            and (this.data_parallel_size == other.data_parallel_size)\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class OptimizerConfig:\n",
      "    \"\"\"Configuration for model optimization during training.\n",
      "\n",
      "    Note:\n",
      "        Set type to \"empty\" for models that won't be trained.\n",
      "    \"\"\"\n",
      "\n",
      "    type: str = field(\n",
      "        default=\"adam\",\n",
      "        metadata={\"help\": \"Optimizer type\", \"choices\": [\"adam\", \"empty\"]},\n",
      "    )\n",
      "    lr: float = field(default=2e-5, metadata={\"help\": \"Learning rate\"})\n",
      "    weight_decay: float = field(default=0.05, metadata={\"help\": \"Weight decay\"})\n",
      "    beta1: float = field(default=0.9, metadata={\"help\": \"Adam beta1 parameter\"})\n",
      "    beta2: float = field(default=0.95, metadata={\"help\": \"Adam beta2 parameter\"})\n",
      "    eps: float = field(default=1e-5, metadata={\"help\": \"Adam epsilon parameter\"})\n",
      "    min_lr_ratio: float = field(\n",
      "        default=0.0,\n",
      "        metadata={\n",
      "            \"help\": \"Minimum learning rate ratio after annealing\",\n",
      "        },\n",
      "    )\n",
      "    lr_scheduler_type: str = field(\n",
      "        default=\"constant\",\n",
      "        metadata={\n",
      "            \"help\": \"Learning rate scheduler type\",\n",
      "            \"choices\": [\"linear\", \"cosine\", \"constant\"],\n",
      "        },\n",
      "    )\n",
      "    warmup_steps_proportion: float = field(\n",
      "        default=0.001,\n",
      "        metadata={\n",
      "            \"help\": \"Proportion of training steps for warmup\",\n",
      "        },\n",
      "    )\n",
      "    offload: bool = field(\n",
      "        default=False, metadata={\"help\": \"Enable optimizer state offloading\"}\n",
      "    )\n",
      "    initial_loss_scale: float = field(\n",
      "        default=2**32, metadata={\"help\": \"Initial loss scaling factor\"}\n",
      "    )\n",
      "    min_loss_scale: float = field(\n",
      "        default=1.0, metadata={\"help\": \"Minimum loss scaling factor\"}\n",
      "    )\n",
      "    loss_scale_window: float = field(\n",
      "        default=5, metadata={\"help\": \"Window size for loss scaling adjustment\"}\n",
      "    )\n",
      "    hysteresis: int = field(\n",
      "        default=2, metadata={\"help\": \"Hysteresis (scaling factor) for loss scaling\"}\n",
      "    )\n",
      "    gradient_clipping: float = field(\n",
      "        default=1.0, metadata={\"help\": \"Gradient clipping threshold\"}\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class vLLMConfig:\n",
      "    \"\"\"Configuration for vLLM inference engine. Refer to:\n",
      "    https://github.com/vllm-project/vllm for detailed documentation.\n",
      "    \"\"\"\n",
      "\n",
      "    max_num_seqs: int = 256\n",
      "    dtype: str = \"float16\"\n",
      "    kv_cache_type: str = \"auto\"\n",
      "    num_scheduler_steps: int = 1\n",
      "    multi_step_stream_outputs: bool = True\n",
      "    block_size: int = 16\n",
      "    swap_space: int = 4\n",
      "    cpu_offload_gb: float = 0\n",
      "    max_seq_len_to_capture: int = 32768\n",
      "\n",
      "    disable_sliding_window: bool = True\n",
      "\n",
      "    # NOTE: Defaults max_model_len to 32k because a larger value\n",
      "    # will enable chunked prefill in vLLM, which will cause\n",
      "    # evalution performance degeneration.\n",
      "    max_model_len: Optional[int] = 32768\n",
      "    enable_chunked_prefill: bool = False\n",
      "\n",
      "    # NOTE: Setting enable_prefix_caching to False\n",
      "    # because it will reuse the block after\n",
      "    # model weights are updated. Using v0.7.2 reset_prefix_cache\n",
      "    # will fix this issue.\n",
      "    enable_prefix_caching: bool = False\n",
      "\n",
      "    gpu_memory_utilization: float = 0.9\n",
      "\n",
      "    enforce_eager: bool = False\n",
      "    hybrid_train: bool = False\n",
      "    additional_engine_args: Dict = field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class SGLangConfig:\n",
      "    \"\"\"Configuration for SGLang runtime. Refer to:\n",
      "    https://github.com/sgl-project/sglang for detailed documentation.\n",
      "    \"\"\"\n",
      "\n",
      "    disable_cuda_graph: bool = False\n",
      "    disable_radix_cache: bool = False\n",
      "    disable_cuda_graph_padding: bool = False\n",
      "    enable_nccl_nvls: bool = False\n",
      "    disable_outlines_disk_cache: bool = False\n",
      "    disable_custom_all_reduce: bool = False\n",
      "    disable_overlap_schedule: bool = False\n",
      "    enable_mixed_chunk: bool = False\n",
      "    enable_dp_attention: bool = False\n",
      "    enable_ep_moe: bool = False\n",
      "    enable_torch_compile: bool = False\n",
      "    torch_compile_max_bs: int = 32\n",
      "    cuda_graph_max_bs: Optional[int] = None\n",
      "    cuda_graph_bs: Optional[List[int]] = None\n",
      "    torchao_config: str = \"\"\n",
      "    enable_nan_detection: bool = False\n",
      "    enable_p2p_check: bool = False\n",
      "    triton_attention_reduce_in_fp32: bool = False\n",
      "    triton_attention_num_kv_splits: int = 8\n",
      "    num_continuous_decode_steps: int = 1\n",
      "    enable_memory_saver: bool = False\n",
      "    allow_auto_truncate: bool = False\n",
      "    # NOTE: to avoid the illegal memory access error\n",
      "    attention_backend: Optional[str] = \"flashinfer\"\n",
      "    sampling_backend: Optional[str] = None\n",
      "    context_length: Optional[int] = 32768\n",
      "    mem_fraction_static: Optional[float] = 0.9\n",
      "    max_running_requests: Optional[int] = None\n",
      "    # NOTE: chunked_prefill_size is by default 8192 on GPUs with 80GB mem in SGLang,\n",
      "    # but we disable it to avoid precision issues\n",
      "    chunked_prefill_size: Optional[int] = -1\n",
      "    max_prefill_tokens: int = 32768\n",
      "    schedule_policy: str = \"lpm\"\n",
      "    schedule_conservativeness: float = 1.0\n",
      "    cpu_offload_gb: int = 0\n",
      "    hybrid_train: bool = False\n",
      "    dtype: str = \"float16\"\n",
      "    kv_cache_dtype: str = \"auto\"\n",
      "\n",
      "    # logging\n",
      "    log_level: str = \"warning\"\n",
      "    log_level_http: Optional[str] = \"warning\"\n",
      "    log_requests: bool = False\n",
      "    log_requests_level: int = 0\n",
      "    show_time_cost: bool = False\n",
      "    enable_metrics: bool = True  # Exports Prometheus-like metrics\n",
      "    # The interval (in decoding iterations) to log throughput\n",
      "    # and update prometheus metrics\n",
      "    decode_log_interval: int = 1\n",
      "\n",
      "    # Use staticmethod to make OmegaConf happy.\n",
      "    @staticmethod\n",
      "    def build_cmd(\n",
      "        sglang_config: \"SGLangConfig\",\n",
      "        model_path,\n",
      "        tp_size,\n",
      "        server_index,\n",
      "        base_gpu_id,\n",
      "        dist_init_addr: Optional[str] = None,\n",
      "    ):\n",
      "        from realhf.base import constants, network, pkg_version, seeding\n",
      "        from realhf.experiments.common.utils import asdict as conf_as_dict\n",
      "\n",
      "        args: Dict = conf_as_dict(sglang_config)\n",
      "        args.pop(\"hybrid_train\")\n",
      "        args[\"random_seed\"] = seeding.get_seed()\n",
      "\n",
      "        host_ip = network.gethostip()\n",
      "        host = \"localhost\" if not sglang_config.enable_metrics else host_ip\n",
      "        args = dict(\n",
      "            host=host,\n",
      "            model_path=model_path,\n",
      "            # Model and tokenizer\n",
      "            tokenizer_path=model_path,\n",
      "            tokenizer_mode=\"auto\",\n",
      "            load_format=\"auto\",\n",
      "            trust_remote_code=True,\n",
      "            device=\"cuda\",\n",
      "            served_model_name=f\"{constants.experiment_name()}/{constants.trial_name()}/{model_path}\",\n",
      "            is_embedding=False,\n",
      "            skip_tokenizer_init=True,\n",
      "            # Other runtime options\n",
      "            tp_size=tp_size,\n",
      "            # Because we have set CUDA_VISIBLE_DEVICES to a single GPU in each process\n",
      "            base_gpu_id=base_gpu_id,\n",
      "            file_storage_path=os.path.join(\n",
      "                constants.SGLANG_CACHE_PATH,\n",
      "                f\"sglang_storage{server_index}\",\n",
      "            ),\n",
      "            # Data parallelism\n",
      "            dp_size=1,  # TODO: check whether we require SGLang dp\n",
      "            load_balance_method=\"round_robin\",\n",
      "            # Expert parallelism\n",
      "            ep_size=1,  # TODO: check\n",
      "            nnodes=1,\n",
      "            node_rank=0,\n",
      "            dist_init_addr=dist_init_addr,\n",
      "            **args,\n",
      "        )\n",
      "\n",
      "        if pkg_version.is_version_less(\"sglang\", \"0.4.4\"):\n",
      "            args.pop(\"log_requests_level\")\n",
      "        if pkg_version.is_version_less(\"sglang\", \"0.4.3\"):\n",
      "            args.pop(\"enable_nccl_nvls\")\n",
      "            args.pop(\"triton_attention_num_kv_splits\")\n",
      "            args.pop(\"cuda_graph_bs\")\n",
      "            args.pop(\"enable_memory_saver\")\n",
      "            args.pop(\"allow_auto_truncate\")\n",
      "            args.pop(\"file_storage_path\")\n",
      "\n",
      "        flags = []\n",
      "        for k, v in args.items():\n",
      "            if v is None or v is False or v == \"\":\n",
      "                continue\n",
      "            if v is True:\n",
      "                flags.append(f\"--{k.replace('_','-')} \")\n",
      "                continue\n",
      "            if isinstance(v, list):\n",
      "                values = \" \".join(map(str, v))\n",
      "                flags.append(f\"--{k.replace('_','-')} {values}\")\n",
      "                continue\n",
      "            flags.append(f\"--{k.replace('_','-')} {v}\")\n",
      "        flags = \" \".join(flags)\n",
      "        return f\"python3 -m sglang.launch_server {flags}\"\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class DistributedDataParallelConfig:\n",
      "    \"\"\"Configuration for Megatron's DistributedDataParallel.\n",
      "    Refer to Megatron-LM documentation for details.\n",
      "    \"\"\"\n",
      "\n",
      "    grad_reduce_in_fp32: bool = True\n",
      "    overlap_grad_reduce: bool = True\n",
      "    overlap_param_gather: bool = False\n",
      "    align_param_gather: bool = False\n",
      "    use_distributed_optimizer: bool = True\n",
      "    check_for_nan_in_grad: bool = False\n",
      "    bucket_size: Optional[int] = None\n",
      "    average_in_collective: bool = False\n",
      "    fp8_param_gather: bool = False\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class MegatronConfig:\n",
      "    \"\"\"Configuration for Megatron-LM training framework.\n",
      "    Refer to Megatron-LM documentation for implementation details.\n",
      "    \"\"\"\n",
      "\n",
      "    # Distributed Training Configuration\n",
      "    ddp: DistributedDataParallelConfig = field(\n",
      "        default_factory=DistributedDataParallelConfig\n",
      "    )\n",
      "    # Don't use MegatronOptimizerConfig here because OmegaConf\n",
      "    # does not recognize the annotation \"torch.dtype\"\n",
      "    overlap_param_gather_with_optimizer_step: bool = False\n",
      "\n",
      "    # Precision Configuration\n",
      "    use_precision_aware_optimizer: bool = False\n",
      "    main_grads_dtype: str = \"float32\"\n",
      "    main_params_dtype: str = \"float32\"\n",
      "    exp_avg_dtype: str = \"float32\"\n",
      "    exp_avg_sq_dtype: str = \"float32\"\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ModelTrainEvalConfig:\n",
      "    \"\"\"Runtime configuration for LLMs in ReaL framework.\n",
      "\n",
      "    Uses a custom model implementation supporting:\n",
      "    - 3D and sequence parallelism\n",
      "    - Flash attention for training/generation\n",
      "    - Packed 1D tensor inputs for memory efficiency\n",
      "\n",
      "    Note: Requires manual conversion from HuggingFace models.\n",
      "    Implemented conversions are in `realhf/api/from_hf/`.\n",
      "    \"\"\"\n",
      "\n",
      "    # Model Architecture Configuration\n",
      "    type: ModelFamily = field(\n",
      "        default=ModelFamily(\"llama\", False),\n",
      "        metadata={\"help\": \"Model family specification\"},\n",
      "    )\n",
      "    path: str = field(default=\"\", metadata={\"help\": \"Path to HuggingFace checkpoint\"})\n",
      "    init_from_scratch: bool = field(\n",
      "        default=False, metadata={\"help\": \"Initialize model weights randomly\"}\n",
      "    )\n",
      "    init_critic_from_actor: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Initialize critic/reward model from LM checkpoint\"},\n",
      "    )\n",
      "\n",
      "    # Training Backend Configuration\n",
      "    backend: str = field(\n",
      "        default=\"megatron\",\n",
      "        metadata={\"help\": \"Training backend\", \"choices\": [\"megatron\"]},\n",
      "    )\n",
      "    gradient_checkpointing: bool = field(\n",
      "        default=True, metadata={\"help\": \"Enable memory-saving gradient checkpointing\"}\n",
      "    )\n",
      "    bf16: bool = field(\n",
      "        default=False, metadata={\"help\": \"Use bf16 precision (otherwise fp16)\"}\n",
      "    )\n",
      "\n",
      "    # Backend-Specific Configurations\n",
      "    optimizer: Optional[OptimizerConfig] = field(\n",
      "        default_factory=OptimizerConfig, metadata={\"help\": \"Optimizer configuration\"}\n",
      "    )\n",
      "    megatron: MegatronConfig = field(\n",
      "        default_factory=MegatronConfig,\n",
      "        metadata={\n",
      "            \"help\": \"Megatron-specific configuration. Can be ignored if this model is not trained.\"\n",
      "        },\n",
      "    )\n",
      "    vllm: vLLMConfig = field(\n",
      "        default_factory=vLLMConfig,\n",
      "        metadata={\n",
      "            \"help\": \"vLLM inference configuration. Can be ignored if this model doesn't use vLLM.\"\n",
      "        },\n",
      "    )\n",
      "    sglang: SGLangConfig = field(\n",
      "        default_factory=SGLangConfig,\n",
      "        metadata={\n",
      "            \"help\": \"SGLang runtime configuration.  Can be ignored if this model doesn't use SGLang.\"\n",
      "        },\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class MFCConfig:\n",
      "    \"\"\"Configuration for a single Micro-Function Chain (MFC).\n",
      "\n",
      "    Contains specifications for micro-batch splitting and parallel execution.\n",
      "\n",
      "    device_mesh format depends on scope:\n",
      "    - Multi-node: SLURM nodelist (e.g., 'node[01-02]' or 'node01,node02')\n",
      "    - Single-node: MPI-style hostfile format (e.g., 'node01:0,1,2,3' for first 4 GPUs)\n",
      "    Must use 1, 2, 4, or 8 contiguous GPUs on single node\n",
      "    \"\"\"\n",
      "\n",
      "    mb_spec: MicroBatchSpec = field(\n",
      "        default_factory=MicroBatchSpec,\n",
      "        metadata={\n",
      "            \"help\": \"Micro-batch splitting specification\",\n",
      "        },\n",
      "    )\n",
      "    parallel: ParallelismConfig = field(\n",
      "        default_factory=ParallelismConfig,\n",
      "        metadata={\n",
      "            \"help\": \"Parallelism strategy.\",\n",
      "        },\n",
      "    )\n",
      "    device_mesh: Optional[str] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Device mesh specification for manual allocation\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "\n",
      "## RL related. ##\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GenerationHyperparameters:\n",
      "    \"\"\"Controls text generation behavior for PPO training.\"\"\"\n",
      "\n",
      "    n: int = field(\n",
      "        default=1, metadata={\"help\": \"Number of sequences to generate per prompt.\"}\n",
      "    )\n",
      "    max_new_tokens: int = field(\n",
      "        default=16384, metadata={\"help\": \"Maximum number of tokens to generate.\"}\n",
      "    )\n",
      "    min_new_tokens: int = field(\n",
      "        default=0, metadata={\"help\": \"Minimum number of tokens to generate.\"}\n",
      "    )\n",
      "    greedy: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Whether to use greedy decoding (max probability).\"},\n",
      "    )\n",
      "    top_p: float = field(\n",
      "        default=1.0,\n",
      "        metadata={\"help\": \"Nucleus sampling probability threshold (0.0, 1.0].\"},\n",
      "    )\n",
      "    top_k: int = field(\n",
      "        default=int(1e8),\n",
      "        metadata={\"help\": \"Number of highest probability tokens to consider.\"},\n",
      "    )\n",
      "    temperature: float = field(\n",
      "        default=1.0,\n",
      "        metadata={\"help\": \"Sampling temperature. Higher values increase diversity.\"},\n",
      "    )\n",
      "\n",
      "    # Deprecated parameters\n",
      "    use_cuda_graph: bool = field(\n",
      "        default=True,\n",
      "        metadata={\"help\": \"[Deprecated] Whether to use CUDA graph optimization.\"},\n",
      "    )\n",
      "    force_cudagraph_recapture: bool = field(\n",
      "        default=True,\n",
      "        metadata={\"help\": \"[Deprecated] Force CUDA graph recapture to release memory.\"},\n",
      "    )\n",
      "    force_no_logits_mask: bool = field(\n",
      "        default=True,\n",
      "        metadata={\n",
      "            \"help\": \"[Deprecated] Disable logits masking (reduces stability but saves memory).\"\n",
      "        },\n",
      "    )\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.temperature == 0.0:\n",
      "            self.greedy = True\n",
      "            self.temperature = 1.0\n",
      "        if self.top_p <= 0.0 or self.top_p > 1:\n",
      "            raise ValueError(\"top_p must be in (0.0, 1.0].\")\n",
      "        if self.top_k <= 0:\n",
      "            raise ValueError(\"top_k must be a positive integer.\")\n",
      "\n",
      "        if self.use_cuda_graph and pkg_version.is_version_less(\"torch\", \"2.3.0\"):\n",
      "            raise ValueError(\n",
      "                f\"To use CUDAGraph, ReaL's PyTorch version should be at least 2.3.0.\"\n",
      "            )\n",
      "\n",
      "    def new(self, **kwargs):\n",
      "        args = asdict(self)\n",
      "        args.update(kwargs)\n",
      "        return GenerationHyperparameters(**args)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class PPOHyperparameters:\n",
      "    \"\"\"Configuration for Proximal Policy Optimization (PPO) training parameters.\"\"\"\n",
      "\n",
      "    # Generation Configuration\n",
      "    gen: GenerationHyperparameters = field(\n",
      "        default_factory=GenerationHyperparameters,\n",
      "        metadata={\"help\": \"Text generation hyperparameters\"},\n",
      "    )\n",
      "\n",
      "    # Core PPO Parameters\n",
      "    ppo_n_minibatches: int = field(\n",
      "        default=4, metadata={\"help\": \"Number of minibatches for each PPO update\"}\n",
      "    )\n",
      "    eps_clip: float = field(\n",
      "        default=0.2, metadata={\"help\": \"Clipping factor for policy ratio\"}\n",
      "    )\n",
      "    c_clip: Optional[float] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Dual clipping factor for policy ratio, must > 1.0. None disables dual clipping.\"\n",
      "        },\n",
      "    )\n",
      "    value_eps_clip: float = field(\n",
      "        default=0.2, metadata={\"help\": \"Clipping factor for value updates\"}\n",
      "    )\n",
      "    early_stop_imp_ratio: float = field(\n",
      "        default=5.0, metadata={\"help\": \"Early stop threshold for importance ratio\"}\n",
      "    )\n",
      "    actor_sample_reuse: int = field(\n",
      "        default=1, metadata={\"help\": \"The data reuse (aka PPO epoch) for actor.\"}\n",
      "    )\n",
      "    critic_sample_reuse: int = field(\n",
      "        default=1, metadata={\"help\": \"The data reuse (aka PPO epoch) for critic.\"}\n",
      "    )\n",
      "\n",
      "    # Reward Processing\n",
      "    max_reward_clip: float = field(\n",
      "        default=20.0, metadata={\"help\": \"Maximum absolute value for clipped rewards\"}\n",
      "    )\n",
      "    reward_output_scaling: float = field(\n",
      "        default=1.0, metadata={\"help\": \"Scaling factor for reward model outputs\"}\n",
      "    )\n",
      "    reward_output_bias: float = field(\n",
      "        default=0.0, metadata={\"help\": \"Bias term for reward model outputs\"}\n",
      "    )\n",
      "    fuse_rew_ref: bool = field(\n",
      "        default=True,\n",
      "        metadata={\"help\": \"Whether to fuse reward and reference model computations\"},\n",
      "    )\n",
      "\n",
      "    # Advantage Estimation\n",
      "    discount: float = field(\n",
      "        default=1.0, metadata={\"help\": \"Discount factor for future rewards\"}\n",
      "    )\n",
      "    gae_lambda: float = field(\n",
      "        default=1.0, metadata={\"help\": \"Lambda parameter for GAE\"}\n",
      "    )\n",
      "    adv_norm: bool = field(\n",
      "        default=True, metadata={\"help\": \"Enable advantage normalization\"}\n",
      "    )\n",
      "\n",
      "    # KL Control\n",
      "    kl_ctl: float = field(default=0.1, metadata={\"help\": \"KL divergence coefficient\"})\n",
      "    use_adaptive_kl_ctl: bool = field(\n",
      "        default=False, metadata={\"help\": \"Use adaptive KL coefficient control\"}\n",
      "    )\n",
      "\n",
      "    # Value Function Configuration\n",
      "    disable_value: bool = field(\n",
      "        default=False, metadata={\"help\": \"Disable value/critic model\"}\n",
      "    )\n",
      "    value_norm: bool = field(\n",
      "        default=True, metadata={\"help\": \"Enable value normalization\"}\n",
      "    )\n",
      "    value_norm_type: str = field(\n",
      "        default=\"exp\",\n",
      "        metadata={\"help\": \"Type of value normalization\", \"choices\": [\"exp\", \"ma\"]},\n",
      "    )\n",
      "    value_norm_beta: float = field(\n",
      "        default=0.99995,\n",
      "        metadata={\"help\": \"Decay factor for exponential moving average\"},\n",
      "    )\n",
      "    value_norm_eps: float = field(\n",
      "        default=1e-5, metadata={\"help\": \"Epsilon term for numerical stability\"}\n",
      "    )\n",
      "    recompute_logprob: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Recompute logp and replace the logp returned by inference.\"},\n",
      "    )\n",
      "    use_decoupled_loss: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Use the decoupled loss. recompute_logprob must be True.\"},\n",
      "    )\n",
      "    behav_imp_weight_cap: Optional[float] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"We filter out the tokens where behav_imp_weight exceeds behav_imp_weight_cap when computing the loss, must be > 1.0, use_decoupled_loss must be true\"\n",
      "        },\n",
      "    )\n",
      "\n",
      "\n",
      "## Experiment utilities. ##\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ExperimentSaveEvalControl:\n",
      "    \"\"\"Controls the frequency of model saving and evaluation during training.\n",
      "\n",
      "    Manages independent counters for epochs, steps, and seconds. The model will be saved\n",
      "    or evaluated when any specified frequency condition is met.\n",
      "\n",
      "    Note:\n",
      "        - Epoch: Number of full passes through the training dataset\n",
      "        - Step: Number of individual training iterations\n",
      "        - Seconds: Wall-clock time duration\n",
      "    \"\"\"\n",
      "\n",
      "    total_train_epochs: int = field(\n",
      "        default=1, metadata={\"help\": \"Total number of epochs to train the model.\"}\n",
      "    )\n",
      "    # Save control\n",
      "    save_freq_epochs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Save frequency in epochs. None disables epoch-based saving.\"\n",
      "        },\n",
      "    )\n",
      "    save_freq_steps: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\"help\": \"Save frequency in steps. None disables step-based saving.\"},\n",
      "    )\n",
      "    save_freq_secs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Save frequency in seconds. None disables time-based saving.\"\n",
      "        },\n",
      "    )\n",
      "    # Checkpointing control\n",
      "    ckpt_freq_epochs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Checkpoint frequency in epochs. None uses save_freq_epochs. \"\n",
      "            \"Checkpointing is used for recover. Previous checkpoint is overwritten to save space.\"\n",
      "        },\n",
      "    )\n",
      "    ckpt_freq_steps: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Checkpoint frequency in steps. None disables step-based checkpointing.\"\n",
      "        },\n",
      "    )\n",
      "    ckpt_freq_secs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Checkpoint frequency in seconds. None disables time-based checkpointing.\"\n",
      "        },\n",
      "    )\n",
      "    # Evaluation control\n",
      "    eval_freq_epochs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Evaluation frequency in epochs. None disables epoch-based evaluation.\"\n",
      "        },\n",
      "    )\n",
      "    eval_freq_steps: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Evaluation frequency in steps. None disables step-based evaluation.\"\n",
      "        },\n",
      "    )\n",
      "    eval_freq_secs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Evaluation frequency in seconds. None disables time-based evaluation.\"\n",
      "        },\n",
      "    )\n",
      "    # Benchmark control\n",
      "    benchmark_steps: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Terminate training after this number of steps. \"\n",
      "            \"For benchmarking purposes only. None indicates normal training.\"\n",
      "        },\n",
      "    )\n",
      "    benchmark_n_seqs: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Terminate training after consuming this number of samples. \"\n",
      "            \"For benchmarking purposes only. None indicates normal training.\"\n",
      "        },\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class AutomaticEvaluator:\n",
      "    \"\"\"Configuration for automatic model evaluation during training.\n",
      "\n",
      "    Controls how and when evaluation jobs are launched to assess model performance\n",
      "    on specified datasets.\n",
      "    \"\"\"\n",
      "\n",
      "    data_names: str = field(\n",
      "        default=\"aime24\",\n",
      "        metadata={\n",
      "            \"help\": \"Comma-separated dataset names for evaluation. \"\n",
      "            \"Supported datasets: 'aime24', 'amc23', 'math_500'.\"\n",
      "        },\n",
      "    )\n",
      "    max_gen_tokens: int = field(\n",
      "        default=32768,\n",
      "        metadata={\"help\": \"Maximum number of tokens to generate during evaluation.\"},\n",
      "    )\n",
      "    max_concurrent_jobs: int = field(\n",
      "        default=3,\n",
      "        metadata={\n",
      "            \"help\": \"Maximum number of concurrent evaluation jobs. \"\n",
      "            \"New jobs wait when this limit is reached.\"\n",
      "        },\n",
      "    )\n",
      "    eval_job_image: Optional[str] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Container image for evaluation jobs. \"\n",
      "            \"None uses the training GPU image.\"\n",
      "        },\n",
      "    )\n",
      "    initial_checkpoint_path: Optional[str] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Initial checkpoint to evaluate. \"\n",
      "            \"Results stored as global_step=0 if specified.\"\n",
      "        },\n",
      "    )\n",
      "    prompt_type: str = field(\n",
      "        default=\"deepscaler\",\n",
      "        metadata={\"help\": \"Prompt format to use during evaluation.\"},\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class WandBConfig:\n",
      "    mode: str = \"disabled\"\n",
      "    entity: Optional[str] = None\n",
      "    project: Optional[str] = None\n",
      "    name: Optional[str] = None\n",
      "    job_type: Optional[str] = None\n",
      "    group: Optional[str] = None\n",
      "    notes: Optional[str] = None\n",
      "    tags: Optional[List[str]] = None\n",
      "    config: Optional[Dict] = None\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class TensorBoardConfig:\n",
      "    path: Optional[str] = None\n",
      "\n",
      "\n",
      "def get_user_tmp():\n",
      "    user = getpass.getuser()\n",
      "    user_tmp = os.path.join(\"/home\", user, \".cache\", \"realhf\")\n",
      "    os.makedirs(user_tmp, exist_ok=True)\n",
      "    return user_tmp\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ClusterSpecConfig:\n",
      "    config_path: str = field(\n",
      "        default=\"\",\n",
      "        metadata={\n",
      "            \"help\": \"JSON config path. If not given, use the following CLI args.\"\n",
      "        },\n",
      "    )\n",
      "    cluster_name: str = field(\n",
      "        default=\"local\",\n",
      "        metadata={\"help\": \"Name of the cluster. Used to set specific environs.\"},\n",
      "    )\n",
      "    fileroot: str = field(\n",
      "        default=get_user_tmp(),\n",
      "        metadata={\n",
      "            \"help\": \"Root for logs and checkpoints. Should be available to all nodes.\"\n",
      "        },\n",
      "    )\n",
      "    gpu_type: str = field(\n",
      "        default=\"tesla\", metadata={\"help\": \"GPU type of the cluster. Used by slurm.\"}\n",
      "    )\n",
      "    mount: str = field(\n",
      "        default=\"/storage:/storage\", metadata={\"help\": \"Mount path for slurm.\"}\n",
      "    )\n",
      "    gpu_image: str = field(default=\"\", metadata={\"help\": \"slurm image for trainers.\"})\n",
      "    cpu_image: str = field(default=\"\", metadata={\"help\": \"slurm image for CPU jobs.\"})\n",
      "    gpu_infer_image: str = field(\n",
      "        default=\"\", metadata={\"help\": \"slurm image for LLM inference.\"}\n",
      "    )\n",
      "    node_name_prefix: str = field(\n",
      "        default=\"slurmd-\", metadata={\"help\": \"Node prefix for a slurm cluster.\"}\n",
      "    )\n",
      "    n_nodes: int = field(\n",
      "        default=32,\n",
      "        metadata={\n",
      "            \"help\": \"The size of the cluster. Used to decide slurm hostname suffix.\"\n",
      "        },\n",
      "    )\n",
      "    n_gpus_per_node: int = field(\n",
      "        default=8,\n",
      "        metadata={\"help\": \"GPUs per node (physically).\"},\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class BaseExperimentConfig:\n",
      "    \"\"\"Configuration for quickstart experiments.\n",
      "\n",
      "    All parameters can be modified via command line arguments. Supports various\n",
      "    recovery modes and parallelization strategies.\n",
      "\n",
      "    Note:\n",
      "        - Recovery modes: auto, fault, resume, disabled\n",
      "        - Allocation modes: manual, heuristic, or pattern-based\n",
      "    \"\"\"\n",
      "\n",
      "    experiment_name: str = field(\n",
      "        default=MISSING,\n",
      "        metadata={\"help\": \"Name of the experiment (no '_' or '/'). Required.\"},\n",
      "    )\n",
      "    trial_name: str = field(\n",
      "        default=MISSING,\n",
      "        metadata={\"help\": \"Name of the trial (no '-' or '/'). Required.\"},\n",
      "    )\n",
      "    mode: str = field(\n",
      "        default=\"slurm\",\n",
      "        metadata={\n",
      "            \"help\": \"Experiment launching mode.\",\n",
      "            \"choices\": [\"slurm\", \"local\", \"ray\"],\n",
      "        },\n",
      "    )\n",
      "    debug: bool = field(\n",
      "        default=True,\n",
      "        metadata={\n",
      "            \"help\": \"Debug mode. False disables assertions for better performance.\"\n",
      "        },\n",
      "    )\n",
      "    metric_discovery_port: int = field(\n",
      "        default=0,\n",
      "        metadata={\"help\": \"Discovery port for prometheus metrics service discovery.\"},\n",
      "    )\n",
      "    partition: str = field(\n",
      "        default=\"dev\", metadata={\"help\": \"SLURM partition for running the experiment.\"}\n",
      "    )\n",
      "    schedule_strategy: str = field(\n",
      "        default=\"empty_first\", metadata={\"help\": \"Resource scheduling strategy.\"}\n",
      "    )\n",
      "    wandb: WandBConfig = field(\n",
      "        default_factory=WandBConfig,\n",
      "        metadata={\"help\": \"Weights & Biases configuration.\"},\n",
      "    )\n",
      "    tensorboard: TensorBoardConfig = field(\n",
      "        default_factory=TensorBoardConfig,\n",
      "        metadata={\"help\": \"TensorBoard configuration. Only 'path' field required.\"},\n",
      "    )\n",
      "    recover_mode: str = field(\n",
      "        default=\"disabled\",\n",
      "        metadata={\n",
      "            \"help\": \"Recovery mode (auto/fault/resume/disabled). \"\n",
      "            \"Use 'disabled' if unfamiliar with recovery mechanism.\"\n",
      "        },\n",
      "    )\n",
      "    recover_retries: int = field(\n",
      "        default=1,\n",
      "        metadata={\"help\": \"Number of recovery retries (auto/fault modes only).\"},\n",
      "    )\n",
      "    recover_after: int = field(\n",
      "        default=10,\n",
      "        metadata={\"help\": \"Recovery interval in seconds (auto/fault modes only).\"},\n",
      "    )\n",
      "    ignore_worker_error: bool = field(\n",
      "        default=False,\n",
      "        metadata={\n",
      "            \"help\": \"Ignore worker runtime errors (disabled mode only). \"\n",
      "            \"Only enable if certain errors can be safely ignored.\"\n",
      "        },\n",
      "    )\n",
      "    allocation_mode: str = field(\n",
      "        default=\"\",\n",
      "        metadata={\n",
      "            \"help\": \"GPU parallel strategy allocation mode. \"\n",
      "            \"Options: manual/heuristic or pattern-based.\"\n",
      "        },\n",
      "    )\n",
      "    n_nodes: int = field(\n",
      "        default=1, metadata={\"help\": \"Number of nodes for experiment.\"}\n",
      "    )\n",
      "    n_gpus_per_node: int = field(\n",
      "        default=8, metadata={\"help\": \"Number of GPUs per node for this experiment.\"}\n",
      "    )\n",
      "    nodelist: Optional[str] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"SLURM nodelist for manual allocation. \"\n",
      "            \"Format: 'slurmd-01:0,1,2,3' or 'slurmd-[01-02,03,07],COM08'.\"\n",
      "        },\n",
      "    )\n",
      "    exclude: Optional[str] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"SLURM nodelist to exclude from allocation. \"\n",
      "            \"Format: 'slurmd-01:0,1,2,3' or 'slurmd-[01-02,03,07],COM08'.\"\n",
      "        },\n",
      "    )\n",
      "    seed: int = field(default=1, metadata={\"help\": \"Random seed for reproducibility.\"})\n",
      "    cache_clear_freq: Optional[int] = field(\n",
      "        default=10,\n",
      "        metadata={\n",
      "            \"help\": \"Clear data transfer cache every N steps. \"\n",
      "            \"Set lower if OOM occurs. None disables clearing.\"\n",
      "        },\n",
      "    )\n",
      "    exp_ctrl: ExperimentSaveEvalControl = field(\n",
      "        default_factory=ExperimentSaveEvalControl,\n",
      "        metadata={\"help\": \"Experiment save/evaluation control configuration.\"},\n",
      "    )\n",
      "    torch_cache_mysophobia: bool = field(\n",
      "        default=True,\n",
      "        metadata={\n",
      "            \"help\": \"Clear torch cache before each RPC (~0.1s overhead per RPC).\"\n",
      "        },\n",
      "    )\n",
      "    auto_eval: bool = field(\n",
      "        default=False,\n",
      "        metadata={\n",
      "            \"help\": \"Enable automatic evaluation during training. \"\n",
      "            \"Results logged to disk and WandB (if active).\"\n",
      "        },\n",
      "    )\n",
      "    auto_eval_config: AutomaticEvaluator = field(\n",
      "        default_factory=AutomaticEvaluator,\n",
      "        metadata={\"help\": \"Automatic evaluation configuration.\"},\n",
      "    )\n",
      "    cpus_per_master_worker: int = field(\n",
      "        default=4, metadata={\"help\": \"CPU cores per master worker.\"}\n",
      "    )\n",
      "    mem_per_master_worker: int = field(\n",
      "        default=20000, metadata={\"help\": \"Memory per master worker (MB).\"}\n",
      "    )\n",
      "    cpus_per_model_worker: int = field(\n",
      "        default=4, metadata={\"help\": \"CPU cores per model worker.\"}\n",
      "    )\n",
      "    mem_per_model_worker: int = field(\n",
      "        default=90000, metadata={\"help\": \"Memory per model worker (MB).\"}\n",
      "    )\n",
      "    shuffle_dataset: bool = field(\n",
      "        default=True, metadata={\"help\": \"Shuffle in each epoch.\"}\n",
      "    )\n",
      "    ray_temp_path: str = field(\n",
      "        default=\"/tmp/ray\", metadata={\"help\": \"Absolute path for Ray's log.\"}\n",
      "    )\n",
      "    cluster: ClusterSpecConfig = field(\n",
      "        default_factory=ClusterSpecConfig,\n",
      "        metadata={\"help\": \"Cluster specification. Mainly used by slurm.\"},\n",
      "    )\n",
      "\n",
      "\n",
      "## Configuration options of asynchronous experiments. ##\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class AsyncRLOptions:\n",
      "    schedule_policy: str = field(\n",
      "        default=\"round_robin\",\n",
      "        metadata={\n",
      "            \"help\": \"The request schedule policy during generation. Available options: [round_robin].\"\n",
      "        },\n",
      "    )\n",
      "    new_tokens_per_chunk: int = field(\n",
      "        default=int(1e10),\n",
      "        metadata={\n",
      "            \"help\": \"The length of chunked generation. Only valid if inference can't be interrupted.\"\n",
      "        },\n",
      "    )\n",
      "    max_head_offpolicyness: int = field(\n",
      "        default=0,\n",
      "        metadata={\"help\": \"Maximum off-policyness tolerance for the first token.\"},\n",
      "    )\n",
      "\n",
      "    n_rollout_workers: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Number of rollout workers. None defaults to train world size.\"\n",
      "        },\n",
      "    )\n",
      "    max_concurrent_rollouts: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Max concurrent rollouts globally. Defaults to train batch size.\"\n",
      "        },\n",
      "    )\n",
      "    flush_request_timeout: int = field(\n",
      "        default=300,\n",
      "        metadata={\"help\": \"The timeout of flushing requests upon weight update.\"},\n",
      "    )\n",
      "\n",
      "    cpus_per_generation_server: int = field(\n",
      "        default=4, metadata={\"help\": \"Generation server CPUs.\"}\n",
      "    )\n",
      "    mem_per_generation_server: int = field(\n",
      "        default=60 * 1024, metadata={\"help\": \"Generation server CPU memory in MB.\"}\n",
      "    )\n",
      "    cpus_per_gserver_manager: int = field(\n",
      "        default=4, metadata={\"help\": \"Generation manager CPUs.\"}\n",
      "    )\n",
      "    mem_per_gserver_manager: int = field(\n",
      "        default=10 * 1024, metadata={\"help\": \"Generation manager CPU memory in MB.\"}\n",
      "    )\n",
      "    cpus_per_rollout_worker: int = field(\n",
      "        default=4, metadata={\"help\": \"Rollout worker CPUs.\"}\n",
      "    )\n",
      "    mem_per_rollout_worker: int = field(\n",
      "        default=20 * 1024, metadata={\"help\": \"Rollout worker CPU memory in MB.\"}\n",
      "    )\n",
      "\n",
      "\n",
      "## Configurations for practical experiments. ##\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class NullPPOExperimentOptions:\n",
      "    \"\"\"Configuration for a null PPO experiment (testing purposes only).\"\"\"\n",
      "\n",
      "    model: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Model configuration for testing.\"},\n",
      "    )\n",
      "    inf: MFCConfig = field(\n",
      "        default_factory=MFCConfig,\n",
      "        metadata={\"help\": \"Inference model function call configuration.\"},\n",
      "    )\n",
      "    train: MFCConfig = field(\n",
      "        default_factory=MFCConfig,\n",
      "        metadata={\"help\": \"Training model function call configuration.\"},\n",
      "    )\n",
      "    dataset: PromptOnlyDatasetConfig = field(\n",
      "        default_factory=PromptOnlyDatasetConfig,\n",
      "        metadata={\"help\": \"Dataset configuration for testing.\"},\n",
      "    )\n",
      "    dataset_filter_threshold: float = field(\n",
      "        default=0.2,\n",
      "        metadata={\"help\": \"Threshold value for dataset filtering in tests.\"},\n",
      "    )\n",
      "    dataset_max_filter_percentage: float = field(\n",
      "        default=0.1,\n",
      "        metadata={\"help\": \"Maximum percentage of dataset to filter in tests.\"},\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class SFTExperimentOptions:\n",
      "    \"\"\"Configuration for supervised fine-tuning (SFT) experiments.\"\"\"\n",
      "\n",
      "    model: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Model runtime configuration.\"},\n",
      "    )\n",
      "    allocation: MFCConfig = field(\n",
      "        default_factory=MFCConfig,\n",
      "        metadata={\"help\": \"Device allocation and parallelism configuration.\"},\n",
      "    )\n",
      "    dataset: PromptAnswerDatasetConfig = field(\n",
      "        default_factory=PromptAnswerDatasetConfig,\n",
      "        metadata={\"help\": \"Dataset configuration.\"},\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class PPOMATHExperimentOptions:\n",
      "    \"\"\"Configuration for PPO (Proximal Policy Optimization) experiments.\n",
      "\n",
      "    Manages four distinct models and their interactions through model function calls.\n",
      "\n",
      "    Note:\n",
      "        Models:\n",
      "        - Actor: Primary LLM for text generation\n",
      "        - Critic: Value function estimator\n",
      "        - Ref: Reference model for KL regularization\n",
      "        - Rew: Reward model (or function) for reward signals\n",
      "    \"\"\"\n",
      "\n",
      "    # Model configurations\n",
      "    actor: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Primary LLM configuration.\"},\n",
      "    )\n",
      "    critic: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Critic model configuration.\"},\n",
      "    )\n",
      "    ref: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Reference model configuration.\"},\n",
      "    )\n",
      "    rew: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Reward model configuration.\"},\n",
      "    )\n",
      "\n",
      "    # Model function call configurations\n",
      "    actor_train: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"TrainActor MFC configuration.\"}\n",
      "    )\n",
      "    critic_train: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"TrainCritic MFC configuration.\"}\n",
      "    )\n",
      "    actor_gen: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"Rollout MFC configuration.\"}\n",
      "    )\n",
      "    critic_inf: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"InfValues MFC configuration.\"}\n",
      "    )\n",
      "    rew_inf: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"InfReward MFC configuration.\"}\n",
      "    )\n",
      "    ref_inf: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"InfRef MFC configuration.\"}\n",
      "    )\n",
      "    actor_inf: MFCConfig = field(\n",
      "        default_factory=MFCConfig,\n",
      "        metadata={\"help\": \"Actor inference MFC configuration.\"},\n",
      "    )\n",
      "\n",
      "    # Dataset and algorithm configurations\n",
      "    dataset: PromptOnlyDatasetConfig = field(\n",
      "        default_factory=PromptOnlyDatasetConfig,\n",
      "        metadata={\"help\": \"Dataset configuration.\"},\n",
      "    )\n",
      "    ppo: PPOHyperparameters = field(\n",
      "        default_factory=PPOHyperparameters,\n",
      "        metadata={\"help\": \"PPO algorithm hyperparameters.\"},\n",
      "    )\n",
      "\n",
      "    # Sampling and reward processing\n",
      "    group_size: int = field(\n",
      "        default=1,\n",
      "        metadata={\"help\": \"Number of answers retained per prompt (best-of-n).\"},\n",
      "    )\n",
      "    generation_size: Optional[int] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"Number of answers sampled per prompt. None uses group_size.\"\n",
      "        },\n",
      "    )\n",
      "    mask_no_eos_with_zero: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Mask reward for truncated answers (no EOS token).\"},\n",
      "    )\n",
      "    mask_too_long: bool = field(\n",
      "        default=False, metadata={\"help\": \"Mask PPO loss for length-truncated answers.\"}\n",
      "    )\n",
      "    check_verifier_status: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Raise error if reward is all-zero (verifier bug check).\"},\n",
      "    )\n",
      "    group_adv_norm: bool = field(\n",
      "        default=False, metadata={\"help\": \"Use grouped advantage normalization in GRPO.\"}\n",
      "    )\n",
      "    ref_ema_eta: Optional[float] = field(\n",
      "        default=None,\n",
      "        metadata={\n",
      "            \"help\": \"EMA decay rate for reference model updates. 1.0 means full update.\"\n",
      "        },\n",
      "    )\n",
      "    rw_type: Optional[str] = field(\n",
      "        default=\"sparse\",\n",
      "        metadata={\n",
      "            \"help\": \"Type of reward processing. Only `sparse` is valid for now.\",\n",
      "            \"choices\": [\"sparse\"],\n",
      "        },\n",
      "    )\n",
      "    check_xml_format: bool = field(\n",
      "        default=False, metadata={\"help\": \"Validate XML format in generated responses.\"}\n",
      "    )\n",
      "\n",
      "    # Dataset filtering\n",
      "    dataset_filter_threshold: float = field(\n",
      "        default=100.0,\n",
      "        metadata={\n",
      "            \"help\": \"Rewards higher than this value will be filtered out after each epoch's training.\"\n",
      "        },\n",
      "    )\n",
      "    dataset_max_filter_percentage: float = field(\n",
      "        default=0.0, metadata={\"help\": \"Maximum percentage of dataset to each filter.\"}\n",
      "    )\n",
      "\n",
      "    success_rate_ub: float = field(\n",
      "        default=1.0,\n",
      "        metadata={\n",
      "            \"help\": \"Success rate higher than this value will be filtered out after generation. Valid for async training.\"\n",
      "        },\n",
      "    )\n",
      "    success_rate_lb: float = field(\n",
      "        default=0.0,\n",
      "        metadata={\n",
      "            \"help\": \"Success rate lower than this value will be filtered out after generation. Valid for async training.\"\n",
      "        },\n",
      "    )\n",
      "\n",
      "    # testing only\n",
      "    no_training: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Run without training. Test-only.\"},\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class MathCodeEvalOptions:\n",
      "    gen_config: GenerationHyperparameters = field(\n",
      "        default_factory=GenerationHyperparameters\n",
      "    )\n",
      "\n",
      "    actor: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Primary LLM configuration.\"},\n",
      "    )\n",
      "    rew: ModelTrainEvalConfig = field(\n",
      "        default_factory=ModelTrainEvalConfig,\n",
      "        metadata={\"help\": \"Reward model configuration.\"},\n",
      "    )\n",
      "\n",
      "    actor_gen: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"Rollout MFC configuration.\"}\n",
      "    )\n",
      "    rew_inf: MFCConfig = field(\n",
      "        default_factory=MFCConfig, metadata={\"help\": \"InfReward MFC configuration.\"}\n",
      "    )\n",
      "\n",
      "    dataset: PromptOnlyDatasetConfig = field(\n",
      "        default_factory=PromptOnlyDatasetConfig,\n",
      "        metadata={\"help\": \"Dataset configuration.\"},\n",
      "    )\n",
      "\n",
      "    group_size: int = field(\n",
      "        default=1,\n",
      "        metadata={\"help\": \"Number of answers retained per prompt (best-of-n).\"},\n",
      "    )\n",
      "    rw_type: Optional[str] = field(\n",
      "        default=\"sparse\",\n",
      "        metadata={\n",
      "            \"help\": \"Type of reward processing. Only `sparse` is valid for now.\",\n",
      "            \"choices\": [\"sparse\"],\n",
      "        },\n",
      "    )\n",
      "    check_xml_format: bool = field(\n",
      "        default=False, metadata={\"help\": \"Validate XML format in generated responses.\"}\n",
      "    )\n",
      "\n",
      "    check_verifier_status: bool = field(\n",
      "        default=False,\n",
      "        metadata={\"help\": \"Raise error if reward is all-zero (verifier bug check).\"},\n",
      "    )\n",
      "\n",
      "\n",
      "## A helper function to visualize the helper messages. ##\n",
      "from rich.console import Console\n",
      "from rich.highlighter import RegexHighlighter\n",
      "from rich.panel import Panel\n",
      "from rich.rule import Rule\n",
      "from rich.theme import Theme\n",
      "\n",
      "# Custom theme for colors\n",
      "help_theme = Theme(\n",
      "    {\n",
      "        \"title\": \"bold cyan\",\n",
      "        \"header\": \"bold green\",\n",
      "        \"field\": \"bold yellow\",\n",
      "        \"type\": \"italic blue\",\n",
      "        \"help\": \"dim white\",\n",
      "        \"default\": \"dim green\",\n",
      "        \"example\": \"italic cyan\",\n",
      "        \"border\": \"dim blue\",\n",
      "    }\n",
      ")\n",
      "\n",
      "console = Console(theme=help_theme)\n",
      "\n",
      "\n",
      "class CliHighlighter(RegexHighlighter):\n",
      "    base_style = \"example.\"\n",
      "    highlights = [r\"(python -m .+?)(?=\\s|$)\"]\n",
      "\n",
      "\n",
      "highlighter = CliHighlighter()\n",
      "\n",
      "\n",
      "def print_config_help(\n",
      "    config, prefix: str = \"\", parent_name: str = \"\", indent: int = 0\n",
      ") -> None:\n",
      "    \"\"\"Prints help for a structured config with proper indentation and smart default display\"\"\"\n",
      "    if not is_dataclass(config):\n",
      "        return\n",
      "\n",
      "    for field in fields(config):\n",
      "        value = getattr(config, field.name)\n",
      "        full_name = f\"{parent_name}.{field.name}\" if parent_name else field.name\n",
      "        indent_space = \"    \" * indent\n",
      "\n",
      "        # Field type handling\n",
      "        type_name = (\n",
      "            field.type.__name__ if isinstance(field.type, Type) else str(field.type)\n",
      "        )\n",
      "\n",
      "        # Create help text components\n",
      "        help_parts = []\n",
      "        if \"help\" in field.metadata:\n",
      "            help_parts.append(field.metadata[\"help\"])\n",
      "\n",
      "        # Only show default for leaf nodes (non-dataclass fields)\n",
      "        if not is_dataclass(value):\n",
      "            default_value = field.default if hasattr(field, \"default\") else MISSING\n",
      "            help_parts.append(f\"[default]Default: {default_value}[/default]\")\n",
      "\n",
      "        # Print the field info\n",
      "        console.print(f\"{indent_space}[field]{full_name}[/field]\", end=\" \")\n",
      "        console.print(f\"[type]({type_name})[/type]\", end=\" \")\n",
      "        if help_parts:\n",
      "            console.print(\"- \" + \" \".join(help_parts))\n",
      "        else:\n",
      "            console.print()\n",
      "\n",
      "        # Handle nested dataclasses with increased indentation\n",
      "        if is_dataclass(value):\n",
      "            print_config_help(value, prefix, full_name, indent + 1)\n",
      "\n",
      "\n",
      "def print_config_values(\n",
      "    config,\n",
      "    prefix: str = \"\",\n",
      "    parent_name: str = \"\",\n",
      "    indent: int = 0,\n",
      "    show_types: bool = True,\n",
      ") -> None:\n",
      "    \"\"\"Prints current values with clean indentation and subtle separation\"\"\"\n",
      "    console.print()  # Add space before\n",
      "\n",
      "    top_rule = Rule(\"Current Configuration Begin\", style=\"bold cyan\", align=\"center\")\n",
      "    bottom_rule = Rule(\"Current Configuration End\", style=\"bold cyan\", align=\"center\")\n",
      "\n",
      "    # Title with subtle underline\n",
      "    console.print(top_rule)\n",
      "\n",
      "    # Print config directly to main console\n",
      "    _print_config_values_internal(\n",
      "        console, config, prefix, parent_name, indent, show_types\n",
      "    )\n",
      "\n",
      "    # Closing rule\n",
      "    console.print(bottom_rule)\n",
      "    console.print()  # Add space after\n",
      "\n",
      "\n",
      "def _print_config_values_internal(\n",
      "    console: Console,\n",
      "    config,\n",
      "    prefix: str,\n",
      "    parent_name: str,\n",
      "    indent: int,\n",
      "    show_types: bool,\n",
      ") -> None:\n",
      "    \"\"\"Internal recursive function that does the actual printing\"\"\"\n",
      "    if not is_dataclass(config):\n",
      "        return\n",
      "\n",
      "    for field in fields(config):\n",
      "        value = getattr(config, field.name)\n",
      "        full_name = f\"{parent_name}.{field.name}\" if parent_name else field.name\n",
      "        indent_space = \"    \" * indent\n",
      "\n",
      "        # Field type handling\n",
      "        type_name = (\n",
      "            field.type.__name__ if isinstance(field.type, Type) else str(field.type)\n",
      "        )\n",
      "\n",
      "        # Create help text components\n",
      "        help_parts = []\n",
      "\n",
      "        # Print the field info\n",
      "        console.print(f\"{indent_space}[field]{full_name}[/field]\", end=\" \")\n",
      "        if show_types:\n",
      "            console.print(f\"[type]({type_name})[/type]\", end=\" \")\n",
      "\n",
      "        # Always show current value\n",
      "        value_str = str(value)\n",
      "        if isinstance(value, (list, dict)):\n",
      "            value_str = f\"{type(value).__name__}(len={len(value)})\"\n",
      "        if not is_dataclass(value):\n",
      "            help_parts.append(f\"[value]{value_str}[/value]\")\n",
      "\n",
      "        if help_parts:\n",
      "            console.print(\"- \" + \" \".join(help_parts))\n",
      "        else:\n",
      "            console.print()\n",
      "\n",
      "        # Handle nested dataclasses\n",
      "        if is_dataclass(value):\n",
      "            _print_config_values_internal(\n",
      "                console, value, prefix, full_name, indent + 1, show_types\n",
      "            )\n",
      "\n",
      "\n",
      "def print_runtime_helper(args):\n",
      "    \"\"\"Print comprehensive help with rich formatting\"\"\"\n",
      "\n",
      "    exp_type = args.__class__.__name__\n",
      "    # Main help panel\n",
      "    console.print(\n",
      "        Panel.fit(\n",
      "            f\"[header]Setting {exp_type} with the Following Values[/header]\",\n",
      "            border_style=\"border\",\n",
      "        ),\n",
      "        justify=\"center\",\n",
      "    )\n",
      "\n",
      "    # Configuration options section\n",
      "    print_config_values(args)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/gemma.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, register_hf_family\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "from .llama import (\n",
      "    convert_state_dict_llama,\n",
      "    llama_embedding_layer_names,\n",
      "    llama_output_head_param_name,\n",
      "    llama_transformer_block_param_name,\n",
      "    to_llama_state_dict,\n",
      ")\n",
      "\n",
      "\n",
      "def convert_config_gemma(\n",
      "    hf_config: transformers.GemmaConfig,\n",
      ") -> ReaLModelConfig:\n",
      "    if hf_config.hidden_activation is None:\n",
      "        act = \"gelu_pytorch_tanh\"\n",
      "    else:\n",
      "        act = hf_config.hidden_activation\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.num_hidden_layers,\n",
      "        n_kv_heads=hf_config.num_key_value_heads,\n",
      "        hidden_dim=hf_config.hidden_size,\n",
      "        n_q_heads=hf_config.num_attention_heads,\n",
      "        head_dim=hf_config.head_dim,\n",
      "        intermediate_dim=hf_config.intermediate_size,\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        n_positions=hf_config.max_position_embeddings,\n",
      "        embd_pdrop=0.0,\n",
      "        attn_pdrop=(\n",
      "            hf_config.attention_dropout\n",
      "            if hasattr(hf_config, \"attention_dropout\")\n",
      "            else 0.1\n",
      "        ),\n",
      "        layer_norm_epsilon=hf_config.rms_norm_eps,\n",
      "        activation_function=act,  # NOTE: here is different than LLaMA\n",
      "        use_attention_bias=hf_config.attention_bias,\n",
      "        use_attn_proj_bias=hf_config.attention_bias,\n",
      "        scale_attn_by_inverse_layer_idx=False,\n",
      "        layer_norm_type=\"gemma\",\n",
      "        mlp_type=\"llama\",\n",
      "        apply_rotary=True,\n",
      "        rotary_base=hf_config.rope_theta,\n",
      "        rotary_interleaved=False,\n",
      "        tied_embedding=hf_config.tie_word_embeddings,\n",
      "        normalize_embed=True,\n",
      "    )\n",
      "\n",
      "\n",
      "def convert_config_back_gemma(\n",
      "    config: ReaLModelConfig,\n",
      ") -> transformers.GemmaConfig:\n",
      "    return transformers.GemmaConfig(\n",
      "        vocab_size=config.vocab_size,\n",
      "        hidden_size=config.hidden_dim,\n",
      "        intermediate_size=config.intermediate_dim,\n",
      "        num_hidden_layers=config.n_layers,\n",
      "        num_key_value_heads=config.n_kv_heads,\n",
      "        num_attention_heads=config.n_q_heads,\n",
      "        head_dim=config.head_dim,\n",
      "        max_position_embeddings=config.n_positions,\n",
      "        rms_norm_eps=config.layer_norm_epsilon,\n",
      "        hidden_act=config.activation_function,\n",
      "        hidden_activation=config.activation_function,\n",
      "        attention_bias=config.use_attention_bias,\n",
      "        attention_dropout=config.attn_pdrop,\n",
      "        rope_theta=config.rotary_base,\n",
      "        tie_word_embeddings=config.tied_embedding,\n",
      "        architectures=[\"GemmaForCausalLM\"],\n",
      "    )\n",
      "\n",
      "\n",
      "def gemma_config_maker() -> ReaLModelConfig:\n",
      "    hf_config = transformers.GemmaConfig(\n",
      "        attention_bias=False,\n",
      "        hidden_act=\"gelu\",\n",
      "        hidden_size=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        intermediate_size=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        max_position_embeddings=TESTING_MODEL_N_POSITIONS,\n",
      "        num_attention_heads=TESTING_MODEL_N_HEADS,\n",
      "        num_hidden_layers=TESTING_MODEL_N_LAYERS,\n",
      "        num_key_value_heads=4,\n",
      "        head_dim=TESTING_MODEL_HEAD_DIM,\n",
      "        rms_norm_eps=1e-06,\n",
      "        rope_theta=10000.0,\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "    )\n",
      "    return convert_config_gemma(hf_config)\n",
      "\n",
      "\n",
      "register_hf_family(\n",
      "    name=\"gemma\",\n",
      "    hf_cls_name=\"GemmaForCausalLM\",\n",
      "    config_from_hf_converter=convert_config_gemma,\n",
      "    config_to_hf_converter=convert_config_back_gemma,\n",
      "    sd_from_hf_converter=convert_state_dict_llama,\n",
      "    sd_to_hf_converter=to_llama_state_dict,\n",
      "    embedding_param_names=llama_embedding_layer_names,\n",
      "    tblock_param_names=llama_transformer_block_param_name,\n",
      "    head_param_names=llama_output_head_param_name,\n",
      "    real_config_maker=gemma_config_maker,\n",
      ")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/mistral.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, register_hf_family\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "from .llama import (\n",
      "    convert_state_dict_llama,\n",
      "    llama_embedding_layer_names,\n",
      "    llama_output_head_param_name,\n",
      "    llama_transformer_block_param_name,\n",
      "    to_llama_state_dict,\n",
      ")\n",
      "\n",
      "\n",
      "def config_from_mistral(hf_config: transformers.MistralConfig) -> ReaLModelConfig:\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.num_hidden_layers,\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        hidden_dim=hf_config.hidden_size,\n",
      "        n_q_heads=hf_config.num_attention_heads,\n",
      "        n_kv_heads=hf_config.num_key_value_heads,\n",
      "        head_dim=hf_config.hidden_size // hf_config.num_attention_heads,\n",
      "        intermediate_dim=hf_config.intermediate_size,\n",
      "        activation_function=hf_config.hidden_act,\n",
      "        n_positions=hf_config.max_position_embeddings,\n",
      "        layer_norm_epsilon=hf_config.rms_norm_eps,\n",
      "        layer_norm_type=\"rms\",\n",
      "        tied_embedding=hf_config.tie_word_embeddings,\n",
      "        mlp_type=\"llama\",\n",
      "        rotary_base=hf_config.rope_theta,\n",
      "        apply_rotary=True,\n",
      "        attn_pdrop=hf_config.attention_dropout,\n",
      "        resid_pdrop=0.0,\n",
      "        use_attention_bias=False,\n",
      "        use_attn_proj_bias=False,\n",
      "        embd_pdrop=0.0,\n",
      "        sliding_window=hf_config.sliding_window,\n",
      "        scale_attn_by_inverse_layer_idx=False,\n",
      "    )\n",
      "\n",
      "\n",
      "def config_to_mistral(config: ReaLModelConfig) -> transformers.MistralConfig:\n",
      "    return transformers.MistralConfig(\n",
      "        num_hidden_layers=config.n_layers,\n",
      "        vocab_size=config.vocab_size,\n",
      "        hidden_size=config.hidden_dim,\n",
      "        num_attention_heads=config.n_q_heads,\n",
      "        num_key_value_heads=config.n_kv_heads,\n",
      "        intermediate_size=config.intermediate_dim,\n",
      "        hidden_act=config.activation_function,\n",
      "        max_position_embeddings=config.n_positions,\n",
      "        rms_norm_eps=config.layer_norm_epsilon,\n",
      "        tie_word_embeddings=False,\n",
      "        rope_theta=config.rotary_base,\n",
      "        attention_dropout=config.attn_pdrop,\n",
      "        sliding_window=config.sliding_window,\n",
      "        architectures=[\"MistralForCausalLM\"],\n",
      "    )\n",
      "\n",
      "\n",
      "def get_real_config_mistral() -> ReaLModelConfig:\n",
      "    hf_config = transformers.MistralConfig(\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "        max_position_embeddings=TESTING_MODEL_N_POSITIONS,\n",
      "        hidden_size=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        intermediate_size=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        num_hidden_layers=TESTING_MODEL_N_LAYERS,\n",
      "        num_attention_heads=TESTING_MODEL_N_HEADS,\n",
      "        num_key_value_heads=2,\n",
      "    )\n",
      "    return config_from_mistral(hf_config)\n",
      "\n",
      "\n",
      "register_hf_family(\n",
      "    \"mistral\",\n",
      "    \"MistralForCausalLM\",\n",
      "    config_from_hf_converter=config_from_mistral,\n",
      "    config_to_hf_converter=config_to_mistral,\n",
      "    sd_from_hf_converter=convert_state_dict_llama,\n",
      "    sd_to_hf_converter=to_llama_state_dict,\n",
      "    embedding_param_names=llama_embedding_layer_names,\n",
      "    tblock_param_names=llama_transformer_block_param_name,\n",
      "    head_param_names=llama_output_head_param_name,\n",
      "    real_config_maker=get_real_config_mistral,\n",
      ")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/qwen2.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, register_hf_family\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "from .llama import (\n",
      "    convert_state_dict_llama,\n",
      "    llama_embedding_layer_names,\n",
      "    llama_output_head_param_name,\n",
      "    llama_transformer_block_param_name,\n",
      "    to_llama_state_dict,\n",
      ")\n",
      "\n",
      "\n",
      "def convert_config_qwen2(\n",
      "    hf_config: transformers.Qwen2Config,\n",
      ") -> ReaLModelConfig:\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.num_hidden_layers,\n",
      "        n_kv_heads=hf_config.num_key_value_heads,\n",
      "        hidden_dim=hf_config.hidden_size,\n",
      "        n_q_heads=hf_config.num_attention_heads,\n",
      "        head_dim=hf_config.hidden_size // hf_config.num_attention_heads,\n",
      "        intermediate_dim=hf_config.intermediate_size,\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        n_positions=hf_config.max_position_embeddings,\n",
      "        embd_pdrop=0.0,\n",
      "        attn_pdrop=(\n",
      "            hf_config.attention_dropout\n",
      "            if hasattr(hf_config, \"attention_dropout\")\n",
      "            else 0.1\n",
      "        ),\n",
      "        layer_norm_epsilon=hf_config.rms_norm_eps,\n",
      "        activation_function=hf_config.hidden_act,\n",
      "        use_attention_bias=True,\n",
      "        use_attn_proj_bias=False,\n",
      "        scale_attn_by_inverse_layer_idx=False,\n",
      "        layer_norm_type=\"rms\",\n",
      "        mlp_type=\"llama\",\n",
      "        apply_rotary=True,\n",
      "        rotary_base=hf_config.rope_theta,\n",
      "        rotary_interleaved=False,\n",
      "        tied_embedding=hf_config.tie_word_embeddings,\n",
      "    )\n",
      "\n",
      "\n",
      "def convert_config_back_qwen2(\n",
      "    config: ReaLModelConfig,\n",
      ") -> transformers.Qwen2Config:\n",
      "    return transformers.Qwen2Config(\n",
      "        vocab_size=config.vocab_size,\n",
      "        hidden_size=config.hidden_dim,\n",
      "        intermediate_size=config.intermediate_dim,\n",
      "        num_hidden_layers=config.n_layers,\n",
      "        num_key_value_heads=config.n_kv_heads,\n",
      "        num_attention_heads=config.n_q_heads,\n",
      "        max_position_embeddings=config.n_positions,\n",
      "        rms_norm_eps=config.layer_norm_epsilon,\n",
      "        hidden_act=config.activation_function,\n",
      "        attention_dropout=config.attn_pdrop,\n",
      "        rope_theta=config.rotary_base,\n",
      "        architectures=[\"Qwen2ForCausalLM\"],\n",
      "        tie_word_embeddings=config.tied_embedding,\n",
      "    )\n",
      "\n",
      "\n",
      "def qwen2_config_maker():\n",
      "    hf_config = transformers.Qwen2Config(\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "        max_position_embeddings=TESTING_MODEL_N_POSITIONS,\n",
      "        hidden_size=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        intermediate_size=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        num_hidden_layers=TESTING_MODEL_N_LAYERS,\n",
      "        num_attention_heads=TESTING_MODEL_N_HEADS,\n",
      "        num_key_value_heads=8,\n",
      "        hidden_act=\"silu\",\n",
      "        rms_norm_eps=1e-5,\n",
      "    )\n",
      "    return convert_config_qwen2(hf_config)\n",
      "\n",
      "\n",
      "register_hf_family(\n",
      "    name=\"qwen2\",\n",
      "    hf_cls_name=\"Qwen2ForCausalLM\",\n",
      "    config_from_hf_converter=convert_config_qwen2,\n",
      "    config_to_hf_converter=convert_config_back_qwen2,\n",
      "    sd_from_hf_converter=convert_state_dict_llama,\n",
      "    sd_to_hf_converter=to_llama_state_dict,\n",
      "    embedding_param_names=llama_embedding_layer_names,\n",
      "    tblock_param_names=llama_transformer_block_param_name,\n",
      "    head_param_names=llama_output_head_param_name,\n",
      "    real_config_maker=qwen2_config_maker,\n",
      ")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/mixtral.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, ReaLMoEConfig, register_hf_family\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "from .llama import llama_embedding_layer_names, llama_output_head_param_name\n",
      "\n",
      "\n",
      "def config_from_mixtral(hf_config: transformers.MixtralConfig) -> ReaLModelConfig:\n",
      "    moe = ReaLMoEConfig(\n",
      "        num_experts=hf_config.num_local_experts,\n",
      "        top_k=hf_config.num_experts_per_tok,\n",
      "        aux_loss_coeff=hf_config.router_aux_loss_coef,\n",
      "        input_jitter_eps=hf_config.router_jitter_noise,\n",
      "    )\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.num_hidden_layers,\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        hidden_dim=hf_config.hidden_size,\n",
      "        n_q_heads=hf_config.num_attention_heads,\n",
      "        n_kv_heads=hf_config.num_key_value_heads,\n",
      "        head_dim=hf_config.hidden_size // hf_config.num_attention_heads,\n",
      "        intermediate_dim=hf_config.intermediate_size,\n",
      "        activation_function=hf_config.hidden_act,\n",
      "        n_positions=hf_config.max_position_embeddings,\n",
      "        layer_norm_epsilon=hf_config.rms_norm_eps,\n",
      "        layer_norm_type=\"rms\",\n",
      "        mlp_type=\"moe\",\n",
      "        tied_embedding=hf_config.tie_word_embeddings,\n",
      "        rotary_base=hf_config.rope_theta,\n",
      "        apply_rotary=True,\n",
      "        attn_pdrop=hf_config.attention_dropout,\n",
      "        resid_pdrop=0.0,\n",
      "        use_attention_bias=False,\n",
      "        use_attn_proj_bias=False,\n",
      "        embd_pdrop=0.0,\n",
      "        sliding_window=hf_config.sliding_window,\n",
      "        moe=moe,\n",
      "        scale_attn_by_inverse_layer_idx=False,\n",
      "    )\n",
      "\n",
      "\n",
      "def config_to_mixtral(config: ReaLModelConfig) -> transformers.MistralConfig:\n",
      "    return transformers.MixtralConfig(\n",
      "        num_hidden_layers=config.n_layers,\n",
      "        vocab_size=config.vocab_size,\n",
      "        hidden_size=config.hidden_dim,\n",
      "        num_attention_heads=config.n_q_heads,\n",
      "        num_key_value_heads=config.n_kv_heads,\n",
      "        intermediate_size=config.intermediate_dim,\n",
      "        hidden_act=config.activation_function,\n",
      "        max_position_embeddings=config.n_positions,\n",
      "        rms_norm_eps=config.layer_norm_epsilon,\n",
      "        tie_word_embeddings=False,\n",
      "        rope_theta=config.rotary_base,\n",
      "        attention_dropout=config.attn_pdrop,\n",
      "        sliding_window=config.sliding_window,\n",
      "        num_local_experts=config.moe.num_experts,\n",
      "        num_experts_per_tok=config.moe.top_k,\n",
      "        router_aux_loss_coef=config.moe.aux_loss_coeff,\n",
      "        router_jitter_noise=config.moe.input_jitter_eps,\n",
      "    )\n",
      "\n",
      "\n",
      "def convert_state_dict_mixtral(state_dict: Dict, config: ReaLModelConfig) -> Dict:\n",
      "    new_state_dict = {}\n",
      "    for k, v in state_dict.items():\n",
      "        if k == \"model.embed_tokens.weight\":\n",
      "            new_state_dict[\"0.wte.weight\"] = v\n",
      "        elif k == \"lm_head.weight\":\n",
      "            new_state_dict[f\"{config.n_layers + 1}.weight\"] = v\n",
      "        elif k == \"model.norm.weight\":\n",
      "            new_state_dict[f\"{config.n_layers}.ln_f.weight\"] = v\n",
      "        elif \"inv_freq\" in k:\n",
      "            continue\n",
      "        else:\n",
      "            block_idx = int(k.split(\".\")[2])\n",
      "            name = k.split(\".\", 3)[3]\n",
      "            replace_pairs = [\n",
      "                (\"self_attn.\", \"attn.\"),\n",
      "                (\"post_attention_layernorm.\", \"mlp.ln.\"),\n",
      "                (\"input_layernorm.\", \"attn.c_attn.ln.\"),\n",
      "                (\"attn.o_proj.\", \"attn.c_proj.\"),\n",
      "                (\"q_proj.\", \"c_attn.q_attn.\"),\n",
      "                (\"k_proj.\", \"c_attn.k_attn.\"),\n",
      "                (\"v_proj.\", \"c_attn.v_attn.\"),\n",
      "                (\"block_sparse_moe.gate.\", \"mlp.router.\"),\n",
      "                (\"block_sparse_moe.experts\", \"mlp.experts.local_experts\"),\n",
      "                (\"w1\", \"gate_proj\"),\n",
      "                (\"w2\", \"down_proj\"),\n",
      "                (\"w3\", \"up_proj\"),\n",
      "            ]\n",
      "            for k1, k2 in replace_pairs:\n",
      "                if k1 in name:\n",
      "                    name = name.replace(k1, k2)\n",
      "            new_state_dict[f\"{block_idx + 1}.{name}\"] = v\n",
      "    return new_state_dict\n",
      "\n",
      "\n",
      "def to_mixtral_state_dict(\n",
      "    state_dict: Dict[str, torch.Tensor], config: ReaLModelConfig\n",
      ") -> Dict:\n",
      "    _k = list(state_dict.keys())[0]\n",
      "    device = state_dict[_k].device\n",
      "    num_experts = config.moe.num_experts\n",
      "\n",
      "    layer_indices = list(set([int(k.split(\".\")[0]) for k in state_dict.keys()]))\n",
      "\n",
      "    new_sd = {}\n",
      "    for i in layer_indices:\n",
      "        if i == 0:\n",
      "            new_sd[\"model.embed_tokens.weight\"] = state_dict[\"0.wte.weight\"]\n",
      "        elif i == config.n_layers + 1:\n",
      "            if config.is_critic or not config.tied_embedding:\n",
      "                new_sd[\"lm_head.weight\"] = state_dict[f\"{i}.weight\"]\n",
      "        else:\n",
      "            # moe layer\n",
      "            new_sd[f\"model.layers.{i-1}.block_sparse_moe.gate.weight\"] = state_dict[\n",
      "                f\"{i}.mlp.router.weight\"\n",
      "            ]\n",
      "            for j in range(num_experts):\n",
      "                new_sd[f\"model.layers.{i-1}.block_sparse_moe.experts.{j}.w1.weight\"] = (\n",
      "                    state_dict[f\"{i}.mlp.experts.local_experts.{j}.gate_proj.weight\"]\n",
      "                )\n",
      "                new_sd[f\"model.layers.{i-1}.block_sparse_moe.experts.{j}.w2.weight\"] = (\n",
      "                    state_dict[f\"{i}.mlp.experts.local_experts.{j}.down_proj.weight\"]\n",
      "                )\n",
      "                new_sd[f\"model.layers.{i-1}.block_sparse_moe.experts.{j}.w3.weight\"] = (\n",
      "                    state_dict[f\"{i}.mlp.experts.local_experts.{j}.up_proj.weight\"]\n",
      "                )\n",
      "            # others\n",
      "            new_sd[f\"model.layers.{i-1}.input_layernorm.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.ln.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.post_attention_layernorm.weight\"] = state_dict[\n",
      "                f\"{i}.mlp.ln.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.k_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.k_attn.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.o_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_proj.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.q_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.q_attn.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.v_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.v_attn.weight\"\n",
      "            ]\n",
      "            if config.use_attention_bias:\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.q_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_attn.q_attn.bias\"\n",
      "                ]\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.k_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_attn.k_attn.bias\"\n",
      "                ]\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.v_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_attn.v_attn.bias\"\n",
      "                ]\n",
      "            if config.use_attn_proj_bias:\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.o_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_proj.bias\"\n",
      "                ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.rotary_emb.inv_freq\"] = 1.0 / (\n",
      "                config.rotary_base\n",
      "                ** (\n",
      "                    torch.arange(\n",
      "                        0,\n",
      "                        config.head_dim,\n",
      "                        2,\n",
      "                        device=device,\n",
      "                        dtype=torch.float32,\n",
      "                    )\n",
      "                    / config.head_dim\n",
      "                )\n",
      "            )\n",
      "            if i == config.n_layers:\n",
      "                new_sd[\"model.norm.weight\"] = state_dict[f\"{i}.ln_f.weight\"]\n",
      "\n",
      "    return new_sd\n",
      "\n",
      "\n",
      "def mixtral_transformer_block_param_name(\n",
      "    config: ReaLModelConfig, idx: int\n",
      ") -> List[str]:\n",
      "    names = []\n",
      "    num_experts = config.moe.num_experts\n",
      "    for k in [\"weight\", \"bias\"]:\n",
      "        names += [\n",
      "            f\"model.layers.{idx}.input_layernorm.{k}\",\n",
      "            f\"model.layers.{idx}.block_sparse_moe.gate.{k}\",\n",
      "            f\"model.layers.{idx}.post_attention_layernorm.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.k_proj.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.o_proj.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.q_proj.{k}\",\n",
      "            # f\"model.layers.{idx}.self_attn.rotary_emb.inv_freq\",\n",
      "            f\"model.layers.{idx}.self_attn.v_proj.{k}\",\n",
      "        ]\n",
      "        for j in range(num_experts):\n",
      "            names += [\n",
      "                f\"model.layers.{idx}.block_sparse_moe.experts.{j}.w1.{k}\",\n",
      "                f\"model.layers.{idx}.block_sparse_moe.experts.{j}.w2.{k}\",\n",
      "                f\"model.layers.{idx}.block_sparse_moe.experts.{j}.w3.{k}\",\n",
      "            ]\n",
      "        if idx == config.n_layers - 1:\n",
      "            names += [f\"model.norm.{k}\"]\n",
      "    return names\n",
      "\n",
      "\n",
      "def get_real_config_mixtral() -> ReaLModelConfig:\n",
      "    hf_config = transformers.MixtralConfig(\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "        max_position_embeddings=TESTING_MODEL_N_POSITIONS,\n",
      "        hidden_size=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        intermediate_size=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        num_hidden_layers=TESTING_MODEL_N_LAYERS,\n",
      "        num_attention_heads=TESTING_MODEL_N_HEADS,\n",
      "        num_key_value_heads=8,\n",
      "        num_local_experts=4,\n",
      "        num_experts_per_tok=2,\n",
      "        router_jitter_noise=None,\n",
      "    )\n",
      "    return config_from_mixtral(hf_config)\n",
      "\n",
      "\n",
      "register_hf_family(\n",
      "    \"mixtral\",\n",
      "    \"MixtralForCausalLM\",\n",
      "    config_from_hf_converter=config_from_mixtral,\n",
      "    config_to_hf_converter=config_to_mixtral,\n",
      "    sd_from_hf_converter=convert_state_dict_mixtral,\n",
      "    sd_to_hf_converter=to_mixtral_state_dict,\n",
      "    embedding_param_names=llama_embedding_layer_names,\n",
      "    tblock_param_names=mixtral_transformer_block_param_name,\n",
      "    head_param_names=llama_output_head_param_name,\n",
      "    real_config_maker=get_real_config_mixtral,\n",
      ")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/qwen3.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import *\n",
      "\n",
      "from transformers.configuration_utils import PretrainedConfig\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, register_hf_family\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "from .llama import (\n",
      "    convert_state_dict_llama,\n",
      "    llama_embedding_layer_names,\n",
      "    llama_output_head_param_name,\n",
      "    to_llama_state_dict,\n",
      ")\n",
      "\n",
      "\n",
      "class Qwen3Config(PretrainedConfig):\n",
      "\n",
      "    model_type = \"qwen3\"\n",
      "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
      "\n",
      "    # Default tensor parallel plan for base model `Qwen3`\n",
      "    base_model_tp_plan = {\n",
      "        \"layers.*.self_attn.q_proj\": \"colwise\",\n",
      "        \"layers.*.self_attn.k_proj\": \"colwise\",\n",
      "        \"layers.*.self_attn.v_proj\": \"colwise\",\n",
      "        \"layers.*.self_attn.o_proj\": \"rowwise\",\n",
      "        \"layers.*.mlp.gate_proj\": \"colwise\",\n",
      "        \"layers.*.mlp.up_proj\": \"colwise\",\n",
      "        \"layers.*.mlp.down_proj\": \"rowwise\",\n",
      "    }\n",
      "    base_model_pp_plan = {\n",
      "        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n",
      "        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n",
      "        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n",
      "    }\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        vocab_size=151936,\n",
      "        hidden_size=4096,\n",
      "        intermediate_size=22016,\n",
      "        num_hidden_layers=32,\n",
      "        num_attention_heads=32,\n",
      "        num_key_value_heads=32,\n",
      "        head_dim=128,\n",
      "        hidden_act=\"silu\",\n",
      "        max_position_embeddings=32768,\n",
      "        initializer_range=0.02,\n",
      "        rms_norm_eps=1e-6,\n",
      "        use_cache=True,\n",
      "        tie_word_embeddings=False,\n",
      "        rope_theta=10000.0,\n",
      "        rope_scaling=None,\n",
      "        attention_bias=False,\n",
      "        use_sliding_window=False,\n",
      "        sliding_window=4096,\n",
      "        max_window_layers=28,\n",
      "        attention_dropout=0.0,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        from transformers.modeling_rope_utils import rope_config_validation\n",
      "\n",
      "        self.vocab_size = vocab_size\n",
      "        self.max_position_embeddings = max_position_embeddings\n",
      "        self.hidden_size = hidden_size\n",
      "        self.intermediate_size = intermediate_size\n",
      "        self.num_hidden_layers = num_hidden_layers\n",
      "        self.num_attention_heads = num_attention_heads\n",
      "        self.use_sliding_window = use_sliding_window\n",
      "        self.sliding_window = (\n",
      "            sliding_window  # we check `use_sliding_window` in the modeling code\n",
      "        )\n",
      "        self.max_window_layers = max_window_layers\n",
      "\n",
      "        # for backward compatibility\n",
      "        if num_key_value_heads is None:\n",
      "            num_key_value_heads = num_attention_heads\n",
      "\n",
      "        self.num_key_value_heads = num_key_value_heads\n",
      "        self.head_dim = head_dim\n",
      "        self.hidden_act = hidden_act\n",
      "        self.initializer_range = initializer_range\n",
      "        self.rms_norm_eps = rms_norm_eps\n",
      "        self.use_cache = use_cache\n",
      "        self.rope_theta = rope_theta\n",
      "        self.rope_scaling = rope_scaling\n",
      "        self.attention_bias = attention_bias\n",
      "        self.attention_dropout = attention_dropout\n",
      "        # Validate the correctness of rotary position embeddings parameters\n",
      "        # BC: if there is a 'type' field, move it to 'rope_type'.\n",
      "        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n",
      "            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n",
      "        rope_config_validation(self)\n",
      "\n",
      "        super().__init__(\n",
      "            tie_word_embeddings=tie_word_embeddings,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "\n",
      "def convert_config_qwen3(\n",
      "    hf_config: Qwen3Config,\n",
      ") -> ReaLModelConfig:\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.num_hidden_layers,\n",
      "        n_kv_heads=hf_config.num_key_value_heads,\n",
      "        hidden_dim=hf_config.hidden_size,\n",
      "        n_q_heads=hf_config.num_attention_heads,\n",
      "        head_dim=getattr(\n",
      "            hf_config,\n",
      "            \"head_dim\",\n",
      "            hf_config.hidden_size // hf_config.num_attention_heads,\n",
      "        ),\n",
      "        intermediate_dim=hf_config.intermediate_size,\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        n_positions=hf_config.max_position_embeddings,\n",
      "        embd_pdrop=0.0,\n",
      "        attn_pdrop=(\n",
      "            hf_config.attention_dropout\n",
      "            if hasattr(hf_config, \"attention_dropout\")\n",
      "            else 0.1\n",
      "        ),\n",
      "        layer_norm_epsilon=hf_config.rms_norm_eps,\n",
      "        activation_function=hf_config.hidden_act,\n",
      "        use_attention_bias=False,\n",
      "        use_attn_proj_bias=False,\n",
      "        scale_attn_by_inverse_layer_idx=False,\n",
      "        layer_norm_type=\"rms\",\n",
      "        qk_layernorm=True,\n",
      "        mlp_type=\"llama\",\n",
      "        apply_rotary=True,\n",
      "        rotary_base=hf_config.rope_theta,\n",
      "        rotary_interleaved=False,\n",
      "        tied_embedding=hf_config.tie_word_embeddings,\n",
      "    )\n",
      "\n",
      "\n",
      "def convert_config_back_qwen3(\n",
      "    config: ReaLModelConfig,\n",
      ") -> Qwen3Config:\n",
      "    return Qwen3Config(\n",
      "        vocab_size=config.vocab_size,\n",
      "        hidden_size=config.hidden_dim,\n",
      "        intermediate_size=config.intermediate_dim,\n",
      "        num_hidden_layers=config.n_layers,\n",
      "        num_key_value_heads=config.n_kv_heads,\n",
      "        num_attention_heads=config.n_q_heads,\n",
      "        head_dim=config.head_dim,\n",
      "        max_position_embeddings=config.n_positions,\n",
      "        rms_norm_eps=config.layer_norm_epsilon,\n",
      "        hidden_act=config.activation_function,\n",
      "        attention_dropout=config.attn_pdrop,\n",
      "        rope_theta=config.rotary_base,\n",
      "        architectures=[\"Qwen3ForCausalLM\"],  # [\"Qwen3ForCausalLM\"],\n",
      "        tie_word_embeddings=config.tied_embedding,\n",
      "    )\n",
      "\n",
      "\n",
      "def qwen3_config_maker():\n",
      "    hf_config = Qwen3Config(\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "        max_position_embeddings=TESTING_MODEL_N_POSITIONS,\n",
      "        hidden_size=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        intermediate_size=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        num_hidden_layers=TESTING_MODEL_N_LAYERS,\n",
      "        num_attention_heads=TESTING_MODEL_N_HEADS,\n",
      "        head_dim=TESTING_MODEL_HEAD_DIM,\n",
      "        num_key_value_heads=8,\n",
      "        hidden_act=\"silu\",\n",
      "        rms_norm_eps=1e-5,\n",
      "    )\n",
      "    return convert_config_qwen3(hf_config)\n",
      "\n",
      "\n",
      "def convert_state_dict_qwen3(state_dict: Dict, config: ReaLModelConfig) -> Dict:\n",
      "    llama_state_dict = convert_state_dict_llama(state_dict, config)\n",
      "    # model.layers.0.self_attn.k_norm.weight -> 1.attn.k_ln.weight\n",
      "    new_state_dict = {}\n",
      "    for k, v in llama_state_dict.items():\n",
      "        if \"k_norm\" in k:\n",
      "            k = k.replace(\"k_norm\", \"k_ln\")\n",
      "        if \"q_norm\" in k:\n",
      "            k = k.replace(\"q_norm\", \"q_ln\")\n",
      "        new_state_dict[k] = v\n",
      "    return new_state_dict\n",
      "\n",
      "\n",
      "def convert_state_dict_back_qwen3(state_dict: Dict, config: ReaLModelConfig) -> Dict:\n",
      "    new_sd = to_llama_state_dict(state_dict, config)\n",
      "    layer_indices = list(set([int(k.split(\".\")[0]) for k in state_dict.keys()]))\n",
      "    for i in layer_indices:\n",
      "        if i == 0 or i == config.n_layers + 1:\n",
      "            continue\n",
      "        new_sd[f\"model.layers.{i - 1}.self_attn.k_norm.weight\"] = state_dict[\n",
      "            f\"{i}.attn.k_ln.weight\"\n",
      "        ]\n",
      "        new_sd[f\"model.layers.{i - 1}.self_attn.q_norm.weight\"] = state_dict[\n",
      "            f\"{i}.attn.q_ln.weight\"\n",
      "        ]\n",
      "    return new_sd\n",
      "\n",
      "\n",
      "def qwen3_transformer_block_param_name(config: ReaLModelConfig, idx: int) -> List[str]:\n",
      "    names = []\n",
      "    for k in [\"weight\", \"bias\"]:\n",
      "        names += [\n",
      "            f\"model.layers.{idx}.input_layernorm.{k}\",\n",
      "            f\"model.layers.{idx}.mlp.down_proj.{k}\",\n",
      "            f\"model.layers.{idx}.mlp.gate_proj.{k}\",\n",
      "            f\"model.layers.{idx}.mlp.up_proj.{k}\",\n",
      "            f\"model.layers.{idx}.post_attention_layernorm.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.k_proj.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.o_proj.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.q_proj.{k}\",\n",
      "            # f\"model.layers.{idx}.self_attn.rotary_emb.inv_freq\",\n",
      "            f\"model.layers.{idx}.self_attn.v_proj.{k}\",\n",
      "        ]\n",
      "        if idx == config.n_layers - 1:\n",
      "            names += [f\"model.norm.{k}\"]\n",
      "    # Qwen3\n",
      "    if config.qk_layernorm:\n",
      "        names += [\n",
      "            f\"model.layers.{idx}.self_attn.q_norm.weight\",\n",
      "            f\"model.layers.{idx}.self_attn.k_norm.weight\",\n",
      "        ]\n",
      "    return names\n",
      "\n",
      "\n",
      "register_hf_family(\n",
      "    name=\"qwen3\",\n",
      "    hf_cls_name=\"Qwen3ForCausalLM\",  # \"Qwen3ForCausalLM\"\n",
      "    config_from_hf_converter=convert_config_qwen3,\n",
      "    config_to_hf_converter=convert_config_back_qwen3,\n",
      "    sd_from_hf_converter=convert_state_dict_qwen3,\n",
      "    sd_to_hf_converter=convert_state_dict_back_qwen3,\n",
      "    embedding_param_names=llama_embedding_layer_names,\n",
      "    tblock_param_names=qwen3_transformer_block_param_name,\n",
      "    head_param_names=llama_output_head_param_name,\n",
      "    real_config_maker=qwen3_config_maker,\n",
      ")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/llama.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, register_hf_family\n",
      "from realhf.base.constants import use_te_impl\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "\n",
      "def convert_state_dict_llama(state_dict: Dict, config: ReaLModelConfig) -> Dict:\n",
      "    new_state_dict = {}\n",
      "    for k, v in state_dict.items():\n",
      "        if k == \"model.embed_tokens.weight\":\n",
      "            new_state_dict[\"0.wte.weight\"] = v\n",
      "        elif k == \"lm_head.weight\":\n",
      "            new_state_dict[f\"{config.n_layers + 1}.weight\"] = v\n",
      "        elif k == \"model.norm.weight\":\n",
      "            new_state_dict[f\"{config.n_layers}.ln_f.weight\"] = v\n",
      "        elif \"inv_freq\" in k:\n",
      "            continue\n",
      "        else:\n",
      "            block_idx = int(k.split(\".\")[2])\n",
      "            name = k.split(\".\", 3)[3]\n",
      "            replace_pairs = [\n",
      "                (\"self_attn.\", \"attn.\"),\n",
      "                (\"post_attention_layernorm.\", \"mlp.ln.\"),\n",
      "                (\"input_layernorm.\", \"attn.c_attn.ln.\"),\n",
      "                (\"attn.o_proj.\", \"attn.c_proj.\"),\n",
      "                (\"q_proj.\", \"c_attn.q_attn.\"),\n",
      "                (\"k_proj.\", \"c_attn.k_attn.\"),\n",
      "                (\"v_proj.\", \"c_attn.v_attn.\"),\n",
      "            ]\n",
      "            for k1, k2 in replace_pairs:\n",
      "                if k1 in name:\n",
      "                    name = name.replace(k1, k2)\n",
      "            new_state_dict[f\"{block_idx + 1}.{name}\"] = v\n",
      "\n",
      "    if use_te_impl():\n",
      "        state_dict = new_state_dict\n",
      "        new_state_dict = {}\n",
      "        te_replace_pairs = [\n",
      "            (\".mlp.ln.weight\", \".mlp.layer_norm_weight\"),\n",
      "            (\".mlp.down_proj.weight\", \".mlp.fc2_weight\"),\n",
      "        ]\n",
      "        for k, v in state_dict.items():\n",
      "            for k1, k2 in te_replace_pairs:\n",
      "                if k1 in k:\n",
      "                    k = k.replace(k1, k2)\n",
      "            new_state_dict[k] = v\n",
      "\n",
      "        # fuse gate && up weight\n",
      "        for i in range(config.n_layers):\n",
      "            gate_w = new_state_dict[f\"{i+1}.mlp.gate_proj.weight\"]\n",
      "            upproj_w = new_state_dict[f\"{i+1}.mlp.up_proj.weight\"]\n",
      "            w = torch.cat([gate_w, upproj_w], dim=0)\n",
      "            new_state_dict[f\"{i+1}.mlp.fc1_weight\"] = w\n",
      "            new_state_dict[f\"{i+1}.mlp._extra_state\"] = None\n",
      "            new_state_dict.pop(f\"{i+1}.mlp.gate_proj.weight\")\n",
      "            new_state_dict.pop(f\"{i+1}.mlp.up_proj.weight\")\n",
      "    return new_state_dict\n",
      "\n",
      "\n",
      "def to_llama_state_dict(\n",
      "    state_dict: Dict[str, torch.Tensor], config: ReaLModelConfig\n",
      ") -> Dict:\n",
      "    if use_te_impl():\n",
      "        # remove all extra states\n",
      "        keys = list(state_dict.keys())\n",
      "        for k in keys:\n",
      "            if k.endswith(\"_extra_state\"):\n",
      "                state_dict.pop(k)\n",
      "\n",
      "        # split gate && up weight\n",
      "        for i in range(config.n_layers):\n",
      "            w = state_dict[f\"{i + 1}.mlp.fc1_weight\"]\n",
      "            gate_w, upproj_w = w.split((w.shape[0] // 2, w.shape[0] // 2), dim=0)\n",
      "            state_dict[f\"{i + 1}.mlp.gate_proj.weight\"] = gate_w.contiguous()\n",
      "            state_dict[f\"{i + 1}.mlp.up_proj.weight\"] = upproj_w.contiguous()\n",
      "            state_dict.pop(f\"{i + 1}.mlp.fc1_weight\")\n",
      "\n",
      "        # rename\n",
      "        new_state_dict = {}\n",
      "        te_replace_pairs = [\n",
      "            (\".mlp.layer_norm_weight\", \".mlp.ln.weight\"),\n",
      "            (\".mlp.fc2_weight\", \".mlp.down_proj.weight\"),\n",
      "        ]\n",
      "        for k, v in state_dict.items():\n",
      "            for k1, k2 in te_replace_pairs:\n",
      "                if k1 in k:\n",
      "                    k = k.replace(k1, k2)\n",
      "            new_state_dict[k] = v\n",
      "        state_dict = new_state_dict\n",
      "\n",
      "    _k = list(state_dict.keys())[0]\n",
      "    device = state_dict[_k].device\n",
      "\n",
      "    layer_indices = list(set([int(k.split(\".\")[0]) for k in state_dict.keys()]))\n",
      "\n",
      "    new_sd = {}\n",
      "    for i in layer_indices:\n",
      "        if i == 0:\n",
      "            new_sd[\"model.embed_tokens.weight\"] = state_dict[\"0.wte.weight\"]\n",
      "        elif i == config.n_layers + 1:\n",
      "            if config.is_critic or not config.tied_embedding:\n",
      "                new_sd[\"lm_head.weight\"] = state_dict[f\"{i}.weight\"]\n",
      "        else:\n",
      "            new_sd[f\"model.layers.{i-1}.input_layernorm.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.ln.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.mlp.down_proj.weight\"] = state_dict[\n",
      "                f\"{i}.mlp.down_proj.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.mlp.gate_proj.weight\"] = state_dict[\n",
      "                f\"{i}.mlp.gate_proj.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.mlp.up_proj.weight\"] = state_dict[\n",
      "                f\"{i}.mlp.up_proj.weight\"\n",
      "            ]\n",
      "            if config.use_mlp_bias:\n",
      "                new_sd[f\"model.layers.{i-1}.mlp.down_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.mlp.down_proj.bias\"\n",
      "                ]\n",
      "                new_sd[f\"model.layers.{i-1}.mlp.gate_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.mlp.gate_proj.bias\"\n",
      "                ]\n",
      "                new_sd[f\"model.layers.{i-1}.mlp.up_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.mlp.up_proj.bias\"\n",
      "                ]\n",
      "            new_sd[f\"model.layers.{i-1}.post_attention_layernorm.weight\"] = state_dict[\n",
      "                f\"{i}.mlp.ln.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.k_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.k_attn.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.o_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_proj.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.q_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.q_attn.weight\"\n",
      "            ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.v_proj.weight\"] = state_dict[\n",
      "                f\"{i}.attn.c_attn.v_attn.weight\"\n",
      "            ]\n",
      "            if config.use_attention_bias:\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.q_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_attn.q_attn.bias\"\n",
      "                ]\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.k_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_attn.k_attn.bias\"\n",
      "                ]\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.v_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_attn.v_attn.bias\"\n",
      "                ]\n",
      "            if config.use_attn_proj_bias:\n",
      "                new_sd[f\"model.layers.{i-1}.self_attn.o_proj.bias\"] = state_dict[\n",
      "                    f\"{i}.attn.c_proj.bias\"\n",
      "                ]\n",
      "            new_sd[f\"model.layers.{i-1}.self_attn.rotary_emb.inv_freq\"] = 1.0 / (\n",
      "                config.rotary_base\n",
      "                ** (\n",
      "                    torch.arange(\n",
      "                        0,\n",
      "                        config.head_dim,\n",
      "                        2,\n",
      "                        device=device,\n",
      "                        dtype=torch.float32,\n",
      "                    )\n",
      "                    / config.head_dim\n",
      "                )\n",
      "            )\n",
      "            if i == config.n_layers:\n",
      "                new_sd[\"model.norm.weight\"] = state_dict[f\"{i}.ln_f.weight\"]\n",
      "\n",
      "    return new_sd\n",
      "\n",
      "\n",
      "# param name is used to load directly from huggingface checkpoints\n",
      "def llama_embedding_layer_names(config: ReaLModelConfig) -> List[str]:\n",
      "    return [\"model.embed_tokens.weight\"]\n",
      "\n",
      "\n",
      "def llama_transformer_block_param_name(config: ReaLModelConfig, idx: int) -> List[str]:\n",
      "    names = []\n",
      "    for k in [\"weight\", \"bias\"]:\n",
      "        names += [\n",
      "            f\"model.layers.{idx}.input_layernorm.{k}\",\n",
      "            f\"model.layers.{idx}.mlp.down_proj.{k}\",\n",
      "            f\"model.layers.{idx}.mlp.gate_proj.{k}\",\n",
      "            f\"model.layers.{idx}.mlp.up_proj.{k}\",\n",
      "            f\"model.layers.{idx}.post_attention_layernorm.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.k_proj.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.o_proj.{k}\",\n",
      "            f\"model.layers.{idx}.self_attn.q_proj.{k}\",\n",
      "            # f\"model.layers.{idx}.self_attn.rotary_emb.inv_freq\",\n",
      "            f\"model.layers.{idx}.self_attn.v_proj.{k}\",\n",
      "        ]\n",
      "        if idx == config.n_layers - 1:\n",
      "            names += [f\"model.norm.{k}\"]\n",
      "    return names\n",
      "\n",
      "\n",
      "def llama_output_head_param_name(config: ReaLModelConfig) -> List[str]:\n",
      "    if config.tied_embedding and not config.is_critic:\n",
      "        return [\"model.embed_tokens.weight\"]\n",
      "    else:\n",
      "        return [\"lm_head.weight\"]\n",
      "\n",
      "\n",
      "def convert_config_llama(\n",
      "    hf_config: transformers.LlamaConfig,\n",
      ") -> ReaLModelConfig:\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.num_hidden_layers,\n",
      "        n_kv_heads=hf_config.num_key_value_heads,\n",
      "        n_q_heads=hf_config.num_attention_heads,\n",
      "        hidden_dim=hf_config.hidden_size,\n",
      "        head_dim=hf_config.hidden_size // hf_config.num_attention_heads,\n",
      "        intermediate_dim=hf_config.intermediate_size,\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        n_positions=hf_config.max_position_embeddings,\n",
      "        embd_pdrop=0.0,\n",
      "        attn_pdrop=(\n",
      "            hf_config.attention_dropout\n",
      "            if hasattr(hf_config, \"attention_dropout\")\n",
      "            else 0.1\n",
      "        ),\n",
      "        layer_norm_epsilon=hf_config.rms_norm_eps,\n",
      "        activation_function=hf_config.hidden_act,\n",
      "        use_attention_bias=hf_config.attention_bias,\n",
      "        use_attn_proj_bias=hf_config.attention_bias,\n",
      "        scale_attn_by_inverse_layer_idx=False,\n",
      "        layer_norm_type=\"rms\",\n",
      "        mlp_type=\"llama\",\n",
      "        apply_rotary=True,\n",
      "        rotary_base=hf_config.rope_theta,\n",
      "        rotary_interleaved=False,\n",
      "        rotary_scaling=(\n",
      "            None if hf_config.rope_scaling is None else hf_config.rope_scaling[\"factor\"]\n",
      "        ),\n",
      "        rotary_scaling_type=(\n",
      "            None if hf_config.rope_scaling is None else hf_config.rope_scaling[\"type\"]\n",
      "        ),\n",
      "    )\n",
      "\n",
      "\n",
      "def convert_config_back_llama(\n",
      "    config: ReaLModelConfig,\n",
      ") -> transformers.LlamaConfig:\n",
      "    rope_scaling = {}\n",
      "    if config.rotary_scaling is not None:\n",
      "        rope_scaling[\"factor\"] = config.rotary_scaling\n",
      "    if config.rotary_scaling_type is not None:\n",
      "        rope_scaling[\"type\"] = config.rotary_scaling_type\n",
      "    return transformers.LlamaConfig(\n",
      "        vocab_size=config.vocab_size,\n",
      "        hidden_size=config.hidden_dim,\n",
      "        intermediate_size=config.intermediate_dim,\n",
      "        num_hidden_layers=config.n_layers,\n",
      "        num_key_value_heads=config.n_kv_heads,\n",
      "        num_attention_heads=config.n_q_heads,\n",
      "        max_position_embeddings=config.n_positions,\n",
      "        rms_norm_eps=config.layer_norm_epsilon,\n",
      "        hidden_act=config.activation_function,\n",
      "        attention_dropout=config.attn_pdrop,\n",
      "        attention_bias=config.use_attention_bias,\n",
      "        rope_theta=config.rotary_base,\n",
      "        rope_scaling=rope_scaling if rope_scaling else None,\n",
      "        architectures=[\"LlamaForCausalLM\"],\n",
      "    )\n",
      "\n",
      "\n",
      "def make_real_config_llama():\n",
      "    hf_config = transformers.LlamaConfig(\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "        max_position_embeddings=TESTING_MODEL_N_POSITIONS,\n",
      "        hidden_size=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        intermediate_size=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        num_hidden_layers=TESTING_MODEL_N_LAYERS,\n",
      "        num_attention_heads=TESTING_MODEL_N_HEADS,\n",
      "        num_key_value_heads=8,\n",
      "        hidden_act=\"silu\",\n",
      "        rms_norm_eps=1e-5,\n",
      "    )\n",
      "    return convert_config_llama(hf_config)\n",
      "\n",
      "\n",
      "for name in [\n",
      "    \"llama\",\n",
      "    \"codellama\",\n",
      "    \"deepseek\",\n",
      "]:\n",
      "    register_hf_family(\n",
      "        name=name,\n",
      "        hf_cls_name=\"LlamaForCausalLM\",\n",
      "        config_from_hf_converter=convert_config_llama,\n",
      "        config_to_hf_converter=convert_config_back_llama,\n",
      "        sd_from_hf_converter=convert_state_dict_llama,\n",
      "        sd_to_hf_converter=to_llama_state_dict,\n",
      "        embedding_param_names=llama_embedding_layer_names,\n",
      "        tblock_param_names=llama_transformer_block_param_name,\n",
      "        head_param_names=llama_output_head_param_name,\n",
      "        real_config_maker=make_real_config_llama,\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/gpt2.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core.model_api import ReaLModelConfig, register_hf_family\n",
      "from realhf.base.testing import (\n",
      "    TESTING_MODEL_HEAD_DIM,\n",
      "    TESTING_MODEL_HIDDEN_SIZE,\n",
      "    TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "    TESTING_MODEL_N_HEADS,\n",
      "    TESTING_MODEL_N_LAYERS,\n",
      "    TESTING_MODEL_N_POSITIONS,\n",
      "    TESTING_MODEL_VOCAB_SIZE,\n",
      ")\n",
      "\n",
      "\n",
      "def sd_from_gpt2(state_dict: Dict, config: ReaLModelConfig) -> Dict:\n",
      "    if any(k.startswith(\"transformer.\") for k in state_dict.keys()):\n",
      "        new_sd = {k.replace(\"transformer.\", \"\"): v for k, v in state_dict.items()}\n",
      "        if \"lm_head.weight\" in new_sd:\n",
      "            head_w = new_sd.pop(\"lm_head.weight\")\n",
      "            assert torch.allclose(head_w, new_sd[\"wte.weight\"])\n",
      "        state_dict = new_sd\n",
      "\n",
      "    new_sd = {}\n",
      "    if \"wte.weight\" in state_dict:\n",
      "        new_sd[\"0.wte.weight\"] = state_dict[\"wte.weight\"]\n",
      "    if \"wpe.weight\" in state_dict:\n",
      "        new_sd[\"0.wpe.weight\"] = state_dict[\"wpe.weight\"]\n",
      "    if \"lm_head.weight\" in state_dict:\n",
      "        new_sd[f\"{config.n_layers + 1}.weight\"] = state_dict[\"lm_head.weight\"]\n",
      "    if \"ln_f.weight\" in state_dict:\n",
      "        new_sd[f\"{config.n_layers}.ln_f.weight\"] = state_dict[\"ln_f.weight\"]\n",
      "        new_sd[f\"{config.n_layers}.ln_f.bias\"] = state_dict[\"ln_f.bias\"]\n",
      "    for i in range(config.n_layers):\n",
      "        if not any(k.startswith(f\"h.{i}.\") for k in state_dict):\n",
      "            continue\n",
      "        new_sd[f\"{i+1}.attn.c_attn.ln.weight\"] = state_dict[f\"h.{i}.ln_1.weight\"]\n",
      "        new_sd[f\"{i+1}.attn.c_attn.ln.bias\"] = state_dict[f\"h.{i}.ln_1.bias\"]\n",
      "\n",
      "        attn_w = state_dict[f\"h.{i}.attn.c_attn.weight\"]\n",
      "        attn_bias = state_dict[f\"h.{i}.attn.c_attn.bias\"]\n",
      "        qw, kw, vw = torch.chunk(attn_w, 3, dim=1)\n",
      "        qb, kb, vb = torch.chunk(attn_bias, 3, dim=0)\n",
      "        new_sd[f\"{i+1}.attn.c_attn.q_attn.weight\"] = qw.transpose(0, 1)\n",
      "        new_sd[f\"{i+1}.attn.c_attn.k_attn.weight\"] = kw.transpose(0, 1)\n",
      "        new_sd[f\"{i+1}.attn.c_attn.v_attn.weight\"] = vw.transpose(0, 1)\n",
      "        new_sd[f\"{i+1}.attn.c_attn.q_attn.bias\"] = qb\n",
      "        new_sd[f\"{i+1}.attn.c_attn.k_attn.bias\"] = kb\n",
      "        new_sd[f\"{i+1}.attn.c_attn.v_attn.bias\"] = vb\n",
      "\n",
      "        new_sd[f\"{i+1}.attn.c_proj.weight\"] = state_dict[\n",
      "            f\"h.{i}.attn.c_proj.weight\"\n",
      "        ].transpose(0, 1)\n",
      "        new_sd[f\"{i+1}.attn.c_proj.bias\"] = state_dict[f\"h.{i}.attn.c_proj.bias\"]\n",
      "\n",
      "        new_sd[f\"{i+1}.mlp.ln.weight\"] = state_dict[f\"h.{i}.ln_2.weight\"]\n",
      "        new_sd[f\"{i+1}.mlp.ln.bias\"] = state_dict[f\"h.{i}.ln_2.bias\"]\n",
      "        new_sd[f\"{i+1}.mlp.c_fc.weight\"] = state_dict[\n",
      "            f\"h.{i}.mlp.c_fc.weight\"\n",
      "        ].transpose(0, 1)\n",
      "        new_sd[f\"{i+1}.mlp.c_fc.bias\"] = state_dict[f\"h.{i}.mlp.c_fc.bias\"]\n",
      "        new_sd[f\"{i+1}.mlp.c_proj.weight\"] = state_dict[\n",
      "            f\"h.{i}.mlp.c_proj.weight\"\n",
      "        ].transpose(0, 1)\n",
      "        new_sd[f\"{i+1}.mlp.c_proj.bias\"] = state_dict[f\"h.{i}.mlp.c_proj.bias\"]\n",
      "    return new_sd\n",
      "\n",
      "\n",
      "def sd_to_gpt2(state_dict: Dict, config: ReaLModelConfig) -> Dict:\n",
      "    max_positions = config.n_positions\n",
      "    bias = torch.tril(\n",
      "        torch.ones((max_positions, max_positions), dtype=torch.bool)\n",
      "    ).view(1, 1, max_positions, max_positions)\n",
      "\n",
      "    new_sd = {}\n",
      "    if \"0.wte.weight\" in state_dict:\n",
      "        new_sd[\"wte.weight\"] = state_dict[\"0.wte.weight\"]\n",
      "        new_sd[\"wpe.weight\"] = state_dict[\"0.wpe.weight\"]\n",
      "    if f\"{config.n_layers}.ln_f.weight\" in state_dict:\n",
      "        new_sd[\"ln_f.weight\"] = state_dict[f\"{config.n_layers}.ln_f.weight\"]\n",
      "        new_sd[\"ln_f.bias\"] = state_dict[f\"{config.n_layers}.ln_f.bias\"]\n",
      "    if f\"{config.n_layers + 1}.weight\" in state_dict:\n",
      "        new_sd[\"lm_head.weight\"] = state_dict[f\"{config.n_layers + 1}.weight\"]\n",
      "    for i in range(config.n_layers):\n",
      "        if not any(k.startswith(f\"{i+1}.\") for k in state_dict):\n",
      "            continue\n",
      "        new_sd[f\"h.{i}.ln_1.weight\"] = state_dict[f\"{i+1}.attn.c_attn.ln.weight\"]\n",
      "        new_sd[f\"h.{i}.ln_1.bias\"] = state_dict[f\"{i+1}.attn.c_attn.ln.bias\"]\n",
      "\n",
      "        qw = state_dict[f\"{i+1}.attn.c_attn.q_attn.weight\"].transpose(0, 1)\n",
      "        kw = state_dict[f\"{i+1}.attn.c_attn.k_attn.weight\"].transpose(0, 1)\n",
      "        vw = state_dict[f\"{i+1}.attn.c_attn.v_attn.weight\"].transpose(0, 1)\n",
      "        qb = state_dict[f\"{i+1}.attn.c_attn.q_attn.bias\"]\n",
      "        kb = state_dict[f\"{i+1}.attn.c_attn.k_attn.bias\"]\n",
      "        vb = state_dict[f\"{i+1}.attn.c_attn.v_attn.bias\"]\n",
      "        attn_w = torch.cat([qw, kw, vw], dim=1)\n",
      "        attn_bias = torch.cat([qb, kb, vb], dim=0)\n",
      "        new_sd[f\"h.{i}.attn.c_attn.weight\"] = attn_w\n",
      "        new_sd[f\"h.{i}.attn.c_attn.bias\"] = attn_bias\n",
      "\n",
      "        new_sd[f\"h.{i}.attn.c_proj.weight\"] = state_dict[\n",
      "            f\"{i+1}.attn.c_proj.weight\"\n",
      "        ].transpose(0, 1)\n",
      "        new_sd[f\"h.{i}.attn.c_proj.bias\"] = state_dict[f\"{i+1}.attn.c_proj.bias\"]\n",
      "\n",
      "        new_sd[f\"h.{i}.ln_2.weight\"] = state_dict[f\"{i+1}.mlp.ln.weight\"]\n",
      "        new_sd[f\"h.{i}.ln_2.bias\"] = state_dict[f\"{i+1}.mlp.ln.bias\"]\n",
      "        new_sd[f\"h.{i}.mlp.c_fc.weight\"] = state_dict[\n",
      "            f\"{i+1}.mlp.c_fc.weight\"\n",
      "        ].transpose(0, 1)\n",
      "        new_sd[f\"h.{i}.mlp.c_fc.bias\"] = state_dict[f\"{i+1}.mlp.c_fc.bias\"]\n",
      "        new_sd[f\"h.{i}.mlp.c_proj.weight\"] = state_dict[\n",
      "            f\"{i+1}.mlp.c_proj.weight\"\n",
      "        ].transpose(0, 1)\n",
      "        new_sd[f\"h.{i}.mlp.c_proj.bias\"] = state_dict[f\"{i+1}.mlp.c_proj.bias\"]\n",
      "        new_sd[f\"h.{i}.attn.bias\"] = bias\n",
      "    return new_sd\n",
      "\n",
      "\n",
      "# param name is used to load directly from huggingface checkpoints\n",
      "def gpt2_embedding_layer_names(config: ReaLModelConfig) -> List[str]:\n",
      "    return [\n",
      "        \"wte.weight\",\n",
      "        \"wpe.weight\",\n",
      "        \"transformer.wte.weight\",\n",
      "        \"transformer.wpe.weight\",\n",
      "    ]\n",
      "\n",
      "\n",
      "def gpt2_transformer_block_param_name(config: ReaLModelConfig, idx: int) -> List[str]:\n",
      "    names = [\n",
      "        f\"h.{idx}.ln_1.weight\",\n",
      "        f\"h.{idx}.ln_1.bias\",\n",
      "        # f'h.{idx}.attn.bias',\n",
      "        f\"h.{idx}.attn.c_attn.weight\",\n",
      "        f\"h.{idx}.attn.c_attn.bias\",\n",
      "        f\"h.{idx}.attn.c_proj.weight\",\n",
      "        f\"h.{idx}.attn.c_proj.bias\",\n",
      "        f\"h.{idx}.ln_2.weight\",\n",
      "        f\"h.{idx}.ln_2.bias\",\n",
      "        f\"h.{idx}.mlp.c_fc.weight\",\n",
      "        f\"h.{idx}.mlp.c_fc.bias\",\n",
      "        f\"h.{idx}.mlp.c_proj.weight\",\n",
      "        f\"h.{idx}.mlp.c_proj.bias\",\n",
      "    ]\n",
      "    if idx == config.n_layers - 1:\n",
      "        names += [\"ln_f.weight\", \"ln_f.bias\"]\n",
      "    _names = [\"transformer.\" + name for name in names]\n",
      "    return names + _names\n",
      "\n",
      "\n",
      "def gpt2_output_head_param_name(config: ReaLModelConfig) -> List[str]:\n",
      "    return [\"wte.weight\", \"transformer.wte.weight\", \"lm_head.weight\"]\n",
      "\n",
      "\n",
      "def convert_config_gpt2(\n",
      "    hf_config: transformers.GPT2Config,\n",
      ") -> ReaLModelConfig:\n",
      "    return ReaLModelConfig(\n",
      "        n_layers=hf_config.n_layer,\n",
      "        n_kv_heads=hf_config.n_head,\n",
      "        hidden_dim=hf_config.n_embd,\n",
      "        head_dim=hf_config.n_embd // hf_config.n_head,\n",
      "        n_q_heads=hf_config.n_head,\n",
      "        use_mlp_bias=True,\n",
      "        intermediate_dim=(\n",
      "            hf_config.n_inner if hf_config.n_inner is not None else 4 * hf_config.n_embd\n",
      "        ),\n",
      "        vocab_size=hf_config.vocab_size,\n",
      "        n_positions=hf_config.n_positions,\n",
      "        embd_pdrop=hf_config.embd_pdrop,\n",
      "        attn_pdrop=hf_config.attn_pdrop,\n",
      "        resid_pdrop=hf_config.resid_pdrop,\n",
      "        layer_norm_epsilon=hf_config.layer_norm_epsilon,\n",
      "        activation_function=hf_config.activation_function,\n",
      "        use_attention_bias=True,\n",
      "        use_attn_proj_bias=True,\n",
      "        scale_attn_by_inverse_layer_idx=hf_config.scale_attn_by_inverse_layer_idx,\n",
      "        tied_embedding=True,\n",
      "        scale_attn_weights=hf_config.scale_attn_weights,\n",
      "    )\n",
      "\n",
      "\n",
      "def to_gpt2_config(config: ReaLModelConfig) -> transformers.GPT2Config:\n",
      "    return transformers.GPT2Config(\n",
      "        vocab_size=config.vocab_size,\n",
      "        n_positions=config.n_positions,\n",
      "        n_embd=config.hidden_dim,\n",
      "        n_layer=config.n_layers,\n",
      "        n_head=config.n_q_heads,\n",
      "        n_inner=config.intermediate_dim,\n",
      "        activation_function=config.activation_function,\n",
      "        embd_pdrop=config.embd_pdrop,\n",
      "        attn_pdrop=config.attn_pdrop,\n",
      "        resid_pdrop=config.resid_pdrop,\n",
      "        layer_norm_epsilon=config.layer_norm_epsilon,\n",
      "        scale_attn_by_inverse_layer_idx=config.scale_attn_by_inverse_layer_idx,\n",
      "        scale_attn_weights=config.scale_attn_weights,\n",
      "        architectures=[\"GPT2LMHeadModel\"],\n",
      "    )\n",
      "\n",
      "\n",
      "def make_real_config_gpt2() -> ReaLModelConfig:\n",
      "    hf_config = transformers.GPT2Config(\n",
      "        vocab_size=TESTING_MODEL_VOCAB_SIZE,\n",
      "        n_positions=TESTING_MODEL_N_POSITIONS,\n",
      "        n_embd=TESTING_MODEL_HIDDEN_SIZE,\n",
      "        n_layer=TESTING_MODEL_N_LAYERS,\n",
      "        n_head=TESTING_MODEL_N_HEADS,\n",
      "        n_inner=TESTING_MODEL_INTERMEDIATE_SIZE,\n",
      "        activation_function=\"gelu_new\",\n",
      "    )\n",
      "    return convert_config_gpt2(hf_config)\n",
      "\n",
      "\n",
      "register_hf_family(\n",
      "    name=\"gpt2\",\n",
      "    hf_cls_name=\"GPT2LMHeadModel\",\n",
      "    config_from_hf_converter=convert_config_gpt2,\n",
      "    config_to_hf_converter=to_gpt2_config,\n",
      "    sd_from_hf_converter=sd_from_gpt2,\n",
      "    sd_to_hf_converter=sd_to_gpt2,\n",
      "    embedding_param_names=gpt2_embedding_layer_names,\n",
      "    tblock_param_names=gpt2_transformer_block_param_name,\n",
      "    head_param_names=gpt2_output_head_param_name,\n",
      "    real_config_maker=make_real_config_gpt2,\n",
      ")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/from_hf/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "from realhf.base.importing import import_module\n",
      "\n",
      "import_module(os.path.dirname(__file__), re.compile(r\"^(?!.*__init__).*\\.py$\"))\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/quickstart/search.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "from typing import List, Optional\n",
      "\n",
      "from realhf.api.cli_args import ParallelismConfig\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.api.quickstart.device_mesh import DeviceMesh\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RPCExecution:\n",
      "    rpc: MFCDef\n",
      "    device_mesh: DeviceMesh\n",
      "    parallel_strategy: ParallelismConfig\n",
      "    time_cost: Optional[int] = None\n",
      "    mem: Optional[int] = None\n",
      "    static_mem: Optional[int] = None\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"RPCExecution({self.rpc}, {self.device_mesh}, {self.parallel_strategy})\"\n",
      "\n",
      "    def __hash__(self):\n",
      "        return hash(\n",
      "            (\n",
      "                self.rpc.name,\n",
      "                self.device_mesh.cluster_mesh,\n",
      "                self.device_mesh.device_mesh_name,\n",
      "                str(self.parallel_strategy),\n",
      "            )\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RPCInstance:\n",
      "    rpc: MFCDef\n",
      "    iteration_id: int\n",
      "    parents: List[MFCDef]\n",
      "    children: List[MFCDef]\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        return f\"{self.rpc.name}:{self.iteration_id}\"\n",
      "\n",
      "    def __repr__(self):\n",
      "        if len(self.parents) == 0 and len(self.children) == 0:\n",
      "            return f\"RPCInstance({self.rpc.name}, {self.iteration_id})\"\n",
      "        else:\n",
      "            return (\n",
      "                f\"RPCInstance({self.rpc.name}, {self.iteration_id}, \"\n",
      "                f\"{self.parents}, {self.children})\"\n",
      "            )\n",
      "\n",
      "    def __hash__(self):\n",
      "        return hash((self.rpc.name, self.iteration_id))\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/quickstart/device_mesh.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import math\n",
      "from typing import List, Optional, Tuple, Union\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from realhf.api.cli_args import ParallelismConfig\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.base.slurm_utils import are_ones_contiguous, parse_nodelist\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class DeviceMesh:\n",
      "    # number of total nodes, n_gpus_per_node=8\n",
      "    n_nodes: int\n",
      "    n_gpus_per_node: int\n",
      "    # a 2D binary array of current device mesh name\n",
      "    # shape: (n_nodes, n_gpus_per_node)\n",
      "    mapping: np.ndarray\n",
      "\n",
      "    def to_dict(self):\n",
      "        return dict(\n",
      "            n_nodes=self.n_nodes,\n",
      "            n_gpus_per_node=self.n_gpus_per_node,\n",
      "            mapping=self.mapping.tolist(),\n",
      "        )\n",
      "\n",
      "    def split(self, split_n_gpus: int) -> Tuple[\"DeviceMesh\", \"DeviceMesh\"]:\n",
      "        \"\"\"Split the current device into two parts, with the first part having\n",
      "        `split_n_gpus` GPUs.\n",
      "\n",
      "        The second part will have the remaining GPUs.\n",
      "        \"\"\"\n",
      "        assert self._is_valid_mapping()\n",
      "        assert (\n",
      "            self.mapping.sum() > split_n_gpus\n",
      "        ), f\"split size {split_n_gpus} should be smaller than the number of current GPUs {self.mapping.sum()}\"\n",
      "\n",
      "        sub_mapping2 = self.mapping.copy()\n",
      "        t = split_n_gpus\n",
      "        cnt = 0\n",
      "        while t > 0:\n",
      "            i, j = cnt // self.n_gpus_per_node, cnt % self.n_gpus_per_node\n",
      "            if sub_mapping2[i, j] == 0:\n",
      "                cnt += 1\n",
      "            else:\n",
      "                sub_mapping2[i, j] = 0\n",
      "                t -= 1\n",
      "        sub_mapping1 = self.mapping - sub_mapping2\n",
      "\n",
      "        d1, d2 = (\n",
      "            DeviceMesh(\n",
      "                n_nodes=self.n_nodes,\n",
      "                n_gpus_per_node=self.n_gpus_per_node,\n",
      "                mapping=sub_mapping1,\n",
      "            ),\n",
      "            DeviceMesh(\n",
      "                n_nodes=self.n_nodes,\n",
      "                n_gpus_per_node=self.n_gpus_per_node,\n",
      "                mapping=sub_mapping2,\n",
      "            ),\n",
      "        )\n",
      "        assert d1._is_valid_mapping()\n",
      "        assert d2._is_valid_mapping()\n",
      "        return (d1, d2)\n",
      "\n",
      "    @staticmethod\n",
      "    def from_dict(d):\n",
      "        device_mesh = DeviceMesh(**d)\n",
      "        device_mesh.mapping = np.array(d[\"mapping\"])\n",
      "        return device_mesh\n",
      "\n",
      "    def __post_init__(self):\n",
      "        n = cluster_spec.suffix_n_digits\n",
      "        assert self._is_valid_mapping()\n",
      "\n",
      "    def __eq__(self, other: \"DeviceMesh\"):\n",
      "        return np.all(self.mapping == other.mapping)\n",
      "\n",
      "    def __op_assertion(self, other: \"DeviceMesh\"):\n",
      "        assert self.n_nodes == other.n_nodes\n",
      "        assert self.n_gpus_per_node == other.n_gpus_per_node\n",
      "\n",
      "    def overlap(self, other: \"DeviceMesh\") -> bool:\n",
      "        self.__op_assertion(other)\n",
      "        return np.any(self.mapping & other.mapping)\n",
      "\n",
      "    def contain(self, other: \"DeviceMesh\") -> bool:\n",
      "        self.__op_assertion(other)\n",
      "        return np.all(self.mapping & other.mapping == self.mapping)\n",
      "\n",
      "    def contained_by(self, other: \"DeviceMesh\") -> bool:\n",
      "        self.__op_assertion(other)\n",
      "        return np.all(self.mapping & other.mapping == other.mapping)\n",
      "\n",
      "    def sub_device_meshes(self, min_n_gpus: int = 4) -> List[\"DeviceMesh\"]:\n",
      "        \"\"\"Find sub device meshes of this device mesh with at least min_n_gpus\n",
      "        gpus.\n",
      "\n",
      "        Sub device meshes have following constraints:\n",
      "            1. Sub device meshes have the same cluster mesh.\n",
      "            2. Sub device meshes of multiple nodes must contain consecutive nodes\n",
      "               in the cluster mesh.\n",
      "            3. Sub device meshes can only be of shape 1x1, 1x2, 1x4, 1x8 or Nx8\n",
      "            4. If sub device meshes are of shape 1x2 or 1x4, the start GPU id\n",
      "               must be 0, 2, 4, 6 for 1x2 and 0, 4 for 1x4.\n",
      "        \"\"\"\n",
      "        sub_mappings = []\n",
      "        rows, cols = np.where(self.mapping == 1)\n",
      "\n",
      "        unique_rows = np.unique(rows)\n",
      "        for row in unique_rows:\n",
      "            this_cols = cols[rows == row]\n",
      "            assert (\n",
      "                self.n_gpus_per_node % min_n_gpus == 0\n",
      "                or min_n_gpus % self.n_gpus_per_node == 0\n",
      "            )\n",
      "            n_gpus = min_n_gpus\n",
      "            while n_gpus < min(self.n_gpus_per_node, np.sum(self.mapping)):\n",
      "                for start in range(np.min(this_cols), self.n_gpus_per_node, n_gpus):\n",
      "                    sub_mapping = np.zeros(\n",
      "                        (self.n_nodes, self.n_gpus_per_node), dtype=np.int32\n",
      "                    )\n",
      "                    sub_mapping[row, start : start + n_gpus] = 1\n",
      "                    sub_mappings.append(sub_mapping)\n",
      "                n_gpus *= 2\n",
      "\n",
      "        for n_rows in range(1, len(unique_rows) + 1):\n",
      "            for start in range(0, len(unique_rows) - n_rows + 1):\n",
      "                sub_mapping = np.zeros(\n",
      "                    (self.n_nodes, self.n_gpus_per_node), dtype=np.int32\n",
      "                )\n",
      "                sub_mapping[start : start + n_rows, :] = 1\n",
      "                sub_mappings.append(sub_mapping)\n",
      "\n",
      "        return [\n",
      "            DeviceMesh(\n",
      "                n_nodes=self.n_nodes,\n",
      "                n_gpus_per_node=self.n_gpus_per_node,\n",
      "                mapping=sub_mapping,\n",
      "            )\n",
      "            for sub_mapping in sub_mappings\n",
      "        ]\n",
      "\n",
      "    def _is_valid_mapping(self) -> bool:\n",
      "        if self.mapping.shape != (self.n_nodes, self.n_gpus_per_node):\n",
      "            raise RuntimeError(\n",
      "                f\"Invalid mapping shape {self.mapping.shape} n_nodes={self.n_nodes} \"\n",
      "                f\"n_gpus_per_node={self.n_gpus_per_node}\"\n",
      "            )\n",
      "        if not np.all(np.logical_or(self.mapping == 0, self.mapping == 1)):\n",
      "            raise RuntimeError(f\"Invalid mapping value {self.mapping}\")\n",
      "\n",
      "        assert math.log(self.n_gpus_per_node, 2).is_integer()\n",
      "\n",
      "        if self.mapping.sum() >= self.n_gpus_per_node:\n",
      "            if not (\n",
      "                self.mapping.sum() % self.n_gpus_per_node == 0\n",
      "                and np.all(\n",
      "                    np.logical_or(\n",
      "                        self.mapping.sum(1) == self.n_gpus_per_node,\n",
      "                        self.mapping.sum(1) == 0,\n",
      "                    )\n",
      "                )\n",
      "            ):\n",
      "                raise RuntimeError(\n",
      "                    f\"Invalid mapping sum {self.mapping}. \"\n",
      "                    \"If using GPUs more than an entire node, \"\n",
      "                    \"only several complete nodes are allowed.\"\n",
      "                )\n",
      "        if not are_ones_contiguous(self.mapping.flatten()):\n",
      "            raise RuntimeError(f\"mapping devices are not contiguous {self.mapping}\")\n",
      "        return True\n",
      "\n",
      "\n",
      "def make_device_mesh_from_name(\n",
      "    global_mesh_name: str, name: str, n_gpus_per_node: int = 8\n",
      "):\n",
      "    \"\"\"\n",
      "    DeviceMesh name format: <prefix><node_indices>[:<gpu_ids>]\n",
      "        slurm_nodelist is the name of slurm nodes the mesh is on, should follow slurm convention,\n",
      "        for example \"slurmd-[40-43]\" or \"slurmd-[01,11,13-14]\" with prefix slurmd-,\n",
      "        if n_nodes=1, gpu_ids are the gpu id list delimited by comma if n_gpus < n_gpus_per_node,\n",
      "        for example \"0,1,2,3\" or \"0,1\". An example of full device mesh name\n",
      "        in this situation is \"slurmd-40:0,1,2,3\"\n",
      "\n",
      "    Note: cluster device mesh name must occupy entire nodes.\n",
      "    \"\"\"\n",
      "    prefix = cluster_spec.node_name_prefix\n",
      "    node_list = parse_nodelist(global_mesh_name, prefix)\n",
      "    n_nodes = len(node_list)\n",
      "\n",
      "    gpu_ids = None\n",
      "    if \":\" in name:\n",
      "        node_names, gpu_ids = name.split(\":\")\n",
      "        gpu_ids = list(map(int, gpu_ids.split(\",\")))\n",
      "        assert all(gpu_id < n_gpus_per_node for gpu_id in gpu_ids)\n",
      "    else:\n",
      "        node_names = name\n",
      "    node_names = parse_nodelist(node_names, prefix)\n",
      "    mapping = np.zeros((n_nodes, n_gpus_per_node), dtype=np.int32)\n",
      "    if gpu_ids is None:\n",
      "        node_indices = [node_list.index(node_name) for node_name in node_names]\n",
      "        mapping[node_indices, :] = 1\n",
      "    else:\n",
      "        assert len(node_names) == 1\n",
      "        node_index = node_list.index(node_names[0])\n",
      "        mapping[node_index, gpu_ids] = 1\n",
      "\n",
      "    return DeviceMesh(\n",
      "        n_nodes=n_nodes,\n",
      "        n_gpus_per_node=n_gpus_per_node,\n",
      "        mapping=mapping,\n",
      "    )\n",
      "\n",
      "\n",
      "def find_parallel_strategies(\n",
      "    device_mesh: DeviceMesh,\n",
      ") -> List[ParallelismConfig]:\n",
      "    n_gpus = np.sum(device_mesh.mapping)\n",
      "    res = []\n",
      "    for num_tp in [1, 2, 4, 8]:\n",
      "        if n_gpus >= num_tp:\n",
      "            assert n_gpus % num_tp == 0\n",
      "            num_dp_pp = n_gpus // num_tp\n",
      "            num_pp = 1\n",
      "            while num_pp <= num_dp_pp:\n",
      "                num_dp_tp = n_gpus // num_pp\n",
      "                valid = (\n",
      "                    num_dp_tp in [1, 2, 4, 8] or num_dp_tp % 8 == 0\n",
      "                ) and num_dp_pp % num_pp == 0\n",
      "                if valid:\n",
      "                    res.append(ParallelismConfig(num_pp, num_tp, num_dp_pp // num_pp))\n",
      "                num_pp += 1\n",
      "    return res\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RPCAllocation:\n",
      "    rpc: MFCDef\n",
      "    device_mesh: DeviceMesh\n",
      "    parallel: ParallelismConfig\n",
      "\n",
      "    def __post_init__(self):\n",
      "        world_size = (\n",
      "            self.parallel.tensor_parallel_size\n",
      "            * self.parallel.pipeline_parallel_size\n",
      "            * self.parallel.data_parallel_size\n",
      "        )\n",
      "        assert world_size == self.device_mesh.mapping.sum(), (\n",
      "            \"World size of ParallelismConfig does not match number of GPUs in device mesh\"\n",
      "            f\"world_size {world_size} != n GPUs {self.device_mesh.mapping.sum()}\"\n",
      "        )\n",
      "\n",
      "    def to_dict(self):\n",
      "        return dict(\n",
      "            rpc=self.rpc.name,\n",
      "            device_mesh=self.device_mesh.to_dict(),\n",
      "            parallel=dataclasses.asdict(self.parallel),\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def from_dict(d):\n",
      "        return RPCAllocation(\n",
      "            rpc=d[\"rpc\"],\n",
      "            device_mesh=DeviceMesh.from_dict(d[\"device_mesh\"]),\n",
      "            parallel=ParallelismConfig(**d[\"parallel\"]),\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/quickstart/entrypoint.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import datetime\n",
      "import functools\n",
      "import inspect\n",
      "import json\n",
      "import os\n",
      "from typing import Callable\n",
      "\n",
      "import hydra\n",
      "import yaml\n",
      "from hydra.core.config_store import ConfigStore\n",
      "from omegaconf import MISSING, OmegaConf\n",
      "\n",
      "import realhf.api.core.system_api as system_api\n",
      "from realhf.base.constants import init_constants\n",
      "from realhf.base.ray_utils import check_ray_availability\n",
      "from realhf.base.slurm_utils import check_slurm_availability\n",
      "\n",
      "\n",
      "def kind_reminder(config_name, logger, args):\n",
      "    from realhf.base.constants import LOG_ROOT, MODEL_SAVE_ROOT\n",
      "\n",
      "    logger.info(f\"Running {config_name} experiment.\")\n",
      "    logger.info(\n",
      "        f\"Logs will be dumped to {os.path.join(LOG_ROOT, args.experiment_name, args.trial_name)}\"\n",
      "    )\n",
      "    logger.info(\n",
      "        f\"Experiment configs will be dumped to {os.path.join(LOG_ROOT, args.experiment_name, args.trial_name, 'config.yaml')}\"\n",
      "    )\n",
      "    logger.info(\n",
      "        f\"Model checkpoints will be saved to {os.path.join(MODEL_SAVE_ROOT, args.experiment_name, args.trial_name)}\"\n",
      "    )\n",
      "\n",
      "    if args.mode == \"slurm\":\n",
      "        slurm_available = check_slurm_availability()\n",
      "        if slurm_available:\n",
      "            logger.info(\"Launching experiments with SLURM...\")\n",
      "        else:\n",
      "            logger.warning(\"Slurm is not available. Using local mode.\")\n",
      "            args.mode = \"local\"\n",
      "    elif args.mode == \"ray\":\n",
      "        ray_available = check_ray_availability()\n",
      "        if ray_available:\n",
      "            logger.info(\"Launching experiments with RAY...\")\n",
      "        else:\n",
      "            logger.warning(\"Ray is not available. Using local mode.\")\n",
      "            args.mode = \"local\"\n",
      "    elif args.mode == \"local\":\n",
      "        logger.info(\"Launching experiments locally.\")\n",
      "    else:\n",
      "        raise ValueError(f\"Invalid mode {args.mode}\")\n",
      "\n",
      "\n",
      "cs = ConfigStore.instance()\n",
      "QUICKSTART_CONFIG_CLASSES = {}\n",
      "QUICKSTART_USERCODE_PATHS = {}\n",
      "QUICKSTART_FN = {}\n",
      "\n",
      "\n",
      "def register_quickstart_exp(config_name: str, exp_cls: Callable):\n",
      "    usercode_path = os.path.abspath(inspect.getfile(inspect.currentframe().f_back))\n",
      "\n",
      "    @hydra.main(version_base=None, config_name=config_name)\n",
      "    def run(args):\n",
      "        # NOTE: we import logging here to avoid hydra logging overwrite\n",
      "        import realhf.base.logging as logging\n",
      "\n",
      "        logger = logging.getLogger(\"quickstart\", \"colored\")\n",
      "\n",
      "        # print_runtime_helper(OmegaConf.to_object(args))\n",
      "\n",
      "        exp_name = args.experiment_name\n",
      "        if args.trial_name == MISSING:\n",
      "            args.trial_name = trial_name = (\n",
      "                f\"run{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
      "            )\n",
      "        else:\n",
      "            trial_name = args.trial_name\n",
      "        from realhf.apps.main import main_start, main_stop\n",
      "\n",
      "        init_constants(args)\n",
      "        from realhf.base.constants import LOG_ROOT, QUICKSTART_EXPR_CACHE_PATH\n",
      "\n",
      "        config_save_path = os.path.join(\n",
      "            LOG_ROOT, args.experiment_name, args.trial_name, \"config.yaml\"\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(config_save_path), exist_ok=True)\n",
      "        with open(config_save_path, \"w\") as f:\n",
      "            yaml.dump(\n",
      "                dataclasses.asdict(OmegaConf.to_object(args)),\n",
      "                f,\n",
      "                default_flow_style=False,\n",
      "                sort_keys=False,\n",
      "            )\n",
      "\n",
      "        kind_reminder(config_name, logger, args)\n",
      "\n",
      "        exp_fn = functools.partial(exp_cls, **args)\n",
      "\n",
      "        os.makedirs(os.path.dirname(QUICKSTART_EXPR_CACHE_PATH), exist_ok=True)\n",
      "        cache_file = os.path.join(\n",
      "            QUICKSTART_EXPR_CACHE_PATH, f\"{exp_name}_{trial_name}.json\"\n",
      "        )\n",
      "        with open(cache_file, \"w\") as f:\n",
      "            dict_args = OmegaConf.to_container(args)\n",
      "            json.dump(\n",
      "                dict(\n",
      "                    args=dict_args,\n",
      "                    usercode_path=usercode_path,\n",
      "                    config_name=config_name,\n",
      "                ),\n",
      "                f,\n",
      "                indent=4,\n",
      "                ensure_ascii=False,\n",
      "            )\n",
      "\n",
      "        system_api.register_experiment(exp_name, exp_fn)\n",
      "\n",
      "        try:\n",
      "            main_start(args)\n",
      "        except Exception as e:\n",
      "            main_stop(args)\n",
      "            logger.warning(\"Exception occurred. Stopping all workers.\")\n",
      "            raise e\n",
      "\n",
      "    cs.store(name=config_name, node=exp_cls)\n",
      "\n",
      "    # assert config_name not in QUICKSTART_CONFIG_CLASSES\n",
      "    QUICKSTART_CONFIG_CLASSES[config_name] = exp_cls\n",
      "    # assert config_name not in QUICKSTART_USERCODE_PATHS\n",
      "    QUICKSTART_USERCODE_PATHS[config_name] = usercode_path\n",
      "    # assert config_name not in QUICKSTART_FN\n",
      "    QUICKSTART_FN[config_name] = run\n",
      "    return run\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/quickstart/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# NOTE: required by hydra\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/model_api.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import abc\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import keyword\n",
      "from typing import Any, Callable, Dict, Hashable, List, Literal, Optional, Tuple, Union\n",
      "\n",
      "import aiohttp\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.utils.data\n",
      "import transformers\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import GenerationHyperparameters\n",
      "from realhf.api.core.config import (\n",
      "    ModelAbstraction,\n",
      "    ModelBackendAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelName,\n",
      "    ModelWrapperAbstraction,\n",
      ")\n",
      "from realhf.api.core.data_api import MicroBatchSpec, SequenceSample, load_hf_tokenizer\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.base.recover import StepInfo\n",
      "\n",
      "logger = logging.getLogger(\"model_api\")\n",
      "\n",
      "\n",
      "class ZeroTotalLossWeightException(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class GenRespMeta:\n",
      "    qid: str\n",
      "    accepted: bool\n",
      "    n_tokens: int\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class GenReqMeta:\n",
      "    ## Meta info used to schedule the request. ##\n",
      "    qid: Hashable\n",
      "    prompt_len: int\n",
      "    group_size: int\n",
      "    new_token_budget: int\n",
      "    predicted_new_tokens: int | None\n",
      "    previous_server_url: str = \"\"\n",
      "    previous_version: int = -1\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelVersionReq:\n",
      "    server_url: str\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class APIGenerateInput:\n",
      "    # The unique query id of this prompt\n",
      "    qid: Hashable\n",
      "    # prompt token ids\n",
      "    prompt_ids: List[int]\n",
      "    # prompt token ids + generated prefix, the input to server\n",
      "    input_ids: List[int]\n",
      "    # the sampling params to server, may limit n=1 and max_new_tokens\n",
      "    # for partial rollout\n",
      "    gconfig: GenerationHyperparameters\n",
      "    # stop tokens, usually EOS and PAD\n",
      "    stop_token_ids: List[int] = dataclasses.field(default_factory=list)\n",
      "    # whether to return logprobs\n",
      "    return_logprob: bool = True\n",
      "    # logprobs of preivous generation\n",
      "    # length len(input_ids) - len(prompt_ids)\n",
      "    prev_logprobs: List[float] = dataclasses.field(default_factory=list)\n",
      "    # the weight version when submitting this request\n",
      "    version_start: int = -1\n",
      "\n",
      "    # other metadata\n",
      "    metadata: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class APIGenerateOutput:\n",
      "    ## input re-export ##\n",
      "    qid: Hashable\n",
      "    prompt_ids: List[int]\n",
      "    input_ids: List[int]\n",
      "    gconfig: GenerationHyperparameters\n",
      "    prev_logprobs: List[float] = dataclasses.field(default_factory=list)\n",
      "    version_start: int = -1\n",
      "    metadata: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "    ## outputs. To be amended by the reply. ##\n",
      "    # output token ids\n",
      "    output_ids: List[List[int]] = dataclasses.field(default_factory=list)\n",
      "    # output logprobs with the same length as output_ids\n",
      "    output_logprobs: List[List[float]] = dataclasses.field(default_factory=list)\n",
      "    # the weight version when finishing this request\n",
      "    version_end: List[int] = dataclasses.field(default_factory=list)\n",
      "    # whether truncated\n",
      "    no_eos: List[bool] = dataclasses.field(default_factory=list)\n",
      "\n",
      "    # statistics\n",
      "    latency: float = float(\"inf\")\n",
      "    ttft: float = float(\"inf\")  # Time to first token\n",
      "    itl: List[float] = dataclasses.field(\n",
      "        default_factory=list\n",
      "    )  # List of inter-token latencies\n",
      "\n",
      "    @classmethod\n",
      "    def from_input(cls, inp: APIGenerateInput):\n",
      "        return cls(\n",
      "            qid=inp.qid,\n",
      "            prompt_ids=inp.prompt_ids,\n",
      "            input_ids=inp.input_ids,\n",
      "            gconfig=inp.gconfig,\n",
      "            prev_logprobs=inp.prev_logprobs,\n",
      "            version_start=inp.version_start,\n",
      "            metadata=inp.metadata,\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def concat(outputs: List[\"APIGenerateOutput\"]):\n",
      "        assert len(set([o.qid for o in outputs])) == 1\n",
      "        return APIGenerateOutput(\n",
      "            qid=outputs[0].qid,\n",
      "            prompt_ids=outputs[0].prompt_ids,\n",
      "            input_ids=outputs[0].input_ids,\n",
      "            gconfig=outputs[0].gconfig,\n",
      "            prev_logprobs=outputs[0].prev_logprobs,\n",
      "            version_start=outputs[0].version_start,\n",
      "            metadata=outputs[0].metadata,\n",
      "            output_ids=sum([o.output_ids for o in outputs], []),\n",
      "            output_logprobs=sum([o.output_logprobs for o in outputs], []),\n",
      "            version_end=sum([o.version_end for o in outputs], []),\n",
      "            no_eos=sum([o.no_eos for o in outputs], []),\n",
      "            latency=max([o.latency for o in outputs]),\n",
      "            ttft=max([o.ttft for o in outputs]),\n",
      "            itl=sum([o.itl for o in outputs], []),\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def group_size(self):\n",
      "        return len(self.output_ids)\n",
      "\n",
      "    @property\n",
      "    def output_lens(self):\n",
      "        return [len(x) for x in self.output_ids]\n",
      "\n",
      "    @property\n",
      "    def input_len(self):\n",
      "        return len(self.input_ids)\n",
      "\n",
      "    @property\n",
      "    def prompt_len(self):\n",
      "        return len(self.prompt_ids)\n",
      "\n",
      "    @property\n",
      "    def gen_lens(self):\n",
      "        return [len(x) + self.input_len - self.prompt_len for x in self.output_ids]\n",
      "\n",
      "    def get_logprobs(self) -> List[List[float]]:\n",
      "        logprobs = []\n",
      "        for logp in self.output_logprobs:\n",
      "            assert len(self.prev_logprobs) == self.input_len - self.prompt_len, (\n",
      "                len(self.prev_logprobs),\n",
      "                self.input_len,\n",
      "                self.prompt_len,\n",
      "            )\n",
      "            logprobs.append([0.0] * (self.prompt_len - 1) + self.prev_logprobs + logp)\n",
      "        return logprobs\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class BundledGenerationOutputs:\n",
      "    ## Used for collecting generation outputs for env interaction or training. ##\n",
      "\n",
      "    # unique query id in the dataset\n",
      "    qid: Hashable\n",
      "    # prompt token ids\n",
      "    prompt_ids: List[int]\n",
      "    # output token ids excluding the prompt\n",
      "    output_ids: List[List[int]]\n",
      "    # whole sequences including the prompt\n",
      "    seqs: List[List[int]]\n",
      "    # whole logprobs, one token shorter than seq\n",
      "    # logps at prompt tokens are zero\n",
      "    logprobs: List[List[float]]\n",
      "    # whether truncated\n",
      "    no_eos: List[bool]\n",
      "    # server weight version when starting generation\n",
      "    version_start: List[int]\n",
      "    # server weight version when generation ends\n",
      "    version_end: List[int]\n",
      "\n",
      "    @classmethod\n",
      "    def from_api_outputs(cls, outputs: List[APIGenerateOutput]):\n",
      "        assert len(set(o.qid for o in outputs)) == 1\n",
      "        prompt_len = len(outputs[0].prompt_ids)\n",
      "        seqs = []\n",
      "        logprobs = []\n",
      "        version_starts = []\n",
      "        for o in outputs:\n",
      "            for out in o.output_ids:\n",
      "                seqs.append(o.input_ids + out)\n",
      "            for logp in o.get_logprobs():\n",
      "                logprobs.append(logp)\n",
      "            version_starts += [o.version_start] * o.group_size\n",
      "        return cls(\n",
      "            qid=outputs[0].qid,\n",
      "            prompt_ids=outputs[0].prompt_ids,\n",
      "            seqs=seqs,\n",
      "            output_ids=[seq[prompt_len:] for seq in seqs],\n",
      "            logprobs=logprobs,\n",
      "            no_eos=sum([o.no_eos for o in outputs], []),\n",
      "            version_start=version_starts,\n",
      "            version_end=sum([o.version_end for o in outputs], []),\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def output_logprobs(self):\n",
      "        return [lp[self.prompt_len - 1 :] for lp in self.logprobs]\n",
      "\n",
      "    @property\n",
      "    def output_lens(self):\n",
      "        return [len(out) for out in self.output_ids]\n",
      "\n",
      "    @property\n",
      "    def seqlens(self):\n",
      "        return [len(seq) for seq in self.seqs]\n",
      "\n",
      "    @property\n",
      "    def prompt_len(self):\n",
      "        return len(self.prompt_ids)\n",
      "\n",
      "\n",
      "AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(\n",
      "    total=6 * 60 * 60,\n",
      "    connect=300,\n",
      ")\n",
      "\n",
      "\n",
      "class LLMAPIClient:\n",
      "    def __init__(\n",
      "        self, generate_url: str, update_weights_url: str, concurrency_limit: int = -1\n",
      "    ):\n",
      "        self.update_weights_url = update_weights_url\n",
      "        self.generate_url = generate_url\n",
      "        self.concurrency_limit = concurrency_limit\n",
      "\n",
      "        self.session: aiohttp.ClientSession\n",
      "        self.semaphore: asyncio.Semaphore\n",
      "\n",
      "    async def __aenter__(self):\n",
      "        conn = aiohttp.TCPConnector(limit=0, ttl_dns_cache=300, force_close=True)\n",
      "        self.session = aiohttp.ClientSession(\n",
      "            timeout=AIOHTTP_TIMEOUT,\n",
      "            connector=conn,\n",
      "            read_bufsize=1024 * 1024 * 10,\n",
      "        )\n",
      "        if self.concurrency_limit > 0:\n",
      "            self.semaphore = asyncio.Semaphore(self.concurrency_limit)\n",
      "        return self\n",
      "\n",
      "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
      "        if self.session:\n",
      "            await self.session.close()\n",
      "\n",
      "    async def async_add_generate_request(\n",
      "        self, req: APIGenerateInput, stream: bool = True\n",
      "    ) -> APIGenerateOutput:\n",
      "\n",
      "        if self.concurrency_limit > 0:\n",
      "            async with self.semaphore:\n",
      "                return await self._do_generate(req, stream=stream)\n",
      "        else:\n",
      "            return await self._do_generate(req, stream=stream)\n",
      "\n",
      "    async def _do_generate(\n",
      "        self, req: APIGenerateInput, stream: bool = True\n",
      "    ) -> APIGenerateOutput:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    async def async_update_weights_from_disk(self, path):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ReaLMoEConfig:\n",
      "    \"\"\"Configuration for MoE models.\n",
      "\n",
      "    :param num_experts: The number of experts in the mixture of experts.\n",
      "    :type num_experts: int\n",
      "    :param top_k: The number of experts to route per token, also\n",
      "        interpreted as the `top-k` routing parameter.\n",
      "    :type top_k: int\n",
      "    :param routing_type: The load balancing type for the MoE router. Can\n",
      "        be \"aux_loss\", \"sinkhorn\", or \"none\".\n",
      "    :type routing_type: str\n",
      "    :param aux_loss_coeff: The coefficient for the auxiliary loss.\n",
      "        Effective only when routing_type=\"aux_loss\".\n",
      "    :type aux_loss_coeff: float\n",
      "    :param capacity_factor: The capacity factor of each expert. An\n",
      "        expert will drop tokens if the number of tokens exceeds\n",
      "        capacity_factor * (num_tokens / num_experts). No tokens will be\n",
      "        dropped if capacity_factor is None.\n",
      "    :type capacity_factor: float or None\n",
      "    :param pad_to_capacity: Whether to pad the input to the capacity of\n",
      "        the expert.\n",
      "    :type pad_to_capacity: bool\n",
      "    :param token_drop_policy: The token drop policy for the MoE. Can be\n",
      "        either \"prob\" or \"position\". If \"prob\", the tokens with the\n",
      "        lowest probabilities will be dropped. If \"position\", tokens at\n",
      "        the end of each batch will be dropped.\n",
      "    :type token_drop_policy: str\n",
      "    :param z_loss_coeff: The coefficient for the z-loss.\n",
      "    :type z_loss_coeff: float\n",
      "    :param input_jitter_eps: The input jitter noise for the router.\n",
      "    :type input_jitter_eps: float\n",
      "    \"\"\"\n",
      "\n",
      "    num_experts: int = 8\n",
      "    top_k: int = 2\n",
      "    routing_type: str = \"aux_loss\"\n",
      "    aux_loss_coeff: float = 1e-3\n",
      "    capacity_factor: float = None\n",
      "    pad_to_capacity: bool = False\n",
      "    token_drop_policy: str = \"probs\"\n",
      "    z_loss_coeff: float = 0.0\n",
      "    input_jitter_eps: Optional[float] = None\n",
      "    use_grouped_gemm: bool = False\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ReaLModelConfig:\n",
      "    \"\"\"Configuration for the ReaLModel.\n",
      "\n",
      "    :param n_layers: The number of transformer blocks.\n",
      "    :type n_layers: int\n",
      "    :param n_kv_heads: The number of key-value attention heads.\n",
      "    :type n_kv_heads: int\n",
      "    :param n_q_heads: The number of query attention heads.\n",
      "    :type n_q_heads: int\n",
      "    :param head_dim: The dimension of each attention head.\n",
      "        If None, it defaults to hidden_dim // n_q_heads.\n",
      "        If specified, the query layer will have the shape\n",
      "        (hidden_dim, head_dim * n_q_heads).\n",
      "    :type head_dim: int or None\n",
      "    :param hidden_dim: The hidden dimension of the transformer block.\n",
      "    :type hidden_dim: int\n",
      "    :param intermediate_dim: The dimension of the intermediate layer in the MLP.\n",
      "    :type intermediate_dim: int\n",
      "    :param vocab_size: The vocabulary size.\n",
      "    :type vocab_size: int\n",
      "    :param n_positions: The maximum context length. Can be None for\n",
      "        rotary embedding, where the context length is determined during runtime.\n",
      "    :type n_positions: Optional[int]\n",
      "    :param embd_pdrop: The dropout probability for the embedding layer.\n",
      "    :type embd_pdrop: float\n",
      "    :param resid_pdrop: The dropout probability for the residual connections.\n",
      "    :type resid_pdrop: float\n",
      "    :param attn_pdrop: The dropout probability for the attention weights.\n",
      "    :type attn_pdrop: float\n",
      "    :param layer_norm_epsilon: The epsilon value for layer normalization.\n",
      "    :type layer_norm_epsilon: float\n",
      "    :param activation_function: The activation function for the MLP.\n",
      "    :type activation_function: str\n",
      "    :param scale_attn_by_inverse_layer_idx: Whether to scale the attention weights\n",
      "        by the inverse of the layer index.\n",
      "    :type scale_attn_by_inverse_layer_idx: bool\n",
      "    :param use_attention_bias: Whether to use bias for QKV layers.\n",
      "    :type use_attention_bias: bool\n",
      "    :param use_attn_proj_bias: Whether to use bias for the attention projection layer.\n",
      "    :type use_attn_proj_bias: bool\n",
      "    :param layer_norm_type: The type of layer normalization. Can be None, \"rms\", or \"gemma\".\n",
      "    :type layer_norm_type: Optional[str]\n",
      "    :param mlp_type: The type of the MLP. Can be None, \"llama\", or \"moe\".\n",
      "    :type mlp_type: Optional[str]\n",
      "    :param apply_rotary: Whether to apply rotary embedding.\n",
      "    :type apply_rotary: bool\n",
      "    :param rotary_base: The exponential base for the rotary embedding.\n",
      "    :type rotary_base: float\n",
      "    :param rotary_interleaved: Whether to use interleaved rotary embedding.\n",
      "    :type rotary_interleaved: bool\n",
      "    :param rotary_scaling: The scaling factor for the rotary embedding.\n",
      "    :type rotary_scaling: Optional[float]\n",
      "    :param rotary_scaling_type: The type of scaling for the rotary embedding.\n",
      "    :type rotary_scaling_type: Optional[str]\n",
      "    :param normalize_embed: Whether to normalize the embeddings\n",
      "        before passing them through the transformer blocks. Used by Gemma.\n",
      "    :type normalize_embed: bool\n",
      "    :param abs_position_embedding_offset: The offset for the absolute position embedding.\n",
      "        Used by OPT, but OPT is currently not supported.\n",
      "    :type abs_position_embedding_offset: int\n",
      "    :param do_layernorm_before: Whether to apply layer normalization before the attention\n",
      "        rather than after. Used by OPT, but OPT is currently not supported.\n",
      "    :type do_layernorm_before: bool\n",
      "    :param tied_embedding: Whether to share the embeddings and output weights.\n",
      "        Used by models like GPT-2 and Gemma.\n",
      "    :type tied_embedding: bool\n",
      "    :param sliding_window: The sliding window size for the attention.\n",
      "        Currently a placeholder and not supported.\n",
      "    :type sliding_window: Optional[int]\n",
      "    :param moe: Configuration for MoE models, only effective when mlp_type=\"moe\".\n",
      "    :type moe: Optional[ReaLMoEConfig]\n",
      "    :param is_critic: Whether the model is a critic model.\n",
      "    :type is_critic: bool\n",
      "    \"\"\"\n",
      "\n",
      "    ### Architectural configurations. ###\n",
      "    n_layers: int\n",
      "    n_kv_heads: int\n",
      "    n_q_heads: int\n",
      "    hidden_dim: int\n",
      "    intermediate_dim: int  # for mlp, usually 4*h\n",
      "    vocab_size: int\n",
      "    n_positions: int\n",
      "    head_dim: Optional[int] = None\n",
      "    embd_pdrop: float = 0.1\n",
      "    resid_pdrop: float = 0.1\n",
      "    attn_pdrop: float = 0.1\n",
      "    layer_norm_epsilon: float = 1e-5\n",
      "    activation_function: str = \"gelu\"\n",
      "    scale_attn_by_inverse_layer_idx: bool = True\n",
      "    scale_attn_weights: bool = True\n",
      "    # llama does not use attention bias and uses special MLP/LayerNorm layers\n",
      "    use_attention_bias: bool = True\n",
      "    use_attn_proj_bias: bool = True\n",
      "    use_mlp_bias: bool = False\n",
      "    layer_norm_type: Optional[str] = None\n",
      "    mlp_type: Optional[str] = None\n",
      "    # rotary embedding\n",
      "    apply_rotary: bool = False\n",
      "    rotary_base: float = 10000.0\n",
      "    rotary_interleaved: bool = False\n",
      "    rotary_scaling: Optional[float] = None\n",
      "    rotary_scaling_type: Optional[str] = None\n",
      "    rotary_special_impl: Optional[str] = None\n",
      "    # for gemma\n",
      "    normalize_embed: bool = False\n",
      "    # for qwen3\n",
      "    qk_layernorm: bool = False\n",
      "    # for opt, it's 2\n",
      "    abs_position_embedding_offset: int = 0\n",
      "    do_layernorm_before: bool = True\n",
      "    # for bailing\n",
      "    norm_head: bool = False\n",
      "    norm_softmax: bool = False\n",
      "    # Tied embedding\n",
      "    tied_embedding: bool = False\n",
      "    sliding_window: Optional[int] = None\n",
      "    # MoE Config\n",
      "    moe: Optional[ReaLMoEConfig] = None\n",
      "\n",
      "    # Whether it is a critic/reward model that outputs scores.\n",
      "    is_critic: bool = False\n",
      "\n",
      "    # The HuggingFace checkpoint\n",
      "    base_model_path: Optional[str] = None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.is_critic and self.tied_embedding:\n",
      "            raise ValueError(\"Critic model cannot share embeddings and output weights.\")\n",
      "        if self.head_dim is None:\n",
      "            self.head_dim = self.hidden_dim // self.n_q_heads\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class FinetuneSpec:\n",
      "    \"\"\"The specification for the fine-tuning task.\n",
      "\n",
      "    :param total_train_epochs: The total number of epochs for training.\n",
      "    :type total_train_epochs: int\n",
      "    :param dataset_size: The total number of data.\n",
      "    :type dataset_size: int\n",
      "    :param train_batch_size: The batch size for training.\n",
      "    :type train_batch_size: int\n",
      "    \"\"\"\n",
      "\n",
      "    total_train_epochs: int\n",
      "    dataset_size: int\n",
      "    train_batch_size: int\n",
      "\n",
      "    @property\n",
      "    def total_train_steps(self):\n",
      "        dsize = self.dataset_size * self.total_train_epochs\n",
      "        return (dsize + self.train_batch_size - 1) // self.train_batch_size\n",
      "\n",
      "    def is_new_epoch(self, version: StepInfo) -> bool:\n",
      "        return (\n",
      "            version.global_step * self.train_batch_size\n",
      "        ) // self.dataset_size > version.epoch\n",
      "\n",
      "    def is_epoch_last_step(self, version: StepInfo) -> bool:\n",
      "        return (\n",
      "            self.dataset_size\n",
      "            - version.global_step * self.train_batch_size % self.dataset_size\n",
      "        ) <= self.train_batch_size\n",
      "\n",
      "    def inc_version(self, version: StepInfo) -> StepInfo:\n",
      "        if self.is_new_epoch(version):\n",
      "            version.epoch += 1\n",
      "            version.epoch_step = 0\n",
      "        version.epoch_step += 1\n",
      "        version.global_step += 1\n",
      "        return version\n",
      "\n",
      "\n",
      "class PipelinableEngine(abc.ABC):\n",
      "    \"\"\"Defines the signature for modules after backend initialization.\n",
      "\n",
      "    Modules with this signature will be passed to :class:`ModelInterface`\n",
      "    for model function call execution.\n",
      "    \"\"\"\n",
      "\n",
      "    def train_batch(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        loss_fn: Callable[[torch.Tensor, SequenceSample], torch.Tensor],\n",
      "        loss_weight_fn: Callable[[torch.Tensor, SequenceSample], float],\n",
      "        version_steps: int,\n",
      "        token_normalize_scope: Literal[\"global\", \"dp\"] = \"global\",\n",
      "    ) -> Dict:\n",
      "        \"\"\"Update the model with a batch of data and a loss function.\n",
      "\n",
      "        :param input_: The input data. It should contain at least the key ``packed_input_ids``,\n",
      "            which includes the concatenated token sequences. It should also include any other\n",
      "            entries required to compute the loss.\n",
      "        :type input_: SequenceSample\n",
      "        :param loss_fn: The loss function. It takes the output of the forward pass and the\n",
      "            input data, returning the loss.\n",
      "        :type loss_fn: Callable[[torch.Tensor, SequenceSample], torch.Tensor]\n",
      "        :param loss_weight_fn: This function is used to calculate the number of valid tokens\n",
      "            when normalizing loss across micro batches and DP ranks. Can be `lambda: 1`\n",
      "            if just taking the average over batches.\n",
      "        :type loss_weight_fn: Callable[[torch.Tensor, SequenceSample], float]\n",
      "        :param version_steps: The global step counter for this experiment,\n",
      "            used by the backend to determine the learning rate schedule.\n",
      "        :type version_steps: int\n",
      "        :param global_normalize_scope: The scope of token-wise loss normalization. Choices:\n",
      "            global: average across all micro batches across DP ranks.\n",
      "            dp: average across micro batches in current DP rank.\n",
      "            Default to \"global\".\n",
      "        :type global_normalize_scope: Literal[\"global\", \"dp\"]\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def eval_batch(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        loss_fn: Callable[[torch.Tensor, SequenceSample], torch.Tensor],\n",
      "    ) -> torch.Tensor | None:\n",
      "        \"\"\"Evaluate the model using the forward pass and loss function.\n",
      "\n",
      "        This method wraps :meth:`forward` with a customized ``post_hook`` and ``aggregate_fn``.\n",
      "\n",
      "        :param input_: The input data. It should contain at least the key ``packed_input_ids``,\n",
      "            which includes the concatenated token sequences. It should also include any other\n",
      "            entries required to compute the loss.\n",
      "        :type input_: SequenceSample\n",
      "        :param loss_fn: The loss function. It takes the output of the forward pass and the\n",
      "            input data, returning the loss.\n",
      "        :type loss_fn: Callable[[torch.Tensor, SequenceSample], torch.Tensor]\n",
      "        :return: The aggregated scalar loss if on the last pipe stage.\n",
      "        :rtype: torch.Tensor | None\n",
      "        \"\"\"\n",
      "\n",
      "        def _loss_fn(out, inp_):\n",
      "            # To prevent calling data reordering.\n",
      "            return float(loss_fn(out, inp_))\n",
      "\n",
      "        return self.forward(\n",
      "            input_=input_,\n",
      "            mb_spec=mb_spec,\n",
      "            post_hook=_loss_fn,\n",
      "            aggregate_fn=sum,\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        output_seqlens: List[List[int]] | None = None,\n",
      "        post_hook: Callable[[torch.Tensor, SequenceSample], Any] | None = None,\n",
      "        aggregate_fn: Callable[[List[Any]], Any] = torch.cat,\n",
      "    ) -> Any | None:\n",
      "        \"\"\"Run the forward pass or inference on the model. Note that it is\n",
      "        gradient-free.\n",
      "\n",
      "        To train the model, use :meth:`train_batch` instead.\n",
      "\n",
      "        :param input_: The input data. It should contain at least the key ``packed_input_ids``,\n",
      "            which includes the concatenated token sequences.\n",
      "        :type input_: SequenceSample\n",
      "        :param post_hook: A function to apply to the output after the forward pass.\n",
      "            It takes the output tensor and the input data, returning an arbitrary result.\n",
      "            With a post_hook, we can process the output in mini-batches,\n",
      "            reducing memory usage for operations such as gathering log-probabilities.\n",
      "            If None, this function just returns the output tensor.\n",
      "        :type post_hook: Callable[[torch.Tensor, SequenceSample], Any] | None\n",
      "        :param aggregate_fn: A function to aggregate the results of the post_hook.\n",
      "        :type aggregate_fn: Callable[[List[Any]], Any]\n",
      "        :return: The aggregated result of the post_hook from the last pipeline stage. Returns None otherwise.\n",
      "            The output before post_hook is a concatenated tensor along the batch-sequence dimension, similar to\n",
      "            ``packed_input_ids``. For example, if we have 3 sequences with lengths [2, 3, 4],\n",
      "            and the vocabulary size is 1000, ``packed_input_ids`` should have shape [9],\n",
      "            and the logits should have shape [9, 1000].\n",
      "        :rtype: Any | None\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=GenerationHyperparameters\n",
      "        ),\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor | None] | None:\n",
      "        \"\"\"Generate outputs from the model.\n",
      "\n",
      "        :param input_: The input data. It should contain at least the key ``packed_input_ids``,\n",
      "            which includes the concatenated prompts.\n",
      "        :type input_: SequenceSample\n",
      "        :param tokenizer: The tokenizer for the model.\n",
      "        :type tokenizer: transformers.PreTrainedTokenizerFast\n",
      "        :param gconfig: The generation hyperparameters.\n",
      "        :type gconfig: GenerationHyperparameters\n",
      "        :return: For the last pipeline stage, returns the generated tokens, log probabilities, and optionally the logits mask.\n",
      "            See :class:`GenerationHyperparameters` for more details about the logits mask.\n",
      "            Returns None for other stages.\n",
      "            The outputs are stacked tensors along the batch dimension. For example,\n",
      "            if we have 3 prompts with lengths [2, 3, 4], a maximum generated length of 5,\n",
      "            and a vocabulary size of 1000, ``packed_input_ids`` should have shape [9],\n",
      "            generated tokens and log probabilities should have shape [3, 5],\n",
      "            and the logits should have shape [3, 5, 1000].\n",
      "        :rtype: Tuple[torch.Tensor, torch.Tensor, torch.Tensor | None] | None\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class Model:\n",
      "    \"\"\"A collection consisting of a neural network, a tokenizer, and metadata\n",
      "    with a unique name.\n",
      "\n",
      "    :param name: The unique name of the model.\n",
      "    :type name: ModelName\n",
      "    :param module: The neural network module. Its parameters may be\n",
      "        sharded by tensor or pipeline parallelism.\n",
      "    :type module: PipelinableEngine | torch.nn.Module\n",
      "    :param tokenizer: The tokenizer associated with the model.\n",
      "    :type tokenizer: transformers.PreTrainedTokenizerFast\n",
      "    :param device: The device on which to run the model.\n",
      "    :type device: Union[str, torch.device]\n",
      "    :param dtype: The data type of the model. Defaults to torch.float16\n",
      "        if None.\n",
      "    :type dtype: Optional[torch.dtype]\n",
      "    :param version: The version of the model.\n",
      "    :type version: StepInfo\n",
      "    :param ft_spec: The fine-tuning specification for the model.\n",
      "        Generally not used.\n",
      "    :type ft_spec: FinetuneSpec\n",
      "    \"\"\"\n",
      "\n",
      "    name: ModelName\n",
      "    module: PipelinableEngine | torch.nn.Module\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast\n",
      "    device: Union[str, torch.device]\n",
      "    dtype: Optional[torch.dtype] = None\n",
      "    version: StepInfo = dataclasses.field(default_factory=StepInfo)\n",
      "    ft_spec: FinetuneSpec = None  # will be initialized by the backend\n",
      "    backend_name: Optional[str] = None  # will be initialized by the backend\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.module is None:\n",
      "            return\n",
      "        try:\n",
      "            self.module = self.module.to(self.device)\n",
      "        except ValueError as e:\n",
      "            # 4-bit and 8-bit model may fail here\n",
      "            logger.warning(\n",
      "                f\"Failed to move model to device {self.device} because {e}. Abort to device.\"\n",
      "            )\n",
      "\n",
      "    def inc_version(self):\n",
      "        self.ft_spec.inc_version(self.version)\n",
      "\n",
      "\n",
      "class ModelBackend(abc.ABC):\n",
      "    \"\"\"A backend that wraps :class:`Model` to provide additional\n",
      "    functionalities such as pipelined model function calls and ZeRO\n",
      "    optimization.\n",
      "\n",
      "    Current backend implementations include inference, DeepSpeed, and Megatron.\n",
      "    The inference backend provides only inference and generation APIs,\n",
      "    while the DeepSpeed and Megatron backends also support training.\n",
      "\n",
      "    The backend offers two main functionalities:\n",
      "\n",
      "    1. Pipelined generation, inference, and training, implemented in ReaL.\n",
      "\n",
      "    2. ZeRO optimization, implemented in DeepSpeed and Megatron.\n",
      "\n",
      "    After initialization, the ``module`` attribute in :class:`Model`\n",
      "    will have the same signature as :class:`PipelinableEngine`.\n",
      "    See ``realhf/impl/model/backend`` for concrete implementations.\n",
      "    \"\"\"\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _initialize(self, model: Model, spec: FinetuneSpec) -> Model:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def initialize(self, model: Model, spec: FinetuneSpec) -> Model:\n",
      "        \"\"\"Initialize the model with the backend to support pipelining and\n",
      "        distributed optimization.\"\"\"\n",
      "        model.ft_spec = spec\n",
      "        return self._initialize(model, spec)\n",
      "\n",
      "    def destroy(self, model: Model):\n",
      "        \"\"\"Destroy the backend and release GPU memory.\"\"\"\n",
      "        pass\n",
      "\n",
      "    def save(self, model: Model, save_dir: str):\n",
      "        \"\"\"Save backend states, e.g., optimizer states in the Adam\n",
      "        optimizer.\"\"\"\n",
      "        pass\n",
      "\n",
      "    def load(self, model: Model, load_dir: str):\n",
      "        \"\"\"Load backend states during recover.\"\"\"\n",
      "        pass\n",
      "\n",
      "\n",
      "class NullBackend(ModelBackend):\n",
      "\n",
      "    def _initialize(self, model: Model, spec: FinetuneSpec) -> Model:\n",
      "        return model\n",
      "\n",
      "\n",
      "def null_model(name: ModelName, device: Union[str, torch.device]) -> Model:\n",
      "    return Model(name, torch.nn.Identity(), None, device)\n",
      "\n",
      "\n",
      "def tokenizer_only_model(\n",
      "    name: ModelName, device: Union[str, torch.device], tokenizer_path: str\n",
      ") -> Model:\n",
      "    return Model(name, torch.nn.Identity(), load_hf_tokenizer(tokenizer_path), device)\n",
      "\n",
      "\n",
      "class ModelInterface(abc.ABC):\n",
      "    \"\"\"An interface for model training, evaluation, inference, and generation.\n",
      "\n",
      "    This interface is designed to follow the dependency injection pattern.\n",
      "    We pass the model to the interface and call its methods, ensuring that model APIs\n",
      "    and algorithms are fully decoupled. For example, REINFORCE and PPO can exhibit\n",
      "    different behaviors during training. Separate interfaces can be written for these\n",
      "    algorithms while using the same model that provides basic forward-backward-update\n",
      "    functionality (i.e., :class:`PipelinableEngine`).\n",
      "\n",
      "    During runtime, the master worker requests model workers to execute a specific\n",
      "    interface type (e.g., generate) on a specific model. The model worker locates\n",
      "    the corresponding model, passes it into the requested interface, performs the\n",
      "    computation, and returns the result.\n",
      "\n",
      "    Users can easily create new interfaces to support customized usage.\n",
      "    See :doc:`customization` for more details.\n",
      "    \"\"\"\n",
      "\n",
      "    def save(self, model: Model, save_dir: str):\n",
      "        pass\n",
      "\n",
      "    def evaluate(\n",
      "        self,\n",
      "        model: Model,\n",
      "        eval_dataloader: torch.utils.data.DataLoader,\n",
      "    ) -> Dict:\n",
      "        # NOTE: No n_mbs here because the batch size can be configured in the dataloader.\n",
      "        return {}\n",
      "\n",
      "    def inference(\n",
      "        self,\n",
      "        model: Model,\n",
      "        data: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample | None:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def generate(\n",
      "        self,\n",
      "        model: Model,\n",
      "        data: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample | None:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def train_step(\n",
      "        self,\n",
      "        model: Model,\n",
      "        data: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> Dict | List[Dict]:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    # Mock methods for creating data and profiling an individual MFC.\n",
      "    def _mock_generate(self, model: Model, data: SequenceSample):\n",
      "        return data\n",
      "\n",
      "    def _mock_inference(self, model: Model, data: SequenceSample):\n",
      "        return data\n",
      "\n",
      "    def _mock_train_step(self, model: Model, data: SequenceSample):\n",
      "        return data\n",
      "\n",
      "    def mock(\n",
      "        self,\n",
      "        type_: str,\n",
      "        model: Model,\n",
      "        data: SequenceSample,\n",
      "    ) -> SequenceSample:\n",
      "        if type_ == \"generate\":\n",
      "            return self._mock_generate(model, data)\n",
      "        elif type_ == \"inference\":\n",
      "            return self._mock_inference(model, data)\n",
      "        elif type_ == \"train_step\":\n",
      "            return self._mock_train_step(model, data)\n",
      "        else:\n",
      "            raise ValueError(f\"Unsupported interface type {type_}\")\n",
      "\n",
      "\n",
      "class NullInterface(ModelInterface):\n",
      "\n",
      "    def inference(\n",
      "        self, model: Model, data: SequenceSample, mb_spec: MicroBatchSpec\n",
      "    ) -> SequenceSample:\n",
      "        scores = np.random.randn(sum(len(x) for x in data.seqlens[\"packed_prompts\"]))\n",
      "        rewards = torch.from_numpy(scores).to(device=model.device, dtype=torch.float32)\n",
      "        res = SequenceSample(\n",
      "            keys=[\"rewards\"],\n",
      "            trailing_shapes=dict(rewards=()),\n",
      "            dtypes=dict(rewards=torch.float32),\n",
      "            ids=data.ids,\n",
      "            seqlens=dict(\n",
      "                rewards=[\n",
      "                    torch.tensor([1 for _ in range(len(x))], dtype=torch.int32)\n",
      "                    for x in data.seqlens[\"packed_prompts\"]\n",
      "                ],\n",
      "            ),\n",
      "            data=dict(rewards=rewards),\n",
      "        )\n",
      "        # record rewards for each piece of data\n",
      "        avg_scores = []\n",
      "        offset = 0\n",
      "        for i in range(data.bs):\n",
      "            score_lis = scores[offset : offset + len(data.seqlens[\"packed_prompts\"][i])]\n",
      "            avg_scores.append(score_lis.mean().item())\n",
      "            offset += len(data.seqlens[\"packed_prompts\"][i])\n",
      "        assert offset == sum(len(x) for x in data.seqlens[\"packed_prompts\"])\n",
      "        res.metadata[\"scores\"] = avg_scores\n",
      "        return res\n",
      "\n",
      "    def train_step(\n",
      "        self, model: Model, data: SequenceSample, mb_spec: MicroBatchSpec\n",
      "    ) -> Dict | List[Dict]:\n",
      "        from realhf.base import constants\n",
      "\n",
      "        n_tokens = sum(flat2d(data.seqlens[data._get_split_key()]))\n",
      "        n_tokens = torch.tensor(\n",
      "            n_tokens, dtype=torch.long, device=constants.current_device()\n",
      "        )\n",
      "        dist.all_reduce(n_tokens, group=constants.data_parallel_group())\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.info(f\"Number of tokens in NullInterface training: {int(n_tokens)}\")\n",
      "        model.inc_version()\n",
      "        return {}\n",
      "\n",
      "    def save(self, model: Model, save_dir: str):\n",
      "        module = model.module.module\n",
      "        module.save_to_hf(\n",
      "            tokenizer=model.tokenizer,\n",
      "            save_dir=save_dir,\n",
      "        )\n",
      "\n",
      "\n",
      "ALL_MODEL_CLASSES = {}\n",
      "ALL_INTERFACE_CLASSES = {}\n",
      "ALL_BACKEND_CLASSES = {}\n",
      "ALL_WRAPPER_CLASSES = {}\n",
      "\n",
      "\n",
      "def register_model(name, model_cls):\n",
      "    assert name not in ALL_MODEL_CLASSES\n",
      "    ALL_MODEL_CLASSES[name] = model_cls\n",
      "\n",
      "\n",
      "def register_interface(name, cls_):\n",
      "    assert name not in ALL_INTERFACE_CLASSES\n",
      "    assert issubclass(cls_, ModelInterface)\n",
      "    ALL_INTERFACE_CLASSES[name] = cls_\n",
      "\n",
      "\n",
      "def register_backend(name, cls_):\n",
      "    assert name not in ALL_BACKEND_CLASSES\n",
      "    assert issubclass(cls_, ModelBackend)\n",
      "    ALL_BACKEND_CLASSES[name] = cls_\n",
      "\n",
      "\n",
      "def register_wrapper(name, cls_):\n",
      "    assert name not in ALL_WRAPPER_CLASSES\n",
      "    ALL_WRAPPER_CLASSES[name] = cls_\n",
      "\n",
      "\n",
      "def make_model_wrapper(\n",
      "    cfg: ModelWrapperAbstraction,\n",
      ") -> Callable[[Model], Model]:\n",
      "    cls_ = ALL_WRAPPER_CLASSES[cfg.type_]\n",
      "    return cls_(**cfg.args)\n",
      "\n",
      "\n",
      "def make_model(\n",
      "    cfg: ModelAbstraction, name: ModelName, device: Union[str, torch.device]\n",
      ") -> Model:\n",
      "    model_cls = ALL_MODEL_CLASSES[cfg.type_]\n",
      "    model = model_cls(**cfg.args, name=name, device=device)\n",
      "    assert isinstance(model, Model)\n",
      "    for w in cfg.wrappers:\n",
      "        model = make_model_wrapper(w)(model)\n",
      "        assert isinstance(model, Model)\n",
      "    return model\n",
      "\n",
      "\n",
      "def make_interface(cfg: ModelInterfaceAbstraction) -> ModelInterface:\n",
      "    cls_ = ALL_INTERFACE_CLASSES[cfg.type_]\n",
      "    return cls_(**cfg.args)\n",
      "\n",
      "\n",
      "def make_backend(cfg: ModelBackendAbstraction) -> ModelBackend:\n",
      "    cls_ = ALL_BACKEND_CLASSES[cfg.type_]\n",
      "    return cls_(**cfg.args)\n",
      "\n",
      "\n",
      "register_interface(\"null\", NullInterface)\n",
      "register_backend(\"null\", NullBackend)\n",
      "register_model(\"null\", null_model)\n",
      "register_model(\"tokenizer\", tokenizer_only_model)\n",
      "\n",
      "SUPPORTED_MODELS = []\n",
      "HF_MODEL_FAMILY_REGISTRY = {}\n",
      "\n",
      "\n",
      "def is_valid_function_name(name):\n",
      "    if not name.isidentifier():\n",
      "        return False\n",
      "    if keyword.iskeyword(name):\n",
      "        return False\n",
      "    return True\n",
      "\n",
      "\n",
      "def register_hf_family(\n",
      "    name: str,\n",
      "    hf_cls_name: str,\n",
      "    config_from_hf_converter: Callable[\n",
      "        [transformers.PretrainedConfig], ReaLModelConfig\n",
      "    ],\n",
      "    config_to_hf_converter: Callable[[ReaLModelConfig], transformers.PretrainedConfig],\n",
      "    sd_from_hf_converter: Callable[[Dict, ReaLModelConfig], Dict],\n",
      "    sd_to_hf_converter: Callable[[Dict, ReaLModelConfig], Dict],\n",
      "    embedding_param_names: Callable[[ReaLModelConfig], List[str]],\n",
      "    tblock_param_names: Callable[[ReaLModelConfig, int], List[str]],\n",
      "    head_param_names: Callable[[ReaLModelConfig], List[str]],\n",
      "    real_config_maker: Optional[Callable] = None,\n",
      "):\n",
      "    if name in SUPPORTED_MODELS:\n",
      "        raise ValueError(f\"Model {name} is already registered.\")\n",
      "    if not is_valid_function_name(name):\n",
      "        raise ValueError(f\"Model name {name} is not a valid function name in Python.\")\n",
      "    SUPPORTED_MODELS.append(name)\n",
      "    HF_MODEL_FAMILY_REGISTRY[name] = dict(\n",
      "        name=name,\n",
      "        hf_cls_name=hf_cls_name,\n",
      "        config_from_hf_converter=config_from_hf_converter,\n",
      "        config_to_hf_converter=config_to_hf_converter,\n",
      "        sd_from_hf_converter=sd_from_hf_converter,\n",
      "        sd_to_hf_converter=sd_to_hf_converter,\n",
      "        embedding_param_names=embedding_param_names,\n",
      "        tblock_param_names=tblock_param_names,\n",
      "        head_param_names=head_param_names,\n",
      "        real_config_maker=real_config_maker,\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/agent_api.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# Experimental APIs of RL agents.\n",
      "\n",
      "import asyncio\n",
      "from abc import ABC\n",
      "from typing import List\n",
      "\n",
      "from realhf.api.core.config import AgentAbstraction\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "\n",
      "\n",
      "class Agent(ABC):\n",
      "    # TODO: implement type checking inside each queue.\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "ALL_AGNETS = {}\n",
      "\n",
      "\n",
      "def register_agent(name, cls_):\n",
      "    assert name not in ALL_AGNETS\n",
      "    ALL_AGNETS[name] = cls_\n",
      "\n",
      "\n",
      "def make_agent(cfg: AgentAbstraction) -> Agent:\n",
      "    return ALL_AGNETS[cfg.type_](**cfg.args)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/system_api.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import os\n",
      "from enum import Enum\n",
      "from typing import Any, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import realhf.api.core.dfg as dfg\n",
      "from realhf.api.cli_args import (\n",
      "    AutomaticEvaluator,\n",
      "    ExperimentSaveEvalControl,\n",
      "    TensorBoardConfig,\n",
      "    WandBConfig,\n",
      ")\n",
      "from realhf.api.core.config import (\n",
      "    AgentAbstraction,\n",
      "    DatasetAbstraction,\n",
      "    EnvServiceAbstraction,\n",
      "    ModelAbstraction,\n",
      "    ModelName,\n",
      "    ModelShardID,\n",
      "    StandaloneModelShardAbstraction,\n",
      ")\n",
      "from realhf.base import constants, topology\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "\n",
      "\n",
      "class ExpStatus(Enum):\n",
      "    RUNNING = \"RUNNING\"\n",
      "    ABORTED = \"ABORTED\"\n",
      "    COMPLETE = \"COMPLETE\"\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class Scheduling:\n",
      "    # TODO: add partition\n",
      "    cpu: int\n",
      "    gpu: int\n",
      "    mem: int\n",
      "    nodelist: str = None\n",
      "    exclude: str = None\n",
      "    container_image: str = None\n",
      "    env_vars: Dict[str, str] = dataclasses.field(default_factory=dict)\n",
      "    # time utils from \"https://slurm.schedmd.com/sbatch.html\"\n",
      "    time_limit: Optional[str] = None  # see  \"--time\" option for format\n",
      "    begin: Optional[str] = None  # see \"--begin\" option for format\n",
      "    deadline: Optional[str] = None  # see \"--deadline\" option for format\n",
      "\n",
      "    @staticmethod\n",
      "    def master_worker_default(**kwargs):\n",
      "        return Scheduling(\n",
      "            **{\n",
      "                \"cpu\": 16,\n",
      "                \"mem\": 20 * 1024,\n",
      "                \"gpu\": 0,\n",
      "                \"container_image\": cluster_spec.cpu_image,\n",
      "                **kwargs,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def model_worker_default(**kwargs):\n",
      "        return Scheduling(\n",
      "            **{\n",
      "                \"cpu\": 2,\n",
      "                \"gpu\": 1,\n",
      "                \"mem\": 60 * 1024,\n",
      "                \"container_image\": cluster_spec.gpu_image,\n",
      "                **kwargs,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def generation_server_default(**kwargs):\n",
      "        return Scheduling(\n",
      "            **{\n",
      "                \"cpu\": 4,\n",
      "                \"gpu\": 1,\n",
      "                \"mem\": 60 * 1024,\n",
      "                \"container_image\": cluster_spec.gpu_infer_image,\n",
      "                **kwargs,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def gserver_manager_default(**kwargs):\n",
      "        return Scheduling(\n",
      "            **{\n",
      "                \"cpu\": 4,\n",
      "                \"gpu\": 0,\n",
      "                \"mem\": 10 * 1024,\n",
      "                \"container_image\": cluster_spec.gpu_image,\n",
      "                **kwargs,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def rollout_worker_default(**kwargs):\n",
      "        return Scheduling(\n",
      "            **{\n",
      "                \"cpu\": 4,\n",
      "                \"gpu\": 0,\n",
      "                \"mem\": 20 * 1024,\n",
      "                \"container_image\": cluster_spec.gpu_image,\n",
      "                **kwargs,\n",
      "            }\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class WorkerInformation:\n",
      "    \"\"\"The basic information of an worker.\n",
      "\n",
      "    To improve config readability, the experiment starter will fill the\n",
      "    fields, instead of letting the users do so in experiment configs.\n",
      "    \"\"\"\n",
      "\n",
      "    experiment_name: str = \"\"\n",
      "    trial_name: str = \"\"  # Name of the trial of the experiment; e.g. \"{USER}-0\".\n",
      "    worker_type: str = \"\"  # E.g. \"policy\", \"actor\", or \"trainer\".\n",
      "    worker_index: int = (\n",
      "        -1\n",
      "    )  # The index of the worker of the specific type, starting from 0.\n",
      "    worker_count: int = (\n",
      "        0  # Total number of workers; hence, 0 <= worker_index < worker_count.\n",
      "    )\n",
      "    worker_tag: Optional[str] = (\n",
      "        None  # For actor and policy worker, can be \"training\" or \"evaluation\".\n",
      "    )\n",
      "    host_key: Optional[str] = None  # Worker will update and keep this key alive.\n",
      "    watch_keys: Union[str, List[str]] = (\n",
      "        None  # Worker will exit if all of the watching keys are gone.\n",
      "    )\n",
      "\n",
      "    def system_setup(\n",
      "        self,\n",
      "        experiment_name,\n",
      "        trial_name,\n",
      "        worker_type,\n",
      "        worker_index,\n",
      "        worker_count,\n",
      "    ):\n",
      "        \"\"\"Setup system related worker information, while leaving the rest\n",
      "        untouched.\"\"\"\n",
      "        self.experiment_name = experiment_name\n",
      "        self.trial_name = trial_name\n",
      "        self.worker_type = worker_type\n",
      "        self.worker_index = worker_index\n",
      "        self.worker_count = worker_count\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelWorker:\n",
      "    base_seed: int\n",
      "    shards: List[StandaloneModelShardAbstraction]\n",
      "    # dataset, for source model workers\n",
      "    tokenizer_name_or_path: Optional[str] = None\n",
      "    datasets: Optional[List[Union[str, DatasetAbstraction]]] = None\n",
      "    use_dataset_cache: bool = False\n",
      "    dataset_cahce_root: str = constants.DATASET_CACHE_PATH\n",
      "    shuffle_dataset: bool = True\n",
      "    cuda_cache_cleanliness: bool = True\n",
      "    cuda_cache_clear_freq: int = 10\n",
      "    torch_cache_mysophobia: bool = False\n",
      "    # model_topos and worker_info will be configured automatically\n",
      "    model_rpcs: List[dfg.MFCDef] = None\n",
      "    model_topos: Dict[ModelName, topology.ProcessTopology] = None\n",
      "    msid2mwid: Dict[ModelShardID, int] = None\n",
      "    data_transfer_pairs: List[Tuple[ModelName, ModelName]] = None\n",
      "    sync_param_pairs: List[Tuple[ModelName, ModelName]] = None\n",
      "    # profiling\n",
      "    profile_mode: bool = False\n",
      "    worker_info: Optional[WorkerInformation] = None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        model_names = [s.id.model_name for s in self.shards]\n",
      "        if len(set(model_names)) != len(model_names):\n",
      "            raise ValueError(\n",
      "                f\"ModelWorker cannot have multiple shards of the same model name: {model_names}.\"\n",
      "            )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class GenerationServer:\n",
      "    base_seed: int\n",
      "    backend_type: str\n",
      "    backend_args: Any\n",
      "    model_path: str\n",
      "    tp_size: int\n",
      "    worker_info: WorkerInformation = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class GserverManager:\n",
      "    model_name: ModelName\n",
      "    n_servers: int\n",
      "    schedule_policy: str\n",
      "    max_head_offpolicyness: int\n",
      "    train_batch_size: int\n",
      "    flush_request_timeout: int\n",
      "    max_concurrent_rollouts: int\n",
      "    worker_info: WorkerInformation = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class RolloutWorker:\n",
      "    base_seed: int\n",
      "    model_name: ModelName\n",
      "    tokenizer_path: str\n",
      "    new_tokens_per_chunk: int\n",
      "    rollout_request_timeout: int\n",
      "    env: EnvServiceAbstraction\n",
      "    agent: AgentAbstraction\n",
      "    datasets: List[Union[str, DatasetAbstraction]]\n",
      "    use_dataset_cache: bool = False\n",
      "    dataset_cahce_root: str = constants.DATASET_CACHE_PATH\n",
      "    worker_info: WorkerInformation = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MasterWorker:\n",
      "    base_seed: int\n",
      "    exp_ctrl: ExperimentSaveEvalControl\n",
      "    # main components\n",
      "    n_model_workers: int\n",
      "    shuffle_dataset: bool = True\n",
      "    model_rpcs: List[dfg.MFCDef] = None\n",
      "    model_topos: Dict[ModelName, topology.ProcessTopology] = None\n",
      "    msid2mwid: Dict[ModelShardID | str, int] = None\n",
      "    data_transfer_pairs: List[Tuple[str, str]] = None\n",
      "    sync_param_pairs: List[Tuple[str, str]] = None\n",
      "    worker_info: Optional[WorkerInformation] = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TasksGroup:\n",
      "    count: int\n",
      "    scheduling: Scheduling\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ExperimentScheduling:\n",
      "    model_worker: TasksGroup\n",
      "    master_worker: TasksGroup\n",
      "    generation_server: TasksGroup | None = None\n",
      "    gserver_manager: TasksGroup | None = None\n",
      "    rollout_worker: TasksGroup | None = None\n",
      "    controller_image: str = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ExperimentConfig:\n",
      "    exp_ctrl: ExperimentSaveEvalControl\n",
      "    wandb: WandBConfig\n",
      "    tensorboard: TensorBoardConfig\n",
      "    # dataflow\n",
      "    model_rpcs: List[dfg.MFCDef]\n",
      "    model_worker: List[ModelWorker] = dataclasses.field(default_factory=list)\n",
      "    generation_server: List[GenerationServer] = dataclasses.field(default_factory=list)\n",
      "    gserver_manager: List[GserverManager] = dataclasses.field(default_factory=list)\n",
      "    rollout_worker: List[RolloutWorker] = dataclasses.field(default_factory=list)\n",
      "    # master_worker will be set automatically\n",
      "    master_worker: Optional[List[MasterWorker]] = None\n",
      "    # automatic evaluation\n",
      "    auto_eval: bool = False\n",
      "    evaluator: AutomaticEvaluator = dataclasses.field(\n",
      "        default_factory=AutomaticEvaluator\n",
      "    )\n",
      "\n",
      "    def __post_init__(self):\n",
      "        self.master_worker = [\n",
      "            MasterWorker(\n",
      "                base_seed=self.model_worker[0].base_seed,\n",
      "                exp_ctrl=self.exp_ctrl,\n",
      "                n_model_workers=len(self.model_worker),\n",
      "                shuffle_dataset=self.model_worker[0].shuffle_dataset,\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    def lazy_init(self):\n",
      "        assert self.master_worker is not None and len(self.master_worker) == 1\n",
      "        model_names = set()\n",
      "        for w in self.model_worker:\n",
      "            model_names = model_names.union([s.id.model_name for s in w.shards])\n",
      "        model_names = sorted(list(model_names))\n",
      "\n",
      "        assert constants.trial_name() is not None\n",
      "        assert constants.experiment_name() is not None\n",
      "        graph_path = os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"dataflow_graph.png\",\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
      "        # If verbose set to True here, every worker will print the graph once\n",
      "        # due to lazy init on workers.\n",
      "        G = dfg.build_graph(self.model_rpcs, verbose=False, graph_path=graph_path)\n",
      "        for rpc in self.model_rpcs:\n",
      "            rpc._G = G\n",
      "\n",
      "        self._validate_model_names(model_names)\n",
      "\n",
      "        model_topos = self._collect_topos(model_names)\n",
      "        model_configs = self._collect_model_configs(model_names)\n",
      "\n",
      "        data_transfer_pairs = self._resolve_data_transfer_pairs(model_names)\n",
      "\n",
      "        sync_param_pairs = self._resolve_param_realloc_pairs(model_configs, model_topos)\n",
      "\n",
      "        model_names_to_instantiate = self._resolve_model_names_to_instantiate(\n",
      "            model_names\n",
      "        )\n",
      "\n",
      "        for mw in self.model_worker:\n",
      "            for s in mw.shards:\n",
      "                s.should_instantiate = s.id.model_name in model_names_to_instantiate\n",
      "\n",
      "        msid2mwid = {}\n",
      "        for i, mw in enumerate(self.model_worker):\n",
      "            mw.model_topos = model_topos\n",
      "            for m in mw.shards:\n",
      "                msid2mwid[m.id] = i\n",
      "        for m in self.model_worker:\n",
      "            m.msid2mwid = msid2mwid\n",
      "            m.data_transfer_pairs = data_transfer_pairs\n",
      "            m.sync_param_pairs = sync_param_pairs\n",
      "\n",
      "        for m in self.model_worker:\n",
      "            m.model_rpcs = self.model_rpcs\n",
      "\n",
      "        # setup master worker config\n",
      "        self.master_worker[0].model_rpcs = self.model_rpcs\n",
      "        self.master_worker[0].model_topos = model_topos\n",
      "        self.master_worker[0].msid2mwid = msid2mwid\n",
      "        self.master_worker[0].sync_param_pairs = sync_param_pairs\n",
      "        self.master_worker[0].data_transfer_pairs = data_transfer_pairs\n",
      "\n",
      "    def resolve_worker_config(self, worker_type, worker_index):\n",
      "        return getattr(self, worker_type)[worker_index]\n",
      "\n",
      "    def set_worker_information(self, experiment_name, trial_name):\n",
      "        for worker_type, workers in [\n",
      "            (\"model_worker\", self.model_worker),\n",
      "            (\"master_worker\", self.master_worker),\n",
      "            (\"gserver_manager\", self.gserver_manager),\n",
      "            (\"rollout_worker\", self.rollout_worker),\n",
      "            (\"generation_server\", self.generation_server),\n",
      "        ]:\n",
      "            if len(workers) == 0:\n",
      "                continue\n",
      "            for i, worker in enumerate(workers):\n",
      "                system_worker_info = dict(\n",
      "                    experiment_name=experiment_name,\n",
      "                    trial_name=trial_name,\n",
      "                    worker_type=worker_type,\n",
      "                    worker_index=i,\n",
      "                    worker_count=len(workers),\n",
      "                )\n",
      "                if worker.worker_info is not None:\n",
      "                    worker.worker_info.system_setup(**system_worker_info)\n",
      "                else:\n",
      "                    worker.worker_info = WorkerInformation(**system_worker_info)\n",
      "\n",
      "    def _collect_topos(\n",
      "        self, model_names: List[ModelName]\n",
      "    ) -> Dict[ModelName, topology.ProcessTopology]:\n",
      "        model_topos = {}\n",
      "        model_allocations = {}\n",
      "        for model_name in model_names:\n",
      "            _this_mws_with_indicies = list(\n",
      "                filter(\n",
      "                    lambda i_mw: any(\n",
      "                        x.id.model_name == model_name for x in i_mw[1].shards\n",
      "                    ),\n",
      "                    enumerate(self.model_worker),\n",
      "                )\n",
      "            )\n",
      "            _this_mw_indices, _this_mws = zip(*_this_mws_with_indicies)\n",
      "            _this_mw_indices = tuple(sorted(_this_mw_indices))\n",
      "            all_shards: List[StandaloneModelShardAbstraction] = [\n",
      "                next(filter(lambda x: x.id.model_name == model_name, mw.shards))\n",
      "                for mw in _this_mws\n",
      "            ]\n",
      "            model_topos[model_name] = all_shards[0].id.topo\n",
      "            model_allocations[model_name] = tuple(sorted(_this_mw_indices))\n",
      "\n",
      "            ##### Sanity check of parallelism ranks. #####\n",
      "            ranks = [s.id.parallelism_rank for s in all_shards]\n",
      "            _topos = [s.id.topo for s in all_shards]\n",
      "            if set(ranks) != set(list(range(len(_this_mws)))) or any(\n",
      "                _t.world_size() != _topos[0].world_size() for _t in _topos\n",
      "            ):\n",
      "                raise ValueError(\n",
      "                    f\"Parallelism rank check failed: model name {model_name}, \"\n",
      "                    f\"model shard ids={[s.id for s in all_shards]}.\"\n",
      "                )\n",
      "            ##### Sanity check of parallelism ranks. #####\n",
      "        return model_topos\n",
      "\n",
      "    def _collect_model_configs(\n",
      "        self, model_names: List[ModelName]\n",
      "    ) -> Dict[ModelName, ModelAbstraction]:\n",
      "        model_configs = {}\n",
      "        for model_name in model_names:\n",
      "            _this_mws = list(\n",
      "                filter(\n",
      "                    lambda mw: any(x.id.model_name == model_name for x in mw.shards),\n",
      "                    self.model_worker,\n",
      "                )\n",
      "            )\n",
      "            all_shards: List[StandaloneModelShardAbstraction] = [\n",
      "                next(filter(lambda x: x.id.model_name == model_name, mw.shards))\n",
      "                for mw in _this_mws\n",
      "            ]\n",
      "            model_configs[model_name] = all_shards[0].model\n",
      "        return model_configs\n",
      "\n",
      "    def _validate_model_names(self, model_names: List[ModelName]):\n",
      "        model_names = sorted(model_names)\n",
      "        _roles = set(mn.role for mn in model_names)\n",
      "        _replica_ids = {\n",
      "            _role: sorted([mn.replica_id for mn in model_names if mn.role == _role])\n",
      "            for _role in _roles\n",
      "        }\n",
      "        for v in _replica_ids.values():\n",
      "            if list(sorted(v)) != list(range(len(v))):\n",
      "                raise ValueError(\n",
      "                    f\"Model replica ids should be 0, 1, 2, ... for each role: {_replica_ids}.\"\n",
      "                )\n",
      "\n",
      "    def _resolve_data_transfer_pairs(\n",
      "        self, model_names: List[ModelName]\n",
      "    ) -> List[Tuple[ModelName, ModelName]]:\n",
      "        data_transfer_pairs: List[Tuple[ModelName, ModelName]] = []\n",
      "        G = self.model_rpcs[0]._G\n",
      "        for edge in G.edges():\n",
      "            mn1 = G.nodes[edge[0]][\"object\"].model_name\n",
      "            mn2 = G.nodes[edge[1]][\"object\"].model_name\n",
      "            data_transfer_pairs.append((mn1, mn2))\n",
      "        src_rpcs = [rpc for rpc in self.model_rpcs if rpc.is_src]\n",
      "        data_src_rpc = src_rpcs[0]\n",
      "        for r in src_rpcs:\n",
      "            if (\n",
      "                data_src_rpc.model_name,\n",
      "                r.model_name,\n",
      "            ) not in data_transfer_pairs:\n",
      "                data_transfer_pairs.append((data_src_rpc.model_name, r.model_name))\n",
      "        return data_transfer_pairs\n",
      "\n",
      "    def _resolve_param_realloc_pairs(\n",
      "        self, model_configs, model_topos\n",
      "    ) -> List[Tuple[ModelName, ModelName]]:\n",
      "        sync_param_pairs: List[Tuple[ModelName, ModelName]] = []\n",
      "        for rpc in self.model_rpcs:\n",
      "            for hook in rpc._pre_hooks + rpc._post_hooks:\n",
      "                if not isinstance(hook, dfg.ParamReallocHook):\n",
      "                    continue\n",
      "                other_model_name = (\n",
      "                    hook.target if hook.target is not None else hook.source\n",
      "                )\n",
      "                other_topo = (\n",
      "                    model_topos[hook.target]\n",
      "                    if hook.target is not None\n",
      "                    else model_topos[hook.source]\n",
      "                )\n",
      "                self_topo = model_topos[rpc.model_name]\n",
      "                if (\n",
      "                    self_topo.get_dim(\"tensor\") % other_topo.get_dim(\"tensor\") != 0\n",
      "                    and other_topo.get_dim(\"tensor\") % self_topo.get_dim(\"tensor\") != 0\n",
      "                ):\n",
      "                    raise ValueError(\n",
      "                        \"To synchronize parameters between two models, \"\n",
      "                        \"their model parallel size must be a multiple of each other.\"\n",
      "                    )\n",
      "                if rpc.model_name == other_model_name:\n",
      "                    raise ValueError(\n",
      "                        f\"Cannot synchronize parameters within the same model \"\n",
      "                        f\"(in {rpc}, {rpc.model_name} and {hook.target}).\"\n",
      "                    )\n",
      "                if hook.target is not None:\n",
      "                    if not (rpc.model_name, hook.target) in sync_param_pairs:\n",
      "                        sync_param_pairs.append((rpc.model_name, hook.target))\n",
      "                else:\n",
      "                    if not (hook.source, rpc.model_name) in sync_param_pairs:\n",
      "                        sync_param_pairs.append((hook.source, rpc.model_name))\n",
      "        return sync_param_pairs\n",
      "\n",
      "    def _resolve_model_names_to_instantiate(\n",
      "        self, model_names: List[ModelName]\n",
      "    ) -> List[ModelName]:\n",
      "        # Mark which shard of the same role should be instantiated.\n",
      "        roles = set([model_name.role for model_name in model_names])\n",
      "        role_is_trainable = {role: False for role in roles}\n",
      "        role_trainable_idx = {}\n",
      "        role_idx_collection = {role: set() for role in roles}\n",
      "        for role in roles:\n",
      "            for rpc in self.model_rpcs:\n",
      "                if rpc.role != role:\n",
      "                    continue\n",
      "                if rpc.interface_type == dfg.ModelInterfaceType.TRAIN_STEP:\n",
      "                    if role_is_trainable[role]:\n",
      "                        raise ValueError(\n",
      "                            f\"Multiple train_step for the same role {role} is not allowed.\"\n",
      "                        )\n",
      "                    role_is_trainable[role] = True\n",
      "                    role_trainable_idx[role] = rpc.model_name.replica_id\n",
      "                role_idx_collection[role].add(rpc.model_name.replica_id)\n",
      "        role_cnt = {role: len(v) for role, v in role_idx_collection.items()}\n",
      "\n",
      "        model_names_to_instantiate = []\n",
      "        for role in roles:\n",
      "            if role_is_trainable[role]:\n",
      "                model_names_to_instantiate.append(\n",
      "                    ModelName(role, role_trainable_idx[role])\n",
      "                )\n",
      "            else:\n",
      "                model_names_to_instantiate += [\n",
      "                    ModelName(role, i) for i in range(role_cnt[role])\n",
      "                ]\n",
      "\n",
      "        return model_names_to_instantiate\n",
      "\n",
      "\n",
      "class Experiment:\n",
      "    \"\"\"Base class for defining the procedure of an experiment.\"\"\"\n",
      "\n",
      "    def scheduling_setup(self) -> ExperimentScheduling:\n",
      "        \"\"\"Returns the Scheduling of all workers.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def initial_setup(self) -> ExperimentConfig | List[ExperimentConfig]:\n",
      "        \"\"\"Returns a list of workers to create when a trial of the experiment\n",
      "        is initialized.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "ALL_EXPERIMENT_CLASSES = {}\n",
      "\n",
      "\n",
      "def register_experiment(name, cls):\n",
      "    assert name not in ALL_EXPERIMENT_CLASSES\n",
      "    ALL_EXPERIMENT_CLASSES[name] = cls\n",
      "\n",
      "\n",
      "def make_experiment(name) -> Experiment:\n",
      "    cls = ALL_EXPERIMENT_CLASSES[name]\n",
      "    return cls()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/dfg.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import collections\n",
      "import dataclasses\n",
      "from typing import Any, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import networkx as nx\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.core.config import (\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelInterfaceType,\n",
      "    ModelName,\n",
      ")\n",
      "from realhf.api.core.data_api import MicroBatchSpec\n",
      "\n",
      "logger = logging.getLogger(\"DataFlowGraph\", \"benchmark\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class OffloadHook:\n",
      "    pass\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ParamReallocHook:\n",
      "    \"\"\"Hook for reallocating weights between source and target.\n",
      "\n",
      "    Weights are transferred from the source model to the target model.\n",
      "    Only one of `source` or `target` should be provided; the other should be\n",
      "    the model name of the hooked MFC.\n",
      "\n",
      "    The weights are updated using the formula: `target = eta * source + (1 - eta) * target`.\n",
      "\n",
      "    :param source: The model name of the source from which weights are transferred.\n",
      "    :type source: Optional[ModelName]\n",
      "    :param target: The model name of the target to which weights are transferred.\n",
      "    :type target: Optional[ModelName]\n",
      "    :param eta: The weight for the source in the update formula. The default is 1.0,\n",
      "        meaning that the target will be completely overwritten by the source.\n",
      "    :type eta: float\n",
      "    \"\"\"\n",
      "\n",
      "    source: Optional[ModelName] = None\n",
      "    target: Optional[ModelName] = None\n",
      "    eta: float = 1.0\n",
      "\n",
      "\n",
      "RPCHook = Union[OffloadHook, ParamReallocHook]\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MFCDef:\n",
      "    \"\"\"A model function call (MFC) object used by the workers.\n",
      "\n",
      "    MFC stands for Model Function Call. This object serves as the interface for\n",
      "    developing new algorithms and will be inserted into an `nx.DiGraph` as nodes.\n",
      "    Edges will be automatically resolved based on input/output keys.\n",
      "\n",
      "    Fields starting with an underscore are filled automatically.\n",
      "\n",
      "    **Note:** In the ReaL implementation, the term RPC also refers to MFC.\n",
      "\n",
      "    :param name: The unique identifier for this model function call.\n",
      "    :type name: str\n",
      "    :param n_seqs: The number of sequences to be processed in a batch.\n",
      "    :type n_seqs: int\n",
      "    :param interface_type: The type of interface used by the node (e.g., generate, train_step).\n",
      "    :type interface_type: ModelInterfaceType\n",
      "    :param interface_impl: The actual implementation of the interface when running this node.\n",
      "    :type interface_impl: ModelInterface\n",
      "    :param model_name: The model identifier used by the node, corresponding to a unique LLM.\n",
      "        The user-provided model name can be a string; the replica ID will be resolved in ReaL.\n",
      "    :type model_name: str or ModelName\n",
      "    :param input_keys: Input data keys used to resolve dependencies.\n",
      "    :type input_keys: Tuple\n",
      "    :param output_keys: Output data keys used to resolve dependencies.\n",
      "    :type output_keys: Tuple\n",
      "    :param input_key_remap: Remap input keys to identifiers recognized by the interface implementation.\n",
      "        Keys are from `input_keys` and values are identifiers known to the interface.\n",
      "    :type input_key_remap: Dict[str, str]\n",
      "    :param output_key_remap: Remap output keys to identifiers recognized by MFC.\n",
      "        Keys are identifiers known to the interface, and values are from `output_keys`.\n",
      "    :type output_key_remap: Dict[str, str]\n",
      "    :param mb_spec: The approach to dividing micro-batches. Check MicroBatchSpec for details.\n",
      "    :type mb_spec: MicroBatchSpec.\n",
      "    :param min_n_seqs_per_pass: The minimum number of sequences for each model interface pass.\n",
      "        If the interface does not further split the batch, this value should be 1. Otherwise,\n",
      "        it should be the minimum number of required mini-batches, e.g., PPO minibatch.\n",
      "    :type min_n_seqs_per_pass: int\n",
      "    :param log_return_value: Whether to log the return value of the interface implementation.\n",
      "    :type log_return_value: bool\n",
      "    \"\"\"\n",
      "\n",
      "    # The unique identifier of this model function call.\n",
      "    name: str\n",
      "\n",
      "    # batch size\n",
      "    n_seqs: int\n",
      "\n",
      "    # The interface type to be used by the node (e.g., generate, train_step).\n",
      "    interface_type: ModelInterfaceType\n",
      "    interface_impl: ModelInterfaceAbstraction\n",
      "\n",
      "    # The model identifier to be used by the node.\n",
      "    model_name: str | ModelName\n",
      "\n",
      "    # Input and output keys, used to resolve dependencies.\n",
      "    input_keys: Tuple = dataclasses.field(default_factory=tuple)\n",
      "    input_key_remap: Dict[str, str] = dataclasses.field(default_factory=lambda: {})\n",
      "    output_keys: Tuple = dataclasses.field(default_factory=tuple)\n",
      "    output_key_remap: Dict[str, str] = dataclasses.field(default_factory=lambda: {})\n",
      "\n",
      "    mb_spec: MicroBatchSpec = dataclasses.field(default_factory=MicroBatchSpec)\n",
      "    min_n_seqs_per_pass: int | float = 1\n",
      "    log_return_value: bool = False\n",
      "\n",
      "    # Reserved dataclasses.fields. Should not be set by the user.\n",
      "    _G: nx.DiGraph = None\n",
      "    _pre_hooks: List[RPCHook] = dataclasses.field(default_factory=lambda: [])\n",
      "    _post_hooks: List[RPCHook] = dataclasses.field(default_factory=lambda: [])\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if isinstance(self.model_name, str):\n",
      "            self.model_name = ModelName(role=self.model_name, replica_id=0)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"MFCDef[{self.name}]\"\n",
      "\n",
      "    def __hash__(self):\n",
      "        return hash(self.name)\n",
      "\n",
      "    @property\n",
      "    def role(self):\n",
      "        return self.model_name.role\n",
      "\n",
      "    def is_train(self):\n",
      "        return self.interface_type in [ModelInterfaceType.TRAIN_STEP]\n",
      "\n",
      "    def is_inference(self):\n",
      "        return self.interface_type in [ModelInterfaceType.INFERENCE]\n",
      "\n",
      "    def is_generate(self):\n",
      "        return self.interface_type in [ModelInterfaceType.GENERATE]\n",
      "\n",
      "    def add_pre_hook(self, h: RPCHook):\n",
      "        assert isinstance(h, RPCHook), type(h)\n",
      "        if isinstance(h, ParamReallocHook):\n",
      "            assert h.target is None or h.source is None\n",
      "        if isinstance(h, OffloadHook):\n",
      "            raise ValueError(\"Offload can only be post hooks!\")\n",
      "        self._pre_hooks.append(h)\n",
      "\n",
      "    def add_post_hook(self, h: RPCHook):\n",
      "        if isinstance(h, ParamReallocHook):\n",
      "            assert h.target is None or h.source is None\n",
      "        self._post_hooks.append(h)\n",
      "\n",
      "    @property\n",
      "    def is_src(self):\n",
      "        return len(list(self._G.predecessors(self.name))) == 0\n",
      "\n",
      "    @property\n",
      "    def is_dst(self):\n",
      "        return len(list(self._G.successors(self.name))) == 0\n",
      "\n",
      "    @property\n",
      "    def data_producers(self) -> Dict[str, ModelName]:\n",
      "        return self._G.graph[\"data_producers\"]\n",
      "\n",
      "    @property\n",
      "    def data_consumers(self) -> Dict[str, List[str]]:\n",
      "        return self._G.graph[\"data_consumers\"]\n",
      "\n",
      "    @property\n",
      "    def parents(self) -> List[\"MFCDef\"]:\n",
      "        return [self._G.nodes[x][\"object\"] for x in self._G.predecessors(self.name)]\n",
      "\n",
      "    @property\n",
      "    def children(self) -> List[\"MFCDef\"]:\n",
      "        return [self._G.nodes[x][\"object\"] for x in self._G.successors(self.name)]\n",
      "\n",
      "    def all_successors(self) -> List[\"MFCDef\"]:\n",
      "        names = list(nx.dfs_preorder_nodes(self._G, self.name))\n",
      "        names.remove(self.name)\n",
      "        return [self._G.nodes[x][\"object\"] for x in names]\n",
      "\n",
      "    @property\n",
      "    def is_dst_of_model_role(self):\n",
      "\n",
      "        def _has_children_of_model_name(rpc: \"MFCDef\", model_name: ModelName):\n",
      "            if rpc.is_dst:\n",
      "                return False\n",
      "            return any(\n",
      "                [\n",
      "                    r.role == model_name.role\n",
      "                    or _has_children_of_model_name(r, model_name)\n",
      "                    for r in rpc.children\n",
      "                ]\n",
      "            )\n",
      "\n",
      "        return not _has_children_of_model_name(self, self.model_name)\n",
      "\n",
      "\n",
      "def _draw_topo_sorted_digraph(G: nx.DiGraph, graph_path: str):\n",
      "    topological_order = list(nx.topological_sort(G))\n",
      "    # Initialize a dictionary to store the depth of each node\n",
      "    node_depth = {node: 0 for node in G.nodes()}\n",
      "\n",
      "    # Calculate the depth of each node\n",
      "    for node in topological_order:\n",
      "        for neighbor in G.successors(node):\n",
      "            node_depth[neighbor] = max(node_depth[neighbor], node_depth[node] + 1)\n",
      "\n",
      "    layers = {\n",
      "        i: [node for node, depth in node_depth.items() if depth == i]\n",
      "        for i in range(max(node_depth.values()) + 1)\n",
      "    }\n",
      "    pos = nx.multipartite_layout(G, subset_key=layers)\n",
      "    nx.draw(\n",
      "        G,\n",
      "        pos,\n",
      "        with_labels=False,\n",
      "        node_size=4000,\n",
      "        node_color=\"lightblue\",\n",
      "        arrows=True,\n",
      "        arrowsize=20,\n",
      "        width=1.5,\n",
      "    )\n",
      "    labels = {node: node for node in G.nodes()}\n",
      "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=12, font_color=\"black\")\n",
      "    plt.savefig(graph_path, dpi=300)\n",
      "\n",
      "\n",
      "def build_graph(\n",
      "    nodes: List[MFCDef],\n",
      "    verbose: bool = False,\n",
      "    graph_path: Optional[str] = None,\n",
      ") -> nx.DiGraph:\n",
      "    if len(set(node.name for node in nodes)) != len(nodes):\n",
      "        raise ValueError(\n",
      "            \"Each model function call should have an unique name. \"\n",
      "            f\"Got {[node.name for node in nodes]}.\"\n",
      "        )\n",
      "\n",
      "    _G = nx.DiGraph()\n",
      "    _G.add_nodes_from([(node.name, dict(object=node)) for node in nodes])\n",
      "\n",
      "    data_producers: Dict[str, MFCDef] = {}\n",
      "    data_consumers: Dict[str, List[MFCDef]] = collections.defaultdict(list)\n",
      "    for node in nodes:\n",
      "        for k in node.output_keys:\n",
      "            data_producers[k] = node\n",
      "        for k in node.input_keys:\n",
      "            data_consumers[k].append(node)\n",
      "\n",
      "    for node in nodes:\n",
      "        for k in node.input_keys:\n",
      "            if k not in data_producers:\n",
      "                # This is a key from the dataset.\n",
      "                continue\n",
      "            src, dst = data_producers[k].name, node.name\n",
      "            if _G.has_edge(src, dst):\n",
      "                _G[src][dst][\"keys\"].append(k)\n",
      "            else:\n",
      "                _G.add_edge(src, dst, keys=[k])\n",
      "    if verbose:\n",
      "        for u, v, data in _G.edges(data=True):\n",
      "            logger.info(f\"Edge: {u} -> {v} with keys {data['keys']}\")\n",
      "        if graph_path is not None:\n",
      "            _draw_topo_sorted_digraph(_G, graph_path)\n",
      "            logger.info(\n",
      "                f\"> Visualization of the dataflow graph in \"\n",
      "                f\"this experiment is saved to: {graph_path}.\"\n",
      "            )\n",
      "\n",
      "    if len(nodes) != len(_G.nodes):\n",
      "        raise ValueError(\"There are replicated nodes in the graph!\")\n",
      "\n",
      "    # Store useful metadata\n",
      "    _G.graph[\"data_producers\"] = {k: v.model_name for k, v in data_producers.items()}\n",
      "    _G.graph[\"data_consumers\"] = {\n",
      "        k: [v.model_name for v in vs] for k, vs in data_consumers.items()\n",
      "    }\n",
      "\n",
      "    return _G\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/config.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import enum\n",
      "from typing import Any, Dict, List, Optional\n",
      "\n",
      "import realhf.base.cluster as cluster\n",
      "import realhf.base.topology as topology\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class DatasetAbstraction:\n",
      "    type_: str\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class EnvServiceAbstraction:\n",
      "    type_: str = \"null\"\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class AgentAbstraction:\n",
      "    type_: str = \"null\"\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelWrapperAbstraction:\n",
      "    type_: str\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelAbstraction:\n",
      "    type_: str\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "    wrappers: List[ModelWrapperAbstraction] = dataclasses.field(default_factory=list)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelBackendAbstraction:\n",
      "    type_: str\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelInterfaceAbstraction:\n",
      "    type_: str  # This type is the\n",
      "    args: Dict[str, Any] = dataclasses.field(default_factory=dict)\n",
      "\n",
      "\n",
      "class ModelInterfaceType(enum.Enum):\n",
      "    GENERATE = \"generate\"\n",
      "    TRAIN_STEP = \"train_step\"\n",
      "    EVALUATE = \"evaluate\"\n",
      "    INFERENCE = \"inference\"\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(unsafe_hash=True, order=True, frozen=True)\n",
      "class ModelName:\n",
      "    \"\"\"A unique identifier for a model.\n",
      "\n",
      "    :param role: The role of the model, e.g., \"actor\" or \"critic\".\n",
      "    :type role: str\n",
      "    :param replica_id: The replica ID of the model. Different replicas\n",
      "        of the same role have the same set of parameters but different\n",
      "        memory locations. For example, if actor generation and training\n",
      "        in PPO use different parallel strategies, they will have the\n",
      "        same role but different replica IDs.\n",
      "    :type replica_id: int\n",
      "    \"\"\"\n",
      "\n",
      "    role: str\n",
      "    replica_id: int\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        return str(self)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ModelShardID:\n",
      "    \"\"\"The ID of a model shard in a specific model worker.\n",
      "\n",
      "    This ID is essentially a combination of the model name and the 3D\n",
      "    parallelism rank, and can be used as a dictionary key. It represents\n",
      "    the identity of a \"model handler\". The master worker maintains a\n",
      "    lookup table mapping the ModelShardID to the model worker index,\n",
      "    which can be a many-to-one mapping. Requests are created with the\n",
      "    ModelShardID; for example, actors with ranks (dp=*, mp=0, pp=0)\n",
      "    should transfer data to the critics. The ModelShardID is then mapped\n",
      "    to the model worker index, and the requests are sent to the\n",
      "    corresponding model workers.\n",
      "\n",
      "    :param model_name: The name of the model.\n",
      "    :type model_name: ModelName\n",
      "    :param dp_rank: The data parallel rank.\n",
      "    :type dp_rank: int\n",
      "    :param tp_rank: The tensor-model parallel rank.\n",
      "    :type tp_rank: int\n",
      "    :param pp_rank: The pipeline-model parallel rank.\n",
      "    :type pp_rank: int\n",
      "    :param topo: The 3D parallelism topology of this model.\n",
      "    :type topo: ProcessTopology\n",
      "    \"\"\"\n",
      "\n",
      "    model_name: ModelName\n",
      "    dp_rank: int\n",
      "    tp_rank: int\n",
      "    pp_rank: int\n",
      "    topo: topology.ProcessTopology\n",
      "\n",
      "    def __post_init__(self):\n",
      "        assert self.dp_rank >= 0 and self.tp_rank >= 0 and self.pp_rank >= 0\n",
      "        if \"@\" in self.model_name.role:\n",
      "            raise ValueError(\"model_name cannot contain @\")\n",
      "        assert self.dp_rank < self.topo.get_dim(\"data\")\n",
      "        assert self.tp_rank < self.topo.get_dim(\"tensor\")\n",
      "        assert self.pp_rank < self.topo.get_dim(\"pipe\")\n",
      "\n",
      "    @property\n",
      "    def parallelism_rank(self):\n",
      "        return self.topo.get_rank(\n",
      "            data=self.dp_rank, tensor=self.tp_rank, pipe=self.pp_rank\n",
      "        )\n",
      "\n",
      "    @classmethod\n",
      "    def from_parallelism_rank(cls, model_name, topo, parallelism_rank):\n",
      "        c = topo.get_coord(parallelism_rank)\n",
      "        return cls(\n",
      "            model_name=model_name,\n",
      "            dp_rank=c.data,\n",
      "            tp_rank=c.tensor,\n",
      "            pp_rank=c.pipe,\n",
      "            topo=topo,\n",
      "        )\n",
      "\n",
      "    def __repr__(self):\n",
      "        n = cluster.spec.suffix_n_digits\n",
      "        return f\"{self.model_name}@pp{self.pp_rank:0{n}d}@tp{self.tp_rank:0{n}d}@dp{self.dp_rank:0{n}d}\"\n",
      "\n",
      "    def __hash__(self):\n",
      "        return hash(str(self))\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        # Compare the key attribute for equality\n",
      "        if isinstance(other, ModelShardID):\n",
      "            return (\n",
      "                self.model_name == other.model_name\n",
      "                and self.dp_rank == other.dp_rank\n",
      "                and self.tp_rank == other.tp_rank\n",
      "                and self.pp_rank == other.pp_rank\n",
      "            )\n",
      "        return False\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class StandaloneModelShardAbstraction:\n",
      "    id: ModelShardID\n",
      "    model: ModelAbstraction\n",
      "    backend: ModelBackendAbstraction\n",
      "    # evaluation\n",
      "    eval_dataset: Optional[DatasetAbstraction] = None\n",
      "    eval_bs: int = 128\n",
      "    should_instantiate: bool = True\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/env_api.py ====\n",
      "\n",
      "import abc\n",
      "import asyncio\n",
      "from typing import Any, Dict, List, Tuple\n",
      "\n",
      "from realhf.api.core.config import EnvServiceAbstraction\n",
      "\n",
      "\n",
      "class EnvironmentService(abc.ABC):\n",
      "\n",
      "    # TODO: import gymnasium, use its types and signatures\n",
      "    async def step(self, action: Any) -> Tuple[Any, Any, bool, bool, Dict]:\n",
      "        # obs, reward, terminated, truncated, info\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    async def reset(self, seed=None, options=None) -> Tuple[Any, Dict]:\n",
      "        # obs, info\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "ALL_ENV_CLASSES = {}\n",
      "\n",
      "\n",
      "def register_environment(name, env_cls):\n",
      "    assert name not in ALL_ENV_CLASSES\n",
      "    assert \"/\" not in name\n",
      "    ALL_ENV_CLASSES[name] = env_cls\n",
      "\n",
      "\n",
      "class NullEnvironment:\n",
      "\n",
      "    async def step(self, action):\n",
      "        await asyncio.sleep(1)\n",
      "        # obs, reward, terminated, truncated, info\n",
      "        return None, 0.0, True, False, {}\n",
      "\n",
      "    async def reset(self, seed=None, options=None) -> Tuple[Any, Dict]:\n",
      "        await asyncio.sleep(0.1)\n",
      "        return None, {}\n",
      "\n",
      "\n",
      "register_environment(\"null\", NullEnvironment)\n",
      "\n",
      "\n",
      "def make_env(\n",
      "    cfg: EnvServiceAbstraction,\n",
      ") -> EnvironmentService:\n",
      "    return ALL_ENV_CLASSES[cfg.type_](**cfg.args)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/api/core/data_api.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import json\n",
      "import os\n",
      "import random\n",
      "import time\n",
      "from contextlib import contextmanager\n",
      "\n",
      "# NOTE: We don't sue wildcard importing here because the type\n",
      "# `Sequence` has a very similar name to `SequenceSample`.\n",
      "# We don't want to confuse them.\n",
      "from typing import (\n",
      "    Any,\n",
      "    Callable,\n",
      "    Dict,\n",
      "    Hashable,\n",
      "    Iterable,\n",
      "    List,\n",
      "    Optional,\n",
      "    Set,\n",
      "    Tuple,\n",
      "    Union,\n",
      ")\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.utils.data\n",
      "import transformers\n",
      "\n",
      "# NOTE: We only use pandatic dataclasses for SequenceSample\n",
      "# such that it will perform automatic checks.\n",
      "from pydantic import Field\n",
      "from pydantic import dataclasses as pdclasses\n",
      "from pydantic import field_validator, model_validator\n",
      "\n",
      "from realhf.api.cli_args import MicroBatchSpec\n",
      "from realhf.api.core import config as config_api\n",
      "from realhf.base import constants, datapack, logging, seeding\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.utils import load_hf_or_local_file\n",
      "\n",
      "logger = logging.getLogger(\"api.data\")\n",
      "\n",
      "RL_TASKS = [\"math\", \"code\", \"rlhf\", \"stem\"]\n",
      "\n",
      "\n",
      "def load_hf_tokenizer(\n",
      "    model_name_or_path: str,\n",
      "    fast_tokenizer=True,\n",
      "    padding_side: Optional[str] = None,\n",
      ") -> transformers.PreTrainedTokenizerFast:\n",
      "    kwargs = {}\n",
      "    if padding_side is not None:\n",
      "        kwargs[\"padding_side\"] = padding_side\n",
      "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
      "        model_name_or_path,\n",
      "        fast_tokenizer=fast_tokenizer,\n",
      "        trust_remote_code=True,\n",
      "        force_download=True,\n",
      "        **kwargs,\n",
      "    )\n",
      "    if tokenizer.pad_token_id is None:\n",
      "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
      "    return tokenizer\n",
      "\n",
      "\n",
      "@pdclasses.dataclass\n",
      "class SequenceSplitSpec:\n",
      "    partitions: Optional[List[Tuple[int, int]]] = None\n",
      "    sizes: Optional[List[int]] = None\n",
      "\n",
      "    @model_validator(mode=\"after\")\n",
      "    def _validate_partitions(self) -> \"SequenceSplitSpec\":\n",
      "        if self.partitions is not None:\n",
      "            bound = 0\n",
      "            for start, end in self.partitions:\n",
      "                if start >= end:\n",
      "                    raise ValueError(f\"Partition {start}-{end} is empty.\")\n",
      "                if start != bound:\n",
      "                    raise ValueError(f\"Partition {start}-{end} is not contiguous.\")\n",
      "                bound = end\n",
      "\n",
      "        if self.sizes is None and self.partitions is None:\n",
      "            raise ValueError(\"Either sizes or partitions must be provided.\")\n",
      "        elif self.sizes is not None and self.partitions is not None:\n",
      "            if len(self.sizes) != len(self.partitions):\n",
      "                raise ValueError(\"Sizes and partitions are not the consistent.\")\n",
      "            if self.sizes != [end - start for start, end in self.partitions]:\n",
      "                raise ValueError(\"Sizes and partitions are not the consistent.\")\n",
      "        elif self.sizes is None:\n",
      "            self.sizes = [end - start for start, end in self.partitions]\n",
      "        elif self.partitions is None:\n",
      "            offsets = np.cumsum([0] + self.sizes)\n",
      "            self.partitions = [\n",
      "                (offsets[i], offsets[i + 1]) for i in range(len(self.sizes))\n",
      "            ]\n",
      "\n",
      "        return self\n",
      "\n",
      "\n",
      "@pdclasses.dataclass(config=dict(arbitrary_types_allowed=True))\n",
      "class SequenceSample:\n",
      "    \"\"\"The data structure used to represent sequence data.\n",
      "\n",
      "    Each piece of data is assumed to have several \"keys\" (like a dictionary),\n",
      "    with each key potentially corresponding to multiple sequences.\n",
      "\n",
      "    For example, when running PPO, multiple responses can be generated for each prompt.\n",
      "    If there are 2 prompts, each with 3 responses, the batch might look like:\n",
      "\n",
      "    .. code-block:: console\n",
      "\n",
      "        >>> s = SequenceSample(...)\n",
      "        >>> s.keys\n",
      "        {'resp', 'prompt'}\n",
      "        >>> s.seqlens\n",
      "        {'prompt': [[13], [6]], 'resp': [[6, 17, 15], [13, 15, 13]]}\n",
      "        >>> s.data\n",
      "        {'prompt': torch.tensor([...]), 'resp': torch.tensor([...])}\n",
      "\n",
      "    Key points:\n",
      "\n",
      "    - Data with different batch indices can have varying lengths (e.g., the first prompt has a length of 13\n",
      "      while the second has a length of 6).\n",
      "\n",
      "    - A key (e.g., \"response\") can correspond to multiple sequences with different lengths.\n",
      "      Additionally, the number of sequences for each key can differ from the number of sequences for the data.\n",
      "      For example, the first prompt may have 2 responses, and the second may have 3.\n",
      "\n",
      "    - Regardless of the batch size or the number of sequences stored for each key,\n",
      "      the data is concatenated into a 1D tensor. The outer dimension represents the batch size,\n",
      "      and the inner dimension represents the number of sequences for the key.\n",
      "\n",
      "    This data structure facilitates easy gathering, splitting,\n",
      "    and transferring of non-padded batches between different GPUs.\n",
      "\n",
      "    :param keys: The keys of the data.\n",
      "    :type keys: Set[str]\n",
      "    :param trailing_shapes: The trailing shapes of the data,\n",
      "        excluding the first dimension, which must be the sequence length.\n",
      "        Used to construct the receiving buffer for data transfer.\n",
      "    :type trailing_shapes: Dict[str, torch.Size | Tuple | None]\n",
      "    :param dtypes: The types of the data. Used to construct\n",
      "        the receiving buffer for data transfer.\n",
      "    :type dtypes: Dict[str, torch.dtype | None]\n",
      "    :param ids: Unique identifiers for each piece of data.\n",
      "        Should be provided in the dataset implementation.\n",
      "        Used to append new data to the buffer after a model function call.\n",
      "    :type ids: List[Hashable]\n",
      "    :param seqlens: The sequence lengths of each sequence in the data. For a given key,\n",
      "        this should be a list of lists of integers. The outer list represents the batch size,\n",
      "        while the inner lists represent the sequence lengths for this key.\n",
      "        Python-native lists are used here because (1) pickling torch.Tensor or numpy array is inefficient,\n",
      "        and (2) the size of the inner lists can vary across the batch, making 2D arrays impractical.\n",
      "    :type seqlens: Dict[str, List[List[int]]]\n",
      "    :param data: The actual concatenated data. If this is None,\n",
      "        the sample is a metadata-only sample used by the master worker.\n",
      "        The specification of the data should be consistent with the seqlens,\n",
      "        dtypes, and trailing_shapes.\n",
      "    :type data: Optional[Dict[str, torch.Tensor | None]]\n",
      "    :param metadata: Metadata for the sample. It should be a\n",
      "        dictionary of lists, provided in the dataset implementation.\n",
      "        Note that adding metadata can slow down data transfer.\n",
      "    :type metadata: Dict[str, List[Any]]\n",
      "    \"\"\"\n",
      "\n",
      "    keys: Set[str]\n",
      "    trailing_shapes: Dict[str, torch.Size | Tuple | None]\n",
      "    dtypes: Dict[str, torch.dtype | None]\n",
      "\n",
      "    ids: List[Hashable]\n",
      "\n",
      "    seqlens: Dict[str, List[List[int]]]\n",
      "\n",
      "    data: Optional[Dict[str, torch.Tensor | None]] = None\n",
      "\n",
      "    metadata: Dict[str, List[Any]] = Field(default_factory=dict)\n",
      "\n",
      "    @field_validator(\"ids\")\n",
      "    @classmethod\n",
      "    def _validate_ids(cls, ids: List[Hashable]) -> List[str]:\n",
      "        ids = list(map(str, ids))\n",
      "        if len(ids) != len(set(ids)):\n",
      "            raise ValueError(f\"IDs contain duplicates: {ids}.\")\n",
      "        return ids\n",
      "\n",
      "    @field_validator(\"trailing_shapes\")\n",
      "    @classmethod\n",
      "    def _validate_trailing_shapes(\n",
      "        cls, trailing_shapes: Dict\n",
      "    ) -> Dict[str, Tuple | None]:\n",
      "        for k, v in trailing_shapes.items():\n",
      "            if v is not None:\n",
      "                trailing_shapes[k] = tuple(v)\n",
      "        return trailing_shapes\n",
      "\n",
      "    @field_validator(\"keys\")\n",
      "    @classmethod\n",
      "    def _validate_keys_type(cls, keys: Iterable) -> Set[str]:\n",
      "        keys_ = set(keys)\n",
      "        if len(keys_) != len(keys):\n",
      "            raise ValueError(f\"Keys contain duplicates: {keys}.\")\n",
      "        return keys_\n",
      "\n",
      "    @field_validator(\"seqlens\")\n",
      "    @classmethod\n",
      "    def _validate_seqlens_device_dtype(\n",
      "        cls, seqlens: Dict[str, List[torch.Tensor]]\n",
      "    ) -> Dict[str, List[torch.Tensor]]:\n",
      "        for k, lens in seqlens.items():\n",
      "            assert isinstance(lens, list)\n",
      "            assert all(isinstance(l, list) for l in lens)\n",
      "            for i, lens_ in enumerate(lens):\n",
      "                assert all(isinstance(l_, int) for l_ in lens_)\n",
      "        return seqlens\n",
      "\n",
      "    @model_validator(mode=\"after\")\n",
      "    def _validate_list_length(self) -> \"SequenceSample\":\n",
      "        cond = True\n",
      "        l = len(self.ids)\n",
      "        cond &= all(len(lens) == l for lens in self.seqlens.values())\n",
      "        if not cond:\n",
      "            raise ValueError(\n",
      "                f\"Lengths of ids({len(self.ids)})\"\n",
      "                f\"/seqlens({self.seqlens}) \"\n",
      "                \"are not the same.\"\n",
      "            )\n",
      "\n",
      "        return self\n",
      "\n",
      "    @model_validator(mode=\"after\")\n",
      "    def _validate_keys(self) -> \"SequenceSample\":\n",
      "        cond = True\n",
      "        cond &= self.keys == set(self.seqlens.keys())\n",
      "        cond &= self.keys == set(self.trailing_shapes.keys())\n",
      "        cond &= self.keys == set(self.dtypes.keys())\n",
      "        if self.data is not None:\n",
      "            cond &= self.keys == set(self.data.keys())\n",
      "        if not cond:\n",
      "            err = (\n",
      "                f\"Keys are mismatched. \"\n",
      "                f\"keys={self.keys}, \"\n",
      "                f\"seqlens keys={set(self.seqlens.keys())}, \"\n",
      "                f\"trailing_shapes keys={set(self.trailing_shapes.keys())}, \"\n",
      "                f\"dtypes keys={set(self.dtypes.keys())}\"\n",
      "            )\n",
      "            if self.data is not None:\n",
      "                err += f\", data keys={set(self.data.keys())}\"\n",
      "            raise KeyError(err)\n",
      "        return self\n",
      "\n",
      "    @model_validator(mode=\"after\")\n",
      "    def _validate_shapes(self) -> \"SequenceSample\":\n",
      "        if self.data is None:\n",
      "            return self\n",
      "        acc_seqlen = {\n",
      "            k: sum(sum(lens) for lens in lens_list)\n",
      "            for k, lens_list in self.seqlens.items()\n",
      "        }\n",
      "        for k, v in self.data.items():\n",
      "            if v is None:\n",
      "                continue\n",
      "            if v.shape != (acc_seqlen[k], *self.trailing_shapes[k]):\n",
      "                raise ValueError(\n",
      "                    f\"Key: {k}, Data shape {v.shape} does not match \"\n",
      "                    f\"configured shape {(acc_seqlen[k], *self.trailing_shapes[k])}.\"\n",
      "                )\n",
      "        return self\n",
      "\n",
      "    @model_validator(mode=\"after\")\n",
      "    def _validate_dtypes(self) -> \"SequenceSample\":\n",
      "        if self.data is None:\n",
      "            return self\n",
      "        for k, v in self.data.items():\n",
      "            if v is None:\n",
      "                continue\n",
      "            if v.dtype != self.dtypes[k]:\n",
      "                raise ValueError(\n",
      "                    f\"Data dtype {v.dtype} \"\n",
      "                    f\"does not match configured \"\n",
      "                    f\"dtype {self.dtypes[k]}.\"\n",
      "                )\n",
      "        return self\n",
      "\n",
      "    @classmethod\n",
      "    def gather(cls, samples: List[\"SequenceSample\"], keys: Optional[List[str]] = None):\n",
      "        \"\"\"Gather a list of SequenceSample objects into a single batch.\n",
      "\n",
      "        :param samples: A list of SequenceSample objects to be gathered.\n",
      "        :type samples: List[SequenceSample]\n",
      "        :param keys: The keys to be gathered. Only a subset of keys can\n",
      "            be gathered. If None, the keys from the first sample will be\n",
      "            used.\n",
      "        :type keys: Optional[List[str]]\n",
      "        \"\"\"\n",
      "        if keys is None:\n",
      "            keys = samples[0].keys\n",
      "        else:\n",
      "            keys = set(keys)\n",
      "\n",
      "        seqlens = {k: sum([s.seqlens[k] for s in samples], []) for k in keys}\n",
      "        if samples[0].data is not None:\n",
      "            data = {\n",
      "                k: (\n",
      "                    torch.cat([s.data[k] for s in samples], dim=0)\n",
      "                    if samples[0].data[k] is not None\n",
      "                    else None\n",
      "                )\n",
      "                for k in keys\n",
      "            }\n",
      "        else:\n",
      "            data = None\n",
      "        id_ = sum([s.ids for s in samples], [])\n",
      "        metadata = {\n",
      "            k: sum([s.metadata[k] for s in samples], []) for k in samples[0].metadata\n",
      "        }\n",
      "        with cls.disable_validation():\n",
      "            return cls(\n",
      "                keys=keys,\n",
      "                dtypes={key: samples[0].dtypes[key] for key in keys},\n",
      "                trailing_shapes={key: samples[0].trailing_shapes[key] for key in keys},\n",
      "                ids=id_,\n",
      "                seqlens=seqlens,\n",
      "                data=data,\n",
      "                metadata=metadata,\n",
      "            )\n",
      "\n",
      "    def _get_split_key(self) -> str:\n",
      "        acc_seqlen = {k: sum(sum(l) for l in lens) for k, lens in self.seqlens.items()}\n",
      "        return max(acc_seqlen, key=acc_seqlen.get)\n",
      "\n",
      "    def split_with_spec(self, spec: SequenceSplitSpec) -> List[\"SequenceSample\"]:\n",
      "        \"\"\"Split the data according to the given spec.\"\"\"\n",
      "        samples = []\n",
      "        data_offset = {k: 0 for k in self.keys}\n",
      "        for start, end in spec.partitions:\n",
      "            new_seqlens = {\n",
      "                k: lens_list[start:end] for k, lens_list in self.seqlens.items()\n",
      "            }\n",
      "            _data_len = {\n",
      "                k: sum(sum(lens) for lens in lens_list)\n",
      "                for k, lens_list in new_seqlens.items()\n",
      "            }\n",
      "            if self.data is not None:\n",
      "                new_data = {\n",
      "                    k: (\n",
      "                        v[data_offset[k] : _data_len[k] + data_offset[k]]\n",
      "                        if v is not None\n",
      "                        else None\n",
      "                    )\n",
      "                    for k, v in self.data.items()\n",
      "                }\n",
      "            else:\n",
      "                new_data = None\n",
      "            for k in self.keys:\n",
      "                data_offset[k] += _data_len[k]\n",
      "            new_id = self.ids[start:end]\n",
      "            for k, v in self.metadata.items():\n",
      "                if not isinstance(v, list):\n",
      "                    raise ValueError(\n",
      "                        f\"Unknown how to split non-list metadata: ({k}, {v}).\"\n",
      "                    )\n",
      "            with self.disable_validation():\n",
      "                samples.append(\n",
      "                    SequenceSample(\n",
      "                        dtypes=self.dtypes,\n",
      "                        trailing_shapes=self.trailing_shapes,\n",
      "                        keys=self.keys,\n",
      "                        ids=new_id,\n",
      "                        seqlens=new_seqlens,\n",
      "                        data=new_data,\n",
      "                        metadata={k: v[start:end] for k, v in self.metadata.items()},\n",
      "                    )\n",
      "                )\n",
      "        return samples\n",
      "\n",
      "    def split_with_lengths(\n",
      "        self, mb_spec: MicroBatchSpec, lens: List[int]\n",
      "    ) -> Tuple[List[\"SequenceSample\"], List[int] | np.ndarray, List[int] | np.ndarray]:\n",
      "        group_indices = datapack.ffd_allocate(\n",
      "            lens, mb_spec.max_tokens_per_mb, min_groups=mb_spec.n_mbs\n",
      "        )\n",
      "        group_indices = sorted([sorted(g) for g in group_indices])\n",
      "\n",
      "        forward_indices = datapack.flat2d(group_indices)\n",
      "        sample = SequenceSample.reorder(self, forward_indices)\n",
      "\n",
      "        backward_indices = np.zeros(self.bs, dtype=np.int64)\n",
      "        backward_indices[forward_indices] = np.arange(self.bs)\n",
      "\n",
      "        spec = SequenceSplitSpec(sizes=[len(group) for group in group_indices])\n",
      "\n",
      "        return sample.split_with_spec(spec), forward_indices, backward_indices\n",
      "\n",
      "    def split(\n",
      "        self, mb_spec: MicroBatchSpec\n",
      "    ) -> Tuple[List[\"SequenceSample\"], List[int] | np.ndarray, List[int] | np.ndarray]:\n",
      "        \"\"\"Split the data into `n_mbs` parts.\n",
      "\n",
      "        :param mb_spec: The configuration to split the data into.\n",
      "            `n_mbs` is the minimum number of micro-batches,\n",
      "            `max_tokens_per_mb` is the maximum number of tokens in each micro-batch.\n",
      "            If `max_tokens_per_mb` is a large value, defaults to balanced split.\n",
      "        :type mb_spec: MicroBatchSpec\n",
      "        \"\"\"\n",
      "        lens = [sum(lens) for lens in self.seqlens[self._get_split_key()]]\n",
      "        return self.split_with_lengths(mb_spec, lens)\n",
      "\n",
      "    def synced_data_parallel_split(\n",
      "        self, mb_spec: MicroBatchSpec\n",
      "    ) -> List[\"SequenceSample\"]:\n",
      "        mb_inputs, *_ = self.split(mb_spec)\n",
      "        all_n_mbs = [None for _ in range(constants.data_parallel_world_size())]\n",
      "        dist.all_gather_object(\n",
      "            all_n_mbs, len(mb_inputs), group=constants.data_parallel_group()\n",
      "        )\n",
      "        if all(mbs == len(mb_inputs) for mbs in all_n_mbs):\n",
      "            return mb_inputs\n",
      "        # This method is called when max_tokens_per_mb is given and during training.\n",
      "        # In this case, we evenly partition sequences across DP ranks,\n",
      "        # so the recursion will always terminate when n_mbs = bs // dp_size\n",
      "        return self.synced_data_parallel_split(\n",
      "            MicroBatchSpec.new(mb_spec, n_mbs=max(all_n_mbs))\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def reorder(\n",
      "        sample: \"SequenceSample\", indices: List[int] | np.ndarray\n",
      "    ) -> \"SequenceSample\":\n",
      "        assert set(list(indices)) == set(range(sample.bs))\n",
      "        samples = sample.unpack()\n",
      "        return SequenceSample.gather([samples[i] for i in indices])\n",
      "\n",
      "    @staticmethod\n",
      "    def reorder_output(\n",
      "        x: torch.Tensor,\n",
      "        expected_seqlens: List[List[int]],\n",
      "        forward_indices: List[int] | np.ndarray,\n",
      "        backward_indices: List[int] | np.ndarray,\n",
      "    ) -> torch.Tensor:\n",
      "        assert len(forward_indices) == len(backward_indices) == len(expected_seqlens)\n",
      "        actual_seqlens = [expected_seqlens[i] for i in forward_indices]\n",
      "\n",
      "        group_seqlens = [sum(s) for s in actual_seqlens]\n",
      "        assert x.shape[0] == sum(group_seqlens), (\n",
      "            x.shape[0],\n",
      "            group_seqlens,\n",
      "            len(group_seqlens),\n",
      "            sum(group_seqlens),\n",
      "        )\n",
      "        offsets = [0] + np.cumsum(group_seqlens, axis=0).tolist()\n",
      "        mbs = [x[s:e] for s, e in zip(offsets[:-1], offsets[1:])]\n",
      "        return torch.cat([mbs[i] for i in backward_indices])\n",
      "\n",
      "    def unpack(self):\n",
      "        \"\"\"Unpack a batch of data into individual pieces of data.\"\"\"\n",
      "        partitions = [(i, i + 1) for i in range(self.bs)]\n",
      "        return self.split_with_spec(SequenceSplitSpec(partitions=partitions))\n",
      "\n",
      "    def cuda(self):\n",
      "        return self.to_device(\"cuda\")\n",
      "\n",
      "    def cpu(self):\n",
      "        return self.to_device(\"cpu\")\n",
      "\n",
      "    def to_device(self, device: torch.device):\n",
      "        \"\"\"Move the data to device inplace.\"\"\"\n",
      "        if self.data is None:\n",
      "            return self\n",
      "        self.data = {\n",
      "            k: v.to(device) if v is not None else None for k, v in self.data.items()\n",
      "        }\n",
      "        return self\n",
      "\n",
      "    @property\n",
      "    def bs(self):\n",
      "        \"\"\"The batch size or the number of data pieces in the sample.\"\"\"\n",
      "        return len(self.ids)\n",
      "\n",
      "    def meta(self) -> \"SequenceSample\":\n",
      "        \"\"\"Create a new SequenceSample that does not contain any data.\"\"\"\n",
      "        with self.disable_validation():\n",
      "            return SequenceSample(\n",
      "                keys=self.keys,\n",
      "                trailing_shapes=self.trailing_shapes,\n",
      "                dtypes=self.dtypes,\n",
      "                ids=self.ids,\n",
      "                data=None,\n",
      "                seqlens=self.seqlens,\n",
      "                metadata=self.metadata,\n",
      "            )\n",
      "\n",
      "    def update_(self, other: \"SequenceSample\"):\n",
      "        \"\"\"Inplace update data from another SequenceSample.\n",
      "\n",
      "        Used to amend newly produced data after a model function call.\n",
      "        \"\"\"\n",
      "        self.keys = self.keys.union(other.keys)\n",
      "        self.trailing_shapes.update(other.trailing_shapes)\n",
      "        self.dtypes.update(other.dtypes)\n",
      "        assert self.ids == other.ids, (self.ids, other.ids)\n",
      "        if self.data is not None:\n",
      "            self.data.update(other.data)\n",
      "        self.seqlens.update(other.seqlens)\n",
      "        self.metadata.update(other.metadata)\n",
      "\n",
      "    @staticmethod\n",
      "    def shuffled(sample: \"SequenceSample\") -> \"SequenceSample\":\n",
      "        \"\"\"Create a shuffled sample.\n",
      "        Define it as a staticmethod because it is an out-of-place operation.\n",
      "        (Think about the difference between `sorted` and `l.sort()`).\n",
      "        \"\"\"\n",
      "        seed = seeding.get_shuffle_seed()\n",
      "        rng = np.random.RandomState(seed)\n",
      "        indices = np.arange(sample.bs)\n",
      "        rng.shuffle(indices)\n",
      "        return SequenceSample.reorder(sample, indices)\n",
      "\n",
      "    @staticmethod\n",
      "    def _resolve_seqlen_from_key(key, seqlens: List[int]) -> List[torch.Tensor]:\n",
      "        if key in [\n",
      "            \"seq_no_eos_mask\",\n",
      "            \"greedy_seq_no_eos_mask\",\n",
      "            \"loss_mask\",\n",
      "            \"rewards\",\n",
      "            \"greedy_rewards\",\n",
      "            \"base_scores\",\n",
      "            \"task_ids\",\n",
      "        ]:\n",
      "            return [[1] for _ in seqlens]\n",
      "        elif key in [\n",
      "            \"input_ids\",\n",
      "            \"packed_seq\",\n",
      "            \"seq\",\n",
      "            \"packed_logits_mask\",\n",
      "            \"logits_mask\",\n",
      "            \"prompt_mask\",\n",
      "            \"greedy_prompt_mask\",\n",
      "            \"packed_input_ids\",\n",
      "            \"greedy_packed_input_ids\",\n",
      "            \"values\",\n",
      "            \"packed_prompts\",\n",
      "        ]:\n",
      "            return [[seqlen] for seqlen in seqlens]\n",
      "        elif key in [\n",
      "            \"packed_logprobs\",\n",
      "            \"prox_logp\",\n",
      "            \"logprobs\",\n",
      "            \"packed_ref_logprobs\",\n",
      "            \"ref_logprobs\",\n",
      "            \"old_logp\",\n",
      "            \"ref_logp\",\n",
      "            \"advantages\",\n",
      "            \"ppo_loss_mask\",\n",
      "            \"kl_rewards\",\n",
      "            \"returns\",\n",
      "        ]:\n",
      "            return [[seqlen - 1] for seqlen in seqlens]\n",
      "        else:\n",
      "            raise NotImplementedError(\n",
      "                f\"Seqlen could not be resolved given key {key}. \"\n",
      "                f\"Please explicltly construct the `SequenceSample` object\"\n",
      "                \" without using the `from_default` method.\"\n",
      "            )\n",
      "\n",
      "    @classmethod\n",
      "    def from_default(\n",
      "        cls,\n",
      "        seqlens: List[int],\n",
      "        ids: List[Hashable],\n",
      "        data: Dict[str, torch.Tensor],\n",
      "        metadata: Optional[Dict[str, Any]] = None,\n",
      "    ):\n",
      "        \"\"\"Construct a `SequenceSample` object from default parameters.\n",
      "\n",
      "        This helper function is intended for cases where each piece of data has\n",
      "        a single sequence length (e.g., a single response for each prompt).\n",
      "        The sequence lengths for different keys are resolved automatically\n",
      "        according to the rules in ``_resolve_seqlen_from_key``. While this function\n",
      "        can reduce boilerplate code, it may introduce potential bugs, so it should\n",
      "        be used with caution.\n",
      "\n",
      "        :param seqlens: The sequence lengths of each piece of data. This represents\n",
      "            the length of the main attribute (e.g., `packed_input_ids`). Sequence lengths\n",
      "            for other attributes (e.g., rewards and logprobs) are computed from this parameter.\n",
      "            It is **NOT** the actual length of rewards or logprobs even if it is the only key\n",
      "            in the data.\n",
      "        :type seqlens: List[int]\n",
      "        :param ids: Unique identifiers for each piece of data.\n",
      "        :type ids: List[Hashable]\n",
      "        :param data: The actual data.\n",
      "        :type data: Dict[str, torch.Tensor]\n",
      "        :param metadata: Metadata for the sample. Should be a dictionary where each value\n",
      "            is a list with a length equal to the number of sequence lengths.\n",
      "        :type metadata: Optional[Dict[str, Any]]\n",
      "        \"\"\"\n",
      "        if metadata is None:\n",
      "            metadata = {}\n",
      "        for k, v in metadata.items():\n",
      "            if not isinstance(v, list) or len(v) != len(seqlens):\n",
      "                raise ValueError(\n",
      "                    f\"Metadata `{k}` should be a list of length {len(seqlens)}: {v}.\"\n",
      "                )\n",
      "        keys = set(data.keys())\n",
      "        if isinstance(seqlens[0], list):\n",
      "            assert len(seqlens[0]) == 1\n",
      "            seqlens = [seqlen[0] for seqlen in seqlens]\n",
      "        else:\n",
      "            assert all(isinstance(seqlen, int) for seqlen in seqlens)\n",
      "        seqlens = {key: cls._resolve_seqlen_from_key(key, seqlens) for key in keys}\n",
      "        trailing_shapes = {\n",
      "            key: data[key].shape[1:] if data[key] is not None else None for key in keys\n",
      "        }\n",
      "        dtypes = {\n",
      "            key: data[key].dtype if data[key] is not None else None for key in keys\n",
      "        }\n",
      "        return cls(\n",
      "            keys=keys,\n",
      "            ids=ids,\n",
      "            seqlens=seqlens,\n",
      "            trailing_shapes=trailing_shapes,\n",
      "            dtypes=dtypes,\n",
      "            data=data,\n",
      "            metadata=metadata,\n",
      "        )\n",
      "\n",
      "    def select(self, keys: List[str]):\n",
      "        \"\"\"Select a subset of keys inside the SequenceSample.\"\"\"\n",
      "        with self.disable_validation():\n",
      "            keys = set(keys)\n",
      "            return SequenceSample(\n",
      "                keys=keys,\n",
      "                dtypes={key: self.dtypes[key] for key in keys},\n",
      "                trailing_shapes={key: self.trailing_shapes[key] for key in keys},\n",
      "                ids=self.ids,\n",
      "                seqlens={key: self.seqlens[key] for key in keys},\n",
      "                data=(\n",
      "                    None if self.data is None else {key: self.data[key] for key in keys}\n",
      "                ),\n",
      "                metadata=self.metadata,\n",
      "            )\n",
      "\n",
      "    def remap_keys_(self, remap: Dict[str, str]):\n",
      "        \"\"\"Inplace remap keys of the data.\n",
      "\n",
      "        Useful for reusing the same interface implementation in\n",
      "        different algorithms, where the data can be named differently.\n",
      "        \"\"\"\n",
      "        for k in self.keys:\n",
      "            if k in remap:\n",
      "                new_k = remap[k]\n",
      "                self.seqlens[new_k] = self.seqlens.pop(k)\n",
      "                self.trailing_shapes[new_k] = self.trailing_shapes.pop(k)\n",
      "                self.dtypes[new_k] = self.dtypes.pop(k)\n",
      "                if self.data is not None:\n",
      "                    self.data[new_k] = self.data.pop(k)\n",
      "        self.keys = set(remap.get(k, k) for k in self.keys)\n",
      "\n",
      "    @classmethod\n",
      "    @contextmanager\n",
      "    def disable_validation(cls):\n",
      "        \"\"\"Disable the expensive pydantic validation within this context.\n",
      "\n",
      "        Used to accelerate gather/split/transfer operations since we\n",
      "        have ensured that the data created in datasets and interfaces\n",
      "        are valid.\n",
      "        \"\"\"\n",
      "        original_init = cls.__init__\n",
      "\n",
      "        def no_validation_init(self, *args, **kwargs):\n",
      "            kwargs[\"keys\"] = set(kwargs[\"keys\"])\n",
      "            self.__dict__.update(kwargs)\n",
      "\n",
      "        cls.__init__ = no_validation_init\n",
      "        try:\n",
      "            yield\n",
      "        finally:\n",
      "            cls.__init__ = original_init\n",
      "\n",
      "    def as_json_compatible(self) -> Dict:\n",
      "        return dict(\n",
      "            ids=self.ids,\n",
      "            keys=list(self.keys),\n",
      "            trailing_shapes={\n",
      "                k: tuple(v) if v is not None else None\n",
      "                for k, v in self.trailing_shapes.items()\n",
      "            },\n",
      "            dtypes={k: str(v) if v is not None else v for k, v in self.dtypes.items()},\n",
      "            seqlens=self.seqlens,\n",
      "            data={\n",
      "                k: v.cpu().numpy().tolist() if v is not None else None\n",
      "                for k, v in self.data.items()\n",
      "            },\n",
      "            metadata=self.metadata,\n",
      "        )\n",
      "\n",
      "    @classmethod\n",
      "    def from_json_compatible(cls, data: Dict):\n",
      "        dtypes = {}\n",
      "        for k, dtype_str in data[\"dtypes\"].items():\n",
      "            if dtype_str is not None:\n",
      "                dtypes[k] = getattr(torch, dtype_str.split(\".\")[1])\n",
      "            else:\n",
      "                dtypes[k] = None\n",
      "        return cls(\n",
      "            ids=data[\"ids\"],\n",
      "            keys=set(data[\"keys\"]),\n",
      "            trailing_shapes=data[\"trailing_shapes\"],\n",
      "            dtypes=dtypes,\n",
      "            seqlens=data[\"seqlens\"],\n",
      "            data={\n",
      "                k: torch.tensor(v, dtype=dtypes[k]) if v is not None else v\n",
      "                for k, v in data[\"data\"].items()\n",
      "            },\n",
      "            metadata=data[\"metadata\"],\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class DataBatchMeta:\n",
      "    dp_rank: int\n",
      "    meta_sample: SequenceSample | None\n",
      "    birth_times: List\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class DatasetUtility:\n",
      "    seed: int\n",
      "    dp_rank: int\n",
      "    world_size: int\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.tokenizer.pad_token_id is None:\n",
      "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
      "            if self.tokenizer.eos_token_id is None:\n",
      "                raise ValueError(\"eos_token_id of tokenizer must be defined.\")\n",
      "\n",
      "\n",
      "def get_shuffle_indices(seed: int, size: int):\n",
      "    \"\"\"Generate shuffled indices given seed and (dataset) size.\"\"\"\n",
      "    np_rng = np.random.RandomState(seed=seed)\n",
      "    dtype_ = np.uint32\n",
      "    if size >= (np.iinfo(np.uint32).max - 1):\n",
      "        dtype_ = np.int64\n",
      "    shuffle_idx = np.arange(start=0, stop=size, step=1, dtype=dtype_)\n",
      "    np_rng.shuffle(shuffle_idx)\n",
      "    return shuffle_idx\n",
      "\n",
      "\n",
      "def load_shuffle_split_dataset(\n",
      "    util: DatasetUtility,\n",
      "    dataset_path: str,\n",
      "    dataset_builder: Optional[Callable[[], List[Dict[str, str]]]] = None,\n",
      "):\n",
      "    dataset_path = load_hf_or_local_file(dataset_path)\n",
      "    if dataset_path is not None:\n",
      "        if dataset_path.endswith(\".jsonl\"):\n",
      "            with open(dataset_path, \"r\") as f:\n",
      "                data = [json.loads(ff) for ff in f]\n",
      "        else:\n",
      "            raise NotImplementedError(f\"Unknown dataset extension: {dataset_path}\")\n",
      "    else:\n",
      "        assert dataset_builder is not None\n",
      "        data = dataset_builder()\n",
      "\n",
      "    if any(\"id\" not in d for d in data):\n",
      "        logger.warning(\n",
      "            f'Key \"id\" not found in the dataset. Use indices as dataset IDs.'\n",
      "        )\n",
      "        for idx, d in enumerate(data):\n",
      "            d[\"id\"] = idx\n",
      "\n",
      "    # NOTE: in the original way of seperating data, there is a chance that some DP rank\n",
      "    # get no data, which will raise error in the dataset tokenizer.\n",
      "    assert (\n",
      "        len(data) >= util.world_size\n",
      "    ), \"Dataset size must not be smaller than data parallel world size.\"\n",
      "    bins = np.zeros(util.world_size, dtype=np.int64)\n",
      "    for idx, d in enumerate(data):\n",
      "        bins[idx % util.world_size] += 1\n",
      "    dp_indices = np.pad(np.cumsum(bins), (1, 0))\n",
      "    shuffle_indices = get_shuffle_indices(util.seed, len(data))\n",
      "    subset_indices = shuffle_indices[\n",
      "        dp_indices[util.dp_rank] : dp_indices[util.dp_rank + 1]\n",
      "    ]\n",
      "    data: List[Dict[str, str]] = [data[i] for i in subset_indices]\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "ALL_DATASET_CLASSES = {}\n",
      "\n",
      "\n",
      "def register_dataset(name, dataset_cls):\n",
      "    assert name not in ALL_DATASET_CLASSES\n",
      "    assert \"/\" not in name\n",
      "    ALL_DATASET_CLASSES[name] = dataset_cls\n",
      "\n",
      "\n",
      "def make_dataset(\n",
      "    cfg: Union[str, config_api.DatasetAbstraction],\n",
      "    seed: int,\n",
      "    dp_rank: int,\n",
      "    world_size: int,\n",
      "    tokenizer_or_tokenizer_name: Union[transformers.PreTrainedTokenizerFast, str],\n",
      "    experiment_name: str,\n",
      "    trial_name: str,\n",
      "    cache_root: Optional[str] = None,\n",
      ") -> torch.utils.data.Dataset:\n",
      "    if isinstance(cfg, str):\n",
      "        cfg = config_api.DatasetAbstraction(type_=cfg)\n",
      "\n",
      "    if isinstance(tokenizer_or_tokenizer_name, str):\n",
      "        tokenizer = load_hf_tokenizer(tokenizer_or_tokenizer_name)\n",
      "    elif tokenizer_or_tokenizer_name is None:\n",
      "        raise RuntimeError(\"tokenizer_or_tokenizer_name cannot be None.\")\n",
      "    else:\n",
      "        tokenizer = tokenizer_or_tokenizer_name\n",
      "    util = DatasetUtility(\n",
      "        seed,\n",
      "        dp_rank,\n",
      "        world_size,\n",
      "        tokenizer,\n",
      "    )\n",
      "\n",
      "    if cache_root is None:\n",
      "        dataset_cls = ALL_DATASET_CLASSES[cfg.type_]\n",
      "        return dataset_cls(util=util, **cfg.args)\n",
      "\n",
      "    # Create and check cache path.\n",
      "    if not cache_root.startswith(cluster_spec.fileroot) and not cache_root.startswith(\n",
      "        \"/home\"\n",
      "    ):\n",
      "        raise ValueError(\n",
      "            f\"Data cache path {cache_root} should be /home or under {cluster_spec.fileroot}.\"\n",
      "        )\n",
      "    if \"_\" in experiment_name or \"_\" in trial_name:\n",
      "        raise ValueError(f\"Invalid experiment/trial name.\")\n",
      "\n",
      "    output_path = os.path.join(\n",
      "        cache_root,\n",
      "        experiment_name,\n",
      "        trial_name,\n",
      "        cfg.type_,\n",
      "        f\"seed{seed}\",\n",
      "        f\"world_size{world_size}\",\n",
      "        f\"rank{dp_rank}\",\n",
      "    )\n",
      "    os.makedirs(output_path, exist_ok=True)\n",
      "\n",
      "    fname = \"dataset.pt\"\n",
      "    cache_found = os.path.isfile(os.path.join(output_path, fname))\n",
      "\n",
      "    tik = time.perf_counter()\n",
      "    if not cache_found:\n",
      "        logger.info(f\"No data cache found for rank {dp_rank}. Create it from scratch.\")\n",
      "        dataset = ALL_DATASET_CLASSES[cfg.type_](seed, dp_rank, world_size, **cfg.args)\n",
      "        torch.save(dataset, os.path.join(output_path, fname))\n",
      "    else:\n",
      "        logger.info(f\"Rank {dp_rank} find existing data cache, load it.\")\n",
      "        dataset = torch.load(os.path.join(output_path, fname))\n",
      "    logger.info(f\"Dataset creation/loading time: {time.perf_counter() - tik:.3f}s\")\n",
      "\n",
      "    return dataset\n",
      "\n",
      "\n",
      "def gather_stat(src: List[Dict]) -> Dict:\n",
      "    cnt, stats = {}, {}\n",
      "    for reply in src:\n",
      "        # FIXME: understand why the reply can be None\n",
      "        if not reply:\n",
      "            continue\n",
      "        for k, v in reply.items():\n",
      "            cnt[k] = cnt.get(k, 0) + 1\n",
      "            stats[k] = stats.get(k, 0) + v\n",
      "    res = {k: v / cnt for k, v, cnt in zip(stats.keys(), stats.values(), cnt.values())}\n",
      "    for k, c in cnt.items():\n",
      "        if c != len(src):\n",
      "            logger.warning(f\"Gathered `{k}` is not present in every returned stats.\")\n",
      "    for k, v in res.items():\n",
      "        if any(abs(v - x.get(k, None)) > 1e-4 for x in src):\n",
      "            logger.warning(\n",
      "                f\"Gathered `{k}` is not all-reduced \"\n",
      "                f\"before returning: ({[x.get(k, None) for x in src]}, {v}).\"\n",
      "            )\n",
      "    return res\n",
      "\n",
      "\n",
      "def tabulate_stats(data: Dict[str, float], col=4, floatfmt=\".4e\") -> str:\n",
      "    from tabulate import tabulate\n",
      "\n",
      "    items = list(data.items())\n",
      "    # Calculate how many rows we'll need\n",
      "    row_count = (len(items) + col - 1) // col\n",
      "\n",
      "    # Reorganize items in column-major order\n",
      "    column_major = []\n",
      "    for i in range(row_count):\n",
      "        row = []\n",
      "        for j in range(col):\n",
      "            index = i + j * row_count\n",
      "            if index < len(items):\n",
      "                row.extend(items[index])\n",
      "        column_major.append(row)\n",
      "\n",
      "    return tabulate(column_major, floatfmt=floatfmt, tablefmt=\"fancy_grid\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/sft_exp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "\n",
      "from realhf.api.cli_args import SFTExperimentOptions\n",
      "from realhf.api.core.config import (\n",
      "    DatasetAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelInterfaceType,\n",
      "    ModelName,\n",
      ")\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.experiments.common.common import CommonExperimentConfig\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class SFTConfig(CommonExperimentConfig, SFTExperimentOptions):\n",
      "\n",
      "    @property\n",
      "    def models(self):\n",
      "        return {\n",
      "            \"default\": self.model,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        rpc = MFCDef(\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "            name=\"trainDefault\",\n",
      "            mb_spec=self.allocation.mb_spec,\n",
      "            interface_type=ModelInterfaceType.TRAIN_STEP,\n",
      "            interface_impl=ModelInterfaceAbstraction(\"sft\"),\n",
      "            model_name=\"default\",\n",
      "            input_keys=(\"packed_input_ids\", \"prompt_mask\"),\n",
      "            log_return_value=True,\n",
      "        )\n",
      "        return {\"trainDefault\": rpc}\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        return {\"trainDefault\": self.allocation}\n",
      "\n",
      "    @property\n",
      "    def datasets(self):\n",
      "        return [\n",
      "            DatasetAbstraction(\n",
      "                \"prompt_answer\",\n",
      "                args=dict(\n",
      "                    max_length=self.dataset.max_seqlen,\n",
      "                    dataset_path=self.dataset.train_path,\n",
      "                    fill_to_max_length=self.dataset.fill_to_max_length,\n",
      "                ),\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    @property\n",
      "    def eval_dataset(self):\n",
      "        return DatasetAbstraction(\n",
      "            \"prompt_answer\",\n",
      "            args=dict(\n",
      "                max_length=self.dataset.max_seqlen,\n",
      "                dataset_path=self.dataset.valid_path,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def eval_bs(self) -> int:\n",
      "        return self.dataset.valid_bs_n_seqs\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self):\n",
      "        return self.model.path\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"sft\", SFTConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/null_exp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import dataclasses\n",
      "\n",
      "from realhf.api.cli_args import (\n",
      "    MFCConfig,\n",
      "    ModelTrainEvalConfig,\n",
      "    NullPPOExperimentOptions,\n",
      "    PromptOnlyDatasetConfig,\n",
      "    SFTExperimentOptions,\n",
      ")\n",
      "from realhf.api.core.config import (\n",
      "    DatasetAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelInterfaceType,\n",
      ")\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.experiments.common.common import CommonExperimentConfig\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class NullSFTConfig(CommonExperimentConfig, SFTExperimentOptions):\n",
      "\n",
      "    @property\n",
      "    def models(self):\n",
      "        return {\n",
      "            \"default\": self.model,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        rpc = MFCDef(\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "            name=\"trainDefault\",\n",
      "            mb_spec=self.allocation.mb_spec,\n",
      "            interface_type=ModelInterfaceType.TRAIN_STEP,\n",
      "            interface_impl=ModelInterfaceAbstraction(\"null\"),\n",
      "            model_name=\"default\",\n",
      "            input_keys=(\"packed_input_ids\", \"prompt_mask\"),\n",
      "            log_return_value=True,\n",
      "        )\n",
      "        return {\"trainDefault\": rpc}\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        return {\"trainDefault\": self.allocation}\n",
      "\n",
      "    @property\n",
      "    def datasets(self):\n",
      "        return [\n",
      "            DatasetAbstraction(\n",
      "                \"prompt_answer\",\n",
      "                args=dict(\n",
      "                    max_length=self.dataset.max_seqlen,\n",
      "                    dataset_path=self.dataset.train_path,\n",
      "                ),\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self):\n",
      "        return self.model.path\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"null-sft\", NullSFTConfig)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class NullPPOConfig(CommonExperimentConfig, NullPPOExperimentOptions):\n",
      "\n",
      "    @property\n",
      "    def models(self):\n",
      "        return {\n",
      "            \"default\": self.model,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        rw = MFCDef(\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "            name=\"reward\",\n",
      "            mb_spec=self.inf.mb_spec,\n",
      "            interface_type=ModelInterfaceType.INFERENCE,\n",
      "            interface_impl=ModelInterfaceAbstraction(\"null\"),\n",
      "            model_name=\"default\",\n",
      "            input_keys=(\"packed_prompts\",),\n",
      "            output_keys=(\"rewards\",),\n",
      "        )\n",
      "        rpc = MFCDef(\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "            name=\"trainDefault\",\n",
      "            mb_spec=self.train.mb_spec,\n",
      "            interface_type=ModelInterfaceType.TRAIN_STEP,\n",
      "            interface_impl=ModelInterfaceAbstraction(\"null\"),\n",
      "            model_name=\"default\",\n",
      "            input_keys=(\"packed_prompts\", \"rewards\"),\n",
      "            log_return_value=True,\n",
      "        )\n",
      "        return {\"trainDefault\": rpc, \"reward\": rw}\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        return {\"trainDefault\": self.train, \"reward\": self.inf}\n",
      "\n",
      "    @property\n",
      "    def datasets(self):\n",
      "        return [\n",
      "            DatasetAbstraction(\n",
      "                \"math_code_prompt\",\n",
      "                args=dict(\n",
      "                    max_length=self.dataset.max_prompt_len,\n",
      "                    dataset_path=self.dataset.path,\n",
      "                    filter_threshold=self.dataset_filter_threshold,\n",
      "                    max_filter_percentage=self.dataset_max_filter_percentage,\n",
      "                ),\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self):\n",
      "        return self.model.path\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"null-ppo\", NullPPOConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/common.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import itertools\n",
      "import os\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "import transformers\n",
      "from omegaconf import MISSING, OmegaConf\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import (\n",
      "    BaseExperimentConfig,\n",
      "    MFCConfig,\n",
      "    ModelTrainEvalConfig,\n",
      "    ParallelismConfig,\n",
      ")\n",
      "from realhf.api.core.config import (\n",
      "    DatasetAbstraction,\n",
      "    ModelAbstraction,\n",
      "    ModelBackendAbstraction,\n",
      "    ModelName,\n",
      "    ModelShardID,\n",
      "    StandaloneModelShardAbstraction,\n",
      ")\n",
      "from realhf.api.core.dfg import MFCDef, ModelInterfaceType\n",
      "from realhf.api.core.model_api import HF_MODEL_FAMILY_REGISTRY\n",
      "from realhf.api.core.system_api import (\n",
      "    Experiment,\n",
      "    ExperimentConfig,\n",
      "    ExperimentScheduling,\n",
      "    ModelWorker,\n",
      "    Scheduling,\n",
      "    TasksGroup,\n",
      ")\n",
      "from realhf.api.quickstart.device_mesh import (\n",
      "    DeviceMesh,\n",
      "    RPCAllocation,\n",
      "    make_device_mesh_from_name,\n",
      ")\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.experiments.common.check import (\n",
      "    check_valid_model_and_path,\n",
      "    check_valid_optimizer,\n",
      "    check_valid_parallel_batch_size,\n",
      "    check_valid_sglang,\n",
      "    check_valid_vllm,\n",
      ")\n",
      "from realhf.experiments.common.utils import (\n",
      "    AllocationMode,\n",
      "    asdict,\n",
      "    get_real_model_config,\n",
      "    get_topo,\n",
      "    make_inf_backend_config,\n",
      "    make_train_backend_config,\n",
      "    resolve_replica_ids,\n",
      "    resolve_rpc_hooks,\n",
      ")\n",
      "\n",
      "# Register all HF models\n",
      "import realhf.api.from_hf  # isort:skip\n",
      "\n",
      "logger = logging.getLogger(\"CommonExperimentConfig\", \"colored\")\n",
      "\n",
      "GEN_HYBRID_TRAIN_DECOUPLE_ALLOC_WARN = False\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class CommonExperimentConfig(BaseExperimentConfig, Experiment):\n",
      "\n",
      "    @property\n",
      "    def models(self) -> Dict[str, ModelTrainEvalConfig]:\n",
      "        \"\"\"A dict mapping from model roles to model configurations.\n",
      "\n",
      "        Should be implemented in all subclasses.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(f\"models is not implemented in {self.__class__}\")\n",
      "\n",
      "    @property\n",
      "    def rpcs(self) -> Dict[str, MFCDef]:\n",
      "        \"\"\"A dict mapping from model function call names to the MFCDef objects.\n",
      "\n",
      "        Note that model function call names are different from model\n",
      "        names. Should be implemented in all subclasses.\n",
      "\n",
      "        NOTE: in implementation of ReaL, term RPC also refers to MFC.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(f\"rpcs is not implemented in {self.__class__}\")\n",
      "\n",
      "    @property\n",
      "    def allocations(self) -> Dict[str, MFCConfig]:\n",
      "        \"\"\"The allocation configuration for each model function call.\n",
      "\n",
      "        A dictionary mapping MFC names to its allocation configuration.\n",
      "        Must be implemented in the subclass.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(\n",
      "            f\"allocations is not implemented in {self.__class__}.\"\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def datasets(self) -> List[DatasetAbstraction]:\n",
      "        \"\"\"A list of dataset configurations used for training.\n",
      "\n",
      "        Should be implemented in all subclasses.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(f\"datasets is not implemented in {self.__class__}\")\n",
      "\n",
      "    @property\n",
      "    def eval_dataset(self) -> DatasetAbstraction | None:\n",
      "        \"\"\"The dataset configuration used for evaluation.\n",
      "\n",
      "        Can be None if runtime evaluation is not needed.\n",
      "        \"\"\"\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def eval_bs(self) -> int:\n",
      "        \"\"\"The batch size for runtime evaluation.\"\"\"\n",
      "        return 128\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self) -> str:\n",
      "        \"\"\"The tokenizer for tokenizing train/validation datasets.\n",
      "\n",
      "        Required for all experiments.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(\n",
      "            f\"tokenizer_name_or_path is not implemented in {self.__class__}.\"\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def max_prompt_len(self) -> int:\n",
      "        \"\"\"The maximum prompt length for generation.\n",
      "\n",
      "        Used by CUDAGraph-enabled generation. If no generation is used\n",
      "        in the algorithm, this property can be None.\n",
      "        \"\"\"\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def global_device_mesh(self) -> DeviceMesh:\n",
      "        return DeviceMesh(\n",
      "            n_nodes=self.n_nodes,\n",
      "            n_gpus_per_node=self.n_gpus_per_node,\n",
      "            mapping=np.ones((self.n_nodes, self.n_gpus_per_node), dtype=np.int32),\n",
      "        )\n",
      "\n",
      "    def _heuristic_rpc_allocation(self) -> List[RPCAllocation]:\n",
      "        raise NotImplementedError(\n",
      "            f\"_heuristic_rpc_allocation is not implemented in {self.__class__}\"\n",
      "        )\n",
      "\n",
      "    def scheduling_setup(self) -> ExperimentScheduling:\n",
      "        \"\"\"The resourced occupied by each worker.\n",
      "\n",
      "        The resource requirements will be sent to SLURM or Ray, while\n",
      "        being ignored in the local mode.\n",
      "        \"\"\"\n",
      "        return ExperimentScheduling(\n",
      "            master_worker=TasksGroup(\n",
      "                count=1,\n",
      "                scheduling=Scheduling.master_worker_default(\n",
      "                    cpu=self.cpus_per_master_worker,\n",
      "                    mem=self.mem_per_master_worker,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "            model_worker=TasksGroup(\n",
      "                count=self.n_nodes * self.n_gpus_per_node,\n",
      "                scheduling=Scheduling.model_worker_default(\n",
      "                    cpu=self.cpus_per_model_worker,\n",
      "                    gpu=1,\n",
      "                    mem=self.mem_per_model_worker,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def _allocation_mode(self):\n",
      "        return AllocationMode.from_str(self.allocation_mode)\n",
      "\n",
      "    def _get_rpc_allocations(self) -> List[RPCAllocation]:\n",
      "        if self.allocation_mode == \"manual\" and self.nodelist is None:\n",
      "            logger.warning(\n",
      "                \"Warning: Nodelist is not set in manual allocation mode, \"\n",
      "                \"in this case you cannot specify device mesh for each model RPC. \"\n",
      "                \"All model RPC will be allocated on GPUs automatically \"\n",
      "                f\"allocated according to n_nodes {self.n_nodes} \"\n",
      "                f\"and n_gpus_per_node {self.n_gpus_per_node}.\"\n",
      "            )\n",
      "\n",
      "        self._check_legal_allocation_options()\n",
      "\n",
      "        rpcs = self.rpcs\n",
      "        if self._allocation_mode.is_decoupled():\n",
      "            paras = self._allocation_mode.parallel_strat\n",
      "\n",
      "            gdp, gpp, gtp = paras[\"gen\"][\"d\"], paras[\"gen\"][\"p\"], paras[\"gen\"][\"m\"]\n",
      "            gen_world_size = gdp * gpp * gtp\n",
      "            assert (\n",
      "                gen_world_size < self.n_gpus_per_node\n",
      "                or gen_world_size % self.n_gpus_per_node == 0\n",
      "            )\n",
      "            gen_device_mesh, train_device_mesh = self.global_device_mesh.split(\n",
      "                gen_world_size\n",
      "            )\n",
      "\n",
      "            self.gen_device_mesh = gen_device_mesh\n",
      "            self.train_device_mesh = train_device_mesh\n",
      "\n",
      "            rpc_allocs = []\n",
      "            flag = False\n",
      "            for rpc in rpcs.values():\n",
      "                if rpc.is_generate():\n",
      "                    if gpp != 1:\n",
      "                        raise NotImplementedError(\n",
      "                            \"vllm/sglang pipeline parallel is not supported yet.\"\n",
      "                        )\n",
      "                    if flag:\n",
      "                        raise NotImplementedError(\n",
      "                            \"vllm/sglang does not support two generation RPCs for now.\"\n",
      "                        )\n",
      "                    alloc = RPCAllocation(\n",
      "                        rpc=rpc,\n",
      "                        device_mesh=gen_device_mesh,\n",
      "                        parallel=ParallelismConfig(\n",
      "                            data_parallel_size=gdp,\n",
      "                            pipeline_parallel_size=gpp,\n",
      "                            tensor_parallel_size=gtp,\n",
      "                            use_sequence_parallel=False,\n",
      "                        ),\n",
      "                    )\n",
      "                    flag = True\n",
      "                else:\n",
      "                    rpc_name = rpc.name\n",
      "                    if rpc_name in paras:\n",
      "                        dp, pp, tp = (\n",
      "                            paras[rpc_name][\"d\"],\n",
      "                            paras[rpc_name][\"p\"],\n",
      "                            paras[rpc_name][\"m\"],\n",
      "                        )\n",
      "                    else:\n",
      "                        if \"*\" not in paras:\n",
      "                            raise ValueError(\n",
      "                                f\"RPC {rpc_name} parallel strategy not given, \"\n",
      "                                \"expect a `*` to specify the default parallel strategy.\"\n",
      "                            )\n",
      "                        dp, pp, tp = paras[\"*\"][\"d\"], paras[\"*\"][\"p\"], paras[\"*\"][\"m\"]\n",
      "                    if (\n",
      "                        dp * pp * tp + gdp * gpp * gtp\n",
      "                        != self.n_nodes * self.n_gpus_per_node\n",
      "                    ):\n",
      "                        raise ValueError(\n",
      "                            \"The multiplication of 3D parallel degrees \"\n",
      "                            \"does not equal to the number of gpus. \"\n",
      "                            \"Note that the device mesh of vLLM/SGLang should be disjoint from the device mesh of other MFCs, \"\n",
      "                            \"so their summation should be equal to the total number of gpus. \"\n",
      "                            f\"dp={dp}, pp={pp}, mp={tp}, gen.dp={gdp}, gen.pp={gpp}, gen.mp={gtp}, \"\n",
      "                            f\"n_nodes={self.n_nodes}, n_gpus_per_node={self.n_gpus_per_node}\"\n",
      "                        )\n",
      "                    alloc = RPCAllocation(\n",
      "                        rpc=rpc,\n",
      "                        device_mesh=train_device_mesh,\n",
      "                        parallel=ParallelismConfig(\n",
      "                            data_parallel_size=dp,\n",
      "                            pipeline_parallel_size=pp,\n",
      "                            tensor_parallel_size=tp,\n",
      "                            use_sequence_parallel=(\n",
      "                                rpc.interface_type == ModelInterfaceType.TRAIN_STEP\n",
      "                                and tp > 1\n",
      "                            ),\n",
      "                        ),\n",
      "                    )\n",
      "                rpc_allocs.append(alloc)\n",
      "            if not flag:\n",
      "                raise ValueError(\n",
      "                    \"No generation RPC found. Please use the hybrid train allocation mode.\"\n",
      "                )\n",
      "        elif self._allocation_mode.is_global_hybrid():\n",
      "            paras = self._allocation_mode.parallel_strat\n",
      "            rpc_allocs = []\n",
      "            for rpc_name, rpc in self.rpcs.items():\n",
      "                if rpc_name in paras:\n",
      "                    dp, pp, tp = (\n",
      "                        paras[rpc_name][\"d\"],\n",
      "                        paras[rpc_name][\"p\"],\n",
      "                        paras[rpc_name][\"m\"],\n",
      "                    )\n",
      "                else:\n",
      "                    if \"*\" not in paras:\n",
      "                        raise ValueError(\n",
      "                            f\"RPC {rpc_name} parallel strategy not given, \"\n",
      "                            \"expect a `*` to specify the default parallel strategy.\"\n",
      "                        )\n",
      "                    dp, pp, tp = paras[\"*\"][\"d\"], paras[\"*\"][\"p\"], paras[\"*\"][\"m\"]\n",
      "                assert dp * pp * tp == self.n_nodes * self.n_gpus_per_node\n",
      "                alloc = RPCAllocation(\n",
      "                    rpc=rpc,\n",
      "                    device_mesh=self.global_device_mesh,\n",
      "                    parallel=ParallelismConfig(\n",
      "                        data_parallel_size=dp,\n",
      "                        pipeline_parallel_size=pp,\n",
      "                        tensor_parallel_size=tp,\n",
      "                        use_sequence_parallel=(\n",
      "                            rpc.interface_type == ModelInterfaceType.TRAIN_STEP\n",
      "                            and tp > 1\n",
      "                        ),\n",
      "                    ),\n",
      "                )\n",
      "                rpc_allocs.append(alloc)\n",
      "        elif self.allocation_mode == \"manual\":\n",
      "            if self.nodelist is None:\n",
      "                raise ValueError(\n",
      "                    \"The 'nodelist' option must be specified when using manual allocation mode.\"\n",
      "                )\n",
      "            rpc_allocs: List[RPCAllocation] = [\n",
      "                RPCAllocation(\n",
      "                    rpc=rpc,\n",
      "                    device_mesh=(\n",
      "                        make_device_mesh_from_name(\n",
      "                            self.nodelist,\n",
      "                            self.allocations[rpc_type].device_mesh,\n",
      "                            self.global_device_mesh.n_gpus_per_node,\n",
      "                        )\n",
      "                        if self.allocations[rpc_type].device_mesh is not None\n",
      "                        else self.global_device_mesh\n",
      "                    ),\n",
      "                    parallel=self.allocations[rpc_type].parallel,\n",
      "                )\n",
      "                for rpc_type, rpc in rpcs.items()\n",
      "            ]\n",
      "        elif self.allocation_mode == \"heuristic\":\n",
      "            rpc_allocs: List[RPCAllocation] = self._heuristic_rpc_allocation()\n",
      "        else:\n",
      "            raise NotImplementedError(\n",
      "                f'Unknown allocation mode \"{self.allocation_mode}\".'\n",
      "            )\n",
      "        return rpc_allocs\n",
      "\n",
      "    def _get_model_worker_configs(\n",
      "        self, rpc_allocs: List[RPCAllocation]\n",
      "    ) -> List[ModelWorker]:\n",
      "        self._run_model_sanity_check(rpc_allocs)\n",
      "\n",
      "        model_worker = []\n",
      "        shard_counter = defaultdict(lambda: 0)\n",
      "\n",
      "        model_name_to_rpc_allocs: Dict[ModelName, List[RPCAllocation]] = defaultdict(\n",
      "            list\n",
      "        )\n",
      "        for rpc_alloc in rpc_allocs:\n",
      "            model_name_to_rpc_allocs[rpc_alloc.rpc.model_name].append(rpc_alloc)\n",
      "\n",
      "        for i, j in itertools.product(range(self.n_nodes), range(self.n_gpus_per_node)):\n",
      "            mw = ModelWorker(\n",
      "                base_seed=self.seed,\n",
      "                shards=[],\n",
      "                datasets=self.datasets,\n",
      "                shuffle_dataset=self.shuffle_dataset,\n",
      "                torch_cache_mysophobia=self.torch_cache_mysophobia,\n",
      "                cuda_cache_cleanliness=self.cache_clear_freq is not None,\n",
      "                cuda_cache_clear_freq=self.cache_clear_freq,\n",
      "                tokenizer_name_or_path=self.tokenizer_name_or_path,\n",
      "            )\n",
      "\n",
      "            # decoupled allocation, shortcut case\n",
      "            if (\n",
      "                self._allocation_mode.is_decoupled()\n",
      "                and self.gen_device_mesh.mapping[i, j]\n",
      "            ):\n",
      "                gen_rpc_alloc = next(\n",
      "                    alloc for alloc in rpc_allocs if alloc.rpc.is_generate()\n",
      "                )\n",
      "                model_name = gen_rpc_alloc.rpc.model_name\n",
      "                topo = get_topo(\n",
      "                    gen_rpc_alloc.parallel,\n",
      "                    gradient_checkpointing=False,\n",
      "                    max_prompt_len=(self.max_prompt_len),\n",
      "                    gradient_accumulation_fusion=False,\n",
      "                    is_train=False,\n",
      "                )\n",
      "                model_cfg = self.models[model_name.role]\n",
      "\n",
      "                gen_backend_name = \"\"\n",
      "                if self._allocation_mode.is_decoupled_vllm():\n",
      "                    gen_backend_name = \"vllm\"\n",
      "                elif self._allocation_mode.is_decoupled_sglang():\n",
      "                    gen_backend_name = \"sglang\"\n",
      "                backend_cfg = getattr(model_cfg, gen_backend_name)\n",
      "\n",
      "                global GEN_HYBRID_TRAIN_DECOUPLE_ALLOC_WARN\n",
      "                if (\n",
      "                    backend_cfg.hybrid_train\n",
      "                    and not GEN_HYBRID_TRAIN_DECOUPLE_ALLOC_WARN\n",
      "                ):\n",
      "                    logger.warning(\n",
      "                        \"hybrid_train=True takes no effect for the decoupled allocation\"\n",
      "                    )\n",
      "                    GEN_HYBRID_TRAIN_DECOUPLE_ALLOC_WARN = True\n",
      "                backend_cfg.hybrid_train = False\n",
      "\n",
      "                if gen_backend_name == \"vllm\":\n",
      "                    check_valid_vllm(model_name.role, model_cfg.vllm, rpc_allocs)\n",
      "                elif gen_backend_name == \"sglang\":\n",
      "                    check_valid_sglang(model_name.role, model_cfg.sglang, rpc_allocs)\n",
      "\n",
      "                shard_idx = shard_counter[model_name]\n",
      "                dict_args: Dict[str, Any] = asdict(backend_cfg)\n",
      "                mw.shards.append(\n",
      "                    StandaloneModelShardAbstraction(\n",
      "                        id=ModelShardID(\n",
      "                            model_name=model_name,\n",
      "                            topo=topo,\n",
      "                            dp_rank=topo.get_coord(shard_idx).data,\n",
      "                            pp_rank=topo.get_coord(shard_idx).pipe,\n",
      "                            tp_rank=topo.get_coord(shard_idx).tensor,\n",
      "                        ),\n",
      "                        model=ModelAbstraction(\n",
      "                            \"tokenizer\", args=dict(tokenizer_path=model_cfg.path)\n",
      "                        ),\n",
      "                        backend=ModelBackendAbstraction(\n",
      "                            gen_backend_name,\n",
      "                            args=dict(\n",
      "                                model_path=model_cfg.path,\n",
      "                                **dict_args,\n",
      "                            ),\n",
      "                        ),\n",
      "                    )\n",
      "                )\n",
      "                shard_counter[model_name] += 1\n",
      "\n",
      "                model_worker.append(mw)\n",
      "                continue\n",
      "\n",
      "            for (\n",
      "                model_name,\n",
      "                model_rpc_allocs,\n",
      "            ) in model_name_to_rpc_allocs.items():\n",
      "                rpcs = [rpc_alloc.rpc for rpc_alloc in model_rpc_allocs]\n",
      "                if self._allocation_mode.is_decoupled() and all(\n",
      "                    rpc.is_generate() for rpc in rpcs\n",
      "                ):\n",
      "                    continue\n",
      "                rpc_alloc = model_rpc_allocs[0]\n",
      "                model_cfg = self.models[model_name.role]\n",
      "                model = get_real_model_config(\n",
      "                    model_path=model_cfg.path,\n",
      "                    hf_model_family=model_cfg.type._class,\n",
      "                    is_critic=model_cfg.type.is_critic,\n",
      "                    init_from_scratch=model_cfg.init_from_scratch,\n",
      "                    init_critic_from_actor=model_cfg.init_critic_from_actor,\n",
      "                    dtype=\"bf16\" if model_cfg.bf16 else \"fp16\",\n",
      "                )\n",
      "                hf_config = transformers.AutoConfig.from_pretrained(\n",
      "                    model_cfg.path,\n",
      "                    trust_remote_code=True,\n",
      "                    force_download=True,\n",
      "                )\n",
      "                model_config = HF_MODEL_FAMILY_REGISTRY[model_cfg.type._class][\n",
      "                    \"config_from_hf_converter\"\n",
      "                ](hf_config)\n",
      "                if (\n",
      "                    model_config.n_kv_heads % rpc_alloc.parallel.tensor_parallel_size\n",
      "                    != 0\n",
      "                ) or (\n",
      "                    model_config.n_q_heads % rpc_alloc.parallel.tensor_parallel_size\n",
      "                    != 0\n",
      "                ):\n",
      "                    raise ValueError(\n",
      "                        f\"The number of KV heads {model_config.n_kv_heads} or \"\n",
      "                        f\"Q heads {model_config.n_q_heads} is not\"\n",
      "                        f\" divisible by the configured TP size \"\n",
      "                        f\"({rpc_alloc.parallel.tensor_parallel_size}). \"\n",
      "                        f\"Please decrease TP size.\"\n",
      "                    )\n",
      "                mapping = rpc_alloc.device_mesh.mapping\n",
      "                gradient_checkpointing = model_cfg.gradient_checkpointing and any(\n",
      "                    rpc.interface_type == ModelInterfaceType.TRAIN_STEP for rpc in rpcs\n",
      "                )\n",
      "\n",
      "                topo = get_topo(\n",
      "                    rpc_alloc.parallel,\n",
      "                    gradient_checkpointing=gradient_checkpointing,\n",
      "                    max_prompt_len=(\n",
      "                        self.max_prompt_len\n",
      "                        if any(\n",
      "                            rpc.interface_type == ModelInterfaceType.GENERATE\n",
      "                            for rpc in rpcs\n",
      "                        )\n",
      "                        else None\n",
      "                    ),\n",
      "                    gradient_accumulation_fusion=(model_cfg.backend == \"megatron\")\n",
      "                    and (model_cfg.type._class != \"bailing\"),\n",
      "                    is_train=any(rpc.is_train() for rpc in rpcs),\n",
      "                )\n",
      "\n",
      "                if any(rpc.is_train() for rpc in rpcs):\n",
      "                    backend = make_train_backend_config(model_cfg, rpc_alloc.parallel)\n",
      "                elif model_cfg.vllm.hybrid_train and any(\n",
      "                    rpc.is_generate() for rpc in rpcs\n",
      "                ):\n",
      "                    assert len(rpcs) == 1 and rpcs[0].is_generate(), rpcs\n",
      "                    assert (\n",
      "                        not model_cfg.sglang.hybrid_train\n",
      "                    ), \"vLLM and SGLang cannot be enabled at the same time\"\n",
      "                    dict_args: Dict[str, Any] = asdict(model_cfg.vllm)\n",
      "                    check_valid_vllm(model_name.role, model_cfg.vllm, rpc_allocs)\n",
      "                    backend = ModelBackendAbstraction(\n",
      "                        \"vllm\",\n",
      "                        args=dict(\n",
      "                            model_path=model_cfg.path,\n",
      "                            **dict_args,\n",
      "                        ),\n",
      "                    )\n",
      "                elif model_cfg.sglang.hybrid_train and any(\n",
      "                    rpc.is_generate() for rpc in rpcs\n",
      "                ):\n",
      "                    raise NotImplementedError(\n",
      "                        \"SGLang hybrid_train=True is not supported yet.\"\n",
      "                    )\n",
      "                else:\n",
      "                    backend = make_inf_backend_config(model_cfg, rpc_alloc.parallel)\n",
      "\n",
      "                if mapping[i, j]:\n",
      "                    shard_idx = shard_counter[model_name]\n",
      "                    mw.shards.append(\n",
      "                        StandaloneModelShardAbstraction(\n",
      "                            id=ModelShardID(\n",
      "                                model_name=model_name,\n",
      "                                topo=topo,\n",
      "                                dp_rank=topo.get_coord(shard_idx).data,\n",
      "                                pp_rank=topo.get_coord(shard_idx).pipe,\n",
      "                                tp_rank=topo.get_coord(shard_idx).tensor,\n",
      "                            ),\n",
      "                            model=model,\n",
      "                            backend=backend,\n",
      "                            eval_dataset=self.eval_dataset,\n",
      "                            eval_bs=self.eval_bs,\n",
      "                        )\n",
      "                    )\n",
      "                    shard_counter[model_name] += 1\n",
      "            model_worker.append(mw)\n",
      "        return model_worker\n",
      "\n",
      "    def initial_setup(self) -> ExperimentConfig:\n",
      "\n",
      "        rpc_allocs = self._get_rpc_allocations()\n",
      "\n",
      "        resolve_replica_ids(rpc_allocs, self.models)\n",
      "        resolve_rpc_hooks(\n",
      "            rpc_allocs, self.models\n",
      "        )  # inplace modify MFCDefs in rpc allocations\n",
      "\n",
      "        model_worker = self._get_model_worker_configs(rpc_allocs)\n",
      "\n",
      "        return ExperimentConfig(\n",
      "            exp_ctrl=self.exp_ctrl,\n",
      "            wandb=self.wandb,\n",
      "            tensorboard=self.tensorboard,\n",
      "            model_rpcs=[rpc_alloc.rpc for rpc_alloc in rpc_allocs],\n",
      "            model_worker=model_worker,\n",
      "            auto_eval=self.auto_eval,\n",
      "            evaluator=self.auto_eval_config,\n",
      "        )\n",
      "\n",
      "    def _check_legal_allocation_options(self):\n",
      "        if self.n_nodes > self.cluster.n_nodes:\n",
      "            raise ValueError(\n",
      "                f\"Number of used nodes {self.n_nodes} should not be larger than the cluster size {self.cluster.n_nodes}\"\n",
      "            )\n",
      "        if self.n_gpus_per_node > self.cluster.n_gpus_per_node:\n",
      "            raise ValueError(\n",
      "                f\"Number of 7used GPUs per node {self.n_gpus_per_node} should not be larger than the cluster limit {self.cluster.n_gpus_per_node}\"\n",
      "            )\n",
      "        if self.n_nodes > 1 and self.n_gpus_per_node != self.cluster.n_gpus_per_node:\n",
      "            raise ValueError(\n",
      "                f\"For distributed experiments, only using all GPUs on each node is allowed.\"\n",
      "            )\n",
      "        if self.n_nodes > 1 and self.mode == \"local\":\n",
      "            raise ValueError(\n",
      "                \"Cannot run multi-node experiment in local mode, \"\n",
      "                \"please setup slurm for distributed runs.\"\n",
      "            )\n",
      "\n",
      "        if self.n_gpus_per_node != 8 and self.allocation_mode == \"heuristic\":\n",
      "            raise ValueError(\n",
      "                f\"Cannot run heuristic allocation with \"\n",
      "                f\"n_gpus_per_node {self.n_gpus_per_node}, \"\n",
      "                \"please set n_gpus_per_node to 8.\"\n",
      "            )\n",
      "\n",
      "        for rpc_name, rpc in self.rpcs.items():\n",
      "            if rpc_name != rpc.name:\n",
      "                raise KeyError(\n",
      "                    f\"RPC name {rpc_name} does not match the name in the MFCDef object {rpc.name}.\"\n",
      "                )\n",
      "            if self.allocation_mode == \"manual\" and rpc_name not in self.allocations:\n",
      "                if rpc_name not in self.allocations:\n",
      "                    raise ValueError(\n",
      "                        f\"RPC {rpc_name} is not in allocations, please implement \"\n",
      "                        f\"`allocations()` method in your config class to enable \"\n",
      "                        f\"manual allocation.\"\n",
      "                    )\n",
      "\n",
      "            if rpc.model_name.role not in self.models.keys():\n",
      "                raise ValueError(\n",
      "                    f\"RPC {rpc.name} model name {rpc.model_name.role} is not in models.\"\n",
      "                )\n",
      "\n",
      "    def _run_model_sanity_check(self, rpc_allocs: List[RPCAllocation]):\n",
      "        for alloc in rpc_allocs:\n",
      "            check_valid_parallel_batch_size(alloc)\n",
      "        for role, model in self.models.items():\n",
      "            check_valid_model_and_path(role, model, self.cluster.fileroot)\n",
      "            check_valid_optimizer(model)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/check.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import os\n",
      "from typing import List, Optional\n",
      "\n",
      "from huggingface_hub import snapshot_download, try_to_load_from_cache\n",
      "\n",
      "from realhf.api.cli_args import ModelTrainEvalConfig, SGLangConfig, vLLMConfig\n",
      "from realhf.api.quickstart.device_mesh import RPCAllocation\n",
      "from realhf.base import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def check_is_realhf_native_impl(_cls):\n",
      "    return _cls.__module__.startswith(\"realhf\")\n",
      "\n",
      "\n",
      "def check_is_realhf_native_model_interface(name):\n",
      "    # NOTE: we should not import iterfaces here,\n",
      "    # such that we can avoid CUDA initialization.\n",
      "    return name in [\"ppo_actor\", \"ppo_critic\", \"sft\", \"rw-math-code\", \"fused-threading\"]\n",
      "\n",
      "\n",
      "def check_valid_vllm(role: str, vllm: vLLMConfig, rpc_allocs: List[RPCAllocation]):\n",
      "    rpcs = [alloc.rpc for alloc in rpc_allocs if alloc.rpc.role == role]\n",
      "    if vllm.hybrid_train and not any(rpc.is_train() for rpc in rpcs):\n",
      "        logger.warning(\"vLLM hybrid_train is enabled, but no training RPCs are found.\")\n",
      "    if vllm.hybrid_train and not vllm.enforce_eager:\n",
      "        logger.warning(\n",
      "            \"For version < 0.7.0, vLLM hybrid_train requires eager mode to be enabled. \"\n",
      "            \"The user has the responsibility to ensure the version is correct.\"\n",
      "        )\n",
      "\n",
      "\n",
      "def check_valid_sglang(\n",
      "    role: str, sglang: SGLangConfig, rpc_allocs: List[RPCAllocation]\n",
      "):\n",
      "    rpcs = [alloc.rpc for alloc in rpc_allocs if alloc.rpc.role == role]\n",
      "    if sglang.hybrid_train and not any(rpc.is_train() for rpc in rpcs):\n",
      "        logger.warning(\n",
      "            \"SGLang hybrid_train is enabled, but no training RPCs are found.\"\n",
      "        )\n",
      "    if sglang.hybrid_train and not sglang.disable_cuda_graph:\n",
      "        raise ValueError(\"SGLang hybrid_train requires CUDA graph to be disabled.\")\n",
      "\n",
      "\n",
      "def check_valid_optimizer(model: ModelTrainEvalConfig):\n",
      "    if model.optimizer.min_lr_ratio < 0.0 or model.optimizer.min_lr_ratio > 1.0:\n",
      "        raise ValueError(f\"Invalid min_lr_ratio: {model.optimizer.min_lr_ratio}\")\n",
      "    if (\n",
      "        model.optimizer.warmup_steps_proportion < 0.0\n",
      "        or model.optimizer.warmup_steps_proportion > 1.0\n",
      "    ):\n",
      "        raise ValueError(\n",
      "            f\"Invalid warmup_steps_proportion: {model.optimizer.warmup_steps_proportion}\"\n",
      "        )\n",
      "\n",
      "\n",
      "def check_valid_model_and_path(role: str, model: ModelTrainEvalConfig, fileroot):\n",
      "    \"\"\"\n",
      "    Check if model path exists locally, download from HuggingFace Hub if not.\n",
      "\n",
      "    Args:\n",
      "        role: The role identifier for the model\n",
      "        model: ModelTrainEvalConfig object containing model configuration\n",
      "\n",
      "    Returns:\n",
      "        str: The local path to the model (either existing or newly downloaded)\n",
      "\n",
      "    Raises:\n",
      "        Exception: If download fails or other errors occur\n",
      "    \"\"\"\n",
      "    if os.path.exists(model.path):\n",
      "        return\n",
      "\n",
      "    logger.info(f\"Model path `{model.path}` for `{role}` does not exist locally.\")\n",
      "\n",
      "    # Extract model name from path or use the path as model identifier\n",
      "    # Adjust this logic based on how your ModelTrainEvalConfig stores the model identifier\n",
      "    model_name = model.path\n",
      "\n",
      "    # First, check if model exists in HuggingFace cache\n",
      "    logger.info(f\"Checking HuggingFace cache for model: {model_name}\")\n",
      "    cached_path = _check_huggingface_cache(model_name)\n",
      "    if cached_path:\n",
      "        logger.info(f\"Found model in HuggingFace cache: {cached_path}\")\n",
      "        model.path = cached_path\n",
      "        return\n",
      "\n",
      "    # If not in cache, download to /models/ directory\n",
      "    logger.info(f\"Model not found in cache. Downloading from HuggingFace Hub...\")\n",
      "    target_path = os.path.join(fileroot, \"models\", model_name.replace(\"/\", \"--\"))\n",
      "    if not os.path.exists(target_path):\n",
      "        snapshot_download(\n",
      "            repo_id=model_name,\n",
      "            local_dir=target_path,  # Replace '/' to avoid path issues\n",
      "        )\n",
      "    logger.info(f\"Model downloaded successfully to: {target_path}\")\n",
      "    model.path = target_path\n",
      "\n",
      "\n",
      "def _check_huggingface_cache(model_name: str) -> Optional[str]:\n",
      "    \"\"\"\n",
      "    Check if a model exists in the HuggingFace cache.\n",
      "\n",
      "    Args:\n",
      "        model_name: The HuggingFace model identifier (e.g., 'bert-base-uncased')\n",
      "\n",
      "    Returns:\n",
      "        Optional[str]: Path to cached model if found, None otherwise\n",
      "    \"\"\"\n",
      "    # Try to find the model files in cache\n",
      "    # We'll check for common files that should exist in a model repo\n",
      "    common_files = [\n",
      "        \"config.json\",\n",
      "        \"pytorch_model.bin\",\n",
      "        \"model.safetensors\",\n",
      "        \"tf_model.h5\",\n",
      "    ]\n",
      "\n",
      "    cached_path = None\n",
      "    for filename in common_files:\n",
      "        file_path = try_to_load_from_cache(\n",
      "            repo_id=model_name, filename=filename, repo_type=\"model\"\n",
      "        )\n",
      "        if file_path is not None:\n",
      "            # Get the directory containing the cached file\n",
      "            cached_path = os.path.dirname(file_path)\n",
      "            break\n",
      "\n",
      "    # Verify the cached directory exists and contains model files\n",
      "    if cached_path and os.path.exists(cached_path):\n",
      "        # Double-check that it's a valid model directory\n",
      "        if any(os.path.exists(os.path.join(cached_path, f)) for f in common_files):\n",
      "            return cached_path\n",
      "\n",
      "    return None\n",
      "\n",
      "    logger.info(f\"Model downloaded successfully to: {target_path}\")\n",
      "    # Update the model object's path to point to the downloaded location\n",
      "    model.path = target_path\n",
      "\n",
      "\n",
      "def _check_huggingface_cache(model_name: str) -> Optional[str]:\n",
      "    \"\"\"\n",
      "    Check if a model exists in the HuggingFace cache.\n",
      "\n",
      "    Args:\n",
      "        model_name: The HuggingFace model identifier (e.g., 'bert-base-uncased')\n",
      "\n",
      "    Returns:\n",
      "        Optional[str]: Path to cached model if found, None otherwise\n",
      "    \"\"\"\n",
      "    # Try to find the model files in cache\n",
      "    # We'll check for common files that should exist in a model repo\n",
      "    common_files = [\n",
      "        \"config.json\",\n",
      "        \"pytorch_model.bin\",\n",
      "        \"model.safetensors\",\n",
      "        \"tf_model.h5\",\n",
      "    ]\n",
      "\n",
      "    cached_path = None\n",
      "    for filename in common_files:\n",
      "        file_path = try_to_load_from_cache(\n",
      "            repo_id=model_name, filename=filename, repo_type=\"model\"\n",
      "        )\n",
      "        if file_path is not None:\n",
      "            # Get the directory containing the cached file\n",
      "            cached_path = os.path.dirname(file_path)\n",
      "            break\n",
      "\n",
      "    # Verify the cached directory exists and contains model files\n",
      "    if cached_path and os.path.exists(cached_path):\n",
      "        # Double-check that it's a valid model directory\n",
      "        if any(os.path.exists(os.path.join(cached_path, f)) for f in common_files):\n",
      "            return cached_path\n",
      "\n",
      "    return None\n",
      "\n",
      "\n",
      "def check_valid_parallel_batch_size(rpc_alloc: RPCAllocation):\n",
      "    try:\n",
      "        rpc = rpc_alloc.rpc\n",
      "        mb_spec = rpc.mb_spec\n",
      "\n",
      "        dp_size = rpc_alloc.parallel.data_parallel_size\n",
      "        tp_size = rpc_alloc.parallel.tensor_parallel_size\n",
      "        pp_size = rpc_alloc.parallel.pipeline_parallel_size\n",
      "\n",
      "        factor = 1\n",
      "        if rpc.is_train() and rpc_alloc.parallel.pipeline_parallel_size > 1:\n",
      "            factor = 2\n",
      "\n",
      "        assert (\n",
      "            rpc.n_seqs\n",
      "            >= factor * dp_size * pp_size * rpc.min_n_seqs_per_pass * mb_spec.n_mbs\n",
      "        ), (\n",
      "            rpc.name,\n",
      "            rpc.n_seqs,\n",
      "            mb_spec,\n",
      "            rpc.min_n_seqs_per_pass,\n",
      "            factor,\n",
      "            dp_size,\n",
      "            pp_size,\n",
      "        )\n",
      "    except AssertionError as e:\n",
      "        raise ValueError(\n",
      "            f\"Invalid parallel batch size and batch size configuration.\"\n",
      "        ) from e\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/math_code_eval_exp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import dataclasses\n",
      "from typing import Dict\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import MathCodeEvalOptions, ModelTrainEvalConfig\n",
      "from realhf.api.core.config import (\n",
      "    DatasetAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelInterfaceType,\n",
      ")\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.experiments.common.common import CommonExperimentConfig\n",
      "from realhf.experiments.common.utils import asdict\n",
      "\n",
      "logger = logging.getLogger(\"Math Cdoe Eval exp\", \"colored\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MathCodeEvalConfig(MathCodeEvalOptions, CommonExperimentConfig):\n",
      "\n",
      "    @property\n",
      "    def models(self) -> Dict[str, ModelTrainEvalConfig]:\n",
      "        return {\n",
      "            \"actor\": self.actor,\n",
      "            \"reward\": self.rew,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        if (\n",
      "            self.dataset.max_prompt_len + self.gen_config.max_new_tokens\n",
      "            > self.actor.vllm.max_seq_len_to_capture\n",
      "        ):\n",
      "            raise RuntimeError(\n",
      "                f\"vllm max seq len to capture {self.actor.vllm.max_seq_len_to_capture} is \"\n",
      "                f\"smaller than the prompt length + generation length {self.dataset.max_prompt_len + self.gen_config.max_new_tokens}\"\n",
      "            )\n",
      "\n",
      "        # interfaces\n",
      "        actor_interface = ModelInterfaceAbstraction(\n",
      "            \"ppo_actor\",\n",
      "            args={\n",
      "                \"generation_config\": asdict(self.gen_config),\n",
      "                \"group_size\": self.group_size,\n",
      "            },\n",
      "        )\n",
      "\n",
      "        rw_interface = ModelInterfaceAbstraction(\n",
      "            \"rw-math-code\",\n",
      "            args=dict(\n",
      "                dataset_path=self.dataset.path,\n",
      "                tokenizer_path=self.actor.path,\n",
      "                rw_type=self.rw_type,\n",
      "                check_xml_format=self.check_xml_format,\n",
      "                group_size=self.group_size,\n",
      "                check_verifier_status=self.check_verifier_status,\n",
      "            ),\n",
      "        )\n",
      "        rollout = MFCDef(\n",
      "            name=\"actor_gen\",\n",
      "            model_name=\"actor\",\n",
      "            mb_spec=self.actor_gen.mb_spec,\n",
      "            interface_type=ModelInterfaceType.GENERATE,\n",
      "            interface_impl=actor_interface,\n",
      "            input_keys=(\"packed_prompts\", \"task_ids\"),\n",
      "            output_keys=(\"packed_input_ids\",),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        inf_reward = MFCDef(\n",
      "            name=\"rew_inf\",\n",
      "            model_name=\"reward\",\n",
      "            mb_spec=self.rew_inf.mb_spec,\n",
      "            interface_type=ModelInterfaceType.INFERENCE,\n",
      "            interface_impl=rw_interface,\n",
      "            min_n_seqs_per_pass=1 / self.group_size,\n",
      "            input_keys=(\"packed_input_ids\", \"packed_prompts\", \"task_ids\"),\n",
      "            output_keys=(\"rewards\",),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            \"actor_gen\": rollout,\n",
      "            \"rew_inf\": inf_reward,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        return {\n",
      "            \"actor_gen\": self.actor_gen,\n",
      "            \"rew_inf\": self.rew_inf,\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def datasets(self):\n",
      "        return [\n",
      "            DatasetAbstraction(\n",
      "                \"math_code_prompt\",\n",
      "                args=dict(\n",
      "                    dataset_path=self.dataset.path,\n",
      "                    max_length=self.dataset.max_prompt_len,\n",
      "                ),\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self) -> str:\n",
      "        return self.actor.path\n",
      "\n",
      "    @property\n",
      "    def max_prompt_len(self):\n",
      "        return self.dataset.max_prompt_len\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"math-code-eval\", MathCodeEvalConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/ppo_math_exp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import copy\n",
      "import dataclasses\n",
      "import os\n",
      "from typing import Dict\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import ModelTrainEvalConfig, PPOMATHExperimentOptions\n",
      "from realhf.api.core.config import (\n",
      "    DatasetAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelInterfaceType,\n",
      ")\n",
      "from realhf.api.core.dfg import MFCDef, ParamReallocHook\n",
      "from realhf.api.core.system_api import ExperimentConfig\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.experiments.common.common import CommonExperimentConfig\n",
      "from realhf.experiments.common.utils import (\n",
      "    asdict,\n",
      "    resolve_replica_ids,\n",
      "    resolve_rpc_hooks,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"PPO Math exp\", \"colored\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PPOMATHConfig(CommonExperimentConfig, PPOMATHExperimentOptions):\n",
      "    @property\n",
      "    def ppo_kwargs(self):\n",
      "        return dict(\n",
      "            n_minibatches=self.ppo.ppo_n_minibatches,\n",
      "            kl_ctl=self.ppo.kl_ctl,\n",
      "            discount=self.ppo.discount,\n",
      "            gae_lambda=self.ppo.gae_lambda,\n",
      "            eps_clip=self.ppo.eps_clip,\n",
      "            value_eps_clip=self.ppo.value_eps_clip,\n",
      "            max_reward_clip=self.ppo.max_reward_clip,\n",
      "            adaptive_kl_ctl=self.ppo.use_adaptive_kl_ctl,\n",
      "            value_norm=self.ppo.value_norm,\n",
      "            value_norm_type=self.ppo.value_norm_type,\n",
      "            value_norm_beta=self.ppo.value_norm_beta,\n",
      "            value_norm_eps=self.ppo.value_norm_eps,\n",
      "            disable_value=self.ppo.disable_value,\n",
      "            mask_no_eos_with_zero=self.mask_no_eos_with_zero,\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def models(self) -> Dict[str, ModelTrainEvalConfig]:\n",
      "        # role to config\n",
      "        reward = copy.deepcopy(self.actor)\n",
      "        models = {\n",
      "            \"actor\": self.actor,\n",
      "            \"critic\": self.critic,\n",
      "            \"ref\": self.ref,\n",
      "            \"reward\": reward,\n",
      "        }\n",
      "        if self.ppo.disable_value:\n",
      "            models.pop(\"critic\")\n",
      "        if self.ppo.kl_ctl == 0:\n",
      "            models.pop(\"ref\")\n",
      "        if self.ppo.fuse_rew_ref and self.ppo.kl_ctl != 0:\n",
      "            models.pop(\"reward\")\n",
      "        return models\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        if (\n",
      "            (self._allocation_mode.is_decoupled_vllm() or self.actor.vllm.hybrid_train)\n",
      "            and self.dataset.max_prompt_len + self.ppo.gen.max_new_tokens\n",
      "            > self.actor.vllm.max_seq_len_to_capture\n",
      "            and not self.actor.vllm.enforce_eager\n",
      "        ):\n",
      "            raise RuntimeError(\n",
      "                f\"vllm max seq len to capture {self.actor.vllm.max_seq_len_to_capture} is \"\n",
      "                f\"smaller than the prompt length + generation length \"\n",
      "                f\"{self.dataset.max_prompt_len + self.ppo.gen.max_new_tokens}\"\n",
      "            )\n",
      "\n",
      "        domain = os.getenv(\"FUNCTIONCALL_SERVICE_DOMAIN\", \"\")\n",
      "        if (\n",
      "            domain\n",
      "            and (not (domain.startswith(\"http://\") and \":\" in domain))\n",
      "            and (not (domain.startswith(\"https://\") and \":\" in domain))\n",
      "        ):\n",
      "            raise RuntimeError(\n",
      "                \"function call address FUNCTIONCALL_SERVICE_DOMAIN is invalid.\"\n",
      "            )\n",
      "\n",
      "        # interfaces\n",
      "        actor_interface = ModelInterfaceAbstraction(\n",
      "            \"ppo_actor\",\n",
      "            args={\n",
      "                **copy.deepcopy(self.ppo_kwargs),\n",
      "                # NOTE: to_container converts the object to a dict\n",
      "                # It is used for unifying the profiling API, which requires to\n",
      "                # pass external interface configurations in the launch command.\n",
      "                # Customized dataclass objects will not work in that case.\n",
      "                \"generation_config\": asdict(self.ppo.gen),\n",
      "                \"early_stop_imp_ratio\": self.ppo.early_stop_imp_ratio,\n",
      "                \"adv_norm\": self.ppo.adv_norm,\n",
      "                \"group_size\": self.group_size,\n",
      "                \"generation_size\": self.generation_size,\n",
      "                \"group_adv_norm\": self.group_adv_norm,\n",
      "                \"mask_too_long\": self.mask_too_long,\n",
      "                \"sample_reuse\": self.ppo.actor_sample_reuse,\n",
      "                \"c_clip\": self.ppo.c_clip,\n",
      "                \"behav_imp_weight_cap\": self.ppo.behav_imp_weight_cap,\n",
      "            },\n",
      "        )\n",
      "\n",
      "        critic_interface = ModelInterfaceAbstraction(\n",
      "            \"ppo_critic\",\n",
      "            args={\n",
      "                **copy.deepcopy(self.ppo_kwargs),\n",
      "                \"group_size\": self.group_size,\n",
      "                \"mask_too_long\": self.mask_too_long,\n",
      "                \"sample_reuse\": self.ppo.critic_sample_reuse,\n",
      "            },\n",
      "        )\n",
      "        critic_interface.args.pop(\"eps_clip\")\n",
      "        rw_interface = ModelInterfaceAbstraction(\n",
      "            \"rw-math-code\",\n",
      "            args=dict(\n",
      "                dataset_path=self.dataset.path,\n",
      "                tokenizer_path=self.actor.path,\n",
      "                output_scaling=self.ppo.reward_output_scaling,\n",
      "                output_bias=self.ppo.reward_output_bias,\n",
      "                rw_type=self.rw_type,\n",
      "                check_xml_format=self.check_xml_format,\n",
      "                group_size=self.group_size,\n",
      "                check_verifier_status=self.check_verifier_status,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        ref_interface = copy.deepcopy(actor_interface)\n",
      "        ref_interface.args[\"enable_save\"] = False\n",
      "        if self.ppo.fuse_rew_ref:\n",
      "            ref_interface = ModelInterfaceAbstraction(\n",
      "                \"fused-threading\",\n",
      "                args=dict(interfaces=dict(rew=rw_interface, ref=ref_interface)),\n",
      "            )\n",
      "\n",
      "        rollout_output_keys = [\n",
      "            \"seq_no_eos_mask\",\n",
      "            \"packed_input_ids\",\n",
      "            \"packed_logprobs\",\n",
      "            \"prompt_mask\",\n",
      "        ]\n",
      "        if self.ppo.recompute_logprob and not self.ppo.use_decoupled_loss:\n",
      "            rollout_output_keys.remove(\"packed_logprobs\")\n",
      "        rollout = MFCDef(\n",
      "            name=\"actor_gen\",\n",
      "            model_name=\"actor\",\n",
      "            mb_spec=self.actor_gen.mb_spec,\n",
      "            interface_type=ModelInterfaceType.GENERATE,\n",
      "            interface_impl=actor_interface,\n",
      "            input_keys=(\"packed_prompts\", \"task_ids\"),\n",
      "            output_keys=tuple(rollout_output_keys),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        actor_inf_outputs = (\"packed_logprobs\",)\n",
      "        if self.ppo.use_decoupled_loss:\n",
      "            actor_inf_outputs = (\"proximal_logprobs\",)\n",
      "        actor_inf = MFCDef(\n",
      "            name=\"actor_inf\",\n",
      "            model_name=\"actor\",\n",
      "            mb_spec=self.actor_inf.mb_spec,\n",
      "            interface_type=ModelInterfaceType.INFERENCE,\n",
      "            interface_impl=actor_interface,\n",
      "            input_keys=(\"packed_input_ids\",),\n",
      "            output_keys=actor_inf_outputs,\n",
      "            output_key_remap=dict(logprobs=actor_inf_outputs[0]),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        inf_reward = MFCDef(\n",
      "            name=\"rew_inf\",\n",
      "            model_name=\"reward\",\n",
      "            interface_type=ModelInterfaceType.INFERENCE,\n",
      "            interface_impl=rw_interface,\n",
      "            min_n_seqs_per_pass=1 / self.group_size,\n",
      "            input_keys=(\"packed_input_ids\", \"packed_prompts\", \"task_ids\"),\n",
      "            output_keys=(\"rewards\",),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        # add rew param into ref MFC\n",
      "        inf_ref_inputs = [\"packed_input_ids\"]\n",
      "        inf_ref_outputs = [\"packed_ref_logprobs\"]\n",
      "        if self.ppo.fuse_rew_ref:\n",
      "            inf_ref_inputs += [\"packed_prompts\", \"task_ids\"]\n",
      "            inf_ref_outputs += [\"rewards\"]\n",
      "\n",
      "        inf_ref_logits = MFCDef(\n",
      "            name=\"ref_inf\",\n",
      "            model_name=\"ref\",\n",
      "            mb_spec=self.ref_inf.mb_spec,\n",
      "            interface_type=ModelInterfaceType.INFERENCE,\n",
      "            interface_impl=ref_interface,\n",
      "            min_n_seqs_per_pass=1 / self.group_size,\n",
      "            input_keys=tuple(inf_ref_inputs),\n",
      "            output_keys=tuple(inf_ref_outputs),\n",
      "            output_key_remap=dict(logprobs=\"packed_ref_logprobs\"),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        inf_values = MFCDef(\n",
      "            name=\"critic_inf\",\n",
      "            model_name=\"critic\",\n",
      "            mb_spec=self.critic_inf.mb_spec,\n",
      "            interface_type=ModelInterfaceType.INFERENCE,\n",
      "            interface_impl=critic_interface,\n",
      "            min_n_seqs_per_pass=1 / self.group_size,\n",
      "            input_keys=(\"packed_input_ids\", \"seq_no_eos_mask\"),\n",
      "            output_keys=(\"values\",),\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        train_actor_inputs = [\n",
      "            \"packed_input_ids\",\n",
      "            \"packed_logprobs\",\n",
      "            \"packed_ref_logprobs\",\n",
      "            \"rewards\",\n",
      "            \"task_ids\",\n",
      "            \"values\",\n",
      "            \"prompt_mask\",\n",
      "            \"seq_no_eos_mask\",\n",
      "        ]\n",
      "        if self.ppo.disable_value:\n",
      "            train_actor_inputs.remove(\"values\")\n",
      "        if self.ppo.kl_ctl == 0:\n",
      "            train_actor_inputs.remove(\"packed_ref_logprobs\")\n",
      "        if self.ppo.use_decoupled_loss:\n",
      "            train_actor_inputs.append(\"proximal_logprobs\")\n",
      "        train_actor = MFCDef(\n",
      "            name=\"actor_train\",\n",
      "            model_name=\"actor\",\n",
      "            mb_spec=self.actor_train.mb_spec,\n",
      "            interface_type=ModelInterfaceType.TRAIN_STEP,\n",
      "            interface_impl=actor_interface,\n",
      "            input_keys=tuple(train_actor_inputs),\n",
      "            log_return_value=True,\n",
      "            min_n_seqs_per_pass=self.ppo.ppo_n_minibatches / self.group_size,\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        train_critic_inputs = [\n",
      "            \"packed_input_ids\",\n",
      "            \"packed_logprobs\",\n",
      "            \"packed_ref_logprobs\",\n",
      "            \"rewards\",\n",
      "            \"values\",\n",
      "            \"prompt_mask\",\n",
      "            \"seq_no_eos_mask\",\n",
      "        ]\n",
      "        if self.ppo.kl_ctl == 0:\n",
      "            train_critic_inputs.remove(\"packed_ref_logprobs\")\n",
      "        train_critic = MFCDef(\n",
      "            name=\"critic_train\",\n",
      "            model_name=\"critic\",\n",
      "            mb_spec=self.critic_train.mb_spec,\n",
      "            interface_type=ModelInterfaceType.TRAIN_STEP,\n",
      "            interface_impl=critic_interface,\n",
      "            input_keys=tuple(train_critic_inputs),\n",
      "            log_return_value=True,\n",
      "            min_n_seqs_per_pass=self.ppo.ppo_n_minibatches / self.group_size,\n",
      "            n_seqs=self.dataset.train_bs_n_seqs,\n",
      "        )\n",
      "\n",
      "        rpcs = {\n",
      "            \"actor_gen\": rollout,\n",
      "            \"actor_train\": train_actor,\n",
      "            \"critic_inf\": inf_values,\n",
      "            \"critic_train\": train_critic,\n",
      "            \"ref_inf\": inf_ref_logits,\n",
      "            \"actor_inf\": actor_inf,\n",
      "            \"rew_inf\": inf_reward,\n",
      "        }\n",
      "        if self.ppo.disable_value:\n",
      "            rpcs.pop(\"critic_inf\")\n",
      "            rpcs.pop(\"critic_train\")\n",
      "        if not self.ppo.recompute_logprob and not self.ppo.use_decoupled_loss:\n",
      "            rpcs.pop(\"actor_inf\")\n",
      "        if self.ppo.kl_ctl == 0:\n",
      "            rpcs.pop(\"ref_inf\")\n",
      "        if self.ppo.fuse_rew_ref and self.ppo.kl_ctl != 0:\n",
      "            rpcs.pop(\"rew_inf\")\n",
      "        return rpcs\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        allocs = {\n",
      "            \"actor_gen\": self.actor_gen,\n",
      "            \"actor_train\": self.actor_train,\n",
      "            \"critic_inf\": self.critic_inf,\n",
      "            \"critic_train\": self.critic_train,\n",
      "            \"ref_inf\": self.ref_inf,\n",
      "            \"actor_inf\": self.actor_inf,\n",
      "            \"rew_inf\": self.rew_inf,\n",
      "        }\n",
      "        if self.ppo.disable_value:\n",
      "            allocs.pop(\"critic_inf\")\n",
      "            allocs.pop(\"critic_train\")\n",
      "        if not self.ppo.recompute_logprob and not self.ppo.use_decoupled_loss:\n",
      "            allocs.pop(\"actor_inf\")\n",
      "        if self.ppo.kl_ctl == 0:\n",
      "            allocs.pop(\"ref_inf\")\n",
      "        if self.ppo.fuse_rew_ref and self.ppo.kl_ctl != 0:\n",
      "            allocs.pop(\"rew_inf\")\n",
      "        return allocs\n",
      "\n",
      "    @property\n",
      "    def datasets(self):\n",
      "        return [\n",
      "            DatasetAbstraction(\n",
      "                \"math_code_prompt\",\n",
      "                args=dict(\n",
      "                    dataset_path=self.dataset.path,\n",
      "                    max_length=self.dataset.max_prompt_len,\n",
      "                    filter_threshold=self.dataset_filter_threshold,\n",
      "                    max_filter_percentage=self.dataset_max_filter_percentage,\n",
      "                ),\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self) -> str:\n",
      "        return self.actor.path\n",
      "\n",
      "    @property\n",
      "    def max_prompt_len(self):\n",
      "        return self.dataset.max_prompt_len\n",
      "\n",
      "    def initial_setup(self) -> ExperimentConfig:\n",
      "        rpc_allocs = self._get_rpc_allocations()\n",
      "\n",
      "        resolve_replica_ids(rpc_allocs, self.models)\n",
      "        resolve_rpc_hooks(\n",
      "            rpc_allocs, self.models\n",
      "        )  # inplace modify MFCDefs in rpc allocations\n",
      "\n",
      "        ######### update ref model using ema, ref_ema_eta = 0 means fixed ref model #########\n",
      "        def _find_rpc(name):\n",
      "            return next(alloc.rpc for alloc in rpc_allocs if alloc.rpc.name == name)\n",
      "\n",
      "        # Remove the offload hook of ref_inf, because\n",
      "        # we need to receive parameters from peer GPUs and update it immediately.\n",
      "        if self.ref_ema_eta is not None:\n",
      "\n",
      "            ref_inf = _find_rpc(\"ref_inf\")\n",
      "            ref_inf._post_hooks = []\n",
      "\n",
      "            # Add an unidirectional parameter reallocation hook.\n",
      "            actor_train = _find_rpc(\"actor_train\")\n",
      "            actor_train.add_post_hook(\n",
      "                ParamReallocHook(\n",
      "                    target=ref_inf.model_name,\n",
      "                    eta=self.ref_ema_eta,\n",
      "                )\n",
      "            )\n",
      "        ######### The main difference from normal PPO #########\n",
      "\n",
      "        model_worker = self._get_model_worker_configs(rpc_allocs)\n",
      "        self.auto_eval_config.initial_checkpoint_path = self.actor.path\n",
      "\n",
      "        return ExperimentConfig(\n",
      "            exp_ctrl=self.exp_ctrl,\n",
      "            wandb=self.wandb,\n",
      "            tensorboard=self.tensorboard,\n",
      "            model_rpcs=[rpc_alloc.rpc for rpc_alloc in rpc_allocs],\n",
      "            model_worker=model_worker,\n",
      "            auto_eval=self.auto_eval,\n",
      "            evaluator=self.auto_eval_config,\n",
      "        )\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"ppo-math\", PPOMATHConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/common/utils.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import collections\n",
      "import dataclasses\n",
      "import enum\n",
      "import itertools\n",
      "import re\n",
      "from typing import (\n",
      "    Any,\n",
      "    Callable,\n",
      "    Dict,\n",
      "    Hashable,\n",
      "    Iterable,\n",
      "    List,\n",
      "    Optional,\n",
      "    Set,\n",
      "    Tuple,\n",
      "    Union,\n",
      ")\n",
      "\n",
      "import numpy as np\n",
      "from omegaconf import DictConfig, OmegaConf\n",
      "\n",
      "from realhf.api.cli_args import ModelTrainEvalConfig, ParallelismConfig\n",
      "from realhf.api.core.config import (\n",
      "    ModelAbstraction,\n",
      "    ModelBackendAbstraction,\n",
      "    ModelInterfaceType,\n",
      "    ModelName,\n",
      ")\n",
      "from realhf.api.core.dfg import OffloadHook, ParamReallocHook\n",
      "from realhf.api.quickstart.device_mesh import RPCAllocation\n",
      "from realhf.base import logging\n",
      "from realhf.base.topology import (\n",
      "    DataPipeTensorParallelTopology,\n",
      "    PipeDataTensorParallelTopology,\n",
      "    ProcessTopology,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"Experiment Common Utils\", \"benchmark\")\n",
      "\n",
      "\n",
      "def get_real_model_config(\n",
      "    model_path: str,\n",
      "    hf_model_family: str,\n",
      "    is_critic: bool,\n",
      "    init_from_scratch: bool,\n",
      "    init_critic_from_actor: bool,\n",
      "    dtype: Optional[str] = None,\n",
      ") -> ModelAbstraction:\n",
      "    \"\"\"Make a configuration to build model.\"\"\"\n",
      "    model = ModelAbstraction(\n",
      "        \"real_model\",\n",
      "        args=dict(\n",
      "            model_path=model_path,\n",
      "            is_critic=is_critic,\n",
      "            init_critic_from_actor=init_critic_from_actor,\n",
      "            dtype=dtype,\n",
      "            hf_model_family=hf_model_family,\n",
      "            init_from_scratch=init_from_scratch,\n",
      "        ),\n",
      "    )\n",
      "    return model\n",
      "\n",
      "\n",
      "def get_topo(\n",
      "    parallel: ParallelismConfig,\n",
      "    gradient_checkpointing: bool,\n",
      "    gradient_accumulation_fusion: bool,\n",
      "    is_train: bool,\n",
      "    max_prompt_len: Optional[int] = None,\n",
      ") -> ProcessTopology:\n",
      "    if is_train:\n",
      "        return PipeDataTensorParallelTopology(\n",
      "            num_tp=parallel.tensor_parallel_size,\n",
      "            num_pp=parallel.pipeline_parallel_size,\n",
      "            num_dp=parallel.data_parallel_size,\n",
      "            sequence_parallel=parallel.use_sequence_parallel,\n",
      "            gradient_checkpointing=gradient_checkpointing,\n",
      "            max_prompt_len=max_prompt_len,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "        )\n",
      "    return DataPipeTensorParallelTopology(\n",
      "        num_tp=parallel.tensor_parallel_size,\n",
      "        num_pp=parallel.pipeline_parallel_size,\n",
      "        num_dp=parallel.data_parallel_size,\n",
      "        sequence_parallel=parallel.use_sequence_parallel,\n",
      "        max_prompt_len=max_prompt_len,\n",
      "    )\n",
      "\n",
      "\n",
      "def get_world_size(parallel: ParallelismConfig) -> int:\n",
      "    return (\n",
      "        parallel.tensor_parallel_size\n",
      "        * parallel.pipeline_parallel_size\n",
      "        * parallel.data_parallel_size\n",
      "    )\n",
      "\n",
      "\n",
      "def make_train_backend_config(\n",
      "    model_cfg: ModelTrainEvalConfig, parallel_cfg: ParallelismConfig\n",
      "):\n",
      "    if model_cfg.backend == \"megatron\":\n",
      "        megatron_args: Dict[str, Any] = asdict(model_cfg.megatron)\n",
      "        return ModelBackendAbstraction(\n",
      "            \"megatron\",\n",
      "            args=dict(\n",
      "                bf16=model_cfg.bf16,\n",
      "                optimizer=model_cfg.optimizer,\n",
      "                **megatron_args,\n",
      "            ),\n",
      "        )\n",
      "    elif model_cfg.backend == \"mock_train\":\n",
      "        return ModelBackendAbstraction(\n",
      "            \"mock_train\",\n",
      "            args=dict(\n",
      "                optimizer_name=\"adam\",\n",
      "                optimizer_config=dict(\n",
      "                    lr=model_cfg.optimizer.lr,\n",
      "                    weight_decay=model_cfg.optimizer.weight_decay,\n",
      "                    eps=model_cfg.optimizer.eps,\n",
      "                    betas=(\n",
      "                        model_cfg.optimizer.beta1,\n",
      "                        model_cfg.optimizer.beta2,\n",
      "                    ),\n",
      "                ),\n",
      "            ),\n",
      "        )\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Backend {model_cfg.backend} is not supported.\")\n",
      "\n",
      "\n",
      "def make_inf_backend_config(\n",
      "    model_cfg: ModelTrainEvalConfig, parallel_cfg: ParallelismConfig\n",
      "):\n",
      "    return ModelBackendAbstraction(\"inference\")\n",
      "\n",
      "\n",
      "def resolve_replica_ids(\n",
      "    rpc_allocs: List[RPCAllocation], models: Dict[str, ModelTrainEvalConfig]\n",
      "):\n",
      "    role_rpcs = collections.defaultdict(list)\n",
      "    for alloc in rpc_allocs:\n",
      "        rpc = alloc.rpc\n",
      "        role_rpcs[rpc.role].append(alloc)\n",
      "\n",
      "    for role, allocs in role_rpcs.items():\n",
      "        cnt = len(allocs)\n",
      "        if cnt == 1:\n",
      "            allocs[0].rpc.model_name = ModelName(role, 0)\n",
      "            continue\n",
      "        rpcs = [alloc.rpc for alloc in allocs]\n",
      "        if any(rpc.is_train() for rpc in rpcs):\n",
      "            main_alloc = next(alloc for alloc in allocs if alloc.rpc.is_train())\n",
      "        elif any(rpc.is_inference() for rpc in rpcs):\n",
      "            main_alloc = next(alloc for alloc in allocs if alloc.rpc.is_inference())\n",
      "        else:\n",
      "            main_alloc = allocs[0]\n",
      "        main_alloc.rpc.model_name = ModelName(role, 0)\n",
      "        i = 1\n",
      "        for alloc in allocs:\n",
      "            if alloc.rpc.name == main_alloc.rpc.name:\n",
      "                continue\n",
      "            same_alloc = (\n",
      "                alloc.device_mesh == main_alloc.device_mesh\n",
      "                and ParallelismConfig.parallelism_eq(\n",
      "                    alloc.parallel, main_alloc.parallel\n",
      "                )\n",
      "            )\n",
      "            if not same_alloc or (\n",
      "                alloc.rpc.is_generate()\n",
      "                and main_alloc.rpc.is_train()\n",
      "                and (models[role].vllm.hybrid_train or models[role].sglang.hybrid_train)\n",
      "            ):\n",
      "                alloc.rpc.model_name = ModelName(role, i)\n",
      "                i += 1\n",
      "            else:\n",
      "                alloc.rpc.model_name = ModelName(role, 0)\n",
      "\n",
      "\n",
      "def resolve_rpc_hooks(\n",
      "    rpc_allocs: List[RPCAllocation], model_configs: Dict[str, ModelTrainEvalConfig]\n",
      "):\n",
      "    role_interface_types = collections.defaultdict(set)\n",
      "    for rpc_alloc in rpc_allocs:\n",
      "        role_interface_types[rpc_alloc.rpc.role].add(rpc_alloc.rpc.interface_type)\n",
      "\n",
      "    for i, rpc_alloc in enumerate(rpc_allocs):\n",
      "        rpc = rpc_alloc.rpc\n",
      "        parallel = rpc_alloc.parallel\n",
      "        device_mesh = rpc_alloc.device_mesh\n",
      "        # check param realloc hooks for train_step rpcs\n",
      "        if rpc.interface_type == ModelInterfaceType.TRAIN_STEP:\n",
      "            for j, other in enumerate(rpc_allocs):\n",
      "                if rpc.name == other.rpc.name:\n",
      "                    continue\n",
      "                if rpc.role != other.rpc.role:\n",
      "                    continue\n",
      "                if (\n",
      "                    ParallelismConfig.parallelism_eq(parallel, other.parallel)\n",
      "                    and device_mesh == other.device_mesh\n",
      "                    and not (\n",
      "                        model_configs[rpc.role].vllm.hybrid_train\n",
      "                        and other.rpc.is_generate()\n",
      "                    )\n",
      "                ):\n",
      "                    continue\n",
      "                self_config = model_configs[rpc.model_name.role]\n",
      "                other_config = model_configs[other.rpc.model_name.role]\n",
      "                if (\n",
      "                    self_config.backend == \"deepspeed\"\n",
      "                    or other_config.backend == \"deepspeed\"\n",
      "                ):\n",
      "                    raise ValueError(\n",
      "                        \"Param realloc hooks are not supported in DeepSpeed backend.\"\n",
      "                    )\n",
      "                other.rpc.add_pre_hook(ParamReallocHook(source=rpc.model_name))\n",
      "                other.rpc.add_post_hook(ParamReallocHook(target=rpc.model_name))\n",
      "                logger.info(\n",
      "                    f\"Add param sync hooks between \"\n",
      "                    f\"{rpc.name} and {other.rpc.name} for role {rpc.role}\"\n",
      "                )\n",
      "\n",
      "        # Add offload hooks for inference and generate rpcs.\n",
      "        # Add the offload hook only if the role will not be trained (e.g., reward model)\n",
      "        # and its allocation is overlapped with at least one other RPCs.\n",
      "        # As a result, a single inference/generate RPC will not be offloaded.\n",
      "        overlapped_with_other = False\n",
      "        for other in rpc_allocs:\n",
      "            if rpc.name == other.rpc.name:\n",
      "                continue\n",
      "            if np.any(np.logical_and(other.device_mesh.mapping, device_mesh.mapping)):\n",
      "                overlapped_with_other = True\n",
      "                break\n",
      "        if (\n",
      "            ModelInterfaceType.TRAIN_STEP not in role_interface_types[rpc.role]\n",
      "            and overlapped_with_other\n",
      "        ):\n",
      "            rpc.add_post_hook(OffloadHook())\n",
      "            logger.info(f\"Add offload hook for rpc {rpc.name} for role {rpc.role}\")\n",
      "\n",
      "\n",
      "class AllocationType(enum.Enum):\n",
      "    DECOUPLED_vLLM = 1\n",
      "    GLOBAL_HYBRID = 2\n",
      "    MANUAL = 3\n",
      "    HEURISTIC = 4\n",
      "    DECOUPLED_SGLANG = 5\n",
      "    DECOUPLED_MOCK = 6\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class AllocationMode:\n",
      "    type_: AllocationType\n",
      "    parallel_strat: None | Dict[str, Dict[str, int]]\n",
      "\n",
      "    def is_decoupled(self):\n",
      "        return self.type_ in [\n",
      "            AllocationType.DECOUPLED_vLLM,\n",
      "            AllocationType.DECOUPLED_SGLANG,\n",
      "            AllocationType.DECOUPLED_MOCK,\n",
      "        ]\n",
      "\n",
      "    def is_decoupled_vllm(self):\n",
      "        return self.type_ == AllocationType.DECOUPLED_vLLM\n",
      "\n",
      "    def is_decoupled_sglang(self):\n",
      "        return self.type_ == AllocationType.DECOUPLED_SGLANG\n",
      "\n",
      "    def is_decoupled_mock(self):\n",
      "        return self.type_ == AllocationType.DECOUPLED_MOCK\n",
      "\n",
      "    def is_global_hybrid(self):\n",
      "        return self.type_ == AllocationType.GLOBAL_HYBRID\n",
      "\n",
      "    def get_gen_size(self):\n",
      "        assert self.is_decoupled()\n",
      "        paras = self.parallel_strat\n",
      "        gdp, gpp, gmp = paras[\"gen\"][\"d\"], paras[\"gen\"][\"p\"], paras[\"gen\"][\"m\"]\n",
      "        return gdp * gpp * gmp\n",
      "\n",
      "    def get_gen_tp_size(self):\n",
      "        assert self.is_decoupled()\n",
      "        paras = self.parallel_strat\n",
      "        return paras[\"gen\"][\"m\"]\n",
      "\n",
      "    @classmethod\n",
      "    def from_str(cls, allocation_mode: str):\n",
      "        if allocation_mode == \"manual\":\n",
      "            return cls(AllocationType.MANUAL, None)\n",
      "        if allocation_mode == \"heuristic\":\n",
      "            return cls(AllocationType.HEURISTIC, None)\n",
      "\n",
      "        alloc_3d = AllocationMode.extract_3d_alloc(allocation_mode)\n",
      "        alloc_hybrid = AllocationMode.extract_key_value_alloc(allocation_mode)\n",
      "        alloc_decoupled = AllocationMode.extract_decoupled_alloc(allocation_mode)\n",
      "        if alloc_decoupled:\n",
      "            if \"vllm\" in allocation_mode:\n",
      "                return cls(AllocationType.DECOUPLED_vLLM, alloc_decoupled)\n",
      "            elif \"sglang\" in allocation_mode:\n",
      "                return cls(AllocationType.DECOUPLED_SGLANG, alloc_decoupled)\n",
      "            elif \"mock\" in allocation_mode:\n",
      "                return cls(AllocationType.DECOUPLED_MOCK, alloc_decoupled)\n",
      "        if alloc_3d:\n",
      "            return cls(AllocationType.GLOBAL_HYBRID, alloc_3d)\n",
      "        if alloc_hybrid:\n",
      "            return cls(AllocationType.GLOBAL_HYBRID, alloc_hybrid)\n",
      "        raise NotImplementedError(f\"Failed to parse allocation: {allocation_mode}\")\n",
      "\n",
      "    @staticmethod\n",
      "    def extract_3d_alloc(allocation_mode: str) -> Dict | None:\n",
      "        for x, y, z in itertools.permutations([\"d\", \"m\", \"p\"]):\n",
      "            pattern = rf\"{x}(\\d+){y}(\\d+){z}(\\d+)\"\n",
      "            m = re.match(pattern, allocation_mode)\n",
      "            if not m:\n",
      "                continue\n",
      "            a, b, c = map(int, m.groups())\n",
      "            # to be consistent with the key-value pattern\n",
      "            return {\n",
      "                \"*\": {\n",
      "                    x: a,\n",
      "                    y: b,\n",
      "                    z: c,\n",
      "                }\n",
      "            }\n",
      "\n",
      "    @staticmethod\n",
      "    def extract_decoupled_alloc(allocation_mode: str) -> Dict | None:\n",
      "        pattern = re.compile(\n",
      "            r\"(?:(?:vllm|sglang|mock)\\.(.+?)\\+(.+))|(?:(.+?)\\+(?:vllm|sglang|mock)\\.(.+))\"\n",
      "        )\n",
      "        m = pattern.match(allocation_mode)\n",
      "        if not m:\n",
      "            return\n",
      "        if m.group(1):\n",
      "            gen_alloc = m.group(1)\n",
      "            other_alloc = m.group(2)\n",
      "        else:\n",
      "            gen_alloc = m.group(4)\n",
      "            other_alloc = m.group(3)\n",
      "        gen_alloc = AllocationMode.extract_3d_alloc(gen_alloc)\n",
      "        if not gen_alloc:\n",
      "            return\n",
      "        other_alloc = AllocationMode.extract_3d_alloc(\n",
      "            other_alloc\n",
      "        ) or AllocationMode.extract_key_value_alloc(other_alloc)\n",
      "        if not other_alloc:\n",
      "            return\n",
      "        other_alloc.update({\"gen\": gen_alloc[\"*\"]})\n",
      "        return other_alloc\n",
      "\n",
      "    @staticmethod\n",
      "    def extract_key_value_alloc(\n",
      "        allocation_mode: str,\n",
      "    ) -> Dict[str, Dict[str, int]] | None:\n",
      "        def parse_key_value_pairs(s: str):\n",
      "            pattern = re.compile(r\"([^:,]+):([^:,]+)\")\n",
      "            matches = pattern.findall(s)\n",
      "            if not matches:\n",
      "                return None\n",
      "            return {key: value for key, value in matches}\n",
      "\n",
      "        allocs = parse_key_value_pairs(allocation_mode)\n",
      "        if not allocs:\n",
      "            return\n",
      "        for k, v in allocs.items():\n",
      "            v = AllocationMode.extract_3d_alloc(v)\n",
      "            if not v:\n",
      "                return\n",
      "            allocs[k] = v[\"*\"]\n",
      "        return allocs\n",
      "\n",
      "\n",
      "def asdict(cfg):\n",
      "    if isinstance(cfg, (OmegaConf, DictConfig)):\n",
      "        return OmegaConf.to_container(cfg, resolve=True)\n",
      "    return dataclasses.asdict(cfg)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/benchmark/profile_exp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import copy\n",
      "import dataclasses\n",
      "import itertools\n",
      "import json\n",
      "import os\n",
      "from typing import *\n",
      "\n",
      "from omegaconf import OmegaConf\n",
      "\n",
      "from realhf.api.cli_args import (\n",
      "    MFCConfig,\n",
      "    ModelTrainEvalConfig,\n",
      "    ParallelismConfig,\n",
      "    PromptOnlyDatasetConfig,\n",
      ")\n",
      "from realhf.api.core.config import (\n",
      "    DatasetAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      "    ModelInterfaceType,\n",
      ")\n",
      "from realhf.api.core.dfg import MFCDef\n",
      "from realhf.api.core.system_api import ExperimentConfig\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.base import constants, logging\n",
      "from realhf.base.topology import decompose_to_three_factors\n",
      "from realhf.experiments.common.common import CommonExperimentConfig\n",
      "\n",
      "logger = logging.getLogger(\"Profiling Experiment\", \"system\")\n",
      "\n",
      "\n",
      "def default_parallel_config(n_gpus: int) -> List[Dict[str, Any]]:\n",
      "    factors = decompose_to_three_factors(n_gpus)\n",
      "    x = [\n",
      "        {\n",
      "            \"data_parallel_size\": dp,\n",
      "            \"tensor_parallel_size\": tp,\n",
      "            \"pipeline_parallel_size\": pp,\n",
      "            \"use_sequence_parallel\": tp > 1,\n",
      "        }\n",
      "        for dp, tp, pp in factors\n",
      "    ]\n",
      "    x += [\n",
      "        {\n",
      "            \"data_parallel_size\": dp,\n",
      "            \"tensor_parallel_size\": tp,\n",
      "            \"pipeline_parallel_size\": pp,\n",
      "            \"use_sequence_parallel\": False,\n",
      "        }\n",
      "        for dp, tp, pp in factors\n",
      "        if tp > 1\n",
      "    ]\n",
      "    return x\n",
      "\n",
      "\n",
      "def dataclass_from_dict(klass, d):\n",
      "    try:\n",
      "        fieldtypes = {f.name: f.type for f in dataclasses.fields(klass)}\n",
      "        return klass(**{f: dataclass_from_dict(fieldtypes[f], d[f]) for f in d})\n",
      "    except:\n",
      "        return d  # Not a dataclass field\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ProfileConfig(CommonExperimentConfig):\n",
      "    \"\"\"The experiment configuration for profiling layers and interfaces.\n",
      "\n",
      "    The `initial_setup` method in this experiment will return a list of\n",
      "    experiment configurations, which will be run sequentially.\n",
      "    All configurations share the same experiment name, trial name,\n",
      "    and the scheduling configuration. They can have different models,\n",
      "    datasets, or parallel strategies, as long as they always occupy\n",
      "    a fixed number of GPUs.\n",
      "\n",
      "    It's important to note that, if any error occurs during the execution,\n",
      "    the experiment will terminate immediately. In particular, the OOM error\n",
      "    should not appear because the profiling setup usually uses a small model.\n",
      "    \"\"\"\n",
      "\n",
      "    interfaces_jsonl: str = \"\"\n",
      "    allocations_jsonl: Optional[str] = None\n",
      "    handle_names: Optional[List[str]] = None\n",
      "    n_mbs: Optional[List[int]] = None\n",
      "    batch_sizes: Optional[List[int]] = None\n",
      "    models_jsonl: str = \"\"\n",
      "    datasets_jsonl: str = \"\"\n",
      "\n",
      "    def __post_init__(self):\n",
      "        # Check that handle_name belones to [\"train_step\", \"generate\", \"inference\"]\n",
      "        self.handle_names = list(set(self.handle_names))\n",
      "        if any(\n",
      "            k not in [\"train_step\", \"generate\", \"inference\"] for k in self.handle_names\n",
      "        ):\n",
      "            raise NotImplementedError(f\"Unknown handle_name: {self.handle_name}\")\n",
      "\n",
      "        # Check the configuration of interfaces\n",
      "        if not os.path.exists(self.interfaces_jsonl):\n",
      "            raise FileNotFoundError(\n",
      "                f\"File not found: {self.interfaces_jsonl}. \"\n",
      "                \"It should be a JSONL file specifying the arguments \"\n",
      "                \"for the interface implementation.\"\n",
      "            )\n",
      "        with open(self.interfaces_jsonl, \"r\") as f:\n",
      "            self.interface_kwargs = [json.loads(l) for l in f.readlines()]\n",
      "\n",
      "        # Check the configuration of parallel strategies.\n",
      "        if self.allocations_jsonl is None:\n",
      "            self.parallel_kwargs = default_parallel_config(\n",
      "                self.n_nodes * self.n_gpus_per_node\n",
      "            )\n",
      "        else:\n",
      "            assert self.allocations_jsonl.endswith(\".jsonl\")\n",
      "            assert os.path.exists(self.allocations_jsonl)\n",
      "            with open(self.allocations_jsonl, \"r\") as f:\n",
      "                self.parallel_kwargs = [json.loads(l) for l in f.readlines()]\n",
      "        for pcfg in self.parallel_kwargs:\n",
      "            assert isinstance(pcfg, dict), type(pcfg)\n",
      "            assert all(\n",
      "                k\n",
      "                in [\n",
      "                    \"data_parallel_size\",\n",
      "                    \"tensor_parallel_size\",\n",
      "                    \"pipeline_parallel_size\",\n",
      "                    \"use_sequence_parallel\",\n",
      "                ]\n",
      "                for k in pcfg.keys()\n",
      "            ), pcfg.keys()\n",
      "            assert (self.n_nodes * self.n_gpus_per_node) == (\n",
      "                pcfg.get(\"data_parallel_size\", 1)\n",
      "                * pcfg.get(\"tensor_parallel_size\", 1)\n",
      "                * pcfg.get(\"pipeline_parallel_size\", 1)\n",
      "            )\n",
      "\n",
      "        if self.n_mbs is None:\n",
      "            self.n_mbs = [1]\n",
      "        else:\n",
      "            self.n_mbs = OmegaConf.to_container(self.n_mbs)\n",
      "            assert isinstance(self.n_mbs, list), type(self.n_mbs)\n",
      "            assert all(isinstance(x, int) for x in self.n_mbs)\n",
      "\n",
      "        assert self.batch_sizes is not None\n",
      "\n",
      "        assert os.path.exists(self.models_jsonl)\n",
      "        with open(self.models_jsonl, \"r\") as f:\n",
      "            self.model_kwargs = [json.loads(l) for l in f.readlines()]\n",
      "\n",
      "        assert os.path.exists(self.datasets_jsonl)\n",
      "        with open(self.datasets_jsonl, \"r\") as f:\n",
      "            self.dataset_kwargs = [json.loads(l) for l in f.readlines()]\n",
      "            assert all(x[\"type_\"] == \"prompt\" for x in self.dataset_kwargs)\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        return dict(default=self._tmp_allocation)\n",
      "\n",
      "    @property\n",
      "    def models(self):\n",
      "        return dict(default=self._tmp_model)\n",
      "\n",
      "    @property\n",
      "    def tokenizer_name_or_path(self):\n",
      "        return self._tmp_model.path\n",
      "\n",
      "    @property\n",
      "    def max_prompt_len(self):\n",
      "        return self._tmp_dataset.args[\"max_length\"]\n",
      "\n",
      "    @property\n",
      "    def datasets(self):\n",
      "        return [self._tmp_dataset]\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        return dict(default=self._tmp_rpc)\n",
      "\n",
      "    def initial_setup(self) -> List[ExperimentConfig]:\n",
      "        self.allocation_mode = \"manual\"\n",
      "        setups = []\n",
      "        setup_log_path = os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            self.experiment_name,\n",
      "            self.trial_name,\n",
      "            \"setups.jsonl\",\n",
      "        )\n",
      "        logger.info(\n",
      "            f\"Experiment setup configurations of the profiling experiment \"\n",
      "            f\"will be saved to: {setup_log_path}\"\n",
      "        )\n",
      "        with open(setup_log_path, \"w\") as f:\n",
      "            # batch size in the most outer loop to delay the possible OOM error\n",
      "            for (\n",
      "                bs,\n",
      "                pcfg,\n",
      "                n_mbs,\n",
      "                model_cfg,\n",
      "                dataset_cfg,\n",
      "                handle_name,\n",
      "                interface_cfg,\n",
      "            ) in itertools.product(\n",
      "                self.batch_sizes,\n",
      "                self.parallel_kwargs,\n",
      "                self.n_mbs,\n",
      "                self.model_kwargs,\n",
      "                self.dataset_kwargs,\n",
      "                self.handle_names,\n",
      "                self.interface_kwargs,\n",
      "            ):\n",
      "                if handle_name == \"generate\" and pcfg[\"use_sequence_parallel\"]:\n",
      "                    continue\n",
      "\n",
      "                kwargs_stat = dict(\n",
      "                    parallel=pcfg,\n",
      "                    n_mbs=n_mbs,\n",
      "                    model=model_cfg,\n",
      "                    dataset=dataset_cfg,\n",
      "                    interface=interface_cfg,\n",
      "                    bs=bs,\n",
      "                )\n",
      "                f.write(json.dumps(kwargs_stat) + \"\\n\")\n",
      "\n",
      "                # Create tmp object for constructing experiment setups\n",
      "                self._tmp_allocation = MFCConfig(\n",
      "                    parallel=ParallelismConfig(**pcfg), n_mbs=n_mbs\n",
      "                )\n",
      "                self._tmp_model = dataclass_from_dict(ModelTrainEvalConfig, model_cfg)\n",
      "                self._tmp_dataset = DatasetAbstraction(**dataset_cfg)\n",
      "                if handle_name == \"train_step\":\n",
      "                    interface_type = ModelInterfaceType.TRAIN_STEP\n",
      "                elif handle_name == \"inference\":\n",
      "                    interface_type = ModelInterfaceType.INFERENCE\n",
      "                elif handle_name == \"generate\":\n",
      "                    interface_type = ModelInterfaceType.GENERATE\n",
      "                else:\n",
      "                    raise NotImplementedError(\n",
      "                        f\"Unknown which handle to run in the interface: {self.handle_name}\"\n",
      "                    )\n",
      "                self._tmp_rpc = MFCDef(\n",
      "                    n_seqs=bs,\n",
      "                    name=\"default\",\n",
      "                    n_mbs=n_mbs,\n",
      "                    interface_type=interface_type,\n",
      "                    interface_impl=ModelInterfaceAbstraction(**interface_cfg),\n",
      "                    model_name=\"default\",\n",
      "                    input_keys=[\"packed_prompts\"],\n",
      "                    log_return_value=False,\n",
      "                    balanced_dp=True,\n",
      "                )\n",
      "\n",
      "                setup = copy.deepcopy(super().initial_setup())\n",
      "                for m in setup.model_worker:\n",
      "                    m.profile_mode = True\n",
      "                setups.append(setup)\n",
      "        return setups\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"profile\", ProfileConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/async_exp/async_rl_exp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import itertools\n",
      "import os\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "import transformers\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import AsyncRLOptions, ParallelismConfig\n",
      "from realhf.api.core.config import (\n",
      "    AgentAbstraction,\n",
      "    DatasetAbstraction,\n",
      "    EnvServiceAbstraction,\n",
      "    ModelAbstraction,\n",
      "    ModelBackendAbstraction,\n",
      "    ModelName,\n",
      "    ModelShardID,\n",
      "    StandaloneModelShardAbstraction,\n",
      ")\n",
      "from realhf.api.core.dfg import ModelInterfaceType\n",
      "from realhf.api.core.model_api import (\n",
      "    HF_MODEL_FAMILY_REGISTRY,\n",
      "    GenerationHyperparameters,\n",
      ")\n",
      "from realhf.api.core.system_api import (\n",
      "    ExperimentConfig,\n",
      "    ExperimentScheduling,\n",
      "    GenerationServer,\n",
      "    GserverManager,\n",
      "    ModelWorker,\n",
      "    RolloutWorker,\n",
      "    Scheduling,\n",
      "    TasksGroup,\n",
      ")\n",
      "from realhf.api.quickstart.device_mesh import RPCAllocation\n",
      "from realhf.base.cluster import spec as cluster_spec\n",
      "from realhf.experiments.common.check import check_valid_sglang, check_valid_vllm\n",
      "from realhf.experiments.common.common import CommonExperimentConfig\n",
      "from realhf.experiments.common.utils import (\n",
      "    AllocationMode,\n",
      "    asdict,\n",
      "    get_real_model_config,\n",
      "    get_topo,\n",
      "    make_inf_backend_config,\n",
      "    make_train_backend_config,\n",
      "    resolve_replica_ids,\n",
      "    resolve_rpc_hooks,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"AsyncRLExperimentConfig\", \"colored\")\n",
      "\n",
      "GEN_HYBRID_TRAIN_DECOUPLE_ALLOC_WARN = False\n",
      "GEN_WORKER_DEFAULT_CAPACITY = 512\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class AsyncRLExperimentConfig(CommonExperimentConfig, AsyncRLOptions):\n",
      "    @property\n",
      "    def generation_config(self) -> GenerationHyperparameters:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def env(self) -> EnvServiceAbstraction:\n",
      "        return EnvServiceAbstraction(\"null\")\n",
      "\n",
      "    @property\n",
      "    def agent(self) -> AgentAbstraction:\n",
      "        return AgentAbstraction(\"null\")\n",
      "\n",
      "    @property\n",
      "    def gen_backend_args(self) -> Any:\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def get_backend_type(self) -> str:\n",
      "        return \"sglang\"\n",
      "\n",
      "    def scheduling_setup(self) -> ExperimentScheduling:\n",
      "        \"\"\"The resourced occupied by each worker.\n",
      "\n",
      "        The resource requirements will be sent to SLURM or Ray, while\n",
      "        being ignored in the local mode.\n",
      "        \"\"\"\n",
      "        gen_world_size = AllocationMode.from_str(self.allocation_mode).get_gen_size()\n",
      "        train_world_size = self.n_nodes * self.n_gpus_per_node - gen_world_size\n",
      "        gen_tp_size = AllocationMode.from_str(self.allocation_mode).get_gen_tp_size()\n",
      "        return ExperimentScheduling(\n",
      "            master_worker=TasksGroup(\n",
      "                count=1,\n",
      "                scheduling=Scheduling.master_worker_default(\n",
      "                    cpu=self.cpus_per_master_worker,\n",
      "                    mem=self.mem_per_master_worker,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "            model_worker=TasksGroup(\n",
      "                count=train_world_size,\n",
      "                scheduling=Scheduling.model_worker_default(\n",
      "                    cpu=self.cpus_per_model_worker,\n",
      "                    gpu=1,\n",
      "                    mem=self.mem_per_model_worker,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "            generation_server=TasksGroup(\n",
      "                count=gen_world_size // gen_tp_size,\n",
      "                scheduling=Scheduling.generation_server_default(\n",
      "                    cpu=self.cpus_per_generation_server,\n",
      "                    gpu=gen_tp_size,\n",
      "                    mem=self.mem_per_generation_server,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "            gserver_manager=TasksGroup(\n",
      "                count=1,\n",
      "                scheduling=Scheduling.gserver_manager_default(\n",
      "                    cpu=self.cpus_per_gserver_manager,\n",
      "                    mem=self.mem_per_gserver_manager,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "            rollout_worker=TasksGroup(\n",
      "                count=self.n_rollout_workers or train_world_size,\n",
      "                scheduling=Scheduling.rollout_worker_default(\n",
      "                    cpu=self.cpus_per_rollout_worker,\n",
      "                    mem=self.mem_per_rollout_worker,\n",
      "                    nodelist=self.nodelist,\n",
      "                    exclude=self.exclude,\n",
      "                ),\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    def _get_model_worker_configs(\n",
      "        self, rpc_allocs: List[RPCAllocation]\n",
      "    ) -> List[ModelWorker]:\n",
      "        self._run_model_sanity_check(rpc_allocs)\n",
      "\n",
      "        model_worker = []\n",
      "        shard_counter = defaultdict(lambda: 0)\n",
      "\n",
      "        model_name_to_rpc_allocs: Dict[ModelName, List[RPCAllocation]] = defaultdict(\n",
      "            list\n",
      "        )\n",
      "        for rpc_alloc in rpc_allocs:\n",
      "            model_name_to_rpc_allocs[rpc_alloc.rpc.model_name].append(rpc_alloc)\n",
      "\n",
      "        for i, j in itertools.product(range(self.n_nodes), range(self.n_gpus_per_node)):\n",
      "            if self.gen_device_mesh.mapping[i, j]:\n",
      "                continue\n",
      "            mw = ModelWorker(\n",
      "                base_seed=self.seed,\n",
      "                shards=[],\n",
      "                # NOTE: here we use puller stream to wrap the original dataset\n",
      "                datasets=[\n",
      "                    DatasetAbstraction(\n",
      "                        \"puller_stream\", args=dict(dataset_cfgs=self.datasets)\n",
      "                    )\n",
      "                ],\n",
      "                torch_cache_mysophobia=self.torch_cache_mysophobia,\n",
      "                cuda_cache_cleanliness=self.cache_clear_freq is not None,\n",
      "                cuda_cache_clear_freq=self.cache_clear_freq,\n",
      "                tokenizer_name_or_path=self.tokenizer_name_or_path,\n",
      "            )\n",
      "            for (\n",
      "                model_name,\n",
      "                model_rpc_allocs,\n",
      "            ) in model_name_to_rpc_allocs.items():\n",
      "                rpcs = [rpc_alloc.rpc for rpc_alloc in model_rpc_allocs]\n",
      "                if self._allocation_mode.is_decoupled() and all(\n",
      "                    rpc.is_generate() for rpc in rpcs\n",
      "                ):\n",
      "                    continue\n",
      "                rpc_alloc = model_rpc_allocs[0]\n",
      "                model_cfg = self.models[model_name.role]\n",
      "                model = get_real_model_config(\n",
      "                    model_path=model_cfg.path,\n",
      "                    hf_model_family=model_cfg.type._class,\n",
      "                    is_critic=model_cfg.type.is_critic,\n",
      "                    init_from_scratch=model_cfg.init_from_scratch,\n",
      "                    init_critic_from_actor=model_cfg.init_critic_from_actor,\n",
      "                    dtype=\"bf16\" if model_cfg.bf16 else \"fp16\",\n",
      "                )\n",
      "                hf_config = transformers.AutoConfig.from_pretrained(\n",
      "                    model_cfg.path,\n",
      "                    trust_remote_code=True,\n",
      "                    force_download=True,\n",
      "                )\n",
      "                model_config = HF_MODEL_FAMILY_REGISTRY[model_cfg.type._class][\n",
      "                    \"config_from_hf_converter\"\n",
      "                ](hf_config)\n",
      "                if (\n",
      "                    model_config.n_kv_heads % rpc_alloc.parallel.tensor_parallel_size\n",
      "                    != 0\n",
      "                ) or (\n",
      "                    model_config.n_q_heads % rpc_alloc.parallel.tensor_parallel_size\n",
      "                    != 0\n",
      "                ):\n",
      "                    raise ValueError(\n",
      "                        f\"The number of KV heads {model_config.n_kv_heads} or \"\n",
      "                        f\"Q heads {model_config.n_q_heads} is not\"\n",
      "                        f\" divisible by the configured TP size \"\n",
      "                        f\"({rpc_alloc.parallel.tensor_parallel_size}). \"\n",
      "                        f\"Please decrease TP size.\"\n",
      "                    )\n",
      "                mapping = rpc_alloc.device_mesh.mapping\n",
      "                gradient_checkpointing = model_cfg.gradient_checkpointing and any(\n",
      "                    rpc.interface_type == ModelInterfaceType.TRAIN_STEP for rpc in rpcs\n",
      "                )\n",
      "\n",
      "                topo = get_topo(\n",
      "                    rpc_alloc.parallel,\n",
      "                    gradient_checkpointing=gradient_checkpointing,\n",
      "                    max_prompt_len=(\n",
      "                        self.max_prompt_len\n",
      "                        if any(\n",
      "                            rpc.interface_type == ModelInterfaceType.GENERATE\n",
      "                            for rpc in rpcs\n",
      "                        )\n",
      "                        else None\n",
      "                    ),\n",
      "                    gradient_accumulation_fusion=(model_cfg.backend == \"megatron\")\n",
      "                    and (model_cfg.type._class != \"bailing\"),\n",
      "                    is_train=any(rpc.is_train() for rpc in rpcs),\n",
      "                )\n",
      "\n",
      "                if any(rpc.is_train() for rpc in rpcs):\n",
      "                    backend = make_train_backend_config(model_cfg, rpc_alloc.parallel)\n",
      "                else:\n",
      "                    backend = make_inf_backend_config(model_cfg, rpc_alloc.parallel)\n",
      "\n",
      "                if mapping[i, j]:\n",
      "                    shard_idx = shard_counter[model_name]\n",
      "                    mw.shards.append(\n",
      "                        StandaloneModelShardAbstraction(\n",
      "                            id=ModelShardID(\n",
      "                                model_name=model_name,\n",
      "                                topo=topo,\n",
      "                                dp_rank=topo.get_coord(shard_idx).data,\n",
      "                                pp_rank=topo.get_coord(shard_idx).pipe,\n",
      "                                tp_rank=topo.get_coord(shard_idx).tensor,\n",
      "                            ),\n",
      "                            model=model,\n",
      "                            backend=backend,\n",
      "                            eval_dataset=self.eval_dataset,\n",
      "                            eval_bs=self.eval_bs,\n",
      "                        )\n",
      "                    )\n",
      "                    shard_counter[model_name] += 1\n",
      "            model_worker.append(mw)\n",
      "        return model_worker\n",
      "\n",
      "    def get_rollout_worker_configs(self, rpc_allocs):\n",
      "        gen_world_size = AllocationMode.from_str(self.allocation_mode).get_gen_size()\n",
      "        train_world_size = self.n_nodes * self.n_gpus_per_node - gen_world_size\n",
      "        gen_rpc_alloc = next(alloc for alloc in rpc_allocs if alloc.rpc.is_generate())\n",
      "        model_name = gen_rpc_alloc.rpc.model_name\n",
      "\n",
      "        return [\n",
      "            RolloutWorker(\n",
      "                base_seed=self.seed,\n",
      "                model_name=model_name,\n",
      "                tokenizer_path=self.tokenizer_name_or_path,\n",
      "                new_tokens_per_chunk=self.new_tokens_per_chunk,\n",
      "                env=self.env,\n",
      "                agent=self.agent,\n",
      "                datasets=self.datasets,\n",
      "                rollout_request_timeout=self.flush_request_timeout,\n",
      "            )\n",
      "            for _ in range(self.n_rollout_workers or train_world_size)\n",
      "        ]\n",
      "\n",
      "    def get_generation_server_configs(self, rpc_allocs):\n",
      "        am = AllocationMode.from_str(self.allocation_mode)\n",
      "        gen_world_size = am.get_gen_size()\n",
      "        gen_tp_size = am.get_gen_tp_size()\n",
      "        gen_rpc_alloc = next(alloc for alloc in rpc_allocs if alloc.rpc.is_generate())\n",
      "        model_name = gen_rpc_alloc.rpc.model_name\n",
      "        model_cfg = self.models[model_name.role]\n",
      "        return [\n",
      "            GenerationServer(\n",
      "                base_seed=self.seed,\n",
      "                backend_type=self.get_backend_type,\n",
      "                backend_args=self.gen_backend_args,\n",
      "                model_path=model_cfg.path,\n",
      "                tp_size=gen_tp_size,\n",
      "            )\n",
      "            for _ in range(gen_world_size // gen_tp_size)\n",
      "        ]\n",
      "\n",
      "    def get_gserver_manager_config(self, rpc_allocs):\n",
      "        am = AllocationMode.from_str(self.allocation_mode)\n",
      "        gen_world_size = am.get_gen_size()\n",
      "        gen_tp_size = am.get_gen_tp_size()\n",
      "        gen_rpc_alloc = next(alloc for alloc in rpc_allocs if alloc.rpc.is_generate())\n",
      "        model_name = gen_rpc_alloc.rpc.model_name\n",
      "        train_rpcs = [alloc.rpc for alloc in rpc_allocs if alloc.rpc.is_train()]\n",
      "        assert all(rpc.n_seqs == train_rpcs[0].n_seqs for rpc in train_rpcs)\n",
      "        max_concurrent_rollouts = self.max_concurrent_rollouts\n",
      "        if max_concurrent_rollouts is None:\n",
      "            max_concurrent_rollouts = train_rpcs[0].n_seqs\n",
      "        return [\n",
      "            GserverManager(\n",
      "                model_name=model_name,\n",
      "                flush_request_timeout=self.flush_request_timeout,\n",
      "                n_servers=gen_world_size // gen_tp_size,\n",
      "                schedule_policy=self.schedule_policy,\n",
      "                max_head_offpolicyness=self.max_head_offpolicyness,\n",
      "                train_batch_size=train_rpcs[0].n_seqs,\n",
      "                max_concurrent_rollouts=max_concurrent_rollouts,\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    def initial_setup(self) -> ExperimentConfig:\n",
      "        assert self._allocation_mode.is_decoupled(), self._allocation_mode\n",
      "        rpc_allocs = self._get_rpc_allocations()\n",
      "\n",
      "        resolve_replica_ids(rpc_allocs, self.models)\n",
      "        resolve_rpc_hooks(\n",
      "            rpc_allocs, self.models\n",
      "        )  # inplace modify MFCDefs in rpc allocations\n",
      "\n",
      "        return ExperimentConfig(\n",
      "            exp_ctrl=self.exp_ctrl,\n",
      "            wandb=self.wandb,\n",
      "            tensorboard=self.tensorboard,\n",
      "            # NOTE: master and model worker only see RPCs without generation\n",
      "            model_rpcs=[\n",
      "                rpc_alloc.rpc\n",
      "                for rpc_alloc in rpc_allocs\n",
      "                if not rpc_alloc.rpc.is_generate()\n",
      "            ],\n",
      "            model_worker=self._get_model_worker_configs(rpc_allocs),\n",
      "            generation_server=self.get_generation_server_configs(rpc_allocs),\n",
      "            gserver_manager=self.get_gserver_manager_config(rpc_allocs),\n",
      "            rollout_worker=self.get_rollout_worker_configs(rpc_allocs),\n",
      "            auto_eval=self.auto_eval,\n",
      "            evaluator=self.auto_eval_config,\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/async_exp/async_ppo_math_exp.py ====\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import copy\n",
      "import dataclasses\n",
      "from typing import Any, Dict, List, Tuple\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import ModelTrainEvalConfig, PPOMATHExperimentOptions\n",
      "from realhf.api.core.config import (\n",
      "    AgentAbstraction,\n",
      "    EnvServiceAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      ")\n",
      "from realhf.api.core.model_api import GenerationHyperparameters\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.experiments.async_exp.async_rl_exp import AsyncRLExperimentConfig\n",
      "from realhf.experiments.common.ppo_math_exp import PPOMATHConfig\n",
      "from realhf.experiments.common.utils import asdict\n",
      "\n",
      "logger = logging.getLogger(\"Async PPO Math exp\", \"colored\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class AsyncPPOMATHConfig(AsyncRLExperimentConfig, PPOMATHConfig):\n",
      "\n",
      "    @property\n",
      "    def agent(self) -> AgentAbstraction:\n",
      "        return AgentAbstraction(\n",
      "            \"medical-coding-multi-turn-agent\",\n",
      "            args=dict(\n",
      "                gconfig=self.generation_config,\n",
      "                tokenizer_path=self.actor.path,\n",
      "                # success_rate_lb=self.success_rate_lb,\n",
      "                # success_rate_ub=self.success_rate_ub,\n",
      "                reward_scaling=self.ppo.reward_output_scaling,\n",
      "                reward_bias=self.ppo.reward_output_bias,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def env(self) -> EnvServiceAbstraction:\n",
      "        return EnvServiceAbstraction(\n",
      "            \"medical-coding-final-answer-env\", args=dict(dataset_path=self.dataset.path)\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def gen_backend_args(self) -> Any:\n",
      "        return self.actor.sglang\n",
      "\n",
      "    @property\n",
      "    def generation_config(self) -> GenerationHyperparameters:\n",
      "        return GenerationHyperparameters(**asdict(self.ppo.gen)).new(n=self.group_size)\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        rpcs = super(AsyncPPOMATHConfig, self).rpcs\n",
      "        rpcs[\"actor_gen\"].output_keys = (\n",
      "            *rpcs[\"actor_gen\"].output_keys,\n",
      "            \"packed_prompts\",\n",
      "            \"version_start\",\n",
      "            \"version_end\",\n",
      "            \"rewards\",\n",
      "            \"birth_time\",\n",
      "        )\n",
      "        rpcs[\"actor_train\"].input_keys = (\n",
      "            *rpcs[\"actor_train\"].input_keys,\n",
      "            \"version_start\",\n",
      "            \"version_end\",\n",
      "        )\n",
      "        # Revert the effect of fuse_rew_ref, because we don't have the reward RPC in async experiments.\n",
      "        if \"ref_inf\" in rpcs:\n",
      "            actor_interface = rpcs[\"actor_train\"].interface_impl\n",
      "            rpcs[\"ref_inf\"].interface_impl = copy.deepcopy(actor_interface)\n",
      "            rpcs[\"ref_inf\"].interface_impl.args[\"enable_save\"] = False\n",
      "            rpcs[\"ref_inf\"].input_keys = (\"packed_input_ids\",)\n",
      "            rpcs[\"ref_inf\"].output_keys = (\"packed_ref_logprobs\",)\n",
      "        if \"rew_inf\" in rpcs:\n",
      "            rpcs.pop(\"rew_inf\")\n",
      "        if self.no_training:\n",
      "            rpcs[\"actor_train\"].interface_impl = ModelInterfaceAbstraction(\"null\")\n",
      "            rpcs[\"actor_gen\"].interface_impl = ModelInterfaceAbstraction(\"null\")\n",
      "            if \"actor_inf\" in rpcs:\n",
      "                rpcs[\"actor_inf\"].interface_impl = ModelInterfaceAbstraction(\"null\")\n",
      "        return rpcs\n",
      "\n",
      "    @property\n",
      "    def models(self) -> Dict[str, ModelTrainEvalConfig]:\n",
      "        models = super().models\n",
      "        if \"reward\" in models:\n",
      "            models.pop(\"reward\")\n",
      "        return models\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        allocations = super(AsyncPPOMATHConfig, self).allocations\n",
      "        if \"rew_inf\" in allocations:\n",
      "            allocations.pop(\"rew_inf\")\n",
      "        return allocations\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"async-ppo-math\", AsyncPPOMATHConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/experiments/async_exp/async_ppo_math_exp_medical.py ====\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import copy\n",
      "import dataclasses\n",
      "from typing import Any, Dict, List, Tuple\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.cli_args import ModelTrainEvalConfig, PPOMATHExperimentOptions\n",
      "from realhf.api.core.config import (\n",
      "    AgentAbstraction,\n",
      "    EnvServiceAbstraction,\n",
      "    ModelInterfaceAbstraction,\n",
      ")\n",
      "from realhf.api.core.model_api import GenerationHyperparameters\n",
      "from realhf.api.quickstart.entrypoint import register_quickstart_exp\n",
      "from realhf.experiments.async_exp.async_rl_exp import AsyncRLExperimentConfig\n",
      "from realhf.experiments.common.ppo_math_exp import PPOMATHConfig\n",
      "from realhf.experiments.common.utils import asdict\n",
      "\n",
      "logger = logging.getLogger(\"Async PPO Math exp\", \"colored\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class AsyncPPOMATHConfig(AsyncRLExperimentConfig, PPOMATHConfig):\n",
      "\n",
      "    @property\n",
      "    def agent(self) -> AgentAbstraction:\n",
      "        return AgentAbstraction(\n",
      "            \"medical-coding-multi-turn-agent\",\n",
      "            args=dict(\n",
      "                gconfig=self.generation_config,\n",
      "                tokenizer_path=self.actor.path,\n",
      "                # success_rate_lb=self.success_rate_lb,\n",
      "                # success_rate_ub=self.success_rate_ub,\n",
      "                reward_scaling=self.ppo.reward_output_scaling,\n",
      "                reward_bias=self.ppo.reward_output_bias,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def env(self) -> EnvServiceAbstraction:\n",
      "        return EnvServiceAbstraction(\n",
      "            \"medical-coding-final-answer-env\", args=dict(dataset_path=self.dataset.path)\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def gen_backend_args(self) -> Any:\n",
      "        return self.actor.sglang\n",
      "\n",
      "    @property\n",
      "    def generation_config(self) -> GenerationHyperparameters:\n",
      "        return GenerationHyperparameters(**asdict(self.ppo.gen)).new(n=self.group_size)\n",
      "\n",
      "    @property\n",
      "    def rpcs(self):\n",
      "        rpcs = super(AsyncPPOMATHConfig, self).rpcs\n",
      "        rpcs[\"actor_gen\"].output_keys = (\n",
      "            *rpcs[\"actor_gen\"].output_keys,\n",
      "            \"packed_prompts\",\n",
      "            \"version_start\",\n",
      "            \"version_end\",\n",
      "            \"rewards\",\n",
      "            \"birth_time\",\n",
      "        )\n",
      "        rpcs[\"actor_train\"].input_keys = (\n",
      "            *rpcs[\"actor_train\"].input_keys,\n",
      "            \"version_start\",\n",
      "            \"version_end\",\n",
      "        )\n",
      "        # Revert the effect of fuse_rew_ref, because we don't have the reward RPC in async experiments.\n",
      "        if \"ref_inf\" in rpcs:\n",
      "            actor_interface = rpcs[\"actor_train\"].interface_impl\n",
      "            rpcs[\"ref_inf\"].interface_impl = copy.deepcopy(actor_interface)\n",
      "            rpcs[\"ref_inf\"].interface_impl.args[\"enable_save\"] = False\n",
      "            rpcs[\"ref_inf\"].input_keys = (\"packed_input_ids\",)\n",
      "            rpcs[\"ref_inf\"].output_keys = (\"packed_ref_logprobs\",)\n",
      "        if \"rew_inf\" in rpcs:\n",
      "            rpcs.pop(\"rew_inf\")\n",
      "        if self.no_training:\n",
      "            rpcs[\"actor_train\"].interface_impl = ModelInterfaceAbstraction(\"null\")\n",
      "            rpcs[\"actor_gen\"].interface_impl = ModelInterfaceAbstraction(\"null\")\n",
      "            if \"actor_inf\" in rpcs:\n",
      "                rpcs[\"actor_inf\"].interface_impl = ModelInterfaceAbstraction(\"null\")\n",
      "        return rpcs\n",
      "\n",
      "    @property\n",
      "    def models(self) -> Dict[str, ModelTrainEvalConfig]:\n",
      "        models = super().models\n",
      "        if \"reward\" in models:\n",
      "            models.pop(\"reward\")\n",
      "        return models\n",
      "\n",
      "    @property\n",
      "    def allocations(self):\n",
      "        allocations = super(AsyncPPOMATHConfig, self).allocations\n",
      "        if \"rew_inf\" in allocations:\n",
      "            allocations.pop(\"rew_inf\")\n",
      "        return allocations\n",
      "\n",
      "\n",
      "register_quickstart_exp(\"async-ppo-math\", AsyncPPOMATHConfig)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/medical_coding_multi_turn_agent.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import json\n",
      "import re\n",
      "\n",
      "def load_guideline(index: str,\n",
      "                   root_dir: str = '/home/scadmin/SC_project/ai-coding-reasoning/flat_kodierhandbuch_machine') -> str:\n",
      "    \"\"\"\n",
      "    Look for a file named {index}.txt (or starting with {index}) in root_dir,\n",
      "    and return its text contents. Raises FileNotFoundError if nothing matches.\n",
      "    \"\"\"\n",
      "    # 1) try exact match\n",
      "    fname = f\"{index}.txt\"\n",
      "    path = os.path.join(root_dir, fname)\n",
      "    if os.path.isfile(path):\n",
      "        with open(path, encoding='utf-8') as f:\n",
      "            return f.read()\n",
      "    # 2) fallback: any file whose name starts with the index\n",
      "    for fn in os.listdir(root_dir):\n",
      "        if fn.startswith(index) and fn.lower().endswith('.txt'):\n",
      "            with open(os.path.join(root_dir, fn), encoding='utf-8') as f:\n",
      "                return f.read()\n",
      "    raise FileNotFoundError(f\"No guideline file found for index '{index}' in {root_dir}\")\n",
      "\n",
      "\n",
      "\n",
      "try:\n",
      "    with open('/home/scadmin/SC_project/ai-coding-reasoning/reasoning_generate_sft_data/code_range_mapping.json', 'r', encoding='utf-8') as f:\n",
      "        code_range_mapping = json.load(f)\n",
      "except FileNotFoundError:\n",
      "    print(\"Warning: 'code_range_mapping.json' not found. `retrieve_icd_code_chapter` will fail.\")\n",
      "    code_range_mapping = {}\n",
      "\n",
      "try:\n",
      "    # This variable name must match the one used in the function below\n",
      "    with open(\"/home/scadmin/SC_project/ai-coding-reasoning/sorted_combined_dict_translated_v2.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
      "        sorted_combined_dict_translated_v2 = json.load(json_file)\n",
      "except FileNotFoundError:\n",
      "    print(\"Warning: 'sorted_combined_dict_translated_v2.json' not found. `retrieve_icd_code_chapter` will fail.\")\n",
      "    sorted_combined_dict_translated_v2 = {}\n",
      "\n",
      "\n",
      "try:\n",
      "    with open('/home/scadmin/SC_project/ai-coding-reasoning/reasoning_generate_sft_data/coding_documentation/json_export/coding_groups_data.json', 'r', encoding='utf-8') as file:\n",
      "        coding_groups_data = json.load(file)\n",
      "except FileNotFoundError:\n",
      "    print(\"CRITICAL_ERROR_STARTUP: 'coding_groups_data.json' not found. `retrieve_icd_code_chapter` validation will fail.\")\n",
      "    coding_groups_data = {}\n",
      "    \n",
      "\n",
      "def retrieve_icd_code_chapter(subchapter: str) -> tuple[str, set[str] | None]:\n",
      "    \"\"\"\n",
      "    Retrieves ICD-10-GM code details for a given subchapter index and validates data.\n",
      "\n",
      "    Args:\n",
      "        subchapter: The index number, e.g., \"1.1\" or \"19.9\".\n",
      "\n",
      "    Returns:\n",
      "        A tuple:\n",
      "        - On success: (string_content_for_llm, set_of_3_digit_base_codes_in_this_subchapter)\n",
      "        - On failure/data inconsistency: (error_message_string_for_llm, None)\n",
      "    \"\"\"\n",
      "    if not subchapter or not isinstance(subchapter, str):\n",
      "        error_msg = \"Error: Invalid or missing 'subchapter' argument. Please provide a string like '1.1'.\"\n",
      "        # print(f\"TOOL_ERROR (retrieve_icd_code_chapter): {error_msg}\") # Optional: internal server log\n",
      "        return error_msg, None\n",
      "\n",
      "    match = re.search(r\"\\b\\d{1,2}\\.\\d{1,2}\\b\", subchapter)\n",
      "    if not match:\n",
      "        error_msg = f\"Error: Could not find a valid subchapter format (e.g., '1.1') in your input '{subchapter}'. Please use a valid subchapter index.\"\n",
      "        # print(f\"TOOL_ERROR (retrieve_icd_code_chapter): Invalid format for input '{subchapter}'\") # Optional: internal server log\n",
      "        return error_msg, None\n",
      "    \n",
      "    chapter_key_from_input = match.group(0) # e.g., \"1.1\"\n",
      "\n",
      "    # 1. Validate against code_range_mapping\n",
      "    actual_code_range_str = code_range_mapping.get(chapter_key_from_input)\n",
      "    if not actual_code_range_str:\n",
      "        error_msg = (f\"Error: Subchapter index '{chapter_key_from_input}' provided does not exist or is not recognized. \"\n",
      "                     f\"Please ensure you are using a valid subchapter index from the provided list.\")\n",
      "        # print(f\"TOOL_ERROR_DATA (retrieve_icd_code_chapter): Subchapter '{chapter_key_from_input}' not in code_range_mapping.\") # Optional\n",
      "        return error_msg, None\n",
      "\n",
      "    # 2. Validate against coding_groups_data using the mapped range\n",
      "    group_data_for_range = coding_groups_data.get(actual_code_range_str)\n",
      "\n",
      "    codes_list_from_group = group_data_for_range[\"codes\"]\n",
      "\n",
      "    # 3. Retrieve content from sorted_combined_dict_translated_v2\n",
      "    # The data key for sorted_combined_dict_translated_v2 is lowercase\n",
      "    dict_key_for_content = actual_code_range_str.lower()\n",
      "    chapter_content_text = sorted_combined_dict_translated_v2.get(dict_key_for_content)\n",
      "\n",
      "    # All checks passed, prepare successful return\n",
      "    base_codes_in_subchapter = {code[:3] for code in codes_list_from_group}\n",
      "\n",
      "    return chapter_content_text, base_codes_in_subchapter\n",
      "\n",
      "\n",
      "def retrieve_icd_guideline_chapter(guideline_id: str) -> str:\n",
      "    \"\"\"\n",
      "    Retrieves the content of a specific ICD-10-GM coding guideline.\n",
      "\n",
      "    Args:\n",
      "        guideline_id: The guideline identifier, e.g., \"SD0207a\".\n",
      "\n",
      "    Returns:\n",
      "        The text content of the guideline, or an error message.\n",
      "    \"\"\"\n",
      "    if not guideline_id or not isinstance(guideline_id, str):\n",
      "        return \"Error: Invalid or missing 'guideline_id' argument. Please provide a string like 'SD0207a'.\"\n",
      "        \n",
      "    try:\n",
      "        # The load_guideline function is now imported from utils\n",
      "        guideline_content = load_guideline(guideline_id)\n",
      "        return guideline_content\n",
      "    except FileNotFoundError:\n",
      "        return f\"Error: Guideline with ID '{guideline_id}' was not found.\"\n",
      "    except Exception as e:\n",
      "        return f\"An unexpected error occurred in retrieve_icd_guideline_chapter: {str(e)}\"\n",
      "    \n",
      "\n",
      "\n",
      "def parse_assistant_response_with_tool_calls(llm_response: str) -> tuple[str, dict, str]:\n",
      "    \"\"\"\n",
      "    Parses the full LLM response, separating the think block, text content, and tool calls.\n",
      "\n",
      "    Returns a tuple:\n",
      "    1. The original llm_response string (unchanged input).\n",
      "    2. A dictionary formatted as an assistant message for the history (without the think block).\n",
      "       This message contains 'role', 'content', and optionally 'tool_calls'.\n",
      "    3. A string containing the content of the <think> block (or an empty string if no think block).\n",
      "    \n",
      "    Note: The return signature (str, dict, str) matches the implementation provided in the prompt.\n",
      "    \"\"\"\n",
      "    # 1. Extract the <think> block first\n",
      "    think_content = \"\"\n",
      "    content_after_think = llm_response\n",
      "    \n",
      "    think_match = re.search(r\"<think>(.*?)</think>\", llm_response, re.DOTALL)\n",
      "    if think_match:\n",
      "        think_content = think_match.group(1).strip()\n",
      "        content_after_think = llm_response[think_match.end():].lstrip()\n",
      "\n",
      "    # 2. Now, parse the rest of the content (text and tool calls)\n",
      "    tool_calls = []\n",
      "    pattern = re.compile(r\"<tool_call>(.*?)</tool_call>\", re.DOTALL)\n",
      "    matches = list(pattern.finditer(content_after_think))\n",
      "    \n",
      "    if not matches:\n",
      "        # No tool calls, the entire remaining response is content\n",
      "        # text_content = re.sub(r\"<\\|im_end\\|>$\", \"\", content_after_think).strip()\n",
      "        assistant_message = {\"role\": \"assistant\", \"content\": content_after_think}\n",
      "    else:\n",
      "        # <tool_call> blocks are present.\n",
      "        # Text content is what appears *before* the first <tool_call> block.\n",
      "        first_match_start = matches[0].start()\n",
      "        current_text_content = content_after_think[:first_match_start].strip()\n",
      "        \n",
      "        # Iterate through all found <tool_call> blocks\n",
      "        for match in matches:\n",
      "            json_str = match.group(1).strip()\n",
      "            \n",
      "            fallback_call = {\n",
      "                \"name\": \"unknown_function\",\n",
      "                \"arguments\": {\n",
      "                    \"subchapter\": \"unknown_function\" \n",
      "                }\n",
      "            }\n",
      "\n",
      "            try:\n",
      "                # Attempt to parse the JSON string from the tool call block\n",
      "                parsed_json = json.loads(json_str)\n",
      "\n",
      "                # Validate the structure: must be a dict with 'name' (str) and 'arguments' (dict)\n",
      "                if (isinstance(parsed_json, dict) and\n",
      "                        \"name\" in parsed_json and isinstance(parsed_json[\"name\"], str) and\n",
      "                        \"arguments\" in parsed_json and isinstance(parsed_json[\"arguments\"], dict)):\n",
      "                    # Parsed JSON has the expected structure for a tool call\n",
      "                    tool_calls.append(parsed_json)\n",
      "                else:\n",
      "                    # JSON was valid, but its structure doesn't match a tool call's requirements\n",
      "                    # (e.g., not a dictionary, 'name' or 'arguments' missing or wrong type).\n",
      "                    reason = \"Malformed tool call structure (e.g., not a dict, or missing/invalid 'name' or 'arguments' keys/types).\"\n",
      "                    print(f\"Warning: {reason} '\") # Raw content: '{json_str}\n",
      "                    \n",
      "                    # Add diagnostic information to the fallback call's arguments\n",
      "                    fallback_call[\"arguments\"][\"subchapter\"] = \"No Content\"\n",
      "                    tool_calls.append(fallback_call)\n",
      "\n",
      "            except json.JSONDecodeError as e:\n",
      "                # Failed to parse the string as JSON.\n",
      "                reason = f\"JSONDecodeError: {str(e)}\"\n",
      "                print(f\"Warning: Failed to parse JSON from tool call block: {e}. \")  #Raw content: '{json_str}'\n",
      "                \n",
      "                # Add diagnostic information to the fallback call's arguments\n",
      "                fallback_call[\"arguments\"][\"subchapter\"] =  \"No Content\"\n",
      "                tool_calls.append(fallback_call)\n",
      "\n",
      "        # Construct the assistant message with the extracted text content\n",
      "        assistant_message = {\"role\": \"assistant\", \"content\": current_text_content}\n",
      "        # If tool_calls list has items (it will, if 'matches' was not empty), add it to the message.\n",
      "        if tool_calls: \n",
      "            assistant_message[\"tool_calls\"] = tool_calls\n",
      " \n",
      "    return llm_response, assistant_message, think_content\n",
      "\n",
      "\n",
      "\n",
      "#  (Put this near the top of the file once)\n",
      "def _safe_tool_call(name: str, args: dict[str, str]) -> str:\n",
      "    \"\"\"Execute a tool and always return a string for the LLM.\"\"\"\n",
      "    if name == \"retrieve_icd_code_chapter\":\n",
      "        subchapter = args.get(\"subchapter\")\n",
      "        txt, _ = retrieve_icd_code_chapter(subchapter)\n",
      "        return txt\n",
      "    elif name == \"retrieve_icd_guideline_chapter\":\n",
      "        gid = args.get(\"guideline_id\")\n",
      "        return retrieve_icd_guideline_chapter(gid)\n",
      "    else:\n",
      "        return f\"Error: The tool '{name}' is not a valid tool.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "import colorama\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "from realhf.api.core.agent_api import Agent, register_agent\n",
      "from realhf.api.core.data_api import SequenceSample, load_hf_tokenizer\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "from realhf.api.core.model_api import BundledGenerationOutputs\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"Math Code Agent\")\n",
      "\n",
      "\n",
      "class MathMultiTurnAgent(Agent):\n",
      "    \"\"\"A multi-turn reasoning agent for mathematical tasks.\n",
      "\n",
      "    In each turn the agent produces an answer and receives evaluation results from the environment.\n",
      "\n",
      "    By default, we use 4 turns with a token budget=1K at each round.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        gconfig,\n",
      "        tokenizer_path,\n",
      "        reward_scaling=1.0,\n",
      "        reward_bias=0.0,\n",
      "        turn_level_discount: float = 1.0,\n",
      "        num_turns: int = 5,\n",
      "    ):\n",
      "        self.gconfig = gconfig.new(n=1)\n",
      "        self.tokenizer = load_hf_tokenizer(tokenizer_path)\n",
      "\n",
      "        self.reward_scaling = reward_scaling\n",
      "        self.reward_bias = reward_bias\n",
      "        self.turn_level_discount = turn_level_discount\n",
      "\n",
      "        self.num_turns = num_turns\n",
      "\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "        # reset does nothing, just to make it like multi-step environments\n",
      "        await env.reset()\n",
      "\n",
      "        assert prompt.bs == 1\n",
      "        assert self.gconfig.n == 1\n",
      "\n",
      "        prompt_token_ids = prompt.data[\"packed_prompts\"].cpu().numpy().tolist()\n",
      "        qid = prompt.ids[0]\n",
      "        birth_time = int(datetime.now().timestamp() * 1000)\n",
      "\n",
      "        prompt_str = self.tokenizer.batch_decode(\n",
      "            [prompt_token_ids],\n",
      "            clean_up_tokenization_spaces=False,\n",
      "            skip_special_tokens=True,\n",
      "        )[0]\n",
      "\n",
      "        token_ids = prompt_token_ids\n",
      "        all_rewards = []\n",
      "        all_answers = []\n",
      "        all_success = []\n",
      "        x = dict(\n",
      "            keys=[\n",
      "                \"packed_input_ids\",\n",
      "                \"prompt_mask\",\n",
      "                \"packed_logprobs\",\n",
      "                \"seq_no_eos_mask\",\n",
      "                \"packed_prompts\",\n",
      "                \"version_start\",\n",
      "                \"version_end\",\n",
      "                \"rewards\",\n",
      "                \"birth_time\",\n",
      "            ],\n",
      "            ids=[qid],\n",
      "            dtypes=dict(\n",
      "                packed_prompts=torch.long,\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_mask=torch.bool,\n",
      "                seq_no_eos_mask=torch.bool,\n",
      "                version_start=torch.int,\n",
      "                version_end=torch.int,\n",
      "                packed_logprobs=torch.float32,\n",
      "                rewards=torch.float32,\n",
      "                birth_time=torch.long,\n",
      "            ),\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_mask=(),\n",
      "                seq_no_eos_mask=(),\n",
      "                packed_prompts=(),\n",
      "                version_end=(),\n",
      "                version_start=(),\n",
      "                packed_logprobs=(),\n",
      "                rewards=(),\n",
      "                birth_time=(),\n",
      "            ),\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=[[]],\n",
      "                packed_logprobs=[[]],\n",
      "                packed_prompts=[[len(prompt_token_ids)]],\n",
      "                prompt_mask=[[]],\n",
      "                seq_no_eos_mask = [[]],  # seq_no_eos_mask=[[1 for _ in range(self.num_turns)]],\n",
      "                rewards = [[]],          # rewards=[[1 for _ in range(self.num_turns)]],\n",
      "                version_start = [[]],    # version_start=[[1 for _ in range(self.num_turns)]],\n",
      "                version_end = [[]],      # version_end=[[1 for _ in range(self.num_turns)]],\n",
      "                birth_time=[[1]],\n",
      "            ),\n",
      "            data=dict(\n",
      "                packed_prompts=list(prompt_token_ids),\n",
      "                packed_logprobs=[],\n",
      "                packed_input_ids=[],\n",
      "                seq_no_eos_mask=[],\n",
      "                rewards=[],\n",
      "                version_start=[],\n",
      "                version_end=[],\n",
      "                birth_time=torch.tensor([birth_time], dtype=torch.long),\n",
      "                prompt_mask=[],\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        for turn in range(self.num_turns):\n",
      "            await obs_queue.put((qid, token_ids, self.gconfig))\n",
      "\n",
      "            act: BundledGenerationOutputs = await act_queue.get()\n",
      "\n",
      "            seq_strs = self.tokenizer.batch_decode(\n",
      "                act.seqs,\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )\n",
      "            prompt_str = self.tokenizer.batch_decode(\n",
      "                [act.prompt_ids],\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )[0]\n",
      "\n",
      "            answers = [seq_str.split(prompt_str)[1] for seq_str in seq_strs]\n",
      "\n",
      "\n",
      "\n",
      "            #### My Large modification ####\n",
      "            \n",
      "            llm_response, assistant_message_from_parse, original_think_content = parse_assistant_response_with_tool_calls(answers[0])\n",
      "\n",
      "            tool_feedback = \"\"\n",
      "            for call in assistant_message_from_parse.get(\"tool_calls\", []):\n",
      "                tool_name = call[\"name\"]\n",
      "                tool_args = call[\"arguments\"] or {}\n",
      "                print(f\"Executing tool: {tool_name} with args: {tool_args}\")\n",
      "                tool_resp = _safe_tool_call(tool_name, tool_args)\n",
      "                tool_feedback += f\"\\n<tool_response>\\n{tool_resp}\\n</tool_response>\"\n",
      "                logger.debug(f\"Tool Call: {tool_feedback[:1000]}\")\n",
      "\n",
      "                \n",
      "            #### My modification END ####\n",
      "\n",
      "            # single-step env for evaluating generated solutions\n",
      "            if assistant_message_from_parse.get(\"tool_calls\"):\n",
      "                num_generated_sequences = len(answers)\n",
      "                success = [0.0] * num_generated_sequences\n",
      "                rewards = [0.0] * num_generated_sequences\n",
      "                logger.debug(f\"Reward Zero because of Tool Call: {rewards}\")\n",
      "            else:\n",
      "                _, success, *_ = await env.step((qid, answers))\n",
      "                rewards = [\n",
      "                    ((float(r) - 0.5) * 2 - self.reward_bias) * self.reward_scaling\n",
      "                    for r in success\n",
      "                ]\n",
      "                logger.debug(f\"Reward Positive because End of Generation: {rewards}\")\n",
      "\n",
      "            all_success.extend(success)\n",
      "            all_answers.extend(answers)\n",
      "\n",
      "            x[\"data\"][\"packed_input_ids\"].extend(list(act.seqs[0]))\n",
      "            x[\"data\"][\"packed_logprobs\"].extend(list(act.logprobs[0]))\n",
      "            x[\"data\"][\"seq_no_eos_mask\"].append(act.no_eos[0])\n",
      "            all_rewards.append(rewards[0])\n",
      "            x[\"data\"][\"prompt_mask\"].extend(\n",
      "                [1] * act.prompt_len + [0] * (act.seqlens[0] - act.prompt_len)\n",
      "            )\n",
      "\n",
      "            x[\"data\"][\"version_start\"].extend(list(act.version_start))\n",
      "            x[\"data\"][\"version_end\"].extend(list(act.version_end))\n",
      "\n",
      "            x[\"seqlens\"][\"packed_input_ids\"][0].append(act.seqlens[0])\n",
      "            x[\"seqlens\"][\"packed_logprobs\"][0].append(act.seqlens[0] - 1)\n",
      "            x[\"seqlens\"][\"prompt_mask\"][0].append(act.seqlens[0])\n",
      "\n",
      "            x[\"seqlens\"][\"seq_no_eos_mask\"][0].append(1)\n",
      "            x[\"seqlens\"][\"rewards\"][0].append(1)\n",
      "            x[\"seqlens\"][\"version_start\"][0].append(1)\n",
      "            x[\"seqlens\"][\"version_end\"][0].append(1)\n",
      "\n",
      "            token_ids = list(act.seqs[0])\n",
      "\n",
      "            feedback = None\n",
      "\n",
      "            if not assistant_message_from_parse.get(\"tool_calls\") and not success[0]:\n",
      "                break\n",
      "\n",
      "            if success[0]:\n",
      "                break\n",
      "\n",
      "            else:\n",
      "                feedback = tool_feedback\n",
      "\n",
      "            \n",
      "            feedback = \"\\n\" + self.tokenizer.apply_chat_template(\n",
      "                [dict(content=feedback, role=\"user\")],\n",
      "                add_generation_prompt=True,\n",
      "                tokenize=False,\n",
      "            )\n",
      "            \n",
      "            logger.debug(f\"New Feedback: {feedback[:2000]}\")\n",
      "            feedback = self.tokenizer(feedback)[\"input_ids\"]\n",
      "\n",
      "\n",
      "            \n",
      "            token_ids.extend(feedback)\n",
      "            print(f\"LLM Reponse: {llm_response}\")\n",
      "            print(f\"Feedback tokens: {feedback[:2000]}\")\n",
      "            print(f\"Feedback token ids: {len(feedback)}\")\n",
      "            print(f\"Original feedback: {feedback[:2000]}\")\n",
      "            print(f\"Total token length: {len(token_ids)}\")\n",
      "            print(f\"Tokens decoded: {self.tokenizer.decode(token_ids)}\")\n",
      "\n",
      "\n",
      "            max_context = 32768 - self.gconfig.max_new_tokens\n",
      "            if len(token_ids) > max_context:\n",
      "\n",
      "                logger.info(\n",
      "                    f\"Context for next turn ({len(token_ids)} tokens) would exceed \"\n",
      "                    f\"the limit ({max_context}). Terminating trajectory.\"\n",
      "                )\n",
      "                break\n",
      "\n",
      "        self.log_rewards_to_file(\n",
      "            str(qid),\n",
      "            prompt_str,\n",
      "            seqlens=x[\"seqlens\"][\"packed_input_ids\"][0],\n",
      "            answers=all_answers,\n",
      "            prompt_len=len(prompt_token_ids),\n",
      "            rewards=all_rewards,\n",
      "            success=all_success,\n",
      "            version_starts=x[\"data\"][\"version_start\"],\n",
      "            version_ends=x[\"data\"][\"version_end\"],\n",
      "        )\n",
      "\n",
      "        for i in reversed(range(len(all_rewards) - 1)):\n",
      "            all_rewards[i] = (\n",
      "                all_rewards[i] + all_rewards[i + 1] * self.turn_level_discount\n",
      "            )\n",
      "        x[\"data\"][\"rewards\"] = all_rewards\n",
      "\n",
      "        for k in x[\"keys\"]:\n",
      "            if not isinstance(x[\"data\"][k], torch.Tensor):\n",
      "                x[\"data\"][k] = torch.tensor(x[\"data\"][k], dtype=x[\"dtypes\"][k])\n",
      "\n",
      "        x = SequenceSample(**x)\n",
      "\n",
      "        if \"task_ids\" in prompt.keys:\n",
      "            y = SequenceSample(\n",
      "                keys=[\"task_ids\"],\n",
      "                ids=[qid],\n",
      "                dtypes=dict(task_ids=torch.long),\n",
      "                trailing_shapes=dict(task_ids=()),\n",
      "                seqlens=dict(task_ids=[[1]]),\n",
      "                data=dict(task_ids=prompt.data[\"task_ids\"]),\n",
      "            )\n",
      "            x.update_(y)\n",
      "\n",
      "        return [x]\n",
      "\n",
      "    def log_rewards_to_file(\n",
      "        self,\n",
      "        qid: str,\n",
      "        prompt: str,\n",
      "        prompt_len: int,\n",
      "        answers: List[str],\n",
      "        seqlens: List[int],\n",
      "        rewards: List[float],\n",
      "        success: List[bool],\n",
      "        version_starts: List[int],\n",
      "        version_ends: List[int],\n",
      "    ):\n",
      "        group_size = len(answers)\n",
      "\n",
      "        for group_idx in range(group_size):\n",
      "            # NOTE: we can ensure that only one process is logging this query id\n",
      "            gen_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"generated\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.txt\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "\n",
      "            version_start = version_starts[group_idx]\n",
      "            version_end = version_ends[group_idx]\n",
      "            reward = rewards[group_idx]\n",
      "            answer = answers[group_idx]\n",
      "            seqlen = seqlens[group_idx]\n",
      "            with open(gen_file_path, \"a\") as _f:\n",
      "                info = \"\\n\".join(\n",
      "                    [\n",
      "                        f\"idx: {group_idx + 1} / {group_size}, seqlen: {seqlen}, \"\n",
      "                        f\"head version: {version_start}, tail version: {version_end}.\",\n",
      "                        f\"reward is {reward}, prompt is {colorama.Fore.YELLOW + colorama.Style.DIM}{prompt}{colorama.Style.RESET_ALL}\",\n",
      "                        f\"sequence is: {colorama.Fore.YELLOW + colorama.Style.DIM}{answer}{colorama.Style.RESET_ALL}.\",\n",
      "                    ]\n",
      "                )\n",
      "                _f.write(info + \"\\n\")\n",
      "\n",
      "            train_pass_monitor_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"training_monitor\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.jsonl\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(train_pass_monitor_file_path), exist_ok=True)\n",
      "\n",
      "            with open(train_pass_monitor_file_path, \"a\") as monitor_file:\n",
      "                monitor_file.write(\n",
      "                    json.dumps(\n",
      "                        {\n",
      "                            \"version_start\": int(version_start),\n",
      "                            \"version_end\": int(version_end),\n",
      "                            \"success\": bool(success),\n",
      "                            \"prompt_len\": prompt_len,\n",
      "                            \"answer_len\": seqlen - prompt_len,\n",
      "                        },\n",
      "                        ensure_ascii=False,\n",
      "                    )\n",
      "                    + \"\\n\"\n",
      "                )\n",
      "\n",
      "\n",
      "register_agent(\"medical-coding-multi-turn-agent\", MathMultiTurnAgent)\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/null_agent.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# A null agent used for testing\n",
      "import asyncio\n",
      "import copy\n",
      "import random\n",
      "from typing import List\n",
      "\n",
      "from realhf.api.core.agent_api import Agent, register_agent\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "from realhf.api.core.model_api import (\n",
      "    BundledGenerationOutputs,\n",
      "    GenerationHyperparameters,\n",
      ")\n",
      "from realhf.base import logging, testing\n",
      "\n",
      "logger = logging.getLogger(\"Null Agent\")\n",
      "\n",
      "\n",
      "class NullAgent(Agent):\n",
      "    OBS_PUT_CNT = 0\n",
      "    ACT_GET_CNT = 0\n",
      "\n",
      "    def __init__(self, episode_length: int = 1, traj_size: int = 1):\n",
      "        self.episode_len = episode_length\n",
      "        self.traj_size = traj_size\n",
      "\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "\n",
      "        qid = prompt.ids[0]\n",
      "        prompt_token_ids = [\n",
      "            random.randint(0, testing.TESTING_MODEL_VOCAB_SIZE - 1)\n",
      "            for _ in range(random.randint(0, 64))\n",
      "        ]\n",
      "        for step in range(self.episode_len):\n",
      "            await obs_queue.put((qid, prompt_token_ids, GenerationHyperparameters()))\n",
      "            self.OBS_PUT_CNT += 1\n",
      "            act = await act_queue.get()\n",
      "            self.ACT_GET_CNT += 1\n",
      "            assert isinstance(act, BundledGenerationOutputs)\n",
      "\n",
      "        ids = [str(qid) + f\"-{idx}\" for idx in range(self.traj_size)]\n",
      "        traj = [copy.deepcopy(prompt) for _ in range(self.traj_size)]\n",
      "        for t, i in zip(traj, ids):\n",
      "            t.ids[0] = i\n",
      "        return traj\n",
      "\n",
      "\n",
      "register_agent(\"null\", NullAgent)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/math_single_step_agent.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "import colorama\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "from realhf.api.core.agent_api import Agent, register_agent\n",
      "from realhf.api.core.data_api import SequenceSample, load_hf_tokenizer\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "from realhf.api.core.model_api import BundledGenerationOutputs\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"Math Code Agent\")\n",
      "\n",
      "\n",
      "class MathSingleStepAgent(Agent):\n",
      "    def __init__(\n",
      "        self,\n",
      "        gconfig,\n",
      "        tokenizer_path,\n",
      "        success_rate_lb,\n",
      "        success_rate_ub,\n",
      "        reward_scaling=1.0,\n",
      "        reward_bias=0.0,\n",
      "    ):\n",
      "        self.gconfig = gconfig\n",
      "        self.tokenizer = load_hf_tokenizer(tokenizer_path)\n",
      "\n",
      "        self.success_rate_lb = success_rate_lb\n",
      "        self.success_rate_ub = success_rate_ub\n",
      "\n",
      "        self.reward_scaling = reward_scaling\n",
      "        self.reward_bias = reward_bias\n",
      "\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "        # reset does nothing, just to make it like multi-step environments\n",
      "        await env.reset()\n",
      "\n",
      "        assert prompt.bs == 1\n",
      "        prompt_token_ids = prompt.data[\"packed_prompts\"].cpu().numpy().tolist()\n",
      "        qid = prompt.ids[0]\n",
      "        birth_time = int(datetime.now().timestamp() * 1000)\n",
      "        await obs_queue.put((qid, prompt_token_ids, self.gconfig))\n",
      "\n",
      "        act: BundledGenerationOutputs = await act_queue.get()\n",
      "\n",
      "        seq_strs = self.tokenizer.batch_decode(\n",
      "            act.seqs,\n",
      "            clean_up_tokenization_spaces=False,\n",
      "            skip_special_tokens=True,\n",
      "        )\n",
      "        prompt_str = self.tokenizer.batch_decode(\n",
      "            [act.prompt_ids],\n",
      "            clean_up_tokenization_spaces=False,\n",
      "            skip_special_tokens=True,\n",
      "        )[0]\n",
      "\n",
      "        answers = [seq_str.split(prompt_str)[1] for seq_str in seq_strs]\n",
      "\n",
      "        # single-step env\n",
      "        _, success, *_ = await env.step((qid, answers))\n",
      "        rewards = [\n",
      "            ((float(r) - 0.5) * 2 - self.reward_bias) * self.reward_scaling\n",
      "            for r in success\n",
      "        ]\n",
      "\n",
      "        self.log_rewards_to_file(\n",
      "            str(qid),\n",
      "            prompt_str,\n",
      "            seqlens=[len(s) for s in act.seqs],\n",
      "            answers=answers,\n",
      "            prompt_len=len(prompt_token_ids),\n",
      "            rewards=rewards,\n",
      "            success=success,\n",
      "            version_starts=act.version_start,\n",
      "            version_ends=act.version_end,\n",
      "        )\n",
      "\n",
      "        r = np.mean([float(s) for s in success])\n",
      "        if r < self.success_rate_lb:\n",
      "            logger.info(f\"Query ID {qid} reward too low: {r} < {self.success_rate_lb}.\")\n",
      "            return []\n",
      "        if r > self.success_rate_ub:\n",
      "            logger.info(\n",
      "                f\"Query ID {qid} reward too high: {r} > {self.success_rate_ub}.\"\n",
      "            )\n",
      "            return []\n",
      "\n",
      "        x = SequenceSample(\n",
      "            keys=[\n",
      "                \"packed_input_ids\",\n",
      "                \"prompt_mask\",\n",
      "                \"packed_logprobs\",\n",
      "                \"seq_no_eos_mask\",\n",
      "                \"packed_prompts\",\n",
      "                \"version_start\",\n",
      "                \"version_end\",\n",
      "                \"rewards\",\n",
      "                \"birth_time\",\n",
      "            ],\n",
      "            ids=[qid],\n",
      "            dtypes=dict(\n",
      "                packed_prompts=torch.long,\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_mask=torch.bool,\n",
      "                seq_no_eos_mask=torch.bool,\n",
      "                version_start=torch.int,\n",
      "                version_end=torch.int,\n",
      "                packed_logprobs=torch.float32,\n",
      "                rewards=torch.float32,\n",
      "                birth_time=torch.long,\n",
      "            ),\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_mask=(),\n",
      "                seq_no_eos_mask=(),\n",
      "                packed_prompts=(),\n",
      "                version_end=(),\n",
      "                version_start=(),\n",
      "                packed_logprobs=(),\n",
      "                rewards=(),\n",
      "                birth_time=(),\n",
      "            ),\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=[act.seqlens],\n",
      "                packed_logprobs=[[s - 1 for s in act.seqlens]],\n",
      "                packed_prompts=[[act.prompt_len]],\n",
      "                prompt_mask=[act.seqlens],\n",
      "                seq_no_eos_mask=[[1 for _ in range(self.gconfig.n)]],\n",
      "                rewards=[[1 for _ in range(self.gconfig.n)]],\n",
      "                version_start=[[1 for _ in range(self.gconfig.n)]],\n",
      "                version_end=[[1 for _ in range(self.gconfig.n)]],\n",
      "                birth_time=[[1]],\n",
      "            ),\n",
      "            data=dict(\n",
      "                packed_prompts=torch.tensor(act.prompt_ids, dtype=torch.long),\n",
      "                packed_logprobs=torch.tensor(\n",
      "                    sum(act.logprobs, []), dtype=torch.float32\n",
      "                ),\n",
      "                packed_input_ids=torch.tensor(sum(act.seqs, []), dtype=torch.long),\n",
      "                seq_no_eos_mask=torch.tensor(act.no_eos, dtype=torch.bool),\n",
      "                rewards=torch.tensor(rewards, dtype=torch.float32),\n",
      "                version_start=torch.tensor(act.version_start, dtype=torch.int),\n",
      "                version_end=torch.tensor(act.version_end, dtype=torch.int),\n",
      "                birth_time=torch.tensor([birth_time], dtype=torch.long),\n",
      "                prompt_mask=torch.tensor(\n",
      "                    sum(\n",
      "                        [\n",
      "                            [1] * act.prompt_len + [0] * (seqlen - act.prompt_len)\n",
      "                            for seqlen in act.seqlens\n",
      "                        ],\n",
      "                        [],\n",
      "                    ),\n",
      "                    dtype=torch.bool,\n",
      "                ),\n",
      "            ),\n",
      "        )\n",
      "        if \"task_ids\" in prompt.keys:\n",
      "            y = SequenceSample(\n",
      "                keys=[\"task_ids\"],\n",
      "                ids=[qid],\n",
      "                dtypes=dict(task_ids=torch.long),\n",
      "                trailing_shapes=dict(task_ids=()),\n",
      "                seqlens=dict(task_ids=[[1]]),\n",
      "                data=dict(task_ids=prompt.data[\"task_ids\"]),\n",
      "            )\n",
      "            x.update_(y)\n",
      "\n",
      "        return [x]\n",
      "\n",
      "    def log_rewards_to_file(\n",
      "        self,\n",
      "        qid: str,\n",
      "        prompt: str,\n",
      "        prompt_len: int,\n",
      "        answers: List[str],\n",
      "        seqlens: List[int],\n",
      "        rewards: List[float],\n",
      "        success: List[bool],\n",
      "        version_starts: List[int],\n",
      "        version_ends: List[int],\n",
      "    ):\n",
      "        group_size = len(answers)\n",
      "\n",
      "        for group_idx in range(group_size):\n",
      "            # NOTE: we can ensure that only one process is logging this query id\n",
      "            gen_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"generated\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.txt\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "\n",
      "            version_start = version_starts[group_idx]\n",
      "            version_end = version_ends[group_idx]\n",
      "            reward = rewards[group_idx]\n",
      "            answer = answers[group_idx]\n",
      "            seqlen = seqlens[group_idx]\n",
      "            with open(gen_file_path, \"a\") as _f:\n",
      "                info = \"\\n\".join(\n",
      "                    [\n",
      "                        f\"idx: {group_idx + 1} / {group_size}, seqlen: {seqlen}, \"\n",
      "                        f\"head version: {version_start}, tail version: {version_end}.\",\n",
      "                        f\"reward is {reward}, prompt is {colorama.Fore.YELLOW + colorama.Style.DIM}{prompt}{colorama.Style.RESET_ALL}\",\n",
      "                        f\"sequence is: {colorama.Fore.YELLOW + colorama.Style.DIM}{answer}{colorama.Style.RESET_ALL}.\",\n",
      "                    ]\n",
      "                )\n",
      "                _f.write(info + \"\\n\")\n",
      "\n",
      "            train_pass_monitor_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"training_monitor\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.jsonl\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(train_pass_monitor_file_path), exist_ok=True)\n",
      "\n",
      "            with open(train_pass_monitor_file_path, \"a\") as monitor_file:\n",
      "                monitor_file.write(\n",
      "                    json.dumps(\n",
      "                        {\n",
      "                            \"version_start\": int(version_start),\n",
      "                            \"version_end\": int(version_end),\n",
      "                            \"success\": bool(success),\n",
      "                            \"prompt_len\": prompt_len,\n",
      "                            \"answer_len\": seqlen - prompt_len,\n",
      "                        },\n",
      "                        ensure_ascii=False,\n",
      "                    )\n",
      "                    + \"\\n\"\n",
      "                )\n",
      "\n",
      "\n",
      "register_agent(\"math-single-step\", MathSingleStepAgent)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/medical_coding_multi_turn_agent_backup.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import json\n",
      "import re\n",
      "\n",
      "def load_guideline(index: str,\n",
      "                   root_dir: str = '/home/scadmin/SC_project/ai-coding-reasoning/flat_kodierhandbuch_machine') -> str:\n",
      "    \"\"\"\n",
      "    Look for a file named {index}.txt (or starting with {index}) in root_dir,\n",
      "    and return its text contents. Raises FileNotFoundError if nothing matches.\n",
      "    \"\"\"\n",
      "    # 1) try exact match\n",
      "    fname = f\"{index}.txt\"\n",
      "    path = os.path.join(root_dir, fname)\n",
      "    if os.path.isfile(path):\n",
      "        with open(path, encoding='utf-8') as f:\n",
      "            return f.read()\n",
      "    # 2) fallback: any file whose name starts with the index\n",
      "    for fn in os.listdir(root_dir):\n",
      "        if fn.startswith(index) and fn.lower().endswith('.txt'):\n",
      "            with open(os.path.join(root_dir, fn), encoding='utf-8') as f:\n",
      "                return f.read()\n",
      "    raise FileNotFoundError(f\"No guideline file found for index '{index}' in {root_dir}\")\n",
      "\n",
      "\n",
      "\n",
      "try:\n",
      "    with open('/home/scadmin/SC_project/ai-coding-reasoning/reasoning_generate_sft_data/code_range_mapping.json', 'r', encoding='utf-8') as f:\n",
      "        code_range_mapping = json.load(f)\n",
      "except FileNotFoundError:\n",
      "    print(\"Warning: 'code_range_mapping.json' not found. `retrieve_icd_code_chapter` will fail.\")\n",
      "    code_range_mapping = {}\n",
      "\n",
      "try:\n",
      "    # This variable name must match the one used in the function below\n",
      "    with open(\"/home/scadmin/SC_project/ai-coding-reasoning/sorted_combined_dict_translated_v2.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
      "        sorted_combined_dict_translated_v2 = json.load(json_file)\n",
      "except FileNotFoundError:\n",
      "    print(\"Warning: 'sorted_combined_dict_translated_v2.json' not found. `retrieve_icd_code_chapter` will fail.\")\n",
      "    sorted_combined_dict_translated_v2 = {}\n",
      "\n",
      "\n",
      "try:\n",
      "    with open('/home/scadmin/SC_project/ai-coding-reasoning/reasoning_generate_sft_data/coding_documentation/json_export/coding_groups_data.json', 'r', encoding='utf-8') as file:\n",
      "        coding_groups_data = json.load(file)\n",
      "except FileNotFoundError:\n",
      "    print(\"CRITICAL_ERROR_STARTUP: 'coding_groups_data.json' not found. `retrieve_icd_code_chapter` validation will fail.\")\n",
      "    coding_groups_data = {}\n",
      "    \n",
      "\n",
      "def retrieve_icd_code_chapter(subchapter: str) -> tuple[str, set[str] | None]:\n",
      "    \"\"\"\n",
      "    Retrieves ICD-10-GM code details for a given subchapter index and validates data.\n",
      "\n",
      "    Args:\n",
      "        subchapter: The index number, e.g., \"1.1\" or \"19.9\".\n",
      "\n",
      "    Returns:\n",
      "        A tuple:\n",
      "        - On success: (string_content_for_llm, set_of_3_digit_base_codes_in_this_subchapter)\n",
      "        - On failure/data inconsistency: (error_message_string_for_llm, None)\n",
      "    \"\"\"\n",
      "    if not subchapter or not isinstance(subchapter, str):\n",
      "        error_msg = \"Error: Invalid or missing 'subchapter' argument. Please provide a string like '1.1'.\"\n",
      "        # print(f\"TOOL_ERROR (retrieve_icd_code_chapter): {error_msg}\") # Optional: internal server log\n",
      "        return error_msg, None\n",
      "\n",
      "    match = re.search(r\"\\b\\d{1,2}\\.\\d{1,2}\\b\", subchapter)\n",
      "    if not match:\n",
      "        error_msg = f\"Error: Could not find a valid subchapter format (e.g., '1.1') in your input '{subchapter}'. Please use a valid subchapter index.\"\n",
      "        # print(f\"TOOL_ERROR (retrieve_icd_code_chapter): Invalid format for input '{subchapter}'\") # Optional: internal server log\n",
      "        return error_msg, None\n",
      "    \n",
      "    chapter_key_from_input = match.group(0) # e.g., \"1.1\"\n",
      "\n",
      "    # 1. Validate against code_range_mapping\n",
      "    actual_code_range_str = code_range_mapping.get(chapter_key_from_input)\n",
      "    if not actual_code_range_str:\n",
      "        error_msg = (f\"Error: Subchapter index '{chapter_key_from_input}' provided does not exist or is not recognized. \"\n",
      "                     f\"Please ensure you are using a valid subchapter index from the provided list.\")\n",
      "        # print(f\"TOOL_ERROR_DATA (retrieve_icd_code_chapter): Subchapter '{chapter_key_from_input}' not in code_range_mapping.\") # Optional\n",
      "        return error_msg, None\n",
      "\n",
      "    # 2. Validate against coding_groups_data using the mapped range\n",
      "    group_data_for_range = coding_groups_data.get(actual_code_range_str)\n",
      "\n",
      "    codes_list_from_group = group_data_for_range[\"codes\"]\n",
      "\n",
      "    # 3. Retrieve content from sorted_combined_dict_translated_v2\n",
      "    # The data key for sorted_combined_dict_translated_v2 is lowercase\n",
      "    dict_key_for_content = actual_code_range_str.lower()\n",
      "    chapter_content_text = sorted_combined_dict_translated_v2.get(dict_key_for_content)\n",
      "\n",
      "    # All checks passed, prepare successful return\n",
      "    base_codes_in_subchapter = {code[:3] for code in codes_list_from_group}\n",
      "\n",
      "    return chapter_content_text, base_codes_in_subchapter\n",
      "\n",
      "\n",
      "def retrieve_icd_guideline_chapter(guideline_id: str) -> str:\n",
      "    \"\"\"\n",
      "    Retrieves the content of a specific ICD-10-GM coding guideline.\n",
      "\n",
      "    Args:\n",
      "        guideline_id: The guideline identifier, e.g., \"SD0207a\".\n",
      "\n",
      "    Returns:\n",
      "        The text content of the guideline, or an error message.\n",
      "    \"\"\"\n",
      "    if not guideline_id or not isinstance(guideline_id, str):\n",
      "        return \"Error: Invalid or missing 'guideline_id' argument. Please provide a string like 'SD0207a'.\"\n",
      "        \n",
      "    try:\n",
      "        # The load_guideline function is now imported from utils\n",
      "        guideline_content = load_guideline(guideline_id)\n",
      "        return guideline_content\n",
      "    except FileNotFoundError:\n",
      "        return f\"Error: Guideline with ID '{guideline_id}' was not found.\"\n",
      "    except Exception as e:\n",
      "        return f\"An unexpected error occurred in retrieve_icd_guideline_chapter: {str(e)}\"\n",
      "    \n",
      "\n",
      "\n",
      "def parse_assistant_response_with_tool_calls(llm_response: str) -> tuple[str, dict, str]:\n",
      "    \"\"\"\n",
      "    Parses the full LLM response, separating the think block, text content, and tool calls.\n",
      "\n",
      "    Returns a tuple:\n",
      "    1. The original llm_response string (unchanged input).\n",
      "    2. A dictionary formatted as an assistant message for the history (without the think block).\n",
      "       This message contains 'role', 'content', and optionally 'tool_calls'.\n",
      "    3. A string containing the content of the <think> block (or an empty string if no think block).\n",
      "    \n",
      "    Note: The return signature (str, dict, str) matches the implementation provided in the prompt.\n",
      "    \"\"\"\n",
      "    # 1. Extract the <think> block first\n",
      "    think_content = \"\"\n",
      "    content_after_think = llm_response\n",
      "    \n",
      "    think_match = re.search(r\"<think>(.*?)</think>\", llm_response, re.DOTALL)\n",
      "    if think_match:\n",
      "        think_content = think_match.group(1).strip()\n",
      "        content_after_think = llm_response[think_match.end():].lstrip()\n",
      "\n",
      "    # 2. Now, parse the rest of the content (text and tool calls)\n",
      "    tool_calls = []\n",
      "    pattern = re.compile(r\"<tool_call>(.*?)</tool_call>\", re.DOTALL)\n",
      "    matches = list(pattern.finditer(content_after_think))\n",
      "    \n",
      "    if not matches:\n",
      "        # No tool calls, the entire remaining response is content\n",
      "        # text_content = re.sub(r\"<\\|im_end\\|>$\", \"\", content_after_think).strip()\n",
      "        assistant_message = {\"role\": \"assistant\", \"content\": content_after_think}\n",
      "    else:\n",
      "        # <tool_call> blocks are present.\n",
      "        # Text content is what appears *before* the first <tool_call> block.\n",
      "        first_match_start = matches[0].start()\n",
      "        current_text_content = content_after_think[:first_match_start].strip()\n",
      "        \n",
      "        # Iterate through all found <tool_call> blocks\n",
      "        for match in matches:\n",
      "            json_str = match.group(1).strip()\n",
      "            \n",
      "            fallback_call = {\n",
      "                \"name\": \"unknown_function\",\n",
      "                \"arguments\": {\n",
      "                    \"subchapter\": \"unknown_function\" \n",
      "                }\n",
      "            }\n",
      "\n",
      "            try:\n",
      "                # Attempt to parse the JSON string from the tool call block\n",
      "                parsed_json = json.loads(json_str)\n",
      "\n",
      "                # Validate the structure: must be a dict with 'name' (str) and 'arguments' (dict)\n",
      "                if (isinstance(parsed_json, dict) and\n",
      "                        \"name\" in parsed_json and isinstance(parsed_json[\"name\"], str) and\n",
      "                        \"arguments\" in parsed_json and isinstance(parsed_json[\"arguments\"], dict)):\n",
      "                    # Parsed JSON has the expected structure for a tool call\n",
      "                    tool_calls.append(parsed_json)\n",
      "                else:\n",
      "                    # JSON was valid, but its structure doesn't match a tool call's requirements\n",
      "                    # (e.g., not a dictionary, 'name' or 'arguments' missing or wrong type).\n",
      "                    reason = \"Malformed tool call structure (e.g., not a dict, or missing/invalid 'name' or 'arguments' keys/types).\"\n",
      "                    print(f\"Warning: {reason} '\") # Raw content: '{json_str}\n",
      "                    \n",
      "                    # Add diagnostic information to the fallback call's arguments\n",
      "                    fallback_call[\"arguments\"][\"subchapter\"] = \"No Content\"\n",
      "                    tool_calls.append(fallback_call)\n",
      "\n",
      "            except json.JSONDecodeError as e:\n",
      "                # Failed to parse the string as JSON.\n",
      "                reason = f\"JSONDecodeError: {str(e)}\"\n",
      "                print(f\"Warning: Failed to parse JSON from tool call block: {e}. \")  #Raw content: '{json_str}'\n",
      "                \n",
      "                # Add diagnostic information to the fallback call's arguments\n",
      "                fallback_call[\"arguments\"][\"subchapter\"] =  \"No Content\"\n",
      "                tool_calls.append(fallback_call)\n",
      "\n",
      "        # Construct the assistant message with the extracted text content\n",
      "        assistant_message = {\"role\": \"assistant\", \"content\": current_text_content}\n",
      "        # If tool_calls list has items (it will, if 'matches' was not empty), add it to the message.\n",
      "        if tool_calls: \n",
      "            assistant_message[\"tool_calls\"] = tool_calls\n",
      " \n",
      "    return llm_response, assistant_message, think_content\n",
      "\n",
      "\n",
      "\n",
      "#  (Put this near the top of the file once)\n",
      "def _safe_tool_call(name: str, args: dict[str, str]) -> str:\n",
      "    \"\"\"Execute a tool and always return a string for the LLM.\"\"\"\n",
      "    if name == \"retrieve_icd_code_chapter\":\n",
      "        subchapter = args.get(\"subchapter\")\n",
      "        txt, _ = retrieve_icd_code_chapter(subchapter)\n",
      "        return txt\n",
      "    elif name == \"retrieve_icd_guideline_chapter\":\n",
      "        gid = args.get(\"guideline_id\")\n",
      "        return retrieve_icd_guideline_chapter(gid)\n",
      "    else:\n",
      "        return f\"Error: The tool '{name}' is not a valid tool.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "import colorama\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "from realhf.api.core.agent_api import Agent, register_agent\n",
      "from realhf.api.core.data_api import SequenceSample, load_hf_tokenizer\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "from realhf.api.core.model_api import BundledGenerationOutputs\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"Math Code Agent\")\n",
      "\n",
      "\n",
      "class MathMultiTurnAgent(Agent):\n",
      "    \"\"\"A multi-turn reasoning agent for mathematical tasks.\n",
      "\n",
      "    In each turn the agent produces an answer and receives evaluation results from the environment.\n",
      "\n",
      "    By default, we use 4 turns with a token budget=1K at each round.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        gconfig,\n",
      "        tokenizer_path,\n",
      "        reward_scaling=1.0,\n",
      "        reward_bias=0.0,\n",
      "        turn_level_discount: float = 1.0,\n",
      "        num_turns: int = 5,\n",
      "    ):\n",
      "        self.gconfig = gconfig.new(n=1)\n",
      "        self.tokenizer = load_hf_tokenizer(tokenizer_path)\n",
      "\n",
      "        self.reward_scaling = reward_scaling\n",
      "        self.reward_bias = reward_bias\n",
      "        self.turn_level_discount = turn_level_discount\n",
      "\n",
      "        self.num_turns = num_turns\n",
      "\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "        # reset does nothing, just to make it like multi-step environments\n",
      "        await env.reset()\n",
      "\n",
      "        assert prompt.bs == 1\n",
      "        assert self.gconfig.n == 1\n",
      "\n",
      "        prompt_token_ids = prompt.data[\"packed_prompts\"].cpu().numpy().tolist()\n",
      "        qid = prompt.ids[0]\n",
      "        birth_time = int(datetime.now().timestamp() * 1000)\n",
      "\n",
      "        prompt_str = self.tokenizer.batch_decode(\n",
      "            [prompt_token_ids],\n",
      "            clean_up_tokenization_spaces=False,\n",
      "            skip_special_tokens=True,\n",
      "        )[0]\n",
      "\n",
      "        token_ids = prompt_token_ids\n",
      "        all_rewards = []\n",
      "        all_answers = []\n",
      "        all_success = []\n",
      "        x = dict(\n",
      "            keys=[\n",
      "                \"packed_input_ids\",\n",
      "                \"prompt_mask\",\n",
      "                \"packed_logprobs\",\n",
      "                \"seq_no_eos_mask\",\n",
      "                \"packed_prompts\",\n",
      "                \"version_start\",\n",
      "                \"version_end\",\n",
      "                \"rewards\",\n",
      "                \"birth_time\",\n",
      "            ],\n",
      "            ids=[qid],\n",
      "            dtypes=dict(\n",
      "                packed_prompts=torch.long,\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_mask=torch.bool,\n",
      "                seq_no_eos_mask=torch.bool,\n",
      "                version_start=torch.int,\n",
      "                version_end=torch.int,\n",
      "                packed_logprobs=torch.float32,\n",
      "                rewards=torch.float32,\n",
      "                birth_time=torch.long,\n",
      "            ),\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_mask=(),\n",
      "                seq_no_eos_mask=(),\n",
      "                packed_prompts=(),\n",
      "                version_end=(),\n",
      "                version_start=(),\n",
      "                packed_logprobs=(),\n",
      "                rewards=(),\n",
      "                birth_time=(),\n",
      "            ),\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=[[]],\n",
      "                packed_logprobs=[[]],\n",
      "                packed_prompts=[[len(prompt_token_ids)]],\n",
      "                prompt_mask=[[]],\n",
      "                seq_no_eos_mask = [[]],  # seq_no_eos_mask=[[1 for _ in range(self.num_turns)]],\n",
      "                rewards = [[]],          # rewards=[[1 for _ in range(self.num_turns)]],\n",
      "                version_start = [[]],    # version_start=[[1 for _ in range(self.num_turns)]],\n",
      "                version_end = [[]],      # version_end=[[1 for _ in range(self.num_turns)]],\n",
      "                birth_time=[[1]],\n",
      "            ),\n",
      "            data=dict(\n",
      "                packed_prompts=list(prompt_token_ids),\n",
      "                packed_logprobs=[],\n",
      "                packed_input_ids=[],\n",
      "                seq_no_eos_mask=[],\n",
      "                rewards=[],\n",
      "                version_start=[],\n",
      "                version_end=[],\n",
      "                birth_time=torch.tensor([birth_time], dtype=torch.long),\n",
      "                prompt_mask=[],\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        for turn in range(self.num_turns):\n",
      "            await obs_queue.put((qid, token_ids, self.gconfig))\n",
      "\n",
      "            act: BundledGenerationOutputs = await act_queue.get()\n",
      "\n",
      "            seq_strs = self.tokenizer.batch_decode(\n",
      "                act.seqs,\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )\n",
      "            prompt_str = self.tokenizer.batch_decode(\n",
      "                [act.prompt_ids],\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )[0]\n",
      "\n",
      "            answers = [seq_str.split(prompt_str)[1] for seq_str in seq_strs]\n",
      "\n",
      "\n",
      "\n",
      "            #### My Large modification ####\n",
      "            \n",
      "            llm_response, assistant_message_from_parse, original_think_content = parse_assistant_response_with_tool_calls(answers[0])\n",
      "\n",
      "            tool_feedback = \"\"\n",
      "            for call in assistant_message_from_parse.get(\"tool_calls\", []):\n",
      "                tool_name = call[\"name\"]\n",
      "                tool_args = call[\"arguments\"] or {}\n",
      "                print(f\"Executing tool: {tool_name} with args: {tool_args}\")\n",
      "                tool_resp = _safe_tool_call(tool_name, tool_args)\n",
      "                tool_feedback += f\"\\n<tool_response>\\n{tool_resp}\\n</tool_response>\"\n",
      "                logger.debug(f\"Tool Call: {tool_feedback[:1000]}\")\n",
      "\n",
      "                \n",
      "            #### My modification END ####\n",
      "\n",
      "            # single-step env for evaluating generated solutions\n",
      "            if assistant_message_from_parse.get(\"tool_calls\"):\n",
      "                num_generated_sequences = len(answers)\n",
      "                success = [0.0] * num_generated_sequences\n",
      "                rewards = [0.0] * num_generated_sequences\n",
      "                logger.debug(f\"Reward Zero because of Tool Call: {rewards}\")\n",
      "            else:\n",
      "                _, success, *_ = await env.step((qid, answers))\n",
      "                rewards = [\n",
      "                    ((float(r) - 0.5) * 2 - self.reward_bias) * self.reward_scaling\n",
      "                    for r in success\n",
      "                ]\n",
      "                logger.debug(f\"Reward Positive because End of Generation: {rewards}\")\n",
      "\n",
      "            all_success.extend(success)\n",
      "            all_answers.extend(answers)\n",
      "\n",
      "            x[\"data\"][\"packed_input_ids\"].extend(list(act.seqs[0]))\n",
      "            x[\"data\"][\"packed_logprobs\"].extend(list(act.logprobs[0]))\n",
      "            x[\"data\"][\"seq_no_eos_mask\"].append(act.no_eos[0])\n",
      "            all_rewards.append(rewards[0])\n",
      "            x[\"data\"][\"prompt_mask\"].extend(\n",
      "                [1] * act.prompt_len + [0] * (act.seqlens[0] - act.prompt_len)\n",
      "            )\n",
      "\n",
      "            x[\"data\"][\"version_start\"].extend(list(act.version_start))\n",
      "            x[\"data\"][\"version_end\"].extend(list(act.version_end))\n",
      "\n",
      "            x[\"seqlens\"][\"packed_input_ids\"][0].append(act.seqlens[0])\n",
      "            x[\"seqlens\"][\"packed_logprobs\"][0].append(act.seqlens[0] - 1)\n",
      "            x[\"seqlens\"][\"prompt_mask\"][0].append(act.seqlens[0])\n",
      "\n",
      "            x[\"seqlens\"][\"seq_no_eos_mask\"][0].append(1)\n",
      "            x[\"seqlens\"][\"rewards\"][0].append(1)\n",
      "            x[\"seqlens\"][\"version_start\"][0].append(1)\n",
      "            x[\"seqlens\"][\"version_end\"][0].append(1)\n",
      "\n",
      "            token_ids = list(act.seqs[0])\n",
      "\n",
      "            feedback = None\n",
      "\n",
      "            if not assistant_message_from_parse.get(\"tool_calls\") and not success[0]:\n",
      "                break\n",
      "\n",
      "            if success[0]:\n",
      "                # feedback = \"Congratulations! You are correct!\"\n",
      "                break\n",
      "            else:\n",
      "                feedback = tool_feedback\n",
      "\n",
      "            \n",
      "            feedback = \"\\n\" + self.tokenizer.apply_chat_template(\n",
      "                [dict(content=feedback, role=\"user\")],\n",
      "                add_generation_prompt=True,\n",
      "                tokenize=False,\n",
      "            )\n",
      "            \n",
      "            logger.debug(f\"New Feedback: {feedback[:2000]}\")\n",
      "            feedback = self.tokenizer(feedback)[\"input_ids\"]\n",
      "\n",
      "            generation_buffer = self.gconfig.max_new_tokens\n",
      "            max_allowed_len = self.max_context_length - generation_buffer\n",
      "\n",
      "            \n",
      "            token_ids.extend(feedback)\n",
      "            print(f\"LLM Reponse: {llm_response}\")\n",
      "            print(f\"Feedback tokens: {feedback[:2000]}\")\n",
      "            print(f\"Feedback token ids: {len(feedback)}\")\n",
      "            print(f\"Original feedback: {feedback[:2000]}\")\n",
      "            print(f\"Total token length: {len(token_ids)}\")\n",
      "            print(f\"Tokens decoded: {self.tokenizer.decode(token_ids)}\")\n",
      "\n",
      "        self.log_rewards_to_file(\n",
      "            str(qid),\n",
      "            prompt_str,\n",
      "            seqlens=x[\"seqlens\"][\"packed_input_ids\"][0],\n",
      "            answers=all_answers,\n",
      "            prompt_len=len(prompt_token_ids),\n",
      "            rewards=all_rewards,\n",
      "            success=all_success,\n",
      "            version_starts=x[\"data\"][\"version_start\"],\n",
      "            version_ends=x[\"data\"][\"version_end\"],\n",
      "        )\n",
      "\n",
      "        for i in reversed(range(len(all_rewards) - 1)):\n",
      "            all_rewards[i] = (\n",
      "                all_rewards[i] + all_rewards[i + 1] * self.turn_level_discount\n",
      "            )\n",
      "        x[\"data\"][\"rewards\"] = all_rewards\n",
      "\n",
      "        for k in x[\"keys\"]:\n",
      "            if not isinstance(x[\"data\"][k], torch.Tensor):\n",
      "                x[\"data\"][k] = torch.tensor(x[\"data\"][k], dtype=x[\"dtypes\"][k])\n",
      "\n",
      "        x = SequenceSample(**x)\n",
      "\n",
      "        if \"task_ids\" in prompt.keys:\n",
      "            y = SequenceSample(\n",
      "                keys=[\"task_ids\"],\n",
      "                ids=[qid],\n",
      "                dtypes=dict(task_ids=torch.long),\n",
      "                trailing_shapes=dict(task_ids=()),\n",
      "                seqlens=dict(task_ids=[[1]]),\n",
      "                data=dict(task_ids=prompt.data[\"task_ids\"]),\n",
      "            )\n",
      "            x.update_(y)\n",
      "\n",
      "        return [x]\n",
      "\n",
      "    def log_rewards_to_file(\n",
      "        self,\n",
      "        qid: str,\n",
      "        prompt: str,\n",
      "        prompt_len: int,\n",
      "        answers: List[str],\n",
      "        seqlens: List[int],\n",
      "        rewards: List[float],\n",
      "        success: List[bool],\n",
      "        version_starts: List[int],\n",
      "        version_ends: List[int],\n",
      "    ):\n",
      "        group_size = len(answers)\n",
      "\n",
      "        for group_idx in range(group_size):\n",
      "            # NOTE: we can ensure that only one process is logging this query id\n",
      "            gen_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"generated\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.txt\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "\n",
      "            version_start = version_starts[group_idx]\n",
      "            version_end = version_ends[group_idx]\n",
      "            reward = rewards[group_idx]\n",
      "            answer = answers[group_idx]\n",
      "            seqlen = seqlens[group_idx]\n",
      "            with open(gen_file_path, \"a\") as _f:\n",
      "                info = \"\\n\".join(\n",
      "                    [\n",
      "                        f\"idx: {group_idx + 1} / {group_size}, seqlen: {seqlen}, \"\n",
      "                        f\"head version: {version_start}, tail version: {version_end}.\",\n",
      "                        f\"reward is {reward}, prompt is {colorama.Fore.YELLOW + colorama.Style.DIM}{prompt}{colorama.Style.RESET_ALL}\",\n",
      "                        f\"sequence is: {colorama.Fore.YELLOW + colorama.Style.DIM}{answer}{colorama.Style.RESET_ALL}.\",\n",
      "                    ]\n",
      "                )\n",
      "                _f.write(info + \"\\n\")\n",
      "\n",
      "            train_pass_monitor_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"training_monitor\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.jsonl\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(train_pass_monitor_file_path), exist_ok=True)\n",
      "\n",
      "            with open(train_pass_monitor_file_path, \"a\") as monitor_file:\n",
      "                monitor_file.write(\n",
      "                    json.dumps(\n",
      "                        {\n",
      "                            \"version_start\": int(version_start),\n",
      "                            \"version_end\": int(version_end),\n",
      "                            \"success\": bool(success),\n",
      "                            \"prompt_len\": prompt_len,\n",
      "                            \"answer_len\": seqlen - prompt_len,\n",
      "                        },\n",
      "                        ensure_ascii=False,\n",
      "                    )\n",
      "                    + \"\\n\"\n",
      "                )\n",
      "\n",
      "\n",
      "register_agent(\"medical-coding-multi-turn-agent\", MathMultiTurnAgent)\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/medical_coding_multi_turn_agent_truncated_history.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "import json\n",
      "import re\n",
      "\n",
      "def load_guideline(index: str,\n",
      "                   root_dir: str = '/home/scadmin/SC_project/ai-coding-reasoning/flat_kodierhandbuch_machine') -> str:\n",
      "    \"\"\"\n",
      "    Look for a file named {index}.txt (or starting with {index}) in root_dir,\n",
      "    and return its text contents. Raises FileNotFoundError if nothing matches.\n",
      "    \"\"\"\n",
      "    # 1) try exact match\n",
      "    fname = f\"{index}.txt\"\n",
      "    path = os.path.join(root_dir, fname)\n",
      "    if os.path.isfile(path):\n",
      "        with open(path, encoding='utf-8') as f:\n",
      "            return f.read()\n",
      "    # 2) fallback: any file whose name starts with the index\n",
      "    for fn in os.listdir(root_dir):\n",
      "        if fn.startswith(index) and fn.lower().endswith('.txt'):\n",
      "            with open(os.path.join(root_dir, fn), encoding='utf-8') as f:\n",
      "                return f.read()\n",
      "    raise FileNotFoundError(f\"No guideline file found for index '{index}' in {root_dir}\")\n",
      "\n",
      "\n",
      "\n",
      "try:\n",
      "    with open('/home/scadmin/SC_project/ai-coding-reasoning/reasoning_generate_sft_data/code_range_mapping.json', 'r', encoding='utf-8') as f:\n",
      "        code_range_mapping = json.load(f)\n",
      "except FileNotFoundError:\n",
      "    print(\"Warning: 'code_range_mapping.json' not found. `retrieve_icd_code_chapter` will fail.\")\n",
      "    code_range_mapping = {}\n",
      "\n",
      "try:\n",
      "    # This variable name must match the one used in the function below\n",
      "    with open(\"/home/scadmin/SC_project/ai-coding-reasoning/sorted_combined_dict_translated_v2.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
      "        sorted_combined_dict_translated_v2 = json.load(json_file)\n",
      "except FileNotFoundError:\n",
      "    print(\"Warning: 'sorted_combined_dict_translated_v2.json' not found. `retrieve_icd_code_chapter` will fail.\")\n",
      "    sorted_combined_dict_translated_v2 = {}\n",
      "\n",
      "\n",
      "try:\n",
      "    with open('/home/scadmin/SC_project/ai-coding-reasoning/reasoning_generate_sft_data/coding_documentation/json_export/coding_groups_data.json', 'r', encoding='utf-8') as file:\n",
      "        coding_groups_data = json.load(file)\n",
      "except FileNotFoundError:\n",
      "    print(\"CRITICAL_ERROR_STARTUP: 'coding_groups_data.json' not found. `retrieve_icd_code_chapter` validation will fail.\")\n",
      "    coding_groups_data = {}\n",
      "    \n",
      "\n",
      "def retrieve_icd_code_chapter(subchapter: str) -> tuple[str, set[str] | None]:\n",
      "    \"\"\"\n",
      "    Retrieves ICD-10-GM code details for a given subchapter index and validates data.\n",
      "\n",
      "    Args:\n",
      "        subchapter: The index number, e.g., \"1.1\" or \"19.9\".\n",
      "\n",
      "    Returns:\n",
      "        A tuple:\n",
      "        - On success: (string_content_for_llm, set_of_3_digit_base_codes_in_this_subchapter)\n",
      "        - On failure/data inconsistency: (error_message_string_for_llm, None)\n",
      "    \"\"\"\n",
      "    if not subchapter or not isinstance(subchapter, str):\n",
      "        error_msg = \"Error: Invalid or missing 'subchapter' argument. Please provide a string like '1.1'.\"\n",
      "        # print(f\"TOOL_ERROR (retrieve_icd_code_chapter): {error_msg}\") # Optional: internal server log\n",
      "        return error_msg, None\n",
      "\n",
      "    match = re.search(r\"\\b\\d{1,2}\\.\\d{1,2}\\b\", subchapter)\n",
      "    if not match:\n",
      "        error_msg = f\"Error: Could not find a valid subchapter format (e.g., '1.1') in your input '{subchapter}'. Please use a valid subchapter index.\"\n",
      "        # print(f\"TOOL_ERROR (retrieve_icd_code_chapter): Invalid format for input '{subchapter}'\") # Optional: internal server log\n",
      "        return error_msg, None\n",
      "    \n",
      "    chapter_key_from_input = match.group(0) # e.g., \"1.1\"\n",
      "\n",
      "    # 1. Validate against code_range_mapping\n",
      "    actual_code_range_str = code_range_mapping.get(chapter_key_from_input)\n",
      "    if not actual_code_range_str:\n",
      "        error_msg = (f\"Error: Subchapter index '{chapter_key_from_input}' provided does not exist or is not recognized. \"\n",
      "                     f\"Please ensure you are using a valid subchapter index from the provided list.\")\n",
      "        # print(f\"TOOL_ERROR_DATA (retrieve_icd_code_chapter): Subchapter '{chapter_key_from_input}' not in code_range_mapping.\") # Optional\n",
      "        return error_msg, None\n",
      "\n",
      "    # 2. Validate against coding_groups_data using the mapped range\n",
      "    group_data_for_range = coding_groups_data.get(actual_code_range_str)\n",
      "\n",
      "    codes_list_from_group = group_data_for_range[\"codes\"]\n",
      "\n",
      "    # 3. Retrieve content from sorted_combined_dict_translated_v2\n",
      "    # The data key for sorted_combined_dict_translated_v2 is lowercase\n",
      "    dict_key_for_content = actual_code_range_str.lower()\n",
      "    chapter_content_text = sorted_combined_dict_translated_v2.get(dict_key_for_content)\n",
      "\n",
      "    # All checks passed, prepare successful return\n",
      "    base_codes_in_subchapter = {code[:3] for code in codes_list_from_group}\n",
      "\n",
      "    return chapter_content_text, base_codes_in_subchapter\n",
      "\n",
      "\n",
      "def retrieve_icd_guideline_chapter(guideline_id: str) -> str:\n",
      "    \"\"\"\n",
      "    Retrieves the content of a specific ICD-10-GM coding guideline.\n",
      "\n",
      "    Args:\n",
      "        guideline_id: The guideline identifier, e.g., \"SD0207a\".\n",
      "\n",
      "    Returns:\n",
      "        The text content of the guideline, or an error message.\n",
      "    \"\"\"\n",
      "    if not guideline_id or not isinstance(guideline_id, str):\n",
      "        return \"Error: Invalid or missing 'guideline_id' argument. Please provide a string like 'SD0207a'.\"\n",
      "        \n",
      "    try:\n",
      "        # The load_guideline function is now imported from utils\n",
      "        guideline_content = load_guideline(guideline_id)\n",
      "        return guideline_content\n",
      "    except FileNotFoundError:\n",
      "        return f\"Error: Guideline with ID '{guideline_id}' was not found.\"\n",
      "    except Exception as e:\n",
      "        return f\"An unexpected error occurred in retrieve_icd_guideline_chapter: {str(e)}\"\n",
      "    \n",
      "\n",
      "\n",
      "def parse_assistant_response_with_tool_calls(llm_response: str) -> tuple[str, dict, str]:\n",
      "    \"\"\"\n",
      "    Parses the full LLM response, separating the think block, text content, and tool calls.\n",
      "\n",
      "    Returns a tuple:\n",
      "    1. The original llm_response string (unchanged input).\n",
      "    2. A dictionary formatted as an assistant message for the history (without the think block).\n",
      "       This message contains 'role', 'content', and optionally 'tool_calls'.\n",
      "    3. A string containing the content of the <think> block (or an empty string if no think block).\n",
      "    \n",
      "    Note: The return signature (str, dict, str) matches the implementation provided in the prompt.\n",
      "    \"\"\"\n",
      "    # 1. Extract the <think> block first\n",
      "    think_content = \"\"\n",
      "    content_after_think = llm_response\n",
      "    \n",
      "    think_match = re.search(r\"<think>(.*?)</think>\", llm_response, re.DOTALL)\n",
      "    if think_match:\n",
      "        think_content = think_match.group(1).strip()\n",
      "        content_after_think = llm_response[think_match.end():].lstrip()\n",
      "\n",
      "    # 2. Now, parse the rest of the content (text and tool calls)\n",
      "    tool_calls = []\n",
      "    pattern = re.compile(r\"<tool_call>(.*?)</tool_call>\", re.DOTALL)\n",
      "    matches = list(pattern.finditer(content_after_think))\n",
      "    \n",
      "    if not matches:\n",
      "        # No tool calls, the entire remaining response is content\n",
      "        # text_content = re.sub(r\"<\\|im_end\\|>$\", \"\", content_after_think).strip()\n",
      "        assistant_message = {\"role\": \"assistant\", \"content\": content_after_think}\n",
      "    else:\n",
      "        # <tool_call> blocks are present.\n",
      "        # Text content is what appears *before* the first <tool_call> block.\n",
      "        first_match_start = matches[0].start()\n",
      "        current_text_content = content_after_think[:first_match_start].strip()\n",
      "        \n",
      "        # Iterate through all found <tool_call> blocks\n",
      "        for match in matches:\n",
      "            json_str = match.group(1).strip()\n",
      "            \n",
      "            fallback_call = {\n",
      "                \"name\": \"unknown_function\",\n",
      "                \"arguments\": {\n",
      "                    \"subchapter\": \"unknown_function\" \n",
      "                }\n",
      "            }\n",
      "\n",
      "            try:\n",
      "                # Attempt to parse the JSON string from the tool call block\n",
      "                parsed_json = json.loads(json_str)\n",
      "\n",
      "                # Validate the structure: must be a dict with 'name' (str) and 'arguments' (dict)\n",
      "                if (isinstance(parsed_json, dict) and\n",
      "                        \"name\" in parsed_json and isinstance(parsed_json[\"name\"], str) and\n",
      "                        \"arguments\" in parsed_json and isinstance(parsed_json[\"arguments\"], dict)):\n",
      "                    # Parsed JSON has the expected structure for a tool call\n",
      "                    tool_calls.append(parsed_json)\n",
      "                else:\n",
      "                    # JSON was valid, but its structure doesn't match a tool call's requirements\n",
      "                    # (e.g., not a dictionary, 'name' or 'arguments' missing or wrong type).\n",
      "                    reason = \"Malformed tool call structure (e.g., not a dict, or missing/invalid 'name' or 'arguments' keys/types).\"\n",
      "                    print(f\"Warning: {reason} '\") # Raw content: '{json_str}\n",
      "                    \n",
      "                    # Add diagnostic information to the fallback call's arguments\n",
      "                    fallback_call[\"arguments\"][\"subchapter\"] = \"No Content\"\n",
      "                    tool_calls.append(fallback_call)\n",
      "\n",
      "            except json.JSONDecodeError as e:\n",
      "                # Failed to parse the string as JSON.\n",
      "                reason = f\"JSONDecodeError: {str(e)}\"\n",
      "                print(f\"Warning: Failed to parse JSON from tool call block: {e}. \")  #Raw content: '{json_str}'\n",
      "                \n",
      "                # Add diagnostic information to the fallback call's arguments\n",
      "                fallback_call[\"arguments\"][\"subchapter\"] =  \"No Content\"\n",
      "                tool_calls.append(fallback_call)\n",
      "\n",
      "        # Construct the assistant message with the extracted text content\n",
      "        assistant_message = {\"role\": \"assistant\", \"content\": current_text_content}\n",
      "        # If tool_calls list has items (it will, if 'matches' was not empty), add it to the message.\n",
      "        if tool_calls: \n",
      "            assistant_message[\"tool_calls\"] = tool_calls\n",
      " \n",
      "    return llm_response, assistant_message, think_content\n",
      "\n",
      "\n",
      "\n",
      "#  (Put this near the top of the file once)\n",
      "def _safe_tool_call(name: str, args: dict[str, str]) -> str:\n",
      "    \"\"\"Execute a tool and always return a string for the LLM.\"\"\"\n",
      "    if name == \"retrieve_icd_code_chapter\":\n",
      "        subchapter = args.get(\"subchapter\")\n",
      "        txt, _ = retrieve_icd_code_chapter(subchapter)\n",
      "        return txt\n",
      "    elif name == \"retrieve_icd_guideline_chapter\":\n",
      "        gid = args.get(\"guideline_id\")\n",
      "        return retrieve_icd_guideline_chapter(gid)\n",
      "    else:\n",
      "        return f\"Error: The tool '{name}' is not a valid tool.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "import colorama\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "from realhf.api.core.agent_api import Agent, register_agent\n",
      "from realhf.api.core.data_api import SequenceSample, load_hf_tokenizer\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "from realhf.api.core.model_api import BundledGenerationOutputs\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"Math Code Agent\")\n",
      "\n",
      "\n",
      "class MathMultiTurnAgent(Agent):\n",
      "    \"\"\"A multi-turn reasoning agent for mathematical tasks.\n",
      "\n",
      "    In each turn the agent produces an answer and receives evaluation results from the environment.\n",
      "\n",
      "    By default, we use 4 turns with a token budget=1K at each round.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        gconfig,\n",
      "        tokenizer_path,\n",
      "        reward_scaling=1.0,\n",
      "        reward_bias=0.0,\n",
      "        turn_level_discount: float = 1.0,\n",
      "        num_turns: int = 5,\n",
      "    ):\n",
      "        self.gconfig = gconfig.new(n=1)\n",
      "        self.tokenizer = load_hf_tokenizer(tokenizer_path)\n",
      "\n",
      "        self.reward_scaling = reward_scaling\n",
      "        self.reward_bias = reward_bias\n",
      "        self.turn_level_discount = turn_level_discount\n",
      "\n",
      "        self.num_turns = num_turns\n",
      "\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "        # reset does nothing, just to make it like multi-step environments\n",
      "        await env.reset()\n",
      "\n",
      "        assert prompt.bs == 1\n",
      "        assert self.gconfig.n == 1\n",
      "\n",
      "        prompt_token_ids = prompt.data[\"packed_prompts\"].cpu().numpy().tolist()\n",
      "        qid = prompt.ids[0]\n",
      "        birth_time = int(datetime.now().timestamp() * 1000)\n",
      "\n",
      "        prompt_str = self.tokenizer.batch_decode(\n",
      "            [prompt_token_ids],\n",
      "            clean_up_tokenization_spaces=False,\n",
      "            skip_special_tokens=True,\n",
      "        )[0]\n",
      "\n",
      "        token_ids = prompt_token_ids\n",
      "        all_rewards = []\n",
      "        all_answers = []\n",
      "        all_success = []\n",
      "        x = dict(\n",
      "            keys=[\n",
      "                \"packed_input_ids\",\n",
      "                \"prompt_mask\",\n",
      "                \"packed_logprobs\",\n",
      "                \"seq_no_eos_mask\",\n",
      "                \"packed_prompts\",\n",
      "                \"version_start\",\n",
      "                \"version_end\",\n",
      "                \"rewards\",\n",
      "                \"birth_time\",\n",
      "            ],\n",
      "            ids=[qid],\n",
      "            dtypes=dict(\n",
      "                packed_prompts=torch.long,\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_mask=torch.bool,\n",
      "                seq_no_eos_mask=torch.bool,\n",
      "                version_start=torch.int,\n",
      "                version_end=torch.int,\n",
      "                packed_logprobs=torch.float32,\n",
      "                rewards=torch.float32,\n",
      "                birth_time=torch.long,\n",
      "            ),\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_mask=(),\n",
      "                seq_no_eos_mask=(),\n",
      "                packed_prompts=(),\n",
      "                version_end=(),\n",
      "                version_start=(),\n",
      "                packed_logprobs=(),\n",
      "                rewards=(),\n",
      "                birth_time=(),\n",
      "            ),\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=[[]],\n",
      "                packed_logprobs=[[]],\n",
      "                packed_prompts=[[len(prompt_token_ids)]],\n",
      "                prompt_mask=[[]],\n",
      "                seq_no_eos_mask = [[]],  # seq_no_eos_mask=[[1 for _ in range(self.num_turns)]],\n",
      "                rewards = [[]],          # rewards=[[1 for _ in range(self.num_turns)]],\n",
      "                version_start = [[]],    # version_start=[[1 for _ in range(self.num_turns)]],\n",
      "                version_end = [[]],      # version_end=[[1 for _ in range(self.num_turns)]],\n",
      "                birth_time=[[1]],\n",
      "            ),\n",
      "            data=dict(\n",
      "                packed_prompts=list(prompt_token_ids),\n",
      "                packed_logprobs=[],\n",
      "                packed_input_ids=[],\n",
      "                seq_no_eos_mask=[],\n",
      "                rewards=[],\n",
      "                version_start=[],\n",
      "                version_end=[],\n",
      "                birth_time=torch.tensor([birth_time], dtype=torch.long),\n",
      "                prompt_mask=[],\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        for turn in range(self.num_turns):\n",
      "            await obs_queue.put((qid, token_ids, self.gconfig))\n",
      "\n",
      "            act: BundledGenerationOutputs = await act_queue.get()\n",
      "\n",
      "            seq_strs = self.tokenizer.batch_decode(\n",
      "                act.seqs,\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )\n",
      "            prompt_str = self.tokenizer.batch_decode(\n",
      "                [act.prompt_ids],\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )[0]\n",
      "\n",
      "            answers = [seq_str.split(prompt_str)[1] for seq_str in seq_strs]\n",
      "\n",
      "\n",
      "\n",
      "            #### My Large modification ####\n",
      "            \n",
      "            llm_response, assistant_message_from_parse, original_think_content = parse_assistant_response_with_tool_calls(answers[0])\n",
      "\n",
      "            tool_feedback = \"\"\n",
      "            for call in assistant_message_from_parse.get(\"tool_calls\", []):\n",
      "                tool_name = call[\"name\"]\n",
      "                tool_args = call[\"arguments\"] or {}\n",
      "                print(f\"Executing tool: {tool_name} with args: {tool_args}\")\n",
      "                tool_resp = _safe_tool_call(tool_name, tool_args)\n",
      "                tool_feedback += f\"\\n<tool_response>\\n{tool_resp}\\n</tool_response>\"\n",
      "                logger.debug(f\"Tool Call: {tool_feedback[:1000]}\")\n",
      "\n",
      "                \n",
      "            #### My modification END ####\n",
      "\n",
      "            # single-step env for evaluating generated solutions\n",
      "            if assistant_message_from_parse.get(\"tool_calls\"):\n",
      "                num_generated_sequences = len(answers)\n",
      "                success = [0.0] * num_generated_sequences\n",
      "                rewards = [0.0] * num_generated_sequences\n",
      "                logger.debug(f\"Reward Zero because of Tool Call: {rewards}\")\n",
      "            else:\n",
      "                _, success, *_ = await env.step((qid, answers))\n",
      "                rewards = [\n",
      "                    ((float(r) - 0.5) * 2 - self.reward_bias) * self.reward_scaling\n",
      "                    for r in success\n",
      "                ]\n",
      "                logger.debug(f\"Reward Positive because End of Generation: {rewards}\")\n",
      "\n",
      "            all_success.extend(success)\n",
      "            all_answers.extend(answers)\n",
      "\n",
      "            x[\"data\"][\"packed_input_ids\"].extend(list(act.seqs[0]))\n",
      "            x[\"data\"][\"packed_logprobs\"].extend(list(act.logprobs[0]))\n",
      "            x[\"data\"][\"seq_no_eos_mask\"].append(act.no_eos[0])\n",
      "            all_rewards.append(rewards[0])\n",
      "            x[\"data\"][\"prompt_mask\"].extend(\n",
      "                [1] * act.prompt_len + [0] * (act.seqlens[0] - act.prompt_len)\n",
      "            )\n",
      "\n",
      "            x[\"data\"][\"version_start\"].extend(list(act.version_start))\n",
      "            x[\"data\"][\"version_end\"].extend(list(act.version_end))\n",
      "\n",
      "            x[\"seqlens\"][\"packed_input_ids\"][0].append(act.seqlens[0])\n",
      "            x[\"seqlens\"][\"packed_logprobs\"][0].append(act.seqlens[0] - 1)\n",
      "            x[\"seqlens\"][\"prompt_mask\"][0].append(act.seqlens[0])\n",
      "\n",
      "            x[\"seqlens\"][\"seq_no_eos_mask\"][0].append(1)\n",
      "            x[\"seqlens\"][\"rewards\"][0].append(1)\n",
      "            x[\"seqlens\"][\"version_start\"][0].append(1)\n",
      "            x[\"seqlens\"][\"version_end\"][0].append(1)\n",
      "\n",
      "            token_ids = list(act.seqs[0])\n",
      "\n",
      "            feedback = None\n",
      "\n",
      "            if not assistant_message_from_parse.get(\"tool_calls\") and not success[0]:\n",
      "                break\n",
      "\n",
      "            if success[0]:\n",
      "                break\n",
      "\n",
      "            else:\n",
      "                feedback = tool_feedback\n",
      "\n",
      "            \n",
      "            feedback = \"\\n\" + self.tokenizer.apply_chat_template(\n",
      "                [dict(content=feedback, role=\"user\")],\n",
      "                add_generation_prompt=True,\n",
      "                tokenize=False,\n",
      "            )\n",
      "            \n",
      "            logger.debug(f\"New Feedback: {feedback[:2000]}\")\n",
      "            feedback = self.tokenizer(feedback)[\"input_ids\"]\n",
      "\n",
      "\n",
      "            \n",
      "            token_ids.extend(feedback)\n",
      "            print(f\"LLM Reponse: {llm_response}\")\n",
      "            print(f\"Feedback tokens: {feedback[:2000]}\")\n",
      "            print(f\"Feedback token ids: {len(feedback)}\")\n",
      "            print(f\"Original feedback: {feedback[:2000]}\")\n",
      "            print(f\"Total token length: {len(token_ids)}\")\n",
      "            print(f\"Tokens decoded: {self.tokenizer.decode(token_ids)}\")\n",
      "\n",
      "\n",
      "            max_context = 32768 - self.gconfig.max_new_tokens\n",
      "            if len(token_ids) > max_context:\n",
      "\n",
      "                logger.info(\n",
      "                    f\"Context for next turn ({len(token_ids)} tokens) would exceed \"\n",
      "                    f\"the limit ({max_context}). Terminating trajectory.\"\n",
      "                )\n",
      "                break\n",
      "\n",
      "        self.log_rewards_to_file(\n",
      "            str(qid),\n",
      "            prompt_str,\n",
      "            seqlens=x[\"seqlens\"][\"packed_input_ids\"][0],\n",
      "            answers=all_answers,\n",
      "            prompt_len=len(prompt_token_ids),\n",
      "            rewards=all_rewards,\n",
      "            success=all_success,\n",
      "            version_starts=x[\"data\"][\"version_start\"],\n",
      "            version_ends=x[\"data\"][\"version_end\"],\n",
      "        )\n",
      "\n",
      "        for i in reversed(range(len(all_rewards) - 1)):\n",
      "            all_rewards[i] = (\n",
      "                all_rewards[i] + all_rewards[i + 1] * self.turn_level_discount\n",
      "            )\n",
      "        x[\"data\"][\"rewards\"] = all_rewards\n",
      "\n",
      "        for k in x[\"keys\"]:\n",
      "            if not isinstance(x[\"data\"][k], torch.Tensor):\n",
      "                x[\"data\"][k] = torch.tensor(x[\"data\"][k], dtype=x[\"dtypes\"][k])\n",
      "\n",
      "        x = SequenceSample(**x)\n",
      "\n",
      "        if \"task_ids\" in prompt.keys:\n",
      "            y = SequenceSample(\n",
      "                keys=[\"task_ids\"],\n",
      "                ids=[qid],\n",
      "                dtypes=dict(task_ids=torch.long),\n",
      "                trailing_shapes=dict(task_ids=()),\n",
      "                seqlens=dict(task_ids=[[1]]),\n",
      "                data=dict(task_ids=prompt.data[\"task_ids\"]),\n",
      "            )\n",
      "            x.update_(y)\n",
      "\n",
      "        return [x]\n",
      "\n",
      "    def log_rewards_to_file(\n",
      "        self,\n",
      "        qid: str,\n",
      "        prompt: str,\n",
      "        prompt_len: int,\n",
      "        answers: List[str],\n",
      "        seqlens: List[int],\n",
      "        rewards: List[float],\n",
      "        success: List[bool],\n",
      "        version_starts: List[int],\n",
      "        version_ends: List[int],\n",
      "    ):\n",
      "        group_size = len(answers)\n",
      "\n",
      "        for group_idx in range(group_size):\n",
      "            # NOTE: we can ensure that only one process is logging this query id\n",
      "            gen_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"generated\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.txt\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "\n",
      "            version_start = version_starts[group_idx]\n",
      "            version_end = version_ends[group_idx]\n",
      "            reward = rewards[group_idx]\n",
      "            answer = answers[group_idx]\n",
      "            seqlen = seqlens[group_idx]\n",
      "            with open(gen_file_path, \"a\") as _f:\n",
      "                info = \"\\n\".join(\n",
      "                    [\n",
      "                        f\"idx: {group_idx + 1} / {group_size}, seqlen: {seqlen}, \"\n",
      "                        f\"head version: {version_start}, tail version: {version_end}.\",\n",
      "                        f\"reward is {reward}, prompt is {colorama.Fore.YELLOW + colorama.Style.DIM}{prompt}{colorama.Style.RESET_ALL}\",\n",
      "                        f\"sequence is: {colorama.Fore.YELLOW + colorama.Style.DIM}{answer}{colorama.Style.RESET_ALL}.\",\n",
      "                    ]\n",
      "                )\n",
      "                _f.write(info + \"\\n\")\n",
      "\n",
      "            train_pass_monitor_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"training_monitor\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.jsonl\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(train_pass_monitor_file_path), exist_ok=True)\n",
      "\n",
      "            with open(train_pass_monitor_file_path, \"a\") as monitor_file:\n",
      "                monitor_file.write(\n",
      "                    json.dumps(\n",
      "                        {\n",
      "                            \"version_start\": int(version_start),\n",
      "                            \"version_end\": int(version_end),\n",
      "                            \"success\": bool(success),\n",
      "                            \"prompt_len\": prompt_len,\n",
      "                            \"answer_len\": seqlen - prompt_len,\n",
      "                        },\n",
      "                        ensure_ascii=False,\n",
      "                    )\n",
      "                    + \"\\n\"\n",
      "                )\n",
      "\n",
      "\n",
      "register_agent(\"medical-coding-multi-turn-agent\", MathMultiTurnAgent)\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/__init__.py ====\n",
      "\n",
      "import realhf.impl.agent.math_multi_turn_agent\n",
      "import realhf.impl.agent.math_single_step_agent\n",
      "import realhf.impl.agent.null_agent\n",
      "import realhf.impl.agent.medical_coding_multi_turn_agent\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/agent/math_multi_turn_agent.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "import colorama\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "from realhf.api.core.agent_api import Agent, register_agent\n",
      "from realhf.api.core.data_api import SequenceSample, load_hf_tokenizer\n",
      "from realhf.api.core.env_api import EnvironmentService\n",
      "from realhf.api.core.model_api import BundledGenerationOutputs\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"Math Code Agent\")\n",
      "\n",
      "\n",
      "class MathMultiTurnAgent(Agent):\n",
      "    \"\"\"A multi-turn reasoning agent for mathematical tasks.\n",
      "\n",
      "    In each turn the agent produces an answer and receives evaluation results from the environment.\n",
      "\n",
      "    By default, we use 4 turns with a token budget=1K at each round.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        gconfig,\n",
      "        tokenizer_path,\n",
      "        reward_scaling=1.0,\n",
      "        reward_bias=0.0,\n",
      "        turn_level_discount: float = 1.0,\n",
      "        num_turns: int = 5,\n",
      "    ):\n",
      "        self.gconfig = gconfig.new(n=1)\n",
      "        self.tokenizer = load_hf_tokenizer(tokenizer_path)\n",
      "\n",
      "        self.reward_scaling = reward_scaling\n",
      "        self.reward_bias = reward_bias\n",
      "        self.turn_level_discount = turn_level_discount\n",
      "\n",
      "        self.num_turns = num_turns\n",
      "\n",
      "    async def collect_trajectory(\n",
      "        self,\n",
      "        prompt: SequenceSample,\n",
      "        env: EnvironmentService,\n",
      "        obs_queue: asyncio.Queue,\n",
      "        act_queue: asyncio.Queue,\n",
      "    ) -> List[SequenceSample]:\n",
      "        # reset does nothing, just to make it like multi-step environments\n",
      "        await env.reset()\n",
      "\n",
      "        assert prompt.bs == 1\n",
      "        assert self.gconfig.n == 1\n",
      "\n",
      "        prompt_token_ids = prompt.data[\"packed_prompts\"].cpu().numpy().tolist()\n",
      "        qid = prompt.ids[0]\n",
      "        birth_time = int(datetime.now().timestamp() * 1000)\n",
      "\n",
      "        prompt_str = self.tokenizer.batch_decode(\n",
      "            [prompt_token_ids],\n",
      "            clean_up_tokenization_spaces=False,\n",
      "            skip_special_tokens=True,\n",
      "        )[0]\n",
      "\n",
      "        token_ids = prompt_token_ids\n",
      "        all_rewards = []\n",
      "        all_answers = []\n",
      "        all_success = []\n",
      "        x = dict(\n",
      "            keys=[\n",
      "                \"packed_input_ids\",\n",
      "                \"prompt_mask\",\n",
      "                \"packed_logprobs\",\n",
      "                \"seq_no_eos_mask\",\n",
      "                \"packed_prompts\",\n",
      "                \"version_start\",\n",
      "                \"version_end\",\n",
      "                \"rewards\",\n",
      "                \"birth_time\",\n",
      "            ],\n",
      "            ids=[qid],\n",
      "            dtypes=dict(\n",
      "                packed_prompts=torch.long,\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_mask=torch.bool,\n",
      "                seq_no_eos_mask=torch.bool,\n",
      "                version_start=torch.int,\n",
      "                version_end=torch.int,\n",
      "                packed_logprobs=torch.float32,\n",
      "                rewards=torch.float32,\n",
      "                birth_time=torch.long,\n",
      "            ),\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_mask=(),\n",
      "                seq_no_eos_mask=(),\n",
      "                packed_prompts=(),\n",
      "                version_end=(),\n",
      "                version_start=(),\n",
      "                packed_logprobs=(),\n",
      "                rewards=(),\n",
      "                birth_time=(),\n",
      "            ),\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=[[]],\n",
      "                packed_logprobs=[[]],\n",
      "                packed_prompts=[[len(prompt_token_ids)]],\n",
      "                prompt_mask=[[]],\n",
      "                seq_no_eos_mask=[[1 for _ in range(self.num_turns)]],\n",
      "                rewards=[[1 for _ in range(self.num_turns)]],\n",
      "                version_start=[[1 for _ in range(self.num_turns)]],\n",
      "                version_end=[[1 for _ in range(self.num_turns)]],\n",
      "                birth_time=[[1]],\n",
      "            ),\n",
      "            data=dict(\n",
      "                packed_prompts=list(prompt_token_ids),\n",
      "                packed_logprobs=[],\n",
      "                packed_input_ids=[],\n",
      "                seq_no_eos_mask=[],\n",
      "                rewards=[],\n",
      "                version_start=[],\n",
      "                version_end=[],\n",
      "                birth_time=torch.tensor([birth_time], dtype=torch.long),\n",
      "                prompt_mask=[],\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        for turn in range(self.num_turns):\n",
      "            await obs_queue.put((qid, token_ids, self.gconfig))\n",
      "\n",
      "            act: BundledGenerationOutputs = await act_queue.get()\n",
      "\n",
      "            seq_strs = self.tokenizer.batch_decode(\n",
      "                act.seqs,\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )\n",
      "            prompt_str = self.tokenizer.batch_decode(\n",
      "                [act.prompt_ids],\n",
      "                clean_up_tokenization_spaces=False,\n",
      "                skip_special_tokens=True,\n",
      "            )[0]\n",
      "\n",
      "            answers = [seq_str.split(prompt_str)[1] for seq_str in seq_strs]\n",
      "\n",
      "            # single-step env for evaluating generated solutions\n",
      "            _, success, *_ = await env.step((qid, answers))\n",
      "            rewards = [\n",
      "                ((float(r) - 0.5) * 2 - self.reward_bias) * self.reward_scaling\n",
      "                for r in success\n",
      "            ]\n",
      "\n",
      "            all_success.extend(success)\n",
      "            all_answers.extend(answers)\n",
      "\n",
      "            x[\"data\"][\"packed_input_ids\"].extend(list(act.seqs[0]))\n",
      "            x[\"data\"][\"packed_logprobs\"].extend(list(act.logprobs[0]))\n",
      "            x[\"data\"][\"seq_no_eos_mask\"].append(act.no_eos[0])\n",
      "            all_rewards.append(rewards[0])\n",
      "            x[\"data\"][\"prompt_mask\"].extend(\n",
      "                [1] * act.prompt_len + [0] * (act.seqlens[0] - act.prompt_len)\n",
      "            )\n",
      "\n",
      "            x[\"data\"][\"version_start\"].extend(list(act.version_start))\n",
      "            x[\"data\"][\"version_end\"].extend(list(act.version_end))\n",
      "\n",
      "            x[\"seqlens\"][\"packed_input_ids\"][0].append(act.seqlens[0])\n",
      "            x[\"seqlens\"][\"packed_logprobs\"][0].append(act.seqlens[0] - 1)\n",
      "            x[\"seqlens\"][\"prompt_mask\"][0].append(act.seqlens[0])\n",
      "\n",
      "            token_ids = list(act.seqs[0])\n",
      "\n",
      "            feedback = None\n",
      "            if success[0]:\n",
      "                feedback = \"Congratulations! You are correct!\"\n",
      "            else:\n",
      "                feedback = \"Unfortunately your answer is wrong. Let's try again.\"\n",
      "\n",
      "            feedback = \"\\n\" + self.tokenizer.apply_chat_template(\n",
      "                [dict(content=feedback, role=\"user\")],\n",
      "                add_generation_prompt=True,\n",
      "                tokenize=False,\n",
      "            )\n",
      "            feedback = self.tokenizer(feedback)[\"input_ids\"]\n",
      "            token_ids.extend(feedback)\n",
      "\n",
      "        self.log_rewards_to_file(\n",
      "            str(qid),\n",
      "            prompt_str,\n",
      "            seqlens=x[\"seqlens\"][\"packed_input_ids\"][0],\n",
      "            answers=all_answers,\n",
      "            prompt_len=len(prompt_token_ids),\n",
      "            rewards=all_rewards,\n",
      "            success=all_success,\n",
      "            version_starts=x[\"data\"][\"version_start\"],\n",
      "            version_ends=x[\"data\"][\"version_end\"],\n",
      "        )\n",
      "\n",
      "        for i in reversed(range(len(all_rewards) - 1)):\n",
      "            all_rewards[i] = (\n",
      "                all_rewards[i] + all_rewards[i + 1] * self.turn_level_discount\n",
      "            )\n",
      "        x[\"data\"][\"rewards\"] = all_rewards\n",
      "\n",
      "        for k in x[\"keys\"]:\n",
      "            if not isinstance(x[\"data\"][k], torch.Tensor):\n",
      "                x[\"data\"][k] = torch.tensor(x[\"data\"][k], dtype=x[\"dtypes\"][k])\n",
      "\n",
      "        x = SequenceSample(**x)\n",
      "\n",
      "        if \"task_ids\" in prompt.keys:\n",
      "            y = SequenceSample(\n",
      "                keys=[\"task_ids\"],\n",
      "                ids=[qid],\n",
      "                dtypes=dict(task_ids=torch.long),\n",
      "                trailing_shapes=dict(task_ids=()),\n",
      "                seqlens=dict(task_ids=[[1]]),\n",
      "                data=dict(task_ids=prompt.data[\"task_ids\"]),\n",
      "            )\n",
      "            x.update_(y)\n",
      "\n",
      "        return [x]\n",
      "\n",
      "    def log_rewards_to_file(\n",
      "        self,\n",
      "        qid: str,\n",
      "        prompt: str,\n",
      "        prompt_len: int,\n",
      "        answers: List[str],\n",
      "        seqlens: List[int],\n",
      "        rewards: List[float],\n",
      "        success: List[bool],\n",
      "        version_starts: List[int],\n",
      "        version_ends: List[int],\n",
      "    ):\n",
      "        group_size = len(answers)\n",
      "\n",
      "        for group_idx in range(group_size):\n",
      "            # NOTE: we can ensure that only one process is logging this query id\n",
      "            gen_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"generated\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.txt\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "\n",
      "            version_start = version_starts[group_idx]\n",
      "            version_end = version_ends[group_idx]\n",
      "            reward = rewards[group_idx]\n",
      "            answer = answers[group_idx]\n",
      "            seqlen = seqlens[group_idx]\n",
      "            with open(gen_file_path, \"a\") as _f:\n",
      "                info = \"\\n\".join(\n",
      "                    [\n",
      "                        f\"idx: {group_idx + 1} / {group_size}, seqlen: {seqlen}, \"\n",
      "                        f\"head version: {version_start}, tail version: {version_end}.\",\n",
      "                        f\"reward is {reward}, prompt is {colorama.Fore.YELLOW + colorama.Style.DIM}{prompt}{colorama.Style.RESET_ALL}\",\n",
      "                        f\"sequence is: {colorama.Fore.YELLOW + colorama.Style.DIM}{answer}{colorama.Style.RESET_ALL}.\",\n",
      "                    ]\n",
      "                )\n",
      "                _f.write(info + \"\\n\")\n",
      "\n",
      "            train_pass_monitor_file_path = os.path.join(\n",
      "                constants.LOG_ROOT,\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"training_monitor\",\n",
      "                str(version_starts[group_idx]),\n",
      "                f\"{qid}.jsonl\",\n",
      "            )\n",
      "            os.makedirs(os.path.dirname(train_pass_monitor_file_path), exist_ok=True)\n",
      "\n",
      "            with open(train_pass_monitor_file_path, \"a\") as monitor_file:\n",
      "                monitor_file.write(\n",
      "                    json.dumps(\n",
      "                        {\n",
      "                            \"version_start\": int(version_start),\n",
      "                            \"version_end\": int(version_end),\n",
      "                            \"success\": bool(success),\n",
      "                            \"prompt_len\": prompt_len,\n",
      "                            \"answer_len\": seqlen - prompt_len,\n",
      "                        },\n",
      "                        ensure_ascii=False,\n",
      "                    )\n",
      "                    + \"\\n\"\n",
      "                )\n",
      "\n",
      "\n",
      "register_agent(\"math-multi-turn\", MathMultiTurnAgent)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/dataset/prompt_dataset.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import Callable, Dict, Hashable, List, Optional\n",
      "\n",
      "import numpy as np\n",
      "import torch.utils.data\n",
      "\n",
      "from realhf.api.core import data_api\n",
      "from realhf.base import logging\n",
      "\n",
      "logger = logging.getLogger(\"Prompt Dataset\")\n",
      "\n",
      "\n",
      "class PromptDataset(torch.utils.data.Dataset):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        util: data_api.DatasetUtility,\n",
      "        max_length: Optional[int] = None,\n",
      "        dataset_path: Optional[str] = None,\n",
      "        dataset_builder: Optional[Callable[[], List[Dict]]] = None,\n",
      "        fill_to_max_length: bool = False,\n",
      "    ):\n",
      "        \"\"\"A dataset with prompts. Usually used for PPO.\n",
      "\n",
      "        Args:\n",
      "            util (api.data.DatasetUtility): .\n",
      "            max_length (Optional[int], optional): The maximum length of each sequence in the batch.\n",
      "            dataset_path (Optional[str], optional): Path to the dataset json/jsonl file.\n",
      "                The json/jsonl file should be a list of dictionary. Each element in the list should have\n",
      "                a key \"prompt\". Defaults to None.\n",
      "            dataset_builder (Optional[Callable[[], List[Dict]]], optional): Alternative to dataset_path.\n",
      "                A callable that returns a list of dictionary. Defaults to None.\n",
      "            fill_to_max_length (bool):Whether to fill prompts to the maximum length. If True,\n",
      "                prompts will be left-filled with non-pad tokens. Only used for testing.\n",
      "        \"\"\"\n",
      "        self._util = util\n",
      "        self.max_length = max_length\n",
      "\n",
      "        data = data_api.load_shuffle_split_dataset(util, dataset_path, dataset_builder)\n",
      "\n",
      "        prompts_str = [x[\"prompt\"] for x in data]\n",
      "        self.ids = [x[\"id\"] for x in data]\n",
      "        util.tokenizer.padding_side = \"left\"\n",
      "        prompt_encodings = util.tokenizer(\n",
      "            prompts_str,\n",
      "            truncation=True,\n",
      "            max_length=max_length,\n",
      "            padding=False,\n",
      "            return_length=True,\n",
      "            return_attention_mask=False,\n",
      "        )\n",
      "\n",
      "        if fill_to_max_length:\n",
      "            for i in range(len(prompt_encodings[\"length\"])):\n",
      "                x = prompt_encodings[\"input_ids\"][i]\n",
      "                if max_length > len(x):\n",
      "                    # Fill with the final non-pad token to the left.\n",
      "                    prompt_encodings[\"input_ids\"][i] = [x[-1]] * (\n",
      "                        max_length - len(x)\n",
      "                    ) + x\n",
      "                    prompt_encodings[\"length\"][i] = max_length\n",
      "\n",
      "        self.prompt_lengths = prompt_encodings[\"length\"]\n",
      "        self.prompts = prompt_encodings[\"input_ids\"]\n",
      "        assert all(len(x) == l for x, l in zip(self.prompts, self.prompt_lengths))\n",
      "\n",
      "        logger.info(f\"Number of prompts in the dataset: {len(self.prompts)}\")\n",
      "\n",
      "    @property\n",
      "    def util(self):\n",
      "        return self._util\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.prompts)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return data_api.SequenceSample.from_default(\n",
      "            ids=[self.ids[idx]],\n",
      "            seqlens=[self.prompt_lengths[idx]],\n",
      "            data=dict(packed_prompts=torch.tensor(self.prompts[idx], dtype=torch.long)),\n",
      "        )\n",
      "\n",
      "\n",
      "data_api.register_dataset(\"prompt\", PromptDataset)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/dataset/math_parser.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import json\n",
      "import multiprocessing\n",
      "import re\n",
      "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
      "from typing import List, Union\n",
      "\n",
      "import regex\n",
      "from latex2sympy2 import latex2sympy\n",
      "from sympy import N, simplify\n",
      "from sympy.parsing.latex import parse_latex\n",
      "from sympy.parsing.sympy_parser import parse_expr\n",
      "from word2number import w2n\n",
      "\n",
      "from realhf.base import logging\n",
      "\n",
      "logger = logging.getLogger(\"math parser\")\n",
      "\n",
      "# units mainly from MathQA\n",
      "unit_texts = [\n",
      "    \"east\",\n",
      "    \"degree\",\n",
      "    \"mph\",\n",
      "    \"kmph\",\n",
      "    \"ft\",\n",
      "    \"m sqaure\",\n",
      "    \" m east\",\n",
      "    \"sq m\",\n",
      "    \"deg\",\n",
      "    \"mile\",\n",
      "    \"q .\",\n",
      "    \"monkey\",\n",
      "    \"prime\",\n",
      "    \"ratio\",\n",
      "    \"profit of rs\",\n",
      "    \"rd\",\n",
      "    \"o\",\n",
      "    \"gm\",\n",
      "    \"p . m\",\n",
      "    \"lb\",\n",
      "    \"tile\",\n",
      "    \"per\",\n",
      "    \"dm\",\n",
      "    \"lt\",\n",
      "    \"gain\",\n",
      "    \"ab\",\n",
      "    \"way\",\n",
      "    \"west\",\n",
      "    \"a .\",\n",
      "    \"b .\",\n",
      "    \"c .\",\n",
      "    \"d .\",\n",
      "    \"e .\",\n",
      "    \"f .\",\n",
      "    \"g .\",\n",
      "    \"h .\",\n",
      "    \"t\",\n",
      "    \"a\",\n",
      "    \"h\",\n",
      "    \"no change\",\n",
      "    \"men\",\n",
      "    \"soldier\",\n",
      "    \"pie\",\n",
      "    \"bc\",\n",
      "    \"excess\",\n",
      "    \"st\",\n",
      "    \"inches\",\n",
      "    \"noon\",\n",
      "    \"percent\",\n",
      "    \"by\",\n",
      "    \"gal\",\n",
      "    \"kmh\",\n",
      "    \"c\",\n",
      "    \"acre\",\n",
      "    \"rise\",\n",
      "    \"a . m\",\n",
      "    \"th\",\n",
      "    \" r 2\",\n",
      "    \"sq\",\n",
      "    \"mark\",\n",
      "    \"l\",\n",
      "    \"toy\",\n",
      "    \"coin\",\n",
      "    \"sq . m\",\n",
      "    \"gallon\",\n",
      "    \" f\",\n",
      "    \"profit\",\n",
      "    \"minw\",\n",
      "    \"yr\",\n",
      "    \"women\",\n",
      "    \"feet\",\n",
      "    \"am\",\n",
      "    \"pm\",\n",
      "    \"hr\",\n",
      "    \"cu cm\",\n",
      "    \"square\",\n",
      "    \"v   \",\n",
      "    \"are\",\n",
      "    \"rupee\",\n",
      "    \"rounds\",\n",
      "    \"cubic\",\n",
      "    \"cc\",\n",
      "    \"mtr\",\n",
      "    \"s\",\n",
      "    \"ohm\",\n",
      "    \"number\",\n",
      "    \"kmph\",\n",
      "    \"day\",\n",
      "    \"hour\",\n",
      "    \"minute\",\n",
      "    \"min\",\n",
      "    \"second\",\n",
      "    \"man\",\n",
      "    \"woman\",\n",
      "    \"sec\",\n",
      "    \"cube\",\n",
      "    \"mt\",\n",
      "    \"sq inch\",\n",
      "    \"mp\",\n",
      "    \" cm \",\n",
      "    \"hectare\",\n",
      "    \"more\",\n",
      "    \"sec\",\n",
      "    \"unit\",\n",
      "    \"cu . m\",\n",
      "    \"cm 2\",\n",
      "    \"rs .\",\n",
      "    \"rs\",\n",
      "    \"kg\",\n",
      "    \"g\",\n",
      "    \"month\",\n",
      "    \"km\",\n",
      "    \"m\",\n",
      "    \"cm\",\n",
      "    \"mm\",\n",
      "    \"apple\",\n",
      "    \"liter\",\n",
      "    \"loss\",\n",
      "    \"yard\",\n",
      "    \"pure\",\n",
      "    \"year\",\n",
      "    \"increase\",\n",
      "    \"decrease\",\n",
      "    \"d\",\n",
      "    \"less\",\n",
      "    \"Surface\",\n",
      "    \"litre\",\n",
      "    \"pi sq m\",\n",
      "    \"s .\",\n",
      "    \"metre\",\n",
      "    \"meter\",\n",
      "    \"inch\",\n",
      "]\n",
      "\n",
      "unit_texts.extend([t + \"s\" for t in unit_texts])\n",
      "\n",
      "\n",
      "def _fix_fracs(string):\n",
      "    substrs = string.split(\"\\\\frac\")\n",
      "    new_str = substrs[0]\n",
      "    if len(substrs) > 1:\n",
      "        substrs = substrs[1:]\n",
      "        for substr in substrs:\n",
      "            new_str += \"\\\\frac\"\n",
      "            if len(substr) > 0 and substr[0] == \"{\":\n",
      "                new_str += substr\n",
      "            else:\n",
      "                try:\n",
      "                    assert len(substr) >= 2\n",
      "                except:\n",
      "                    return string\n",
      "                a = substr[0]\n",
      "                b = substr[1]\n",
      "                if b != \"{\":\n",
      "                    if len(substr) > 2:\n",
      "                        post_substr = substr[2:]\n",
      "                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n",
      "                    else:\n",
      "                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n",
      "                else:\n",
      "                    if len(substr) > 2:\n",
      "                        post_substr = substr[2:]\n",
      "                        new_str += \"{\" + a + \"}\" + b + post_substr\n",
      "                    else:\n",
      "                        new_str += \"{\" + a + \"}\" + b\n",
      "    string = new_str\n",
      "    return string\n",
      "\n",
      "\n",
      "def _fix_a_slash_b(string):\n",
      "    if len(string.split(\"/\")) != 2:\n",
      "        return string\n",
      "    a = string.split(\"/\")[0]\n",
      "    b = string.split(\"/\")[1]\n",
      "    try:\n",
      "        if \"sqrt\" not in a:\n",
      "            a = int(a)\n",
      "        if \"sqrt\" not in b:\n",
      "            b = int(b)\n",
      "        assert string == \"{}/{}\".format(a, b)\n",
      "        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n",
      "        return new_string\n",
      "    except:\n",
      "        return string\n",
      "\n",
      "\n",
      "def _fix_sqrt(string):\n",
      "    _string = re.sub(r\"\\\\sqrt(\\w+)\", r\"\\\\sqrt{\\1}\", string)\n",
      "    return _string\n",
      "\n",
      "\n",
      "def convert_word_number(text: str) -> str:\n",
      "    try:\n",
      "        text = str(w2n.word_to_num(text))\n",
      "    except:\n",
      "        pass\n",
      "    return text\n",
      "\n",
      "\n",
      "def strip_string(string, skip_unit=False):\n",
      "    string = str(string).strip()\n",
      "    # linebreaks\n",
      "    string = string.replace(\"\\n\", \"\")\n",
      "\n",
      "    # right \".\"\n",
      "    string = string.rstrip(\".\")\n",
      "\n",
      "    # remove inverse spaces\n",
      "    # replace \\\\ with \\\n",
      "    string = string.replace(\"\\\\!\", \"\")\n",
      "    # string = string.replace(\"\\\\ \", \"\")\n",
      "    # string = string.replace(\"\\\\\\\\\", \"\\\\\")\n",
      "\n",
      "    # matrix\n",
      "    string = re.sub(r\"\\\\begin\\{array\\}\\{.*?\\}\", r\"\\\\begin{pmatrix}\", string)\n",
      "    string = re.sub(r\"\\\\end\\{array\\}\", r\"\\\\end{pmatrix}\", string)\n",
      "    string = string.replace(\"bmatrix\", \"pmatrix\")\n",
      "\n",
      "    # replace tfrac and dfrac with frac\n",
      "    string = string.replace(\"tfrac\", \"frac\")\n",
      "    string = string.replace(\"dfrac\", \"frac\")\n",
      "    string = (\n",
      "        string.replace(\"\\\\neq\", \"\\\\ne\")\n",
      "        .replace(\"\\\\leq\", \"\\\\le\")\n",
      "        .replace(\"\\\\geq\", \"\\\\ge\")\n",
      "    )\n",
      "\n",
      "    # remove \\left and \\right\n",
      "    string = string.replace(\"\\\\left\", \"\")\n",
      "    string = string.replace(\"\\\\right\", \"\")\n",
      "    string = string.replace(\"\\\\{\", \"{\")\n",
      "    string = string.replace(\"\\\\}\", \"}\")\n",
      "\n",
      "    # Remove unit: miles, dollars if after is not none\n",
      "    _string = re.sub(r\"\\\\text{.*?}$\", \"\", string).strip()\n",
      "    if _string != \"\" and _string != string:\n",
      "        # print(\"Warning: unit not removed: '{}' -> '{}'\".format(string, _string))\n",
      "        string = _string\n",
      "\n",
      "    if not skip_unit:\n",
      "        # Remove unit: texts\n",
      "        for _ in range(2):\n",
      "            for unit_text in unit_texts:\n",
      "                # use regex, the prefix should be either the start of the string or a non-alphanumeric character\n",
      "                # the suffix should be either the end of the string or a non-alphanumeric character\n",
      "                _string = re.sub(r\"(^|\\W)\" + unit_text + r\"($|\\W)\", r\"\\1\\2\", string)\n",
      "                if _string != \"\":\n",
      "                    string = _string\n",
      "\n",
      "    # Remove circ (degrees)\n",
      "    string = string.replace(\"^{\\\\circ}\", \"\")\n",
      "    string = string.replace(\"^\\\\circ\", \"\")\n",
      "\n",
      "    # remove dollar signs\n",
      "    string = string.replace(\"\\\\$\", \"\")\n",
      "    string = string.replace(\"$\", \"\")\n",
      "    string = string.replace(\"\\\\(\", \"\").replace(\"\\\\)\", \"\")\n",
      "\n",
      "    # convert word number to digit\n",
      "    string = convert_word_number(string)\n",
      "\n",
      "    # replace \"\\\\text{...}\" to \"...\"\n",
      "    string = re.sub(r\"\\\\text\\{(.*?)\\}\", r\"\\1\", string)\n",
      "    for key in [\"x=\", \"y=\", \"z=\", \"x\\\\in\", \"y\\\\in\", \"z\\\\in\", \"x\\\\to\", \"y\\\\to\", \"z\\\\to\"]:\n",
      "        string = string.replace(key, \"\")\n",
      "    string = string.replace(\"\\\\emptyset\", r\"{}\")\n",
      "    string = string.replace(\"(-\\\\infty,\\\\infty)\", \"\\\\mathbb{R}\")\n",
      "\n",
      "    # remove percentage\n",
      "    string = string.replace(\"\\\\%\", \"\")\n",
      "    string = string.replace(\"\\%\", \"\")\n",
      "    string = string.replace(\"%\", \"\")\n",
      "\n",
      "    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n",
      "    string = string.replace(\" .\", \" 0.\")\n",
      "    string = string.replace(\"{.\", \"{0.\")\n",
      "\n",
      "    # cdot\n",
      "    # string = string.replace(\"\\\\cdot\", \"\")\n",
      "    if (\n",
      "        string.startswith(\"{\")\n",
      "        and string.endswith(\"}\")\n",
      "        and string.isalnum()\n",
      "        or string.startswith(\"(\")\n",
      "        and string.endswith(\")\")\n",
      "        and string.isalnum()\n",
      "        or string.startswith(\"[\")\n",
      "        and string.endswith(\"]\")\n",
      "        and string.isalnum()\n",
      "    ):\n",
      "        string = string[1:-1]\n",
      "\n",
      "    # inf\n",
      "    string = string.replace(\"infinity\", \"\\\\infty\")\n",
      "    if \"\\\\infty\" not in string:\n",
      "        string = string.replace(\"inf\", \"\\\\infty\")\n",
      "    string = string.replace(\"+\\\\inity\", \"\\\\infty\")\n",
      "\n",
      "    # and\n",
      "    string = string.replace(\"and\", \"\")\n",
      "    string = string.replace(\"\\\\mathbf\", \"\")\n",
      "\n",
      "    # use regex to remove \\mbox{...}\n",
      "    string = re.sub(r\"\\\\mbox{.*?}\", \"\", string)\n",
      "\n",
      "    # quote\n",
      "    string.replace(\"'\", \"\")\n",
      "    string.replace('\"', \"\")\n",
      "\n",
      "    # i, j\n",
      "    if \"j\" in string and \"i\" not in string:\n",
      "        string = string.replace(\"j\", \"i\")\n",
      "\n",
      "    # replace a.000b where b is not number or b is end, with ab, use regex\n",
      "    string = re.sub(r\"(\\d+)\\.0*([^\\d])\", r\"\\1\\2\", string)\n",
      "    string = re.sub(r\"(\\d+)\\.0*$\", r\"\\1\", string)\n",
      "\n",
      "    # if empty, return empty string\n",
      "    if len(string) == 0:\n",
      "        return string\n",
      "    if string[0] == \".\":\n",
      "        string = \"0\" + string\n",
      "\n",
      "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
      "    if len(string.split(\"=\")) == 2:\n",
      "        if len(string.split(\"=\")[0]) <= 2:\n",
      "            string = string.split(\"=\")[1]\n",
      "\n",
      "    string = _fix_sqrt(string)\n",
      "    string = string.replace(\" \", \"\")\n",
      "\n",
      "    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc. Even works with \\frac1{72} (but not \\frac{72}1). Also does a/b --> \\\\frac{a}{b}\n",
      "    string = _fix_fracs(string)\n",
      "\n",
      "    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y\n",
      "    string = _fix_a_slash_b(string)\n",
      "\n",
      "    return string\n",
      "\n",
      "\n",
      "def extract_answer(pred_str, data_name, use_last_number=True):\n",
      "    pred_str = pred_str.replace(\"\\u043a\\u0438\", \"\")\n",
      "    if data_name in [\"mmlu_stem\", \"sat_math\", \"aqua\", \"gaokao2023\"]:\n",
      "        # TODO check multiple choice\n",
      "        return choice_answer_clean(pred_str)\n",
      "\n",
      "    if \"final answer is $\" in pred_str and \"$. I hope\" in pred_str:\n",
      "        # minerva_math\n",
      "        tmp = pred_str.split(\"final answer is $\", 1)[1]\n",
      "        pred = tmp.split(\"$. I hope\", 1)[0].strip()\n",
      "    elif \"boxed\" in pred_str:\n",
      "        ans = pred_str.split(\"boxed\")[-1]\n",
      "        if len(ans) == 0:\n",
      "            return \"\"\n",
      "        elif ans[0] == \"{\":\n",
      "            stack = 1\n",
      "            a = \"\"\n",
      "            for c in ans[1:]:\n",
      "                if c == \"{\":\n",
      "                    stack += 1\n",
      "                    a += c\n",
      "                elif c == \"}\":\n",
      "                    stack -= 1\n",
      "                    if stack == 0:\n",
      "                        break\n",
      "                    a += c\n",
      "                else:\n",
      "                    a += c\n",
      "        else:\n",
      "            a = ans.split(\"$\")[0].strip()\n",
      "        pred = a\n",
      "    elif \"he answer is\" in pred_str:\n",
      "        pred = pred_str.split(\"he answer is\")[-1].strip()\n",
      "    elif \"final answer is\" in pred_str:\n",
      "        pred = pred_str.split(\"final answer is\")[-1].strip()\n",
      "    elif \"\" in pred_str:\n",
      "        # Handle Chinese few-shot multiple choice problem answer extraction\n",
      "        pred = pred_str.split(\"\")[1].strip().split(\"\\n\\n\")[0].strip()\n",
      "    else:  # use the last number\n",
      "        if use_last_number:\n",
      "            pattern = \"-?\\d*\\.?\\d+\"\n",
      "            pred = re.findall(pattern, pred_str.replace(\",\", \"\"))\n",
      "            if len(pred) >= 1:\n",
      "                pred = pred[-1]\n",
      "            else:\n",
      "                pred = \"\"\n",
      "        else:\n",
      "            pred = \"\"\n",
      "\n",
      "    # choice answer\n",
      "    if data_name in [\"sat_math\", \"aqua\"] or \"mmlu\" in data_name:\n",
      "        tmp = re.findall(r\"\\b(A|B|C|D|E)\\b\", pred.upper())\n",
      "        if tmp:\n",
      "            pred = tmp[-1]\n",
      "        else:\n",
      "            pred = pred.strip().strip(\".\")\n",
      "\n",
      "    # multiple line\n",
      "    # pred = pred.split(\"\\n\")[0]\n",
      "    pred = re.sub(r\"\\n\\s*\", \"\", pred)\n",
      "    if pred != \"\" and pred[0] == \":\":\n",
      "        pred = pred[1:]\n",
      "    if pred != \"\" and pred[-1] == \".\":\n",
      "        pred = pred[:-1]\n",
      "    if pred != \"\" and pred[-1] == \"/\":\n",
      "        pred = pred[:-1]\n",
      "    pred = strip_string(pred, skip_unit=data_name in [\"carp_en\", \"minerva_math\"])\n",
      "    return pred\n",
      "\n",
      "\n",
      "def str_to_pmatrix(input_str):\n",
      "    input_str = input_str.strip()\n",
      "    matrix_str = re.findall(r\"\\{.*,.*\\}\", input_str)\n",
      "    pmatrix_list = []\n",
      "\n",
      "    for m in matrix_str:\n",
      "        m = m.strip(\"{}\")\n",
      "        pmatrix = r\"\\begin{pmatrix}\" + m.replace(\",\", \"\\\\\") + r\"\\end{pmatrix}\"\n",
      "        pmatrix_list.append(pmatrix)\n",
      "\n",
      "    return \", \".join(pmatrix_list)\n",
      "\n",
      "\n",
      "def parse_digits(num):\n",
      "    num = regex.sub(\",\", \"\", str(num))\n",
      "    try:\n",
      "        return float(num)\n",
      "    except:\n",
      "        if num.endswith(\"%\"):\n",
      "            num = num[:-1]\n",
      "            if num.endswith(\"\\\\\"):\n",
      "                num = num[:-1]\n",
      "            try:\n",
      "                return float(num) / 100\n",
      "            except:\n",
      "                pass\n",
      "    return None\n",
      "\n",
      "\n",
      "def is_digit(num):\n",
      "    # paired with parse_digits\n",
      "    return parse_digits(num) is not None\n",
      "\n",
      "\n",
      "def choice_answer_clean(pred: str):\n",
      "    pred = pred.strip(\"\\n\").rstrip(\".\").rstrip(\"/\").strip(\" \").lstrip(\":\")\n",
      "    # Clean the answer based on the dataset\n",
      "    tmp = re.findall(r\"\\b(A|B|C|D|E)\\b\", pred.upper())\n",
      "    if tmp:\n",
      "        pred = tmp\n",
      "    else:\n",
      "        pred = [pred.strip().strip(\".\")]\n",
      "    pred = pred[-1]\n",
      "    # Remove the period at the end, again!\n",
      "    pred = pred.rstrip(\".\").rstrip(\"/\")\n",
      "    return pred\n",
      "\n",
      "\n",
      "def numeric_equal(prediction: float, reference: float):\n",
      "    from math import isclose\n",
      "\n",
      "    # Note that relative tolerance has significant impact\n",
      "    # on the result of the synthesized GSM-Hard dataset\n",
      "    # if reference.is_integer():\n",
      "    #     return isclose(reference, round(prediction), abs_tol=1e-4)\n",
      "    # else:\n",
      "    # prediction = round(prediction, len(str(reference).split(\".\")[-1]))\n",
      "    return isclose(reference, prediction, rel_tol=1e-4)\n",
      "\n",
      "\n",
      "def symbolic_equal_process(a, b, output_queue):\n",
      "    result = symbolic_equal(a, b)\n",
      "    output_queue.put(result)\n",
      "\n",
      "\n",
      "def math_equal(\n",
      "    prediction: Union[bool, float, str],\n",
      "    reference: Union[float, str],\n",
      "    include_percentage: bool = True,\n",
      "    is_close: bool = True,\n",
      "    timeout: bool = False,\n",
      ") -> bool:\n",
      "    \"\"\"\n",
      "    Exact match of math if and only if:\n",
      "    1. numerical equal: both can convert to float and are equal\n",
      "    2. symbolic equal: both can convert to sympy expression and are equal\n",
      "    \"\"\"\n",
      "    # print(\"Judge:\", prediction, reference)\n",
      "    if prediction is None or reference is None:\n",
      "        return False\n",
      "    if str(prediction.strip().lower()) == str(reference.strip().lower()):\n",
      "        return True\n",
      "    if (\n",
      "        reference in [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
      "        and choice_answer_clean(prediction) == reference\n",
      "    ):\n",
      "        return True\n",
      "\n",
      "    try:  # 1. numerical equal\n",
      "        if is_digit(prediction) and is_digit(reference):\n",
      "            prediction = parse_digits(prediction)\n",
      "            reference = parse_digits(reference)\n",
      "            # number questions\n",
      "            if include_percentage:\n",
      "                gt_result = [reference / 100, reference, reference * 100]\n",
      "            else:\n",
      "                gt_result = [reference]\n",
      "            for item in gt_result:\n",
      "                try:\n",
      "                    if is_close:\n",
      "                        if numeric_equal(prediction, item):\n",
      "                            return True\n",
      "                    else:\n",
      "                        if item == prediction:\n",
      "                            return True\n",
      "                except Exception:\n",
      "                    continue\n",
      "            return False\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    if not prediction and prediction not in [0, False]:\n",
      "        return False\n",
      "\n",
      "    # 2. symbolic equal\n",
      "    reference = str(reference).strip()\n",
      "    prediction = str(prediction).strip()\n",
      "\n",
      "    ## pmatrix (amps)\n",
      "    if \"pmatrix\" in prediction and not \"pmatrix\" in reference:\n",
      "        reference = str_to_pmatrix(reference)\n",
      "\n",
      "    ## deal with [], (), {}\n",
      "    pred_str, ref_str = prediction, reference\n",
      "    if (\n",
      "        prediction.startswith(\"[\")\n",
      "        and prediction.endswith(\"]\")\n",
      "        and not reference.startswith(\"(\")\n",
      "    ) or (\n",
      "        prediction.startswith(\"(\")\n",
      "        and prediction.endswith(\")\")\n",
      "        and not reference.startswith(\"[\")\n",
      "    ):\n",
      "        pred_str = pred_str.strip(\"[]()\")\n",
      "        ref_str = ref_str.strip(\"[]()\")\n",
      "    for s in [\"{\", \"}\", \"(\", \")\"]:\n",
      "        ref_str = ref_str.replace(s, \"\")\n",
      "        pred_str = pred_str.replace(s, \"\")\n",
      "    if pred_str.lower() == ref_str.lower():\n",
      "        return True\n",
      "\n",
      "    ## [a, b] vs. [c, d], return a==c and b==d\n",
      "    if (\n",
      "        regex.match(r\"(\\(|\\[).+(\\)|\\])\", prediction) is not None\n",
      "        and regex.match(r\"(\\(|\\[).+(\\)|\\])\", reference) is not None\n",
      "    ):\n",
      "        pred_parts = prediction[1:-1].split(\",\")\n",
      "        ref_parts = reference[1:-1].split(\",\")\n",
      "        if len(pred_parts) == len(ref_parts):\n",
      "            if all(\n",
      "                [\n",
      "                    math_equal(\n",
      "                        pred_parts[i], ref_parts[i], include_percentage, is_close\n",
      "                    )\n",
      "                    for i in range(len(pred_parts))\n",
      "                ]\n",
      "            ):\n",
      "                return True\n",
      "    if (\n",
      "        (\n",
      "            prediction.startswith(\"\\\\begin{pmatrix}\")\n",
      "            or prediction.startswith(\"\\\\begin{bmatrix}\")\n",
      "        )\n",
      "        and (\n",
      "            prediction.endswith(\"\\\\end{pmatrix}\")\n",
      "            or prediction.endswith(\"\\\\end{bmatrix}\")\n",
      "        )\n",
      "        and (\n",
      "            reference.startswith(\"\\\\begin{pmatrix}\")\n",
      "            or reference.startswith(\"\\\\begin{bmatrix}\")\n",
      "        )\n",
      "        and (\n",
      "            reference.endswith(\"\\\\end{pmatrix}\") or reference.endswith(\"\\\\end{bmatrix}\")\n",
      "        )\n",
      "    ):\n",
      "        pred_lines = [\n",
      "            line.strip()\n",
      "            for line in prediction[\n",
      "                len(\"\\\\begin{pmatrix}\") : -len(\"\\\\end{pmatrix}\")\n",
      "            ].split(\"\\\\\\\\\")\n",
      "            if line.strip()\n",
      "        ]\n",
      "        ref_lines = [\n",
      "            line.strip()\n",
      "            for line in reference[\n",
      "                len(\"\\\\begin{pmatrix}\") : -len(\"\\\\end{pmatrix}\")\n",
      "            ].split(\"\\\\\\\\\")\n",
      "            if line.strip()\n",
      "        ]\n",
      "        matched = True\n",
      "        if len(pred_lines) == len(ref_lines):\n",
      "            for pred_line, ref_line in zip(pred_lines, ref_lines):\n",
      "                pred_parts = pred_line.split(\"&\")\n",
      "                ref_parts = ref_line.split(\"&\")\n",
      "                if len(pred_parts) == len(ref_parts):\n",
      "                    if not all(\n",
      "                        [\n",
      "                            math_equal(\n",
      "                                pred_parts[i],\n",
      "                                ref_parts[i],\n",
      "                                include_percentage,\n",
      "                                is_close,\n",
      "                            )\n",
      "                            for i in range(len(pred_parts))\n",
      "                        ]\n",
      "                    ):\n",
      "                        matched = False\n",
      "                        break\n",
      "                else:\n",
      "                    matched = False\n",
      "                if not matched:\n",
      "                    break\n",
      "        else:\n",
      "            matched = False\n",
      "        if matched:\n",
      "            return True\n",
      "\n",
      "    if prediction.count(\"=\") == 1 and reference.count(\"=\") == 1:\n",
      "        pred = prediction.split(\"=\")\n",
      "        pred = f\"{pred[0].strip()} - ({pred[1].strip()})\"\n",
      "        ref = reference.split(\"=\")\n",
      "        ref = f\"{ref[0].strip()} - ({ref[1].strip()})\"\n",
      "        if symbolic_equal(pred, ref) or symbolic_equal(f\"-({pred})\", ref):\n",
      "            return True\n",
      "    elif (\n",
      "        prediction.count(\"=\") == 1\n",
      "        and len(prediction.split(\"=\")[0].strip()) <= 2\n",
      "        and \"=\" not in reference\n",
      "    ):\n",
      "        if math_equal(\n",
      "            prediction.split(\"=\")[1], reference, include_percentage, is_close\n",
      "        ):\n",
      "            return True\n",
      "    elif (\n",
      "        reference.count(\"=\") == 1\n",
      "        and len(reference.split(\"=\")[0].strip()) <= 2\n",
      "        and \"=\" not in prediction\n",
      "    ):\n",
      "        if math_equal(\n",
      "            prediction, reference.split(\"=\")[1], include_percentage, is_close\n",
      "        ):\n",
      "            return True\n",
      "\n",
      "    # symbolic equal with sympy\n",
      "    if timeout:\n",
      "        if call_with_timeout(symbolic_equal_process, prediction, reference):\n",
      "            return True\n",
      "    else:\n",
      "        if symbolic_equal(prediction, reference):\n",
      "            return True\n",
      "\n",
      "    return False\n",
      "\n",
      "\n",
      "def call_with_timeout(func, *args, timeout=3, **kwargs):\n",
      "    output_queue = multiprocessing.Queue()\n",
      "    process_args = args + (output_queue,)\n",
      "    process = multiprocessing.Process(target=func, args=process_args, kwargs=kwargs)\n",
      "    process.start()\n",
      "    process.join(timeout)\n",
      "\n",
      "    if process.is_alive():\n",
      "        process.terminate()\n",
      "        process.join()\n",
      "        return False\n",
      "\n",
      "    return output_queue.get()\n",
      "\n",
      "\n",
      "def math_equal_process(param):\n",
      "    return math_equal(param[-2], param[-1])\n",
      "\n",
      "\n",
      "def symbolic_equal(a, b):\n",
      "    def _parse(s):\n",
      "        for f in [parse_latex, parse_expr, latex2sympy]:\n",
      "            try:\n",
      "                return f(s.replace(\"\\\\\\\\\", \"\\\\\"))\n",
      "            except:\n",
      "                try:\n",
      "                    return f(s)\n",
      "                except:\n",
      "                    pass\n",
      "        return s\n",
      "\n",
      "    a = _parse(a)\n",
      "    b = _parse(b)\n",
      "\n",
      "    # direct equal\n",
      "    try:\n",
      "        if str(a) == str(b) or a == b:\n",
      "            return True\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    # simplify equal\n",
      "    try:\n",
      "        if a.equals(b) or simplify(a - b) == 0:\n",
      "            return True\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    # equation equal\n",
      "    try:\n",
      "        if (abs(a.lhs - a.rhs)).equals(abs(b.lhs - b.rhs)):\n",
      "            return True\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    try:\n",
      "        if numeric_equal(float(N(a)), float(N(b))):\n",
      "            return True\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    # matrix\n",
      "    try:\n",
      "        # if a and b are matrix\n",
      "        if a.shape == b.shape:\n",
      "            _a = a.applyfunc(lambda x: round(x, 3))\n",
      "            _b = b.applyfunc(lambda x: round(x, 3))\n",
      "            if _a.equals(_b):\n",
      "                return True\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    return False\n",
      "\n",
      "\n",
      "def process_results(answer, solution):\n",
      "\n",
      "    try:\n",
      "        extracted_answer = extract_answer(answer, \"math\", use_last_number=False)\n",
      "        extracted_solution = extract_answer(solution, \"math\", use_last_number=True)\n",
      "\n",
      "        # if extract_answer.strip() == \"\":\n",
      "        #     print (answer)\n",
      "        # raise\n",
      "        if extracted_answer is None or extracted_answer.strip() in [\"None\", \"none\", \"\"]:\n",
      "            retval = 0\n",
      "        elif extracted_solution is None or extracted_solution.strip() in [\n",
      "            \"None\",\n",
      "            \"none\",\n",
      "            \"\",\n",
      "        ]:\n",
      "            retval = 0\n",
      "        elif math_equal(extracted_answer, extracted_solution, timeout=False):\n",
      "            # elif call_with_timeout(math_equal, extracted_answer, extracted_solution):\n",
      "            retval = 1\n",
      "        else:\n",
      "            retval = 0\n",
      "\n",
      "        return retval, (extracted_answer, extracted_solution)\n",
      "    except:\n",
      "        return 0, (\"None\", \"None\")\n",
      "\n",
      "\n",
      "def process_results_process(a, b, output_queue):\n",
      "    result = process_results(a, b)\n",
      "    output_queue.put(result)\n",
      "\n",
      "\n",
      "def verify_math_solution(answer: str, solution: str):\n",
      "    # answer is generated by the model, solution is the ground truth\n",
      "    tmp = call_with_timeout(\n",
      "        process_results_process,\n",
      "        answer,\n",
      "        solution,\n",
      "    )\n",
      "    if isinstance(tmp, bool):\n",
      "        return 0\n",
      "    return tmp[0]\n",
      "\n",
      "\n",
      "def loadJson(dataDir):\n",
      "    with open(dataDir, \"r\") as f:\n",
      "        if dataDir.endswith(\".jsonl\"):\n",
      "            samples = [json.loads(line) for line in f.readlines()]\n",
      "        else:\n",
      "            samples = json.load(f)\n",
      "\n",
      "    return samples\n",
      "\n",
      "\n",
      "def parse_line(id2info, prompt_str, generated, query_id):\n",
      "    info = id2info[query_id.split(\"@idx:\")[0]]\n",
      "\n",
      "    label = 0\n",
      "    for sol in info[\"solutions\"]:\n",
      "        label = label or verify_math_solution(generated, sol)\n",
      "    return label\n",
      "\n",
      "\n",
      "def parse_lines_in_parallel(\n",
      "    id2info,\n",
      "    generateds: List,\n",
      "    query_ids: List,\n",
      "    max_workers=22,\n",
      "    check_xml_format=False,\n",
      ") -> List:\n",
      "    assert len(generateds) == len(query_ids), (\n",
      "        len(generateds),\n",
      "        len(query_ids),\n",
      "    )\n",
      "\n",
      "    all_jobs = []\n",
      "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
      "        for qid, gen in zip(query_ids, generateds):\n",
      "            info = id2info[qid.split(\"@idx:\")[0]]\n",
      "            jobs = []\n",
      "            for sol in info[\"solutions\"]:\n",
      "                job = executor.submit(verify_math_solution, gen, sol)\n",
      "                jobs.append(job)\n",
      "            all_jobs.append(jobs)\n",
      "\n",
      "    labels = []\n",
      "    for jobs in all_jobs:\n",
      "        label = 0\n",
      "        for job in as_completed(jobs):\n",
      "            x = job.result()\n",
      "            label = label or x\n",
      "        labels.append(label)\n",
      "    return labels\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    sample = {\n",
      "        \"answers\": [\"\\\\boxed{-\\\\frac{2}{3}}\"],\n",
      "        \"solutions\": [\n",
      "            \"1. **Apply the operation $\\\\otimes$ to the innermost parentheses first:**\\n   \\\\[\\n   (1 \\\\otimes 2) \\\\otimes 3 = \\\\left(\\\\frac{1^2}{2}\\\\right) \\\\otimes 3 = \\\\frac{1}{2} \\\\otimes 3\\n   \\\\]\\n   \\\\[\\n   1 \\\\otimes (2 \\\\otimes 3) = 1 \\\\otimes \\\\left(\\\\frac{2^2}{3}\\\\right) = 1 \\\\otimes \\\\frac{4}{3}\\n   \\\\]\\n\\n2. **Calculate each part using the definition of $\\\\otimes$:**\\n   \\\\[\\n   \\\\frac{1}{2} \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n   \\\\]\\n   \\\\[\\n   1 \\\\otimes \\\\frac{4}{3} = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n   \\\\]\\n\\n3. **Subtract the two results:**\\n   \\\\[\\n   \\\\left(\\\\frac{1}{12}\\\\right) - \\\\left(\\\\frac{3}{4}\\\\right) = \\\\frac{1}{12} - \\\\frac{9}{12} = -\\\\frac{8}{12} = -\\\\frac{2}{3}\\n   \\\\]\\n\\n4. **Conclude with the final answer:**\\n   \\\\[\\n   \\\\boxed{A}\\n   \\\\]\",\n",
      "            \"\\\\boxed{-\\\\frac{2}{3}}\",\n",
      "        ],\n",
      "    }\n",
      "    id2info = {\"fe11b471-1aa9-4867-958f-a0a811c85f92\": sample}\n",
      "\n",
      "    print(\n",
      "        parse_lines_in_parallel(\n",
      "            id2info,\n",
      "            sample[\"answers\"] * 100,\n",
      "            [\"fe11b471-1aa9-4867-958f-a0a811c85f92\" for _ in range(100)],\n",
      "            max_workers=8,\n",
      "            check_xml_format=True,\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/dataset/prompt_answer_dataset.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import json\n",
      "from typing import Callable, Dict, List, Optional\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.utils.data\n",
      "\n",
      "from realhf.api.core import data_api\n",
      "from realhf.base import logging\n",
      "\n",
      "logger = logging.getLogger(\"Prompt Answer Dataset\")\n",
      "\n",
      "\n",
      "class PromptAnswerDataset(torch.utils.data.Dataset):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        util: data_api.DatasetUtility,\n",
      "        max_length: int,\n",
      "        dataset_path: Optional[str] = None,\n",
      "        dataset_builder: Optional[Callable[[], List[Dict]]] = None,\n",
      "        fill_to_max_length: bool = False,\n",
      "    ):\n",
      "        \"\"\"A dataset with prompts and corresponding answers. Usually used for\n",
      "        SFT.\n",
      "\n",
      "        Args:\n",
      "            util (api.data.DatasetUtility): .\n",
      "            max_length (Optional[int], optional): The maximum length of each sequence in the batch.\n",
      "            dataset_path (Optional[str], optional): Path to the dataset json/jsonl file.\n",
      "                The json/jsonl file should be a list of dictionary. Each element in the list should have\n",
      "                a key \"prompt\" and a key \"answer\". Defaults to None.\n",
      "            dataset_builder (Optional[Callable[[], List[Dict]]], optional): Alternative to dataset_path.\n",
      "                A callable that returns a list of dictionary. Defaults to None.\n",
      "            fill_to_max_length (bool): Whether to fill sequences to the maximum length. If True,\n",
      "                prompts will be left-filled with non-pad tokens. Only used for testing.\n",
      "        \"\"\"\n",
      "        self._util = util\n",
      "        tokenizer = self.util.tokenizer\n",
      "\n",
      "        data = data_api.load_shuffle_split_dataset(util, dataset_path, dataset_builder)\n",
      "\n",
      "        seqs = [x[\"prompt\"] + x[\"answer\"] + tokenizer.eos_token for x in data]\n",
      "        self.ids = [x[\"id\"] for x in data]\n",
      "        prompts = [x[\"prompt\"] for x in data]\n",
      "\n",
      "        self.tokens = tokenizer(\n",
      "            seqs,\n",
      "            truncation=True,\n",
      "            max_length=max_length,\n",
      "            return_length=True,\n",
      "            return_attention_mask=False,\n",
      "            padding=False,\n",
      "        )\n",
      "        if fill_to_max_length:\n",
      "            for i in range(len(self.tokens[\"input_ids\"])):\n",
      "                x = self.tokens[\"input_ids\"][i]\n",
      "                if max_length > len(x):\n",
      "                    # Fill with the first non-pad token to the right.\n",
      "                    self.tokens[\"input_ids\"][i] = x + [x[0]] * (max_length - len(x))\n",
      "                    self.tokens[\"length\"][i] = max_length\n",
      "        prompt_tokens = tokenizer(\n",
      "            prompts,\n",
      "            padding=False,\n",
      "            truncation=True,\n",
      "            return_length=True,\n",
      "            max_length=max_length,\n",
      "            return_attention_mask=False,\n",
      "        )\n",
      "\n",
      "        prompt_lengths = prompt_tokens[\"length\"]\n",
      "        seq_lengths = self.tokens[\"length\"]\n",
      "        prompt_masks = []\n",
      "        for i in range(len(self)):\n",
      "            prompt_len = prompt_lengths[i]\n",
      "            seqlen = self.tokens[\"length\"][i]\n",
      "            # seq = self.tokens[\"input_ids\"][i]\n",
      "            # prompt = prompt_tokens[\"input_ids\"][i]\n",
      "            # assert seq[:prompt_len] == prompt, (seq, prompt, prompt_len, seqlen)\n",
      "            assert seqlen >= prompt_len, (seqlen, prompt_len)\n",
      "            prompt_mask = [1] * prompt_len + [0] * (seqlen - prompt_len)\n",
      "            prompt_masks.append(prompt_mask)\n",
      "\n",
      "        self.prompt_masks = prompt_masks\n",
      "\n",
      "        logger.info(\n",
      "            f\"Loaded Prompt Answer Dataset with INFO: \"\n",
      "            f\"#seqs={len(self)}, \"\n",
      "            f\"truncation length={max_length}, \"\n",
      "            f\"avg prompt length={np.mean(prompt_lengths):.1f}, \"\n",
      "            f\"avg answer length={np.mean(seq_lengths) - np.mean(prompt_lengths):.1f}\",\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def util(self):\n",
      "        return self._util\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.tokens[\"input_ids\"])\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        d = {\n",
      "            \"packed_input_ids\": torch.tensor(\n",
      "                self.tokens[\"input_ids\"][idx], dtype=torch.long\n",
      "            ),\n",
      "            \"prompt_mask\": torch.tensor(self.prompt_masks[idx], dtype=torch.bool),\n",
      "        }\n",
      "        assert len(d[\"packed_input_ids\"]) == len(d[\"prompt_mask\"])\n",
      "        seqlen = [len(d[\"packed_input_ids\"])]\n",
      "        x = data_api.SequenceSample.from_default(\n",
      "            ids=[self.ids[idx]],\n",
      "            seqlens=seqlen,\n",
      "            data=d,\n",
      "        )\n",
      "        return x\n",
      "\n",
      "\n",
      "data_api.register_dataset(\"prompt_answer\", PromptAnswerDataset)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/dataset/math_code_dataset.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import json\n",
      "import os\n",
      "import sys\n",
      "import traceback\n",
      "from collections import defaultdict\n",
      "from typing import Callable, Dict, Hashable, List, Optional\n",
      "\n",
      "import numpy as np\n",
      "import torch.utils.data\n",
      "\n",
      "from realhf.api.core import data_api\n",
      "from realhf.base import logging\n",
      "from realhf.utils import load_hf_or_local_file\n",
      "\n",
      "logger = logging.getLogger(\"Math Code Dataset\")\n",
      "\n",
      "\n",
      "def check_math_metadata_entries(data):\n",
      "    assert data[\"task\"] == \"math\" or data[\"task\"] == \"stem\"\n",
      "    assert \"query_id\" in data\n",
      "    data[\"query_id\"] = str(data[\"query_id\"])\n",
      "    assert isinstance(data[\"prompt\"], str)\n",
      "    assert isinstance(data[\"solutions\"], list)\n",
      "    for sol in data[\"solutions\"]:\n",
      "        assert isinstance(sol, str)\n",
      "    return data\n",
      "\n",
      "\n",
      "def check_code_metadata_entries(data):\n",
      "    assert data[\"task\"] == \"code\"\n",
      "    assert \"query_id\" in data\n",
      "    data[\"query_id\"] = str(data[\"query_id\"])\n",
      "    if \"problem_id\" not in data:\n",
      "        data[\"problem_id\"] = data[\"query_id\"]\n",
      "    assert isinstance(data[\"prompt\"], str)\n",
      "    case_size = sys.getsizeof(data[\"input_output\"])\n",
      "    if os.getenv(\"FUNCTIONCALL_SERVICE_DOMAIN\", \"\"):\n",
      "        assert (\n",
      "            case_size < 500 * 1024\n",
      "        ), f\"'input_output' exceeds 500KB ({case_size} bytes). Use remote testcase instead.\"\n",
      "    input_output = json.loads(data[\"input_output\"])\n",
      "    assert len(input_output[\"inputs\"]) == len(input_output[\"outputs\"])\n",
      "    for inp, out in zip(input_output[\"inputs\"], input_output[\"outputs\"]):\n",
      "        assert isinstance(inp, str) and isinstance(out, str), (\n",
      "            inp,\n",
      "            out,\n",
      "            input_output.get(\"fn_name\"),\n",
      "        )\n",
      "    return data\n",
      "\n",
      "\n",
      "def load_metadata(path):\n",
      "    assert str(path).endswith(\".jsonl\"), path\n",
      "    path = load_hf_or_local_file(path)\n",
      "    with open(path, \"r\") as f:\n",
      "        data = [json.loads(l) for l in f.readlines()]\n",
      "\n",
      "    id2info = {}\n",
      "    omit_cnt = defaultdict(int)\n",
      "    task_cnt = defaultdict(int)\n",
      "    for d in data:\n",
      "        assert d[\"query_id\"] not in d, (d[\"task\"], d[\"query_id\"])\n",
      "        try:\n",
      "            if \"task\" not in d:\n",
      "                d[\"task\"] = \"math\"\n",
      "                logger.warning(\n",
      "                    f'Key \"task\" not found in the dataset. Use math as default task type.'\n",
      "                )\n",
      "            if d[\"task\"] == \"math\" or d[\"task\"] == \"stem\":\n",
      "                d = check_math_metadata_entries(d)\n",
      "            elif d[\"task\"] == \"code\":\n",
      "                d = check_code_metadata_entries(d)\n",
      "        except Exception as e:\n",
      "            logger.warning(\n",
      "                f\"Data validation failed: query_id {d['query_id']}. \"\n",
      "                f\"Error: {traceback.format_exc()}. Omit it in the dataset.\"\n",
      "            )\n",
      "            omit_cnt[d[\"task\"]] += 1\n",
      "            continue\n",
      "        id2info[d[\"query_id\"]] = d\n",
      "        task_cnt[d[\"task\"]] += 1\n",
      "    logger.warning(f\"Number of ignored data: {dict(**omit_cnt)}\")\n",
      "    return id2info, task_cnt\n",
      "\n",
      "\n",
      "class MATHCodePromptDataset(torch.utils.data.Dataset):\n",
      "    def __init__(\n",
      "        self,\n",
      "        util: data_api.DatasetUtility,\n",
      "        max_length: Optional[int] = None,\n",
      "        dataset_path: Optional[str] = None,\n",
      "        dataset_builder: Optional[Callable[[], List[Dict]]] = None,\n",
      "        filter_threshold: float = 1e4,\n",
      "        max_filter_percentage: float = 0.0,\n",
      "    ):\n",
      "        \"\"\"Required keys: prompt, query_id, task=math/code, solutions.\n",
      "\n",
      "        For code dataset, they additionally require an \"input_output\" key.\n",
      "        \"\"\"\n",
      "        self._util = util\n",
      "        self.max_length = max_length\n",
      "\n",
      "        id2info, task_cnt = load_metadata(dataset_path)\n",
      "\n",
      "        data = data_api.load_shuffle_split_dataset(util, dataset_path, dataset_builder)\n",
      "\n",
      "        prompts_str = [x[\"prompt\"] for x in data]\n",
      "        self.ids = [x[\"query_id\"] for x in data]\n",
      "        self.tasks_ids = [data_api.RL_TASKS.index(x[\"task\"]) for x in data]\n",
      "        if \"scores\" in data[0]:\n",
      "            self.base_scores = [np.mean(x[\"scores\"]) for x in data]\n",
      "        util.tokenizer.padding_side = \"left\"\n",
      "        prompt_encodings = util.tokenizer(\n",
      "            prompts_str,\n",
      "            truncation=True,\n",
      "            # max_length=max_length,\n",
      "            padding=False,\n",
      "            return_length=True,\n",
      "            return_attention_mask=False,\n",
      "        )\n",
      "\n",
      "        logger.info(f\"{len(data)} samples, checking lengths (max_length={max_length})\")\n",
      "        indices = [\n",
      "            i for i, x in enumerate(prompt_encodings[\"length\"]) if x <= max_length\n",
      "        ]\n",
      "        logger.info(\n",
      "            f\"{len(indices)} samples remain, among them {task_cnt['math']} are math data and {task_cnt['code']} are code data\"\n",
      "        )\n",
      "\n",
      "        self.prompt_lengths = [int(prompt_encodings[\"length\"][idx]) for idx in indices]\n",
      "        self.prompts = [prompt_encodings[\"input_ids\"][idx] for idx in indices]\n",
      "        self.ids = [\n",
      "            str(self.ids[idx]) + f\"@idx:{idx}-{util.dp_rank}\" for idx in indices\n",
      "        ]\n",
      "        self.tasks_ids = [self.tasks_ids[idx] for idx in indices]\n",
      "        if \"scores\" in data[0]:\n",
      "            self.base_scores = [self.base_scores[idx] for idx in indices]\n",
      "\n",
      "        assert all(len(x) == l for x, l in zip(self.prompts, self.prompt_lengths))\n",
      "\n",
      "        logger.info(f\"Number of prompts in the dataset: {len(self.prompts)}\")\n",
      "\n",
      "        self.active_indices = list(range(len(self.prompts)))\n",
      "        self.filter_threshold = filter_threshold\n",
      "        self.max_filter_percentage = max_filter_percentage\n",
      "\n",
      "    @property\n",
      "    def util(self):\n",
      "        return self._util\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.active_indices)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        # print(self.base_scores)\n",
      "        idx = self.active_indices[idx]\n",
      "        data = dict(\n",
      "            task_ids=torch.tensor([self.tasks_ids[idx]], dtype=torch.long),\n",
      "            packed_prompts=torch.tensor(self.prompts[idx], dtype=torch.long),\n",
      "        )\n",
      "        if hasattr(self, \"base_scores\"):\n",
      "            data[\"base_scores\"] = torch.tensor(\n",
      "                [self.base_scores[idx]], dtype=torch.float32\n",
      "            )\n",
      "        return data_api.SequenceSample.from_default(\n",
      "            ids=[self.ids[idx]],\n",
      "            seqlens=[self.prompt_lengths[idx]],\n",
      "            data=data,\n",
      "        )\n",
      "\n",
      "    def filter(self, eval_scores: Dict[Hashable, float]):\n",
      "        # Get all data indices that have a higher score than the threshold.\n",
      "        idx2scores_to_remove = {}\n",
      "        for pop_idx, idx in enumerate(self.active_indices):\n",
      "            data_id = self.ids[idx]\n",
      "            if data_id not in eval_scores:\n",
      "                continue\n",
      "            if eval_scores[data_id] > self.filter_threshold:\n",
      "                idx2scores_to_remove[pop_idx] = eval_scores[data_id]\n",
      "\n",
      "        # Control the number of samples to be removed according to max_filter_percentage.\n",
      "        n = int(len(self.active_indices) * self.max_filter_percentage)\n",
      "        indices_to_remove = sorted(\n",
      "            idx2scores_to_remove.keys(),\n",
      "            key=lambda x: idx2scores_to_remove[x],\n",
      "            reverse=True,\n",
      "        )[:n]\n",
      "\n",
      "        for pop_idx in sorted(indices_to_remove, reverse=True):\n",
      "            self.active_indices.pop(pop_idx)\n",
      "        logger.info(\n",
      "            f\"Math prompt dataset DP rank {self.util.dp_rank} filtered\"\n",
      "            f\" {len(indices_to_remove)} samples, {len(self.active_indices)} samples remain. \"\n",
      "            f\"Original dataset size: {len(self.prompts)}. \"\n",
      "            f\"Filter threshold: {self.filter_threshold}. \"\n",
      "            f\"Max filter percentage: {self.max_filter_percentage}. \"\n",
      "            f\"Current number of eval scores: {len(eval_scores)}.\"\n",
      "        )\n",
      "\n",
      "\n",
      "if not __name__ == \"__main__\":\n",
      "    data_api.register_dataset(\"math_code_prompt\", MATHCodePromptDataset)\n",
      "else:\n",
      "    from transformers import AutoTokenizer\n",
      "\n",
      "    dataset = MATHCodePromptDataset(\n",
      "        data_api.DatasetUtility(\n",
      "            seed=0,\n",
      "            dp_rank=0,\n",
      "            world_size=1,\n",
      "            tokenizer=AutoTokenizer.from_pretrained(\n",
      "                \"/storage/openpsi/models/Qwen__Qwen2-1.5B-Instruct/\"\n",
      "            ),\n",
      "        ),\n",
      "        max_length=512,\n",
      "        dataset_path=\"/storage/datasets/full_prompts_for_r1_distilled.jsonl\",\n",
      "    )\n",
      "\n",
      "    dataloader = torch.utils.data.DataLoader(\n",
      "        dataset,\n",
      "        collate_fn=data_api.SequenceSample.gather,\n",
      "        # NOTE: This is *NOT* the actual batch size for training.\n",
      "        # It is just a proper size to load data to workers.\n",
      "        batch_size=4,\n",
      "        shuffle=True,\n",
      "    )\n",
      "    print(f\"size: {len(dataset)}\")\n",
      "    for d in dataloader:\n",
      "        # print(d.ids)\n",
      "        pass\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/dataset/rw_paired_dataset.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import itertools\n",
      "import json\n",
      "from typing import Callable, Dict, List, Optional\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.utils.data\n",
      "\n",
      "from realhf.api.core import data_api\n",
      "\n",
      "\n",
      "class RewardModelingPairedDataset(torch.utils.data.Dataset):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        util: data_api.DatasetUtility,\n",
      "        max_length: int,\n",
      "        max_pairs_per_prompt: int = 2,\n",
      "        dataset_path: Optional[str] = None,\n",
      "        dataset_builder: Optional[Callable[[], List[Dict]]] = None,\n",
      "    ):\n",
      "        \"\"\"Dataset used for reward modeling. Each sample consists of a prompt,\n",
      "        several positive answers, and several negative answers.\n",
      "\n",
      "        Args:\n",
      "            util (api.data.DatasetUtility): Dataset utility.\n",
      "            max_length (int): The maximum sequence length. Sequences will be right-padded to this length\n",
      "            dataset_path (Optional[str], optional): Path to the dataset json/jsonl file.\n",
      "                The json/jsonl file should be a list of dictionary. Each element in the list should have\n",
      "                the key \"prompt\", \"pos_answers\", and \"neg_answers\". Each \"pos_answer\" must correspond to\n",
      "                one and only one \"neg_answer\" (i.e., they are one-to-one pairs). Defaults to None.\n",
      "            dataset_builder (Optional[Callable[[], List[Dict]]], optional): Alternative to dataset_path.\n",
      "                A callable that returns a list of dictionary. Defaults to None.\n",
      "        \"\"\"\n",
      "        self._util = util\n",
      "        seed = util.seed\n",
      "        tokenizer = util.tokenizer\n",
      "\n",
      "        self.max_pairs_per_prompt = max_pairs_per_prompt\n",
      "\n",
      "        self.rng = np.random.RandomState(seed=seed)\n",
      "        data = data_api.load_shuffle_split_dataset(util, dataset_path, dataset_builder)\n",
      "\n",
      "        prompts = [x[\"prompt\"] for x in data]\n",
      "        self.ids = [x[\"id\"] for x in data]\n",
      "\n",
      "        pos_answers = [\n",
      "            [x[\"prompt\"] + c + tokenizer.eos_token for c in x[\"pos_answers\"]]\n",
      "            for x in data\n",
      "        ]\n",
      "        neg_answers = [\n",
      "            [x[\"prompt\"] + c + tokenizer.eos_token for c in x[\"neg_answers\"]]\n",
      "            for x in data\n",
      "        ]\n",
      "\n",
      "        for a, b in zip(pos_answers, neg_answers):\n",
      "            if len(a) != len(b):\n",
      "                raise RuntimeError(\n",
      "                    \"pos_answers and neg_answers must be one-to-one pairs.\"\n",
      "                )\n",
      "            if len(a) == 0:\n",
      "                raise RuntimeError(\"pos_answers and neg_answers must be non-empty.\")\n",
      "\n",
      "        group_sizes = [len(x) for x in pos_answers]\n",
      "\n",
      "        self.prompt_tokens = tokenizer(\n",
      "            prompts,\n",
      "            max_length=max_length,\n",
      "            truncation=True,\n",
      "            padding=False,\n",
      "            return_length=True,\n",
      "        )\n",
      "        _pos_answer_tokens = tokenizer(\n",
      "            list(itertools.chain.from_iterable(pos_answers)),\n",
      "            max_length=max_length,\n",
      "            padding=False,\n",
      "            truncation=True,\n",
      "            return_length=True,\n",
      "        )\n",
      "        _neg_answer_tokens = tokenizer(\n",
      "            list(itertools.chain.from_iterable(neg_answers)),\n",
      "            max_length=max_length,\n",
      "            padding=False,\n",
      "            truncation=True,\n",
      "            return_length=True,\n",
      "        )\n",
      "\n",
      "        pos_answer_tokens = []\n",
      "        neg_answer_tokens = []\n",
      "        offset = 0\n",
      "        for g in group_sizes:\n",
      "            pos_answer_tokens.append(\n",
      "                {k: v[offset : offset + g] for k, v in _pos_answer_tokens.items()}\n",
      "            )\n",
      "            neg_answer_tokens.append(\n",
      "                {k: v[offset : offset + g] for k, v in _neg_answer_tokens.items()}\n",
      "            )\n",
      "            offset += g\n",
      "\n",
      "        self.pos_answer_tokens: List[Dict[str, List[int]]] = pos_answer_tokens\n",
      "        self.neg_answer_tokens: List[Dict[str, List[int]]] = neg_answer_tokens\n",
      "        assert (\n",
      "            len(self.prompt_tokens[\"input_ids\"])\n",
      "            == len(self.pos_answer_tokens)\n",
      "            == len(self.neg_answer_tokens)\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def util(self):\n",
      "        return self._util\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.pos_answer_tokens)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        # Sample a piece of data composed of a single prompt\n",
      "        # and a group of pos-neg answer pairs.\n",
      "\n",
      "        prompt_len = self.prompt_tokens[\"length\"][idx]\n",
      "        n_pairs_this_prompt = len(self.pos_answer_tokens[idx][\"input_ids\"])\n",
      "        # Randomly select a maximum number of `self.max_pairs_per_prompt` pairs for this prompt\n",
      "        group_size = min(self.max_pairs_per_prompt, n_pairs_this_prompt)\n",
      "        pair_indices = self.rng.choice(n_pairs_this_prompt, group_size, replace=False)\n",
      "\n",
      "        packed_input_ids = []\n",
      "        input_lens = []\n",
      "        for i in pair_indices:\n",
      "            packed_input_ids += self.pos_answer_tokens[idx][\"input_ids\"][i]\n",
      "            packed_input_ids += self.neg_answer_tokens[idx][\"input_ids\"][i]\n",
      "            input_lens += [len(self.pos_answer_tokens[idx][\"input_ids\"][i])]\n",
      "            input_lens += [len(self.neg_answer_tokens[idx][\"input_ids\"][i])]\n",
      "\n",
      "        data = dict(\n",
      "            packed_input_ids=torch.tensor(packed_input_ids, dtype=torch.long),\n",
      "            prompt_lens=torch.tensor([prompt_len], dtype=torch.int32),\n",
      "        )\n",
      "\n",
      "        x = data_api.SequenceSample(\n",
      "            keys=[\"packed_input_ids\", \"prompt_lens\"],\n",
      "            data=data,\n",
      "            dtypes=dict(\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_lens=torch.int32,\n",
      "            ),\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_lens=(),\n",
      "            ),\n",
      "            ids=[self.ids[idx]],\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=[input_lens],\n",
      "                prompt_lens=[[1]],\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        return x\n",
      "\n",
      "\n",
      "data_api.register_dataset(\"rw_pair\", RewardModelingPairedDataset)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/dataset/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "from realhf.base.importing import import_module\n",
      "\n",
      "# Import all dataset implementations.\n",
      "_p = re.compile(r\"^(?!.*__init__).*\\.py$\")\n",
      "_filepath = os.path.dirname(__file__)\n",
      "import_module(_filepath, _p)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import functools\n",
      "import os\n",
      "import re\n",
      "\n",
      "import torch\n",
      "\n",
      "# Import all HuggingFace model implementations.\n",
      "import realhf.api.from_hf\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.core.model_api import HF_MODEL_FAMILY_REGISTRY\n",
      "from realhf.base.importing import import_module\n",
      "from realhf.base.pkg_version import is_available, is_version_less\n",
      "from realhf.impl.model.conversion.hf_registry import HFModelRegistry\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "\n",
      "logger = logging.getLogger(\"model init\")\n",
      "\n",
      "# Import all model implementations.\n",
      "_p = re.compile(r\"^(?!.*__init__).*\\.py$\")\n",
      "_filepath = os.path.dirname(__file__)\n",
      "import_module(os.path.join(_filepath, \"interface\"), _p)\n",
      "import_module(os.path.join(_filepath, \"nn\"), _p)\n",
      "\n",
      "# NOTE: skip importing vLLM for now to avoid an\n",
      "# \"invalid device context\" issue for the 25.01 image\n",
      "if is_available(\"vllm\") and is_version_less(\"vllm\", \"0.6.4\"):\n",
      "    import realhf.impl.model.backend.vllm\n",
      "\n",
      "import realhf.impl.model.backend.inference\n",
      "import realhf.impl.model.backend.megatron\n",
      "import realhf.impl.model.backend.mock_train\n",
      "import realhf.impl.model.backend.sglang\n",
      "\n",
      "# Set PyTorch JIT options, following Megatron-LM.\n",
      "if torch.cuda.is_available():\n",
      "    torch._C._jit_set_profiling_executor(True)\n",
      "    torch._C._jit_set_profiling_mode(True)\n",
      "    torch._C._jit_override_can_fuse_on_cpu(False)\n",
      "    torch._C._jit_override_can_fuse_on_gpu(False)\n",
      "    torch._C._jit_set_texpr_fuser_enabled(False)\n",
      "    # torch._C._jit_set_nvfuser_enabled(True)  # disable the deprecated warning\n",
      "    torch._C._debug_set_autodiff_subgraph_inlining(False)\n",
      "\n",
      "# Add HuggingFace hooks to ReaLModel.\n",
      "_HF_REGISTRIES = {}\n",
      "\n",
      "\n",
      "def _load_from_hf(\n",
      "    model: ReaLModel, registry_name, load_dir: str, init_critic_from_actor: bool\n",
      "):\n",
      "    r = _HF_REGISTRIES[registry_name]\n",
      "    return r.load(model, load_dir, init_critic_from_actor)\n",
      "\n",
      "\n",
      "def _save_to_hf(model: ReaLModel, registry_name, tokenizer, save_dir: str):\n",
      "    r = _HF_REGISTRIES[registry_name]\n",
      "    r.save(model, tokenizer, save_dir)\n",
      "\n",
      "\n",
      "def _config_from_hf(registry_name, hf_config=None, model_path=None, is_critic=False):\n",
      "    r = _HF_REGISTRIES[registry_name]\n",
      "    return r.config_from_hf(hf_config, model_path, is_critic)\n",
      "\n",
      "\n",
      "def _config_to_hf(registry_name, config):\n",
      "    r = _HF_REGISTRIES[registry_name]\n",
      "    return r.config_to_hf(config)\n",
      "\n",
      "\n",
      "def _make_real_config(registry_name):\n",
      "    r = _HF_REGISTRIES[registry_name]\n",
      "    if r.real_config_maker is not None:\n",
      "        return r.real_config_maker()\n",
      "    raise NotImplementedError(\n",
      "        f\"`real_config_maker` not implemented for {registry_name}. \"\n",
      "        f\"Please implement and register `real_config_maker` \"\n",
      "        f\"in realhf.api.from_hf.{registry_name} to make customized ReaLModelConfig.\"\n",
      "    )\n",
      "\n",
      "\n",
      "for name, helpers in HF_MODEL_FAMILY_REGISTRY.items():\n",
      "    _HF_REGISTRIES[name] = r = HFModelRegistry(**helpers)\n",
      "\n",
      "    _load_from_hf_ = functools.partialmethod(_load_from_hf, name)\n",
      "    setattr(ReaLModel, f\"from_{name}\", _load_from_hf_)\n",
      "\n",
      "    _save_to_hf_ = functools.partialmethod(_save_to_hf, name)\n",
      "    setattr(ReaLModel, f\"to_{name}\", _save_to_hf_)\n",
      "\n",
      "    _config_from_hf_ = functools.partial(_config_from_hf, name)\n",
      "    setattr(ReaLModel, f\"config_from_{name}\", staticmethod(_config_from_hf_))\n",
      "\n",
      "    _config_to_hf_ = functools.partial(_config_to_hf, name)\n",
      "    setattr(ReaLModel, f\"config_to_{name}\", staticmethod(_config_to_hf_))\n",
      "\n",
      "    # make a ReaLModelConfig from only parameters related to model size, used for testing\n",
      "    _make_real_config_ = functools.partial(_make_real_config, name)\n",
      "    setattr(ReaLModel, f\"make_{name}_config\", staticmethod(_make_real_config_))\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/comm/global_comm.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import itertools\n",
      "import os\n",
      "import socket\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "import torch.distributed\n",
      "\n",
      "from realhf.api.core.config import ModelName, ModelShardID\n",
      "from realhf.base import constants, gpu_utils, name_resolve, names, network, topology\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class NCCLProcessGroupInfo:\n",
      "    world_size: int\n",
      "    global_rank: int\n",
      "    local_gpu_id: int\n",
      "    # 3D parallelism groups of each model.\n",
      "    model_groups: Dict[str, torch.distributed.ProcessGroup]\n",
      "\n",
      "\n",
      "def filter_match_mwids(\n",
      "    model_name: ModelName,\n",
      "    topo: topology.ProcessTopology,\n",
      "    msid2mwid: Dict[ModelShardID, int],\n",
      "    **conditions,\n",
      ") -> List[int]:\n",
      "    if len(conditions) == 0:\n",
      "        mwids_this_model = [\n",
      "            msid2mwid[ModelShardID.from_parallelism_rank(model_name, topo, j)]\n",
      "            for j in range(topo.world_size())\n",
      "        ]\n",
      "    else:\n",
      "        mwids_this_model = [\n",
      "            msid2mwid[ModelShardID.from_parallelism_rank(model_name, topo, j)]\n",
      "            for j in topo.filter_match(**conditions)\n",
      "        ]\n",
      "    mwids_this_model = sorted(mwids_this_model)\n",
      "    assert len(set(mwids_this_model)) == len(mwids_this_model)\n",
      "    return list(mwids_this_model)\n",
      "\n",
      "\n",
      "def setup_global_comm(\n",
      "    expr_name: str,\n",
      "    trial_name: str,\n",
      "    worker_index: int,\n",
      "    model_topos: Optional[Dict[str, topology.ProcessTopology]] = None,\n",
      "    msid2mwid: Optional[Dict[ModelShardID, int]] = None,\n",
      "    backend: str = \"nccl\",\n",
      ") -> NCCLProcessGroupInfo:\n",
      "    peers: List[int] = list(\n",
      "        sorted(\n",
      "            map(\n",
      "                int,\n",
      "                name_resolve.get_subtree(\n",
      "                    names.distributed_peer(\n",
      "                        expr_name,\n",
      "                        trial_name,\n",
      "                        gpu_utils.GLOBAL_PROCESS_GROUP_NAME,\n",
      "                    )\n",
      "                ),\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    assert len(peers) == len(set(peers)), f\"Duplicated trainer worker index. {peers}\"\n",
      "    world_size = len(peers)\n",
      "    global_rank = peers.index(worker_index)\n",
      "\n",
      "    mw_ranks = {}\n",
      "    if model_topos is not None:\n",
      "        assert msid2mwid is not None\n",
      "        for model_name, topo in model_topos.items():\n",
      "            mw_ranks[model_name] = filter_match_mwids(model_name, topo, msid2mwid)\n",
      "\n",
      "    if (\n",
      "        \"GPU_DEVICES_ISOLATED\" not in os.environ\n",
      "        and \"RAY\" not in os.environ[\"REAL_MODE\"]\n",
      "        and constants.use_cuda()\n",
      "    ):\n",
      "        raise RuntimeError(\n",
      "            \"GPU devices not isolated in slurm or local mode. This should not happen.\"\n",
      "        )\n",
      "\n",
      "    if constants.use_cuda():\n",
      "        assert len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")) == 1, os.environ[\n",
      "            \"CUDA_VISIBLE_DEVICES\"\n",
      "        ]\n",
      "        local_gpu_id = int(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
      "    else:\n",
      "        local_gpu_id = global_rank\n",
      "\n",
      "    pg_master_name = names.distributed_master(\n",
      "        expr_name, trial_name, gpu_utils.GLOBAL_PROCESS_GROUP_NAME\n",
      "    )\n",
      "\n",
      "    if worker_index == 0:\n",
      "        host_ip = socket.gethostbyname(socket.gethostname())\n",
      "        port = network.find_free_port(experiment_name=expr_name, trial_name=trial_name)\n",
      "        pg_init_addr = f\"tcp://{host_ip}:{port}\"\n",
      "        name_resolve.add(pg_master_name, pg_init_addr, keepalive_ttl=300)\n",
      "    else:\n",
      "        try:\n",
      "            pg_init_addr = name_resolve.wait(pg_master_name, timeout=300)\n",
      "        except TimeoutError:\n",
      "            raise TimeoutError(\n",
      "                f\"global_rank={global_rank} worker_index={worker_index} wait for process group init timeout.\"\n",
      "            )\n",
      "\n",
      "    if not constants.use_cuda():\n",
      "        backend = \"gloo\"\n",
      "    torch_dist_kwargs = dict(\n",
      "        world_size=world_size,\n",
      "        rank=global_rank,\n",
      "        init_method=pg_init_addr,\n",
      "        backend=backend,\n",
      "        timeout=constants.NCCL_DEFAULT_TIMEOUT,\n",
      "    )\n",
      "    if constants.use_cuda():\n",
      "        torch.cuda.set_device(\n",
      "            0\n",
      "        )  # initialize CUDA here with only a single visible device\n",
      "    # This environment variable is used by DeepSpeed.\n",
      "    os.environ[\"LOCAL_RANK\"] = \"0\"\n",
      "\n",
      "    if not torch.distributed.is_initialized():\n",
      "        torch.distributed.init_process_group(\n",
      "            **torch_dist_kwargs, group_name=gpu_utils.GLOBAL_PROCESS_GROUP_NAME\n",
      "        )\n",
      "\n",
      "    model_groups = {}\n",
      "    for model_name, ranks in mw_ranks.items():\n",
      "        model_groups[model_name] = topology.new_or_get_group(ranks, backend=backend)\n",
      "        constants.set_parallelism_group(model_name, model_groups[model_name], ranks)\n",
      "        cpu_group = topology.new_or_get_group(ranks, backend=\"gloo\")\n",
      "        constants.set_cpu_parallelism_group(model_name, cpu_group)\n",
      "\n",
      "    self_group = None\n",
      "    for i in range(world_size):\n",
      "        group = topology.new_or_get_group([i], backend=backend)\n",
      "        if i == global_rank:\n",
      "            self_group = group\n",
      "            constants.set_self_group(self_group)\n",
      "\n",
      "    # logger.info(f\"Setup process group finishes for worker_index={worker_index}\")\n",
      "\n",
      "    return NCCLProcessGroupInfo(\n",
      "        world_size=world_size,\n",
      "        global_rank=global_rank,\n",
      "        local_gpu_id=local_gpu_id,\n",
      "        model_groups=model_groups,\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/comm/param_realloc.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import itertools\n",
      "from collections import defaultdict\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "import scipy.optimize\n",
      "import torch.distributed\n",
      "import torch.nn as nn\n",
      "\n",
      "from realhf.api.core import model_api\n",
      "from realhf.api.core.config import ModelName, ModelShardID\n",
      "from realhf.base import constants, topology\n",
      "from realhf.impl.model.comm.global_comm import filter_match_mwids\n",
      "from realhf.impl.model.nn.flatten_param import (\n",
      "    ContiguousParamSpec,\n",
      "    build_param_spec,\n",
      "    param_intervals_from_keys,\n",
      "    param_size_from_keys,\n",
      ")\n",
      "from realhf.impl.model.nn.real_llm_base import keys_from_layer_indices\n",
      "from realhf.impl.model.nn.real_llm_parallel import (\n",
      "    partition_pipeline_layers,\n",
      "    pipeline_repartition_strategy,\n",
      ")\n",
      "\n",
      "_TRAINABLE: Dict[ModelName, bool] = {}\n",
      "\n",
      "\n",
      "def set_trainable(model_name: ModelName, trainable: bool):\n",
      "    _TRAINABLE[model_name] = trainable\n",
      "\n",
      "\n",
      "def is_trainable(model_name: ModelName) -> bool:\n",
      "    return _TRAINABLE[model_name]\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(unsafe_hash=True)\n",
      "class ParamReallocPair:\n",
      "    src: ModelName\n",
      "    src_dp_rank: int\n",
      "    src_tp_rank: int\n",
      "    src_pp_rank: int\n",
      "    dst: ModelName\n",
      "    dst_tp_rank: int\n",
      "    dst_pp_rank: int\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(unsafe_hash=True)\n",
      "class ParamReallocModelPair:\n",
      "    src: ModelName\n",
      "    dst: ModelName\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ParamReallocInfo:\n",
      "    # Groups for parameter synchronization.\n",
      "    param_realloc_model_group: Dict[\n",
      "        ParamReallocModelPair, torch.distributed.ProcessGroup\n",
      "    ]\n",
      "    param_realloc_model_cpu_group: Dict[\n",
      "        ParamReallocModelPair, torch.distributed.ProcessGroup\n",
      "    ]\n",
      "    param_realloc_groups: Dict[ParamReallocPair, torch.distributed.ProcessGroup]\n",
      "    param_realloc_src_ranks: Dict[ParamReallocPair, int]\n",
      "    param_realloc_dst_ranks: Dict[ParamReallocPair, List[int]]\n",
      "\n",
      "\n",
      "def _max_match(_src_ranks: List[int], _grouped_dst_ranks: List[List[int]]):\n",
      "    cost_matrix = []\n",
      "    for source in _src_ranks:\n",
      "        costs = []\n",
      "        for destinations in _grouped_dst_ranks:\n",
      "            cost = 0 if source in destinations else 1\n",
      "            costs.append(cost)\n",
      "        cost_matrix.append(costs)\n",
      "\n",
      "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_matrix)\n",
      "    return row_ind, col_ind\n",
      "\n",
      "\n",
      "def _group_mwids_by_node(ranks: List[int]) -> Dict[int, List[int]]:\n",
      "    node2ranks = defaultdict(list)\n",
      "    for r in ranks:\n",
      "        node2ranks[r // 8].append(r)\n",
      "    return {k: node2ranks[k] for k in sorted(node2ranks.keys())}\n",
      "\n",
      "\n",
      "def _squeeze_mwids_by_node(ranks: List[int]) -> List[int]:\n",
      "    node2ranks = _group_mwids_by_node(ranks)\n",
      "    return [ranks[0] for ranks in node2ranks.values()]\n",
      "\n",
      "\n",
      "def _assign_src_to_dsts(\n",
      "    node2srcs: Dict[int, List[int]], node2dsts: Dict[int, List[int]]\n",
      ") -> Dict[int, List[int]]:\n",
      "    \"\"\"Assign nodes with a greedy algorithm.\n",
      "\n",
      "    All ranks in the values of node2srcs have the data required by all dst ranks.\n",
      "    Source ranks can be assigned to zero or multiple destination ranks.\n",
      "    All destination ranks must be assigned to exactly one src rank.\n",
      "\n",
      "    Args:\n",
      "        node2srcs (Dict[int, List[int]]): Node index -> source ranks.\n",
      "        node2dsts (Dict[int, List[int]]): Node index -> destination ranks.\n",
      "\n",
      "    Returns:\n",
      "        Dict[int, List[int]]: src rank -> dst ranks.\n",
      "    \"\"\"\n",
      "    # First, assign all destination ranks to source nodes.\n",
      "    # If a destination node is also a source node, assign it to itself.\n",
      "    # Otherwise, find the node with the minimum workload for load balancing.\n",
      "    dst_src_nodes = {}\n",
      "    for dst_node in node2dsts.keys():\n",
      "        if dst_node in node2srcs:\n",
      "            # dst node is also src node\n",
      "            dst_src_nodes[dst_node] = dst_node\n",
      "    src_node_workloads = {k: int(k in dst_src_nodes) for k in node2srcs}\n",
      "    assert sum(src_node_workloads.values()) == len(dst_src_nodes)\n",
      "    for dst_node in node2dsts.keys():\n",
      "        if dst_node not in node2srcs:\n",
      "            # find a source node with the minimum workload\n",
      "            src_node = min(src_node_workloads, key=src_node_workloads.get)\n",
      "            dst_src_nodes[dst_node] = src_node\n",
      "    assert all(dst_node in dst_src_nodes for dst_node in node2dsts)\n",
      "\n",
      "    # Revert the key-value of the dict.\n",
      "    src_dst_nodes = defaultdict(list)\n",
      "    for dst_node, src_node in dst_src_nodes.items():\n",
      "        src_dst_nodes[src_node].append(dst_node)\n",
      "\n",
      "    # Next, find an appropriate source rank on each source node.\n",
      "    # If the source rank is also the destination rank, assign it to the destination rank.\n",
      "    # Otherwise, assign the first one to the destination ranks.\n",
      "    assignment = {}\n",
      "    for src_node, dst_nodes in src_dst_nodes.items():\n",
      "        assigned = False\n",
      "        src_ranks = node2srcs[src_node]\n",
      "        dst_ranks = sum([node2dsts[dst_node] for dst_node in dst_nodes], start=[])\n",
      "        for s in src_ranks:\n",
      "            if s in dst_ranks:\n",
      "                assignment[s] = dst_ranks\n",
      "                assigned = True\n",
      "                break\n",
      "        if not assigned:\n",
      "            assignment[src_ranks[0]] = dst_ranks\n",
      "    assert len(set(assignment.keys())) == len(assignment.keys())\n",
      "    assert len(set(sum(assignment.values(), []))) == len(sum(assignment.values(), []))\n",
      "\n",
      "    return assignment\n",
      "\n",
      "\n",
      "def _create_param_realloc_groups(\n",
      "    from_topo: topology.ProcessTopology,\n",
      "    to_topo: topology.ProcessTopology,\n",
      "    src: ModelName,\n",
      "    dst: ModelName,\n",
      "    msid2mwid: Dict[ModelShardID, int],\n",
      "    param_realloc_groups: Dict[ParamReallocPair, torch.distributed.ProcessGroup],\n",
      "    param_realloc_src_ranks: Dict[ParamReallocPair, int],\n",
      "    param_realloc_dst_ranks: Dict[ParamReallocPair, List[int]],\n",
      "):\n",
      "    mwid2msid: Dict[int, Dict[ModelName, ModelShardID]] = defaultdict(dict)\n",
      "    for k, v in msid2mwid.items():\n",
      "        mwid2msid[v][k.model_name] = k\n",
      "    for pp_i, pp_j in itertools.product(\n",
      "        range(from_topo.get_dim(\"pipe\")), range(to_topo.get_dim(\"pipe\"))\n",
      "    ):\n",
      "        # create tensor reshard groups\n",
      "        src_tp_size = from_topo.get_dim(\"tensor\")\n",
      "        dst_tp_size = to_topo.get_dim(\"tensor\")\n",
      "\n",
      "        for tp_j in range(dst_tp_size):\n",
      "            _all_dst_ranks = filter_match_mwids(\n",
      "                dst, to_topo, msid2mwid, pipe=pp_j, tensor=tp_j\n",
      "            )\n",
      "            if src_tp_size > dst_tp_size:\n",
      "                factor = src_tp_size // dst_tp_size\n",
      "                tp_is = list(range(factor * tp_j, factor * (tp_j + 1)))\n",
      "                _all_src_ranks = [\n",
      "                    filter_match_mwids(\n",
      "                        src, from_topo, msid2mwid, tensor=tp_i, pipe=pp_i\n",
      "                    )\n",
      "                    for tp_i in tp_is\n",
      "                ]\n",
      "            else:\n",
      "                factor = dst_tp_size // src_tp_size\n",
      "                _all_src_ranks = [\n",
      "                    filter_match_mwids(\n",
      "                        src,\n",
      "                        from_topo,\n",
      "                        msid2mwid,\n",
      "                        tensor=tp_j // factor,\n",
      "                        pipe=pp_i,\n",
      "                    )\n",
      "                ]\n",
      "            # All GPUs in _src_ranks have the data required by (pp_j, tp_j)\n",
      "            for _src_ranks in _all_src_ranks:\n",
      "                # NOTE: inter-node communication cost is significantly larger than intra-node communication cost.\n",
      "                # We only select one sender per host/node to prevent multiple senders occupying the same network bandwidth.\n",
      "                # This is not the optimal solution for intra-node communication\n",
      "                # because there may exist a source rank that is also dst rank,\n",
      "                # but we forcely select the first source rank on each node here.\n",
      "                assignment = _assign_src_to_dsts(\n",
      "                    _group_mwids_by_node(_src_ranks),\n",
      "                    _group_mwids_by_node(_all_dst_ranks),\n",
      "                )\n",
      "                _idle_src_ranks = [r for r in _src_ranks if r not in assignment]\n",
      "                for _src_rank in _idle_src_ranks:\n",
      "                    dp_i, tp_i = (\n",
      "                        from_topo.get_coord(\n",
      "                            mwid2msid[_src_rank][src].parallelism_rank\n",
      "                        ).data,\n",
      "                        from_topo.get_coord(\n",
      "                            mwid2msid[_src_rank][src].parallelism_rank\n",
      "                        ).tensor,\n",
      "                    )\n",
      "                    key = ParamReallocPair(\n",
      "                        src=src,\n",
      "                        src_dp_rank=dp_i,\n",
      "                        src_tp_rank=tp_i,\n",
      "                        src_pp_rank=pp_i,\n",
      "                        dst=dst,\n",
      "                        dst_tp_rank=tp_j,\n",
      "                        dst_pp_rank=pp_j,\n",
      "                    )\n",
      "                    param_realloc_dst_ranks[key] = []\n",
      "                    param_realloc_groups[key] = None\n",
      "                    param_realloc_src_ranks[key] = _src_rank\n",
      "                for _src_rank, _dst_ranks in assignment.items():\n",
      "                    dp_i, tp_i = (\n",
      "                        from_topo.get_coord(\n",
      "                            mwid2msid[_src_rank][src].parallelism_rank\n",
      "                        ).data,\n",
      "                        from_topo.get_coord(\n",
      "                            mwid2msid[_src_rank][src].parallelism_rank\n",
      "                        ).tensor,\n",
      "                    )\n",
      "                    key = ParamReallocPair(\n",
      "                        src=src,\n",
      "                        src_dp_rank=dp_i,\n",
      "                        src_tp_rank=tp_i,\n",
      "                        src_pp_rank=pp_i,\n",
      "                        dst=dst,\n",
      "                        dst_tp_rank=tp_j,\n",
      "                        dst_pp_rank=pp_j,\n",
      "                    )\n",
      "                    param_realloc_dst_ranks[key] = _dst_ranks\n",
      "                    if _src_rank not in _dst_ranks:\n",
      "                        _dst_ranks = [_src_rank] + _dst_ranks\n",
      "                    assert len(set(_dst_ranks)) == len(_dst_ranks)\n",
      "                    if len(_dst_ranks) > 1:\n",
      "                        if torch.distributed.is_initialized():\n",
      "                            param_realloc_groups[key] = topology.new_or_get_group(\n",
      "                                _dst_ranks\n",
      "                            )\n",
      "                        else:\n",
      "                            # for estimating parameter realloc cost\n",
      "                            param_realloc_groups[key] = 1\n",
      "                    else:\n",
      "                        param_realloc_groups[key] = None\n",
      "                    param_realloc_src_ranks[key] = _src_rank\n",
      "\n",
      "\n",
      "def setup_param_realloc(\n",
      "    model_topos: Optional[Dict[str, topology.ProcessTopology]] = None,\n",
      "    msid2mwid: Optional[Dict[ModelShardID, int]] = None,\n",
      "    param_realloc_pairs: Optional[List[Tuple[ModelName, ModelName]]] = None,\n",
      ") -> ParamReallocInfo:\n",
      "    param_realloc_groups = {}\n",
      "    param_realloc_src_ranks = {}\n",
      "    param_realloc_dst_ranks = {}\n",
      "    param_realloc_model_group = {}\n",
      "    param_realloc_model_cpu_group = {}\n",
      "    if param_realloc_pairs is not None:\n",
      "        for src, dst in param_realloc_pairs:\n",
      "            _create_param_realloc_groups(\n",
      "                model_topos[src],\n",
      "                model_topos[dst],\n",
      "                src,\n",
      "                dst,\n",
      "                msid2mwid,\n",
      "                param_realloc_groups,\n",
      "                param_realloc_src_ranks,\n",
      "                param_realloc_dst_ranks,\n",
      "            )\n",
      "            pair_mw_ranks = set()\n",
      "            topo1 = model_topos[src]\n",
      "            topo2 = model_topos[dst]\n",
      "            for i in range(topo1.world_size()):\n",
      "                pair_mw_ranks.add(\n",
      "                    msid2mwid[ModelShardID.from_parallelism_rank(src, topo1, i)]\n",
      "                )\n",
      "            for j in range(topo2.world_size()):\n",
      "                pair_mw_ranks.add(\n",
      "                    msid2mwid[ModelShardID.from_parallelism_rank(dst, topo2, j)]\n",
      "                )\n",
      "            param_realloc_model_group[ParamReallocModelPair(src, dst)] = (\n",
      "                topology.new_or_get_group(list(sorted(pair_mw_ranks)))\n",
      "            )\n",
      "            param_realloc_model_cpu_group[ParamReallocModelPair(src, dst)] = (\n",
      "                topology.new_or_get_group(list(sorted(pair_mw_ranks)), backend=\"gloo\")\n",
      "            )\n",
      "    return ParamReallocInfo(\n",
      "        param_realloc_groups=param_realloc_groups,\n",
      "        param_realloc_src_ranks=param_realloc_src_ranks,\n",
      "        param_realloc_dst_ranks=param_realloc_dst_ranks,\n",
      "        param_realloc_model_group=param_realloc_model_group,\n",
      "        param_realloc_model_cpu_group=param_realloc_model_cpu_group,\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ReparallelizeSenderStep:\n",
      "    rank: int\n",
      "    sender_tp_portion_id: int\n",
      "    receiver_tp_portion_id: int\n",
      "    param_keys: List[str]\n",
      "    param_intervals_cpu: List[Tuple[int, int]]\n",
      "    param_intervals_cuda: torch.Tensor\n",
      "    max_interval_size: int\n",
      "    param_size: int\n",
      "    group: torch.distributed.ProcessGroup\n",
      "    dst_ranks: List[int]\n",
      "    remove: bool = False\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ReparallelizeReceiverStep:\n",
      "    rank: int\n",
      "    sender_tp_portion_id: int\n",
      "    receiver_tp_portion_id: int\n",
      "    sender_param_intervals_cpu: List[Tuple[int, int]]\n",
      "    sender_param_intervals_cuda: torch.Tensor\n",
      "    sender_max_interval_size: int\n",
      "    receiver_param_intervals_cpu: List[Tuple[int, int]]\n",
      "    receiver_param_intervals_cuda: torch.Tensor\n",
      "    receiver_max_interval_size: int\n",
      "    param_size: int\n",
      "    param_keys: List[str]\n",
      "    param_dtype: torch.dtype\n",
      "    src: int\n",
      "    dst_ranks: List[int]\n",
      "    group: torch.distributed.ProcessGroup\n",
      "\n",
      "\n",
      "def _derive_reparallelize_comm_plan(\n",
      "    from_model_name: ModelName,\n",
      "    to_model_name: ModelName,\n",
      "    from_topo: topology.ProcessTopology,\n",
      "    to_topo: topology.ProcessTopology,\n",
      "    from_model_config: model_api.ReaLModelConfig,\n",
      "    to_model_config: model_api.ReaLModelConfig,\n",
      "    pg_info: ParamReallocInfo,\n",
      "    dtype: Optional[torch.dtype] = torch.float16,\n",
      ") -> List[ReparallelizeReceiverStep | ReparallelizeSenderStep]:\n",
      "    src_tp_size = from_topo.get_dim(\"tensor\")\n",
      "    dst_tp_size = to_topo.get_dim(\"tensor\")\n",
      "    assert src_tp_size % dst_tp_size == 0 or dst_tp_size % src_tp_size == 0\n",
      "    for k, v in dataclasses.asdict(to_model_config).items():\n",
      "        if k not in [\"is_critic\"] and v != getattr(from_model_config, k):\n",
      "            raise ValueError(\n",
      "                f\"Can't load a checkpoint with different config (key `{k}`, \"\n",
      "                f\"value in checkpoint is `{v}`, current value is `{getattr(from_model_config, k)}`).\"\n",
      "            )\n",
      "    if from_model_config.n_kv_heads > 1 and (\n",
      "        from_model_config.n_kv_heads % src_tp_size == 0\n",
      "    ) != (from_model_config.n_kv_heads % dst_tp_size == 0):\n",
      "        raise ValueError(\"Whether to partition kv heads should remain the same.\")\n",
      "\n",
      "    from_layer_mapping = partition_pipeline_layers(\n",
      "        from_model_config,\n",
      "        from_topo.get_dim(\"pipe\"),\n",
      "    )\n",
      "    from_layer_mapping = {\n",
      "        k: list(range(v[0], v[1])) for k, v in from_layer_mapping.items()\n",
      "    }\n",
      "    to_layer_mapping = partition_pipeline_layers(\n",
      "        to_model_config,\n",
      "        to_topo.get_dim(\"pipe\"),\n",
      "    )\n",
      "    to_layer_mapping = {k: list(range(v[0], v[1])) for k, v in to_layer_mapping.items()}\n",
      "    repart_strat = pipeline_repartition_strategy(from_layer_mapping, to_layer_mapping)\n",
      "\n",
      "    from_model_head_param_point_to_embedding = (\n",
      "        from_model_config.tied_embedding\n",
      "        and not from_model_config.is_critic\n",
      "        and from_topo.get_dim(\"pipe\") == 1\n",
      "    )\n",
      "    to_model_head_param_point_to_embedding = (\n",
      "        to_model_config.tied_embedding\n",
      "        and not to_model_config.is_critic\n",
      "        and to_topo.get_dim(\"pipe\") == 1\n",
      "    )\n",
      "    if constants.has_model_name(from_model_name):\n",
      "        with constants.model_scope(from_model_name):\n",
      "            from_layer_indices = from_layer_mapping[constants.pipe_parallel_rank()]\n",
      "            from_model_param_specs, _ = build_param_spec(\n",
      "                from_layer_indices,\n",
      "                from_model_config,\n",
      "                tp_size=from_topo.get_dim(\"tensor\"),\n",
      "                dp_size=from_topo.get_dim(\"data\"),\n",
      "                pp_size=from_topo.get_dim(\"pipe\"),\n",
      "                head_param_point_to_embedding=from_model_head_param_point_to_embedding,\n",
      "            )\n",
      "    if constants.has_model_name(to_model_name):\n",
      "        with constants.model_scope(to_model_name):\n",
      "            to_layer_indices = to_layer_mapping[constants.pipe_parallel_rank()]\n",
      "            to_model_param_specs, _ = build_param_spec(\n",
      "                to_layer_indices,\n",
      "                to_model_config,\n",
      "                tp_size=to_topo.get_dim(\"tensor\"),\n",
      "                pp_size=to_topo.get_dim(\"pipe\"),\n",
      "                dp_size=to_topo.get_dim(\"data\"),\n",
      "                head_param_point_to_embedding=to_model_head_param_point_to_embedding,\n",
      "            )\n",
      "\n",
      "    comm_plan = []\n",
      "\n",
      "    src_dp_size = from_topo.get_dim(\"data\")\n",
      "    src_pp_size = from_topo.get_dim(\"pipe\")\n",
      "    dst_pp_size = to_topo.get_dim(\"pipe\")\n",
      "\n",
      "    # derive a global NCCL communication plan\n",
      "    for (pp_i, pp_j), layer_indices in repart_strat.items():\n",
      "        if len(layer_indices) == 0:\n",
      "            continue\n",
      "\n",
      "        for tp_i in range(src_tp_size):\n",
      "            if dst_tp_size > src_tp_size:\n",
      "                factor = dst_tp_size // src_tp_size\n",
      "                tp_js = [i + factor * tp_i for i in range(factor)]\n",
      "                receiver_tp_portion_id = 0\n",
      "            else:\n",
      "                factor = src_tp_size // dst_tp_size\n",
      "                tp_js = [tp_i // factor]\n",
      "                receiver_tp_portion_id = tp_i % factor\n",
      "            for sender_tp_portion_id, tp_j in enumerate(tp_js):\n",
      "\n",
      "                for dp_i in range(src_dp_size):\n",
      "                    key = ParamReallocPair(\n",
      "                        src=from_model_name,\n",
      "                        src_dp_rank=dp_i,\n",
      "                        src_tp_rank=tp_i,\n",
      "                        src_pp_rank=pp_i,\n",
      "                        dst=to_model_name,\n",
      "                        dst_tp_rank=tp_j,\n",
      "                        dst_pp_rank=pp_j,\n",
      "                    )\n",
      "                    src = pg_info.param_realloc_src_ranks[key]\n",
      "                    group = pg_info.param_realloc_groups[key]\n",
      "                    dst_ranks = pg_info.param_realloc_dst_ranks[key]\n",
      "\n",
      "                    param_keys = None\n",
      "                    param_intervals_cpu = receiver_param_intervals_cpu = None\n",
      "                    param_intervals_cuda = receiver_param_intervals_cuda = None\n",
      "                    max_interval_size = max_receiver_interval_size = None\n",
      "                    param_keys = keys_from_layer_indices(\n",
      "                        from_model_config, layer_indices\n",
      "                    )\n",
      "                    param_size = param_size_from_keys(\n",
      "                        config=from_model_config,\n",
      "                        src_tp_size=src_tp_size,\n",
      "                        sd_keys=param_keys,\n",
      "                        src2dst_tp_size=max(dst_tp_size // src_tp_size, 1),\n",
      "                        src2dst_tp_rank=sender_tp_portion_id,\n",
      "                        head_param_point_to_embedding=from_model_head_param_point_to_embedding,\n",
      "                    )\n",
      "                    if torch.distributed.is_initialized():\n",
      "                        # torch.distributed is not initialized when estimating param realloc cost\n",
      "                        if torch.distributed.get_rank() == src:\n",
      "                            param_intervals_cpu = param_intervals_from_keys(\n",
      "                                model_name=from_model_name,\n",
      "                                config=from_model_config,\n",
      "                                tp_size=src_tp_size,\n",
      "                                param_spec=from_model_param_specs,\n",
      "                                sd_keys=param_keys,\n",
      "                                portion_size=max(dst_tp_size // src_tp_size, 1),\n",
      "                                portion_rank=sender_tp_portion_id,\n",
      "                                head_param_point_to_embedding=from_model_head_param_point_to_embedding,\n",
      "                            )\n",
      "                            param_intervals_cuda = torch.tensor(\n",
      "                                param_intervals_cpu,\n",
      "                                dtype=torch.long,\n",
      "                                device=constants.current_device(),\n",
      "                            )\n",
      "                            max_interval_size = max(\n",
      "                                y - x for x, y in param_intervals_cpu\n",
      "                            )\n",
      "                        if torch.distributed.get_rank() in dst_ranks:\n",
      "                            receiver_param_intervals_cpu = param_intervals_from_keys(\n",
      "                                model_name=to_model_name,\n",
      "                                config=to_model_config,\n",
      "                                tp_size=dst_tp_size,\n",
      "                                param_spec=to_model_param_specs,\n",
      "                                sd_keys=param_keys,\n",
      "                                portion_size=max(src_tp_size // dst_tp_size, 1),\n",
      "                                portion_rank=receiver_tp_portion_id,\n",
      "                                head_param_point_to_embedding=to_model_head_param_point_to_embedding,\n",
      "                            )\n",
      "                            receiver_param_intervals_cuda = torch.tensor(\n",
      "                                receiver_param_intervals_cpu,\n",
      "                                dtype=torch.long,\n",
      "                                device=constants.current_device(),\n",
      "                            )\n",
      "                            max_receiver_interval_size = max(\n",
      "                                y - x for x, y in receiver_param_intervals_cpu\n",
      "                            )\n",
      "\n",
      "                    for dst_rank in dst_ranks:\n",
      "                        comm_plan.append(\n",
      "                            ReparallelizeReceiverStep(\n",
      "                                rank=dst_rank,\n",
      "                                sender_tp_portion_id=sender_tp_portion_id,\n",
      "                                receiver_tp_portion_id=receiver_tp_portion_id,\n",
      "                                param_keys=param_keys,\n",
      "                                sender_param_intervals_cpu=param_intervals_cpu,\n",
      "                                sender_param_intervals_cuda=param_intervals_cuda,\n",
      "                                sender_max_interval_size=max_interval_size,\n",
      "                                receiver_param_intervals_cpu=receiver_param_intervals_cpu,\n",
      "                                receiver_param_intervals_cuda=receiver_param_intervals_cuda,\n",
      "                                receiver_max_interval_size=max_receiver_interval_size,\n",
      "                                param_size=param_size,\n",
      "                                param_dtype=dtype,\n",
      "                                src=src,\n",
      "                                dst_ranks=dst_ranks,\n",
      "                                group=group,\n",
      "                            )\n",
      "                        )\n",
      "                    comm_plan.append(\n",
      "                        ReparallelizeSenderStep(\n",
      "                            rank=src,\n",
      "                            sender_tp_portion_id=sender_tp_portion_id,\n",
      "                            receiver_tp_portion_id=receiver_tp_portion_id,\n",
      "                            param_keys=param_keys,\n",
      "                            param_intervals_cpu=param_intervals_cpu,\n",
      "                            param_intervals_cuda=param_intervals_cuda,\n",
      "                            max_interval_size=max_interval_size,\n",
      "                            param_size=param_size,\n",
      "                            group=group,\n",
      "                            dst_ranks=dst_ranks,\n",
      "                        )\n",
      "                    )\n",
      "    for i, step in enumerate(comm_plan):\n",
      "        if isinstance(step, ReparallelizeReceiverStep):\n",
      "            continue\n",
      "        step: ReparallelizeSenderStep\n",
      "        required_by_nex_steps = False\n",
      "        for nex_step in comm_plan[i + 1 :]:\n",
      "            if (\n",
      "                isinstance(nex_step, ReparallelizeSenderStep)\n",
      "                and nex_step.rank == step.rank\n",
      "                and nex_step.param_keys == step.param_keys\n",
      "            ):\n",
      "                required_by_nex_steps = True\n",
      "                break\n",
      "        step.remove = not required_by_nex_steps\n",
      "\n",
      "    return comm_plan\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ReparallelizeTraget:\n",
      "    comm_plan: List[Union[ReparallelizeSenderStep, ReparallelizeReceiverStep]]\n",
      "    to_param_spec: Dict[str, ContiguousParamSpec]\n",
      "    to_param_size: int\n",
      "    to_layers_handle: nn.ModuleList\n",
      "    to_layer_start_idx: int\n",
      "    to_layer_end_idx: int\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/interface/math_rw_interface.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import ast\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import html\n",
      "import json\n",
      "import os\n",
      "import re\n",
      "import time\n",
      "import xml.etree.ElementTree as ET\n",
      "from typing import Dict, List, Tuple\n",
      "\n",
      "import colorama\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "import realhf.api.core.model_api as model_api\n",
      "import realhf.base.logging as logging\n",
      "from functioncall.code.local_verify import code_verify as local_code_verify\n",
      "from functioncall.code.verify import code_verify\n",
      "from functioncall.math.verify import math_verify\n",
      "from realhf.api.core.data_api import (\n",
      "    RL_TASKS,\n",
      "    MicroBatchSpec,\n",
      "    SequenceSample,\n",
      "    load_hf_tokenizer,\n",
      ")\n",
      "from realhf.base import constants\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.dataset.math_code_dataset import load_metadata\n",
      "from realhf.impl.dataset.math_parser import parse_lines_in_parallel as math_verify_local\n",
      "\n",
      "logger = logging.getLogger(\"Packed Reward Modeling Interface\", \"benchmark\")\n",
      "\n",
      "ENABLE_FUNCTION_CALL = True if os.getenv(\"FUNCTIONCALL_SERVICE_DOMAIN\", \"\") else False\n",
      "math_verify_call = math_verify if ENABLE_FUNCTION_CALL else math_verify_local\n",
      "code_verify_call = code_verify if ENABLE_FUNCTION_CALL else local_code_verify\n",
      "\n",
      "\n",
      "class VerifierException(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "def extract_python_code(text, min_length=20, strict_syntax=True):\n",
      "    code_pattern = r\"(?i)```(?:python|py)?\\s*\\n?(.*?)\\n?```\"\n",
      "    code_blocks = re.findall(code_pattern, text, re.DOTALL)\n",
      "    valid_blocks = []\n",
      "    for block in code_blocks:\n",
      "        clean_block = block.strip()\n",
      "        if len(clean_block) < min_length:\n",
      "            continue\n",
      "\n",
      "        # verify code syntax\n",
      "        if strict_syntax:\n",
      "            try:\n",
      "                ast.parse(clean_block, mode=\"exec\")\n",
      "            except (SyntaxError, IndentationError):\n",
      "                continue\n",
      "\n",
      "        valid_blocks.append(clean_block)\n",
      "\n",
      "    if not valid_blocks:\n",
      "        # logger.warning(f\"failed to extract python code from {text}\")\n",
      "        return None\n",
      "    # return the last code block\n",
      "    return valid_blocks[-1]\n",
      "\n",
      "\n",
      "def check_with_elementtree(text):\n",
      "    def escape_between_tags(text, tags=[\"think\", \"answer\"]):\n",
      "        \"\"\".\"\"\"\n",
      "        # \n",
      "        tag_pattern = \"|\".join(tags)\n",
      "        parts = []\n",
      "        current_pos = 0\n",
      "\n",
      "        # \n",
      "        pattern = f\"</?({tag_pattern})[^>]*>\"\n",
      "\n",
      "        for match in re.finditer(pattern, text):\n",
      "            # \n",
      "            if current_pos < match.start():\n",
      "                parts.append(html.escape(text[current_pos : match.start()]))\n",
      "\n",
      "            # \n",
      "            parts.append(match.group())\n",
      "            current_pos = match.end()\n",
      "\n",
      "        # \n",
      "        if current_pos < len(text):\n",
      "            parts.append(html.escape(text[current_pos:]))\n",
      "\n",
      "        return \"\".join(parts)\n",
      "\n",
      "    text = escape_between_tags(text)\n",
      "    if not text.strip().startswith(\"<think>\"):\n",
      "        text = \"<think>\" + text\n",
      "    try:\n",
      "        xml_text = f\"<root>{text}</root>\"\n",
      "        x = ET.fromstring(xml_text)\n",
      "        if x.text is not None and x.text.strip() != \"\":\n",
      "            return False, f\"Error: extra content before <think>. {x.text}\"\n",
      "        if len(x) != 2:\n",
      "            return False, f\"Error: there are {len(x)} tags.\"\n",
      "        if x[0].tag is None or x[0].tag != \"think\":\n",
      "            return False, f\"Error: <think> tag is missing. {x[0].tag}\"\n",
      "        if x[0].tail is not None and x[0].tail.strip() != \"\":\n",
      "            return (\n",
      "                False,\n",
      "                f\"Error: extra content between <think> and <answer>. {x[0].tail}\",\n",
      "            )\n",
      "        if x[1].tag is None or x[1].tag != \"answer\":\n",
      "            return False, f\"Error: <answer> tag is missing. {x[1].tag}\"\n",
      "        if x[1].tail is not None and x[1].tail.strip() != \"\":\n",
      "            return False, f\"Error: extra content after <answer>, {x[1].tail}\"\n",
      "\n",
      "        return True, x[1].text if x[1].text is not None else \"\"\n",
      "    except ET.ParseError as e:\n",
      "        return False, f\"Error: XML, {str(e)}\"\n",
      "\n",
      "\n",
      "id2info = {}\n",
      "\n",
      "\n",
      "def dispatch_reward_calculation(task, answers, query_id_strs) -> List:\n",
      "    global id2info\n",
      "    assert len(answers) == len(query_id_strs)\n",
      "    format_rewards = []\n",
      "    if task == \"math\" or task == \"stem\":\n",
      "        format_rewards = math_verify_call(id2info, answers, query_id_strs)\n",
      "    elif task == \"code\":\n",
      "        codes = [extract_python_code(_answer) for _answer in answers]\n",
      "        format_rewards = code_verify_call(id2info, codes, query_id_strs)\n",
      "    assert len(format_rewards) == len(answers), (\n",
      "        task,\n",
      "        len(format_rewards),\n",
      "        len(answers),\n",
      "        answers,\n",
      "    )\n",
      "    return format_rewards\n",
      "\n",
      "\n",
      "def retokenize_and_verify(\n",
      "    task,\n",
      "    tokenizer,\n",
      "    prompt_ids: List[List[int]],\n",
      "    seq_ids: List[List[int]],\n",
      "    query_ids: List[str],\n",
      "    check_xml_format=False,\n",
      "):\n",
      "    seq_strs = tokenizer.batch_decode(\n",
      "        seq_ids, clean_up_tokenization_spaces=False, skip_special_tokens=True\n",
      "    )\n",
      "    prompt_strs = tokenizer.batch_decode(\n",
      "        prompt_ids, clean_up_tokenization_spaces=False, skip_special_tokens=True\n",
      "    )\n",
      "    # query_id_strs = query_ids\n",
      "    query_id_strs = [query_id.split(\"@\")[0] for query_id in query_ids]\n",
      "\n",
      "    answers = [\n",
      "        seq_str.split(prompt_str)[1]\n",
      "        for seq_str, prompt_str in zip(seq_strs, prompt_strs)\n",
      "    ]\n",
      "\n",
      "    format_rewards = dispatch_reward_calculation(task, answers, query_id_strs)\n",
      "\n",
      "    if check_xml_format:\n",
      "        for idx, answer in enumerate(answers):\n",
      "            xml_reward, _ = check_with_elementtree(answer)\n",
      "            if xml_reward == 1 and format_rewards[idx] == 0:\n",
      "                format_rewards[idx] = -0.8\n",
      "            elif xml_reward == 0 and format_rewards[idx] == 0:\n",
      "                format_rewards[idx] = -1\n",
      "\n",
      "    return format_rewards, prompt_strs, seq_strs\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MultiTaskRewardInterface(model_api.ModelInterface):\n",
      "    dataset_path: str = \"\"\n",
      "    tokenizer_path: str = \"/storage/models/Qwen__Qwen2.5-1.5B\"\n",
      "    output_scaling: float = 1.0\n",
      "    output_bias: float = 0.0\n",
      "    rw_type: str = \"sparse\"\n",
      "    check_xml_format: bool = False\n",
      "    group_size: int = 1\n",
      "    check_verifier_status: bool = False\n",
      "\n",
      "    def __post_init__(self):\n",
      "        global id2info\n",
      "        id2info, _ = load_metadata(self.dataset_path)\n",
      "        self.tokenizer = load_hf_tokenizer(self.tokenizer_path)\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.info(f\"output_scaling: {self.output_scaling}\")\n",
      "            logger.info(f\"output_bias: {self.output_bias}\")\n",
      "            logger.info(f\"rw_type: {self.rw_type}\")\n",
      "\n",
      "    def _dispatch_tasks(self, data: SequenceSample) -> Tuple[Dict, Dict]:\n",
      "        xs = data.unpack()\n",
      "        dispatched = {}\n",
      "        dispatched_indices = {}\n",
      "        for task_idx, task_name in enumerate(RL_TASKS):\n",
      "            indices = (\n",
      "                (data.data[\"task_ids\"] == task_idx).cpu().numpy().nonzero()[0].tolist()\n",
      "            )\n",
      "            if len(indices) > 0:\n",
      "                dispatched[task_name] = SequenceSample.gather([xs[i] for i in indices])\n",
      "                dispatched_indices[task_name] = indices\n",
      "\n",
      "        return dispatched, dispatched_indices\n",
      "\n",
      "    def _gather_tasks(\n",
      "        self, results: Dict, dispatched_indices: Dict, bs: int\n",
      "    ) -> SequenceSample:\n",
      "        xs = [None for _ in range(bs)]\n",
      "        for task_name, indices in dispatched_indices.items():\n",
      "            xxs = results[task_name].unpack()\n",
      "            assert len(indices) == len(xxs), (len(indices), len(xxs))\n",
      "            for i, xx in zip(indices, xxs):\n",
      "                xs[i] = xx\n",
      "        assert all(xs)\n",
      "        return SequenceSample.gather(xs)\n",
      "\n",
      "    def _dispatch_tp_and_pp(self, data: SequenceSample):\n",
      "        tp_pp_size = constants.tp_and_pp_world_size()\n",
      "        if tp_pp_size == 1:\n",
      "            return data, None\n",
      "        splitted, _, backward_indices = data.split(\n",
      "            mb_spec=MicroBatchSpec(n_mbs=tp_pp_size)\n",
      "        )\n",
      "        tp_pp_rank = constants.tp_and_pp_rank()\n",
      "        print(\"dispatched batch size\", [s.bs for s in splitted], flush=True)\n",
      "        return splitted[tp_pp_rank], backward_indices\n",
      "\n",
      "    def _gather_tp_and_pp(self, input_, data: SequenceSample, backward_indices):\n",
      "        tp_pp_size = constants.tp_and_pp_world_size()\n",
      "        if tp_pp_size == 1:\n",
      "            return data\n",
      "        local_rank = constants.grid().topo.get_rank(\n",
      "            data=constants.data_parallel_rank(),\n",
      "            tensor=0,\n",
      "            pipe=constants.pipe_parallel_world_size() - 1,\n",
      "        )\n",
      "        dst = constants.to_global_pg_rank(local_rank)\n",
      "        gather_list = None\n",
      "        if dist.get_rank() == dst:\n",
      "            gather_list = [None for _ in range(tp_pp_size)]\n",
      "        x = data.data[\"rewards\"].cpu().numpy().tolist()\n",
      "        print(x, flush=True)\n",
      "        dist.gather_object(\n",
      "            x, gather_list, dst=dst, group=constants.tp_and_pp_cpu_group()\n",
      "        )\n",
      "        if dist.get_rank() != dst:\n",
      "            return None\n",
      "        gathered = np.array(gather_list).reshape(-1, self.group_size)\n",
      "        assert len(gathered) == len(backward_indices)\n",
      "        rewards = (\n",
      "            np.concatenate([gathered[i] for i in backward_indices]).flatten().tolist()\n",
      "        )\n",
      "        return SequenceSample(\n",
      "            keys=[\"rewards\"],\n",
      "            trailing_shapes=dict(rewards=()),\n",
      "            dtypes=dict(rewards=torch.float32),\n",
      "            ids=input_.ids,\n",
      "            seqlens=dict(\n",
      "                rewards=[[1 for _ in range(self.group_size)] for _ in range(input_.bs)],\n",
      "            ),\n",
      "            data=dict(rewards=torch.tensor(rewards, dtype=torch.float32)),\n",
      "        )\n",
      "\n",
      "    def calculate_task_reward(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        data: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        task_type: str,\n",
      "    ):\n",
      "        # mb_spec is disrespected here\n",
      "        packed_input_ids: torch.Tensor = data.data[\"packed_input_ids\"]\n",
      "        input_seqlens = flat2d(data.seqlens[\"packed_input_ids\"])\n",
      "        seq_ids = []\n",
      "        offset = 0\n",
      "        for slen in input_seqlens:\n",
      "            seq_ids.append(\n",
      "                packed_input_ids[offset : offset + slen].cpu().numpy().tolist()\n",
      "            )\n",
      "            offset += slen\n",
      "        assert offset == packed_input_ids.shape[0], (offset, packed_input_ids.shape)\n",
      "        prompt_input_ids = data.data[\"packed_prompts\"]\n",
      "        prompt_len = flat2d(data.seqlens[\"packed_prompts\"])\n",
      "        prompt_ids = []\n",
      "        offset = 0\n",
      "        for slen in prompt_len:\n",
      "            p = prompt_input_ids[offset : offset + slen].cpu().numpy().tolist()\n",
      "            prompt_ids += [p] * self.group_size\n",
      "            offset += slen\n",
      "        format_rewards, prompt_strs, seq_strs = retokenize_and_verify(\n",
      "            task_type,\n",
      "            self.tokenizer,\n",
      "            prompt_ids=prompt_ids,\n",
      "            seq_ids=seq_ids,\n",
      "            query_ids=[\n",
      "                str(data_id) for data_id in data.ids for _ in range(self.group_size)\n",
      "            ],\n",
      "            check_xml_format=self.check_xml_format,\n",
      "        )\n",
      "        scores = torch.FloatTensor(format_rewards).to(packed_input_ids.device)\n",
      "        scores[scores == 0] = -1\n",
      "\n",
      "        scores = (\n",
      "            scores.to(packed_input_ids.device) - self.output_bias\n",
      "        ) * self.output_scaling\n",
      "\n",
      "        self.log_rewards_to_file(task_type, model, prompt_strs, seq_strs, scores)\n",
      "\n",
      "        res = SequenceSample(\n",
      "            keys=[\"rewards\"],\n",
      "            trailing_shapes=dict(rewards=()),\n",
      "            dtypes=dict(rewards=torch.float32),\n",
      "            ids=data.ids,\n",
      "            seqlens=dict(\n",
      "                rewards=[\n",
      "                    [1 for _ in range(len(x))] for x in data.seqlens[\"packed_input_ids\"]\n",
      "                ],\n",
      "            ),\n",
      "            data=dict(rewards=scores),\n",
      "        )\n",
      "\n",
      "        # record rewards for each piece of data\n",
      "        avg_scores = []\n",
      "        offset = 0\n",
      "        for i in range(data.bs):\n",
      "            score_lis = scores[\n",
      "                offset : offset + len(data.seqlens[\"packed_input_ids\"][i])\n",
      "            ]\n",
      "            avg_scores.append(score_lis.mean().item())\n",
      "            offset += len(data.seqlens[\"packed_input_ids\"][i])\n",
      "        assert offset == sum(len(x) for x in data.seqlens[\"packed_input_ids\"])\n",
      "\n",
      "        res.metadata[\"scores\"] = avg_scores\n",
      "\n",
      "        if self.check_verifier_status:\n",
      "            avg_score = torch.tensor(\n",
      "                np.mean(avg_scores), device=constants.current_device()\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                avg_score, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "            avg_score /= constants.data_parallel_group()\n",
      "            avg_score = avg_score.item()\n",
      "            minimal_score = (-1 - self.output_bias) * self.output_scaling\n",
      "\n",
      "            if avg_score <= minimal_score or np.isclose(avg_score, minimal_score):\n",
      "                raise VerifierException(\n",
      "                    \"All rewards are at minimal value. Probably there are something wrong with the verifier!\"\n",
      "                )\n",
      "        return res\n",
      "\n",
      "    def log_rewards_to_file(\n",
      "        self, task_type: str, model: model_api.Model, prompt_strs, seq_strs, scores\n",
      "    ):\n",
      "        tik = time.perf_counter()\n",
      "        gen_file_path = os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"generated\",\n",
      "            task_type,\n",
      "            f\"v{model.version.global_step}r{dist.get_rank()}.txt\",\n",
      "        )\n",
      "\n",
      "        os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "        with open(gen_file_path, \"w\") as _f:\n",
      "            for idx, (score, prompt_str, seq_str) in enumerate(\n",
      "                zip(scores, prompt_strs, seq_strs)\n",
      "            ):\n",
      "                info = \"\\n\".join(\n",
      "                    [\n",
      "                        f\"idx: {idx} / {len(scores)}\",\n",
      "                        f\"reward is {score.item()}, prompt is {colorama.Fore.YELLOW + colorama.Style.DIM}{prompt_str}{colorama.Style.RESET_ALL}\",\n",
      "                        f\"sequence is: {colorama.Fore.YELLOW + colorama.Style.DIM}{seq_str.split(prompt_str)[1]}{colorama.Style.RESET_ALL}.\",\n",
      "                    ]\n",
      "                )\n",
      "                _f.write(info + \"\\n\")\n",
      "\n",
      "        gen_file_path = os.path.join(\n",
      "            constants.LOG_ROOT,\n",
      "            constants.experiment_name(),\n",
      "            constants.trial_name(),\n",
      "            \"generated_jsonl\",\n",
      "            task_type,\n",
      "            f\"v{model.version.global_step}r{dist.get_rank()}.jsonl\",\n",
      "        )\n",
      "        os.makedirs(os.path.dirname(gen_file_path), exist_ok=True)\n",
      "        with open(gen_file_path, \"w\") as _f:\n",
      "            for idx, (score, prompt_str, seq_str) in enumerate(\n",
      "                zip(scores, prompt_strs, seq_strs)\n",
      "            ):\n",
      "                _f.write(\n",
      "                    json.dumps(\n",
      "                        {\n",
      "                            \"prompt\": prompt_str,\n",
      "                            \"generated\": seq_str.split(prompt_str)[1],\n",
      "                            \"reward\": score.item(),\n",
      "                        },\n",
      "                        ensure_ascii=False,\n",
      "                    )\n",
      "                    + \"\\n\"\n",
      "                )\n",
      "\n",
      "        logger.info(f\"[{task_type}] number of samples: {len(scores)}, {scores.shape}\")\n",
      "        logger.info(f\"[{task_type}] avg reward: {sum(scores) / len(scores)}\")\n",
      "        logger.info(f\"[{task_type}] log to file time: {time.perf_counter()- tik:.2f}s\")\n",
      "\n",
      "    def inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        data: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample | None:\n",
      "        input_ = data\n",
      "        data, backward_indices = self._dispatch_tp_and_pp(data)\n",
      "        task_data, dispatch_indices = self._dispatch_tasks(data)\n",
      "\n",
      "        assert self.rw_type == \"sparse\"\n",
      "\n",
      "        def _task_func(func, task_type: str):\n",
      "            def _wrapped_func(*args, **kwargs):\n",
      "                start_time = time.perf_counter()\n",
      "                try:\n",
      "                    result = func(*args, **kwargs)\n",
      "                except Exception as e:\n",
      "                    raise asyncio.CancelledError(\n",
      "                        f\"[{task_type}] task failed: {e}\"\n",
      "                    ) from e\n",
      "                finally:\n",
      "                    duration = time.perf_counter() - start_time\n",
      "                    logger.info(f\"[{task_type}] time cost: {duration:.4f}s\")\n",
      "                return task_type, result\n",
      "\n",
      "            return _wrapped_func\n",
      "\n",
      "        async def _run_tasks():\n",
      "            tasks = []\n",
      "            for task_type, d in task_data.items():\n",
      "                task_func = _task_func(self.calculate_task_reward, task_type)\n",
      "                task_args = (model, d, mb_spec, task_type)\n",
      "                task = asyncio.create_task(asyncio.to_thread(task_func, *task_args))\n",
      "                tasks.append(task)\n",
      "\n",
      "            results = await asyncio.gather(*tasks)\n",
      "            task_results = {}\n",
      "            for res in results:\n",
      "                task_type, result = res\n",
      "                task_results[task_type] = result\n",
      "\n",
      "            return task_results\n",
      "\n",
      "        def run_in_thread():\n",
      "            # Create a new event loop for this thread\n",
      "            new_loop = asyncio.new_event_loop()\n",
      "            asyncio.set_event_loop(new_loop)\n",
      "            try:\n",
      "                return new_loop.run_until_complete(_run_tasks())\n",
      "            finally:\n",
      "                new_loop.close()\n",
      "\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "\n",
      "        with ThreadPoolExecutor() as executor:\n",
      "            future = executor.submit(run_in_thread)\n",
      "            task_results = future.result()\n",
      "        final_result = self._gather_tasks(task_results, dispatch_indices, data.bs)\n",
      "        final_result = self._gather_tp_and_pp(input_, final_result, backward_indices)\n",
      "\n",
      "        model.inc_version()\n",
      "\n",
      "        return final_result\n",
      "\n",
      "    def _mock_inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        data: SequenceSample,\n",
      "    ) -> SequenceSample:\n",
      "\n",
      "        prompt_lens = flat2d(data.seqlens[\"packed_prompts\"])\n",
      "        task_ids = data.data[\"task_ids\"].cpu().numpy().tolist()\n",
      "        seqlens = []\n",
      "        offset = 0\n",
      "        seq = []\n",
      "        for plen, task_id in zip(prompt_lens, task_ids):\n",
      "            seq += [data.data[\"packed_prompts\"][offset : offset + plen]]\n",
      "            offset += plen\n",
      "            if task_id == RL_TASKS.index(\"math\"):\n",
      "                answer_str = (\n",
      "                    \"something unimportant but the answer is \\\\boxed{-\\\\frac{2}{3}}.\"\n",
      "                )\n",
      "            elif task_id == RL_TASKS.index(\"code\"):\n",
      "                answer_str = (\n",
      "                    \"```python\\ninput()\\nimport time\\ntime.sleep(1e-3)\\nprint(1)\\n```\"\n",
      "                )\n",
      "            else:\n",
      "                answer_str = \"something unimportant\"\n",
      "            encoding = model.tokenizer(\n",
      "                [answer_str], add_special_tokens=True, return_attention_mask=False\n",
      "            )\n",
      "\n",
      "            ans = torch.tensor(encoding[\"input_ids\"], dtype=torch.long).flatten()\n",
      "            seq += [ans]\n",
      "            seqlens.append(plen + len(ans))\n",
      "\n",
      "        x = SequenceSample.from_default(\n",
      "            seqlens=seqlens,\n",
      "            ids=data.ids,\n",
      "            data=dict(packed_input_ids=torch.cat(seq)),\n",
      "        )\n",
      "        data.update_(x)\n",
      "        return data\n",
      "\n",
      "\n",
      "model_api.register_interface(\"rw-math-code\", MultiTaskRewardInterface)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/interface/fused_interface.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import time\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import Dict\n",
      "\n",
      "import torch\n",
      "\n",
      "import realhf.api.core.model_api as model_api\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.core.config import ModelInterfaceAbstraction\n",
      "from realhf.api.core.data_api import RL_TASKS, MicroBatchSpec, SequenceSample\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class FusedThreadingForwardInterface(model_api.ModelInterface):\n",
      "\n",
      "    def __init__(self, interfaces: Dict[str, ModelInterfaceAbstraction]):\n",
      "        self.interfaces = {\n",
      "            key: model_api.make_interface(interface)\n",
      "            for key, interface in interfaces.items()\n",
      "        }\n",
      "\n",
      "    def run_interface(\n",
      "        self,\n",
      "        interface_name: str,\n",
      "        model,\n",
      "        data,\n",
      "        mb_spec,\n",
      "    ) -> SequenceSample | None:\n",
      "        tik = time.perf_counter()\n",
      "        res = self.interfaces[interface_name].inference(model, data, mb_spec)\n",
      "        t = time.perf_counter() - tik\n",
      "        logger.info(f\"Interface {interface_name} cost {t} s\")\n",
      "        return res\n",
      "\n",
      "    def inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        data: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample | None:\n",
      "        with ThreadPoolExecutor(max_workers=len(self.interfaces)) as executor:\n",
      "            tasks = []\n",
      "            for interface_name in self.interfaces:\n",
      "                task = executor.submit(\n",
      "                    self.run_interface, interface_name, model, data, mb_spec\n",
      "                )\n",
      "                tasks.append(task)\n",
      "\n",
      "            final_result = None\n",
      "            for task in as_completed(tasks):\n",
      "                res = task.result()\n",
      "                if res is None:\n",
      "                    continue\n",
      "                if final_result is None:\n",
      "                    final_result = res\n",
      "                else:\n",
      "                    final_result.update_(res)\n",
      "\n",
      "        return final_result\n",
      "\n",
      "\n",
      "model_api.register_interface(\"fused-threading\", FusedThreadingForwardInterface)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/interface/sft_interface.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "from typing import Dict, List, Literal\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.utils.data\n",
      "import tqdm\n",
      "\n",
      "import realhf.api.core.model_api as model_api\n",
      "from realhf.api.core.data_api import MicroBatchSpec, SequenceSample\n",
      "from realhf.base import constants, stats_tracker\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.utils.functional import (\n",
      "    build_shift_one_indices,\n",
      "    gather_packed_shifted_log_probs,\n",
      ")\n",
      "\n",
      "\n",
      "def compute_packed_sft_loss(\n",
      "    logits: torch.Tensor,\n",
      "    input_: SequenceSample,\n",
      ") -> torch.Tensor:\n",
      "    packed_input_ids: torch.Tensor = input_.data[\"packed_input_ids\"]\n",
      "    input_lens = torch.tensor(flat2d(input_.seqlens[\"packed_input_ids\"]))\n",
      "    cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "    prompt_mask = input_.data[\"prompt_mask\"]\n",
      "\n",
      "    shift_one_indices = build_shift_one_indices(logits, cu_seqlens)\n",
      "    logprobs = gather_packed_shifted_log_probs(\n",
      "        logits, cu_seqlens, packed_input_ids\n",
      "    ).float()\n",
      "    prompt_mask = prompt_mask[shift_one_indices]\n",
      "    logprobs = torch.where(prompt_mask, 0, logprobs)\n",
      "\n",
      "    loss = -logprobs.sum() / prompt_mask.logical_not().count_nonzero()\n",
      "\n",
      "    with torch.no_grad():\n",
      "        seqlogp = torch.zeros(\n",
      "            cu_seqlens.shape[0] - 1, device=logits.device, dtype=torch.float64\n",
      "        )\n",
      "        for i in range(cu_seqlens.shape[0] - 1):\n",
      "            m = prompt_mask[cu_seqlens[i] - i : cu_seqlens[i + 1] - i - 1]\n",
      "            logp = logprobs[cu_seqlens[i] - i : cu_seqlens[i + 1] - i - 1]\n",
      "            assert cu_seqlens[i + 1] - i - 1 <= logprobs.shape[0], (\n",
      "                cu_seqlens,\n",
      "                logprobs.shape,\n",
      "            )\n",
      "            seqlogp[i] = torch.where(m, 0.0, logp.detach()).sum() / (\n",
      "                m.numel() - m.count_nonzero()\n",
      "            )\n",
      "\n",
      "    ## Loggin stats\n",
      "    stats_tracker.denominator(\n",
      "        n_seqs=torch.ones(\n",
      "            cu_seqlens.shape[0] - 1, dtype=torch.bool, device=logprobs.device\n",
      "        ),\n",
      "        n_tokens=torch.ones(logits.shape[0], dtype=torch.bool, device=logits.device),\n",
      "        n_valid_tokens=prompt_mask.logical_not(),\n",
      "        prompt_tokens=prompt_mask,\n",
      "    )\n",
      "    stats_tracker.stat(ppl=(-seqlogp).exp().float(), denominator=\"n_seqs\")\n",
      "    stats_tracker.stat(loss=-logprobs.detach(), denominator=\"n_valid_tokens\")\n",
      "    vocab_min_logits = logits.detach().min(-1).values.float()\n",
      "    vocab_max_logits = logits.detach().max(-1).values.float()\n",
      "    dist.all_reduce(\n",
      "        vocab_min_logits, group=constants.tensor_parallel_group(), op=dist.ReduceOp.MIN\n",
      "    )\n",
      "    dist.all_reduce(\n",
      "        vocab_max_logits, group=constants.tensor_parallel_group(), op=dist.ReduceOp.MAX\n",
      "    )\n",
      "    stats_tracker.stat(\n",
      "        vocab_min_logits=vocab_min_logits,\n",
      "        vocab_max_logits=vocab_max_logits,\n",
      "        denominator=\"n_tokens\",\n",
      "    )\n",
      "\n",
      "    return loss\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class SFTInterface(model_api.ModelInterface):\n",
      "    token_normalize_scope: Literal[\"global\", \"dp\"] = \"global\"\n",
      "\n",
      "    def train_step(\n",
      "        self, model: model_api.Model, data: SequenceSample, mb_spec: MicroBatchSpec\n",
      "    ) -> Dict | List[Dict]:\n",
      "        module = model.module\n",
      "\n",
      "        module.train()\n",
      "\n",
      "        with stats_tracker.scope(\"sft\"):\n",
      "            stats = module.train_batch(\n",
      "                input_=data,\n",
      "                loss_fn=compute_packed_sft_loss,\n",
      "                loss_weight_fn=lambda x: x.data[\"prompt_mask\"]\n",
      "                .logical_not()\n",
      "                .count_nonzero(),\n",
      "                token_normalize_scope=self.token_normalize_scope,\n",
      "                mb_spec=mb_spec,\n",
      "                version_steps=model.version.global_step,\n",
      "            )\n",
      "            stats_tracker.scalar(**stats)\n",
      "\n",
      "        model.inc_version()\n",
      "\n",
      "        return stats_tracker.export()\n",
      "\n",
      "    def save(self, model: model_api.Model, save_dir: str):\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        module.save_to_hf(\n",
      "            tokenizer=model.tokenizer,\n",
      "            save_dir=save_dir,\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def evaluate(\n",
      "        self,\n",
      "        model_: model_api.Model,\n",
      "        eval_dataloader: torch.utils.data.DataLoader,\n",
      "    ) -> Dict:\n",
      "        device = model_.device\n",
      "        module = model_.module\n",
      "\n",
      "        module.eval()\n",
      "\n",
      "        for step, x in enumerate(tqdm.tqdm(eval_dataloader)):\n",
      "            x: SequenceSample\n",
      "\n",
      "            with stats_tracker.scope(\"sft-eval\"):\n",
      "                module.eval_batch(\n",
      "                    input_=x.to_device(device),\n",
      "                    loss_fn=compute_packed_sft_loss,\n",
      "                    mb_spec=MicroBatchSpec(),\n",
      "                )\n",
      "\n",
      "        return stats_tracker.export()\n",
      "\n",
      "\n",
      "model_api.register_interface(\"sft\", SFTInterface)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/interface/ppo_interface.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "from typing import Dict, List, Literal, Optional\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "import realhf.api.core.model_api as model_api\n",
      "import realhf.impl.model.utils.ppo_functional as ppo_functional\n",
      "from realhf.api.core.data_api import (\n",
      "    RL_TASKS,\n",
      "    MicroBatchSpec,\n",
      "    SequenceSample,\n",
      "    SequenceSplitSpec,\n",
      ")\n",
      "from realhf.base import constants, logging, stats_tracker\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.dataset.math_parser import parse_lines_in_parallel\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.nn.real_llm_generate import concat_prompt_to_generation_output\n",
      "from realhf.impl.model.utils.functional import (\n",
      "    build_leave_one_indices,\n",
      "    gather_packed_shifted_log_probs,\n",
      "    masked_normalization,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"PackedPPOInterface\")\n",
      "\n",
      "\n",
      "def get_score(prompt_ids, generated, query_ids, tokenizer):\n",
      "    prompt_strs = tokenizer.batch_decode(\n",
      "        prompt_ids, clean_up_tokenization_spaces=False, skip_special_tokens=True\n",
      "    )\n",
      "    seq_strs = tokenizer.batch_decode(\n",
      "        generated, clean_up_tokenization_spaces=False, skip_special_tokens=True\n",
      "    )\n",
      "    query_id_strs = [query_id.split(\"@\")[0] for query_id in query_ids]\n",
      "    return parse_lines_in_parallel(seq_strs, query_id_strs)\n",
      "\n",
      "\n",
      "def topk(scores, gen_lengths, k) -> list:\n",
      "    indexed = list(enumerate(zip(scores, gen_lengths)))\n",
      "\n",
      "    sorted_indices = sorted(indexed, key=lambda x: (x[1][0], x[1][1]), reverse=True)[:k]\n",
      "\n",
      "    return [idx for idx, _ in sorted_indices]\n",
      "\n",
      "\n",
      "@torch.compile\n",
      "@torch.no_grad()\n",
      "def calc_entropy(logits, cu_seqlens):\n",
      "    leave_one_indices = build_leave_one_indices(logits, cu_seqlens)\n",
      "    probs = torch.nn.functional.softmax(logits.detach(), dim=-1)\n",
      "    entropy = -torch.sum(probs * torch.log(probs + 1e-7), dim=-1)[leave_one_indices]\n",
      "    return entropy\n",
      "\n",
      "\n",
      "def _ppo_actor_loss_from_model_outputs(\n",
      "    logits: torch.FloatTensor,  # [tot_seqlen, vocab_size]\n",
      "    input_: SequenceSample,\n",
      "    kl_adapter: ppo_functional.KLController,  # const\n",
      "    eps_clip: float,  # const\n",
      "    c_clip: float | None,\n",
      "    behav_imp_weight_cap: float | None,\n",
      "    early_stop_imp_ratio: Optional[float],  # const\n",
      "    early_stop_kl: Optional[float],  # const\n",
      "    temperature: Optional[float] = 1,\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"Loss function for ppo actor step, all inputs should be splitted into\n",
      "    pipeline micro batches, returns loss and logging stats.\"\"\"\n",
      "    packed_input_ids = input_.data[\"packed_input_ids\"]\n",
      "    cu_seqlens = (\n",
      "        torch.nn.functional.pad(\n",
      "            torch.tensor(flat2d(input_.seqlens[\"packed_input_ids\"])).cumsum(0),\n",
      "            (1, 0),\n",
      "        )\n",
      "        .int()\n",
      "        .to(logits.device)\n",
      "    )\n",
      "    ppo_loss_mask = input_.data[\"ppo_loss_mask\"]\n",
      "    advantages = input_.data[\"advantages\"]\n",
      "    old_logp = input_.data[\"old_logp\"]\n",
      "    kl_rewards = input_.data[\"kl_rewards\"]\n",
      "\n",
      "    if temperature is not None:\n",
      "        logits /= temperature\n",
      "    logprobs = gather_packed_shifted_log_probs(\n",
      "        logits, cu_seqlens, packed_input_ids\n",
      "    ).float()\n",
      "    loss, ppo_stat = ppo_functional.actor_loss_fn(\n",
      "        logprobs=logprobs,\n",
      "        old_logprobs=old_logp,\n",
      "        advantages=advantages,\n",
      "        eps_clip=eps_clip,\n",
      "        loss_mask=ppo_loss_mask,\n",
      "        c_clip=c_clip,\n",
      "        proximal_logprobs=input_.data.get(\"prox_logp\", None),\n",
      "        behav_imp_weight_cap=behav_imp_weight_cap,\n",
      "    )\n",
      "\n",
      "    entropy = calc_entropy(logits=logits, cu_seqlens=cu_seqlens)\n",
      "\n",
      "    # Log training statistics\n",
      "    stats_tracker.denominator(\n",
      "        n_tokens=torch.ones(logits.shape[0], dtype=torch.bool, device=logits.device),\n",
      "        n_valid_tokens=ppo_loss_mask.bool(),\n",
      "        clipped_tokens=ppo_stat[\"clip_mask\"],\n",
      "        dual_clipped_tokens=ppo_stat[\"dual_clip_mask\"],\n",
      "    )\n",
      "\n",
      "    stats_tracker.stat(\n",
      "        importance_weight=ppo_stat[\"importance_weight\"],\n",
      "        approx_kl=ppo_stat[\"approx_kl\"],\n",
      "        new_logp=logprobs.detach(),\n",
      "        old_logp=old_logp,\n",
      "        entropy=entropy.float(),\n",
      "        actor_loss=ppo_stat[\"loss\"],\n",
      "        clip_ratio=ppo_stat[\"clip_mask\"].float(),\n",
      "        dual_clip_ratio=ppo_stat[\"dual_clip_mask\"].float(),\n",
      "        denominator=\"n_valid_tokens\",\n",
      "    )\n",
      "    if \"behave_imp_weight\" in ppo_stat:\n",
      "        stats_tracker.denominator(unclipped_behave_tokens=ppo_stat[\"behave_mask\"])\n",
      "        stats_tracker.stat(\n",
      "            behave_imp_weight=ppo_stat[\"behave_imp_weight\"],\n",
      "            behave_approx_kl=ppo_stat[\"behave_approx_kl\"],\n",
      "            denominator=\"unclipped_behave_tokens\",\n",
      "        )\n",
      "    vocab_min_logits = logits.detach().min(-1).values.float()\n",
      "    vocab_max_logits = logits.detach().max(-1).values.float()\n",
      "    dist.all_reduce(\n",
      "        vocab_min_logits, group=constants.tensor_parallel_group(), op=dist.ReduceOp.MIN\n",
      "    )\n",
      "    dist.all_reduce(\n",
      "        vocab_max_logits, group=constants.tensor_parallel_group(), op=dist.ReduceOp.MAX\n",
      "    )\n",
      "    stats_tracker.stat(\n",
      "        vocab_min_logits=vocab_min_logits,\n",
      "        vocab_max_logits=vocab_max_logits,\n",
      "        denominator=\"n_tokens\",\n",
      "    )\n",
      "\n",
      "    clip_mask = ppo_stat[\"clip_mask\"]\n",
      "    dual_clip_mask = ppo_stat[\"dual_clip_mask\"]\n",
      "    clipped_new_logp = torch.where(clip_mask, logprobs.detach(), 0.0)\n",
      "    dual_clipped_new_logp = torch.where(dual_clip_mask, logprobs.detach(), 0.0)\n",
      "    clipped_old_logp = torch.where(clip_mask, old_logp, 0.0)\n",
      "    dual_clipped_old_logp = torch.where(dual_clip_mask, old_logp, 0.0)\n",
      "    stats_tracker.stat(\n",
      "        clipped_new_logp=clipped_new_logp,\n",
      "        clipped_old_logp=clipped_old_logp,\n",
      "        denominator=\"clipped_tokens\",\n",
      "    )\n",
      "    stats_tracker.stat(\n",
      "        dual_clipped_new_logp=dual_clipped_new_logp,\n",
      "        dual_clipped_old_logp=dual_clipped_old_logp,\n",
      "        denominator=\"dual_clipped_tokens\",\n",
      "    )\n",
      "\n",
      "    # Logging and early stopping according to KL (logp vs ref) or importance ratio (new logp vs old logp).\n",
      "    mean_ref_kl = (kl_rewards.detach().float() * ppo_loss_mask).sum()\n",
      "    dist.all_reduce(mean_ref_kl, group=constants.data_parallel_group())\n",
      "    _imp = (ppo_stat[\"importance_weight\"].float() * ppo_loss_mask).sum()\n",
      "    dist.all_reduce(_imp, group=constants.data_parallel_group())\n",
      "    _kl = (ppo_stat[\"approx_kl\"].float() * ppo_loss_mask).sum()\n",
      "    dist.all_reduce(_kl, group=constants.data_parallel_group())\n",
      "    _n_valid_tokens = ppo_loss_mask.count_nonzero().clone()\n",
      "    dist.all_reduce(_n_valid_tokens, group=constants.data_parallel_group())\n",
      "    mean_ref_kl /= _n_valid_tokens\n",
      "    _imp /= _n_valid_tokens\n",
      "    _kl /= _n_valid_tokens\n",
      "    # Early stopping.\n",
      "    kl_adapter.update(mean_ref_kl, n_steps=cu_seqlens.shape[0] - 1)\n",
      "    if early_stop_imp_ratio is not None and _imp > early_stop_imp_ratio:\n",
      "        logger.warning(\n",
      "            f\"Current importance ratio {_imp.item():.4f} is larger \"\n",
      "            f\"than early stop threshold {early_stop_imp_ratio}. Abandon this minibatch.\"\n",
      "        )\n",
      "        loss = loss * 0.0\n",
      "    if early_stop_kl is not None and _kl > early_stop_kl:\n",
      "        logger.warning(\n",
      "            f\"Current approximate KL divergence {_kl.item():.4f} is larger \"\n",
      "            f\"than early stop threshold {early_stop_kl}. Abort actor update.\"\n",
      "        )\n",
      "        loss = loss * 0.0\n",
      "\n",
      "    return loss\n",
      "\n",
      "\n",
      "def splited_sum_bool_tensor(t: torch.BoolTensor, chunk_size=256 * 1024 * 1024) -> int:\n",
      "    \"\"\"Sum a boolean tensor by splitting them into chunks and sum the chunks\n",
      "    separately.\n",
      "\n",
      "    to avoid memory overhead introduced by torch default sum method\n",
      "    (which will apply for a block of memory of size `8 * t.numel()`\n",
      "    bytes.)\n",
      "    \"\"\"\n",
      "    flatten = t.flatten()\n",
      "    splitted = flatten.split(chunk_size // 8, dim=0)\n",
      "    r = 0\n",
      "    for chunk in splitted:\n",
      "        r += chunk.sum()\n",
      "    return r\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PPOActorInterface(model_api.ModelInterface):\n",
      "    n_minibatches: int = 4\n",
      "\n",
      "    # Use dict here to allow argument passing through commandline.\n",
      "    generation_config: Dict = dataclasses.field(default_factory=dict)\n",
      "\n",
      "    kl_ctl: float = 0.1\n",
      "\n",
      "    adv_norm: bool = True\n",
      "    discount: float = 1.0\n",
      "    gae_lambda: float = 1.0\n",
      "\n",
      "    eps_clip: float = 0.2\n",
      "    c_clip: Optional[float] = None\n",
      "    behav_imp_weight_cap: Optional[float] = None\n",
      "    value_eps_clip: float = 0.2\n",
      "    max_reward_clip: float = 5.0\n",
      "\n",
      "    disable_value: bool = False\n",
      "\n",
      "    early_stop_kl: Optional[float] = None  # e.g. 0.1\n",
      "    early_stop_imp_ratio: Optional[float] = None  # e.g., 10.0\n",
      "\n",
      "    adaptive_kl_ctl: bool = False\n",
      "    adaptive_kl_target: Optional[float] = 6\n",
      "    adaptive_kl_horizon: Optional[float] = 10000\n",
      "\n",
      "    enable_save: bool = True\n",
      "\n",
      "    value_norm: bool = False\n",
      "    value_norm_type: str = dataclasses.field(\n",
      "        metadata={\"choices\": [\"exp\", \"ma\"]}, default=\"exp\"\n",
      "    )\n",
      "    value_norm_beta: float = 0.99995\n",
      "    value_norm_eps: float = 1e-5\n",
      "\n",
      "    group_size: int = 1\n",
      "    generation_size: Optional[int] = None\n",
      "    mask_no_eos_with_zero: bool = False\n",
      "    group_adv_norm: bool = False\n",
      "    mask_too_long: bool = False\n",
      "    use_dense_reward: bool = False\n",
      "    reward_delta: bool = True\n",
      "    token_normalize_scope: Literal[\"global\", \"dp\"] = \"global\"\n",
      "\n",
      "    sample_reuse: int = 1\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.adaptive_kl_ctl:\n",
      "            assert self.adaptive_kl_target is not None\n",
      "            assert self.adaptive_kl_horizon is not None\n",
      "            self.kl_adapter = ppo_functional.AdaptiveKLController(\n",
      "                self.kl_ctl, self.adaptive_kl_target, self.adaptive_kl_horizon\n",
      "            )\n",
      "        else:\n",
      "            self.kl_adapter = ppo_functional.FixedKLController(self.kl_ctl)\n",
      "        if self.value_norm:\n",
      "            from realhf.impl.model.modules import (\n",
      "                ExponentialRunningMeanStd,\n",
      "                MovingAverageRunningMeanStd,\n",
      "            )\n",
      "\n",
      "            if self.value_norm_type == \"exp\":\n",
      "                self.rms = ExponentialRunningMeanStd(\n",
      "                    beta=self.value_norm_beta, epsilon=self.value_norm_eps\n",
      "                )\n",
      "            elif self.value_norm_type == \"ma\":\n",
      "                self.rms = MovingAverageRunningMeanStd()\n",
      "            else:\n",
      "                raise ValueError(f\"Unknown value_norm_type {self.value_norm_type}\")\n",
      "        self.kl_ctl = None\n",
      "\n",
      "        self.gconfig = model_api.GenerationHyperparameters(**self.generation_config)\n",
      "        if self.generation_size is not None:\n",
      "            assert self.generation_size >= self.group_size\n",
      "        else:\n",
      "            self.generation_size = self.group_size\n",
      "        self.gconfig.n = self.generation_size\n",
      "\n",
      "    def save(self, model: model_api.Model, save_dir: str):\n",
      "        if not self.enable_save:\n",
      "            return\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        module.save_to_hf(\n",
      "            tokenizer=model.tokenizer,\n",
      "            save_dir=save_dir,\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample:\n",
      "        module = model.module\n",
      "\n",
      "        module.eval()\n",
      "\n",
      "        # Remap the key `packed_prompts` to `packed_input_ids`,\n",
      "        # because the pipe runner only recognizes `packed_input_ids`.\n",
      "        # x = SequenceSample.from_default(\n",
      "        #     ids=input_.ids,\n",
      "        #     seqlens=input_.seqlens[\"packed_prompts\"],\n",
      "        #     data=dict(packed_input_ids=input_.data[\"packed_prompts\"]),\n",
      "        # )\n",
      "\n",
      "        packed_input_ids = input_.data[\"packed_prompts\"]\n",
      "        new_input_ids = []\n",
      "        offset = 0\n",
      "        for x in input_.seqlens[\"packed_prompts\"]:\n",
      "            new_input_ids += [\n",
      "                packed_input_ids[offset : offset + x[0]]\n",
      "            ] * self.generation_size\n",
      "            offset += x[0]\n",
      "        assert offset == sum(x[0] for x in input_.seqlens[\"packed_prompts\"])\n",
      "\n",
      "        if model.backend_name not in [\"vllm\", \"sglang\"]:\n",
      "            # Replicate prompts\n",
      "            grouped_input = SequenceSample.from_default(\n",
      "                ids=list(range(input_.bs * self.generation_size)),\n",
      "                seqlens=[\n",
      "                    x[0]\n",
      "                    for x in input_.seqlens[\"packed_prompts\"]\n",
      "                    for _ in range(self.generation_size)\n",
      "                ],\n",
      "                data=dict(packed_input_ids=torch.cat(new_input_ids)),\n",
      "            )\n",
      "        else:\n",
      "            grouped_input = SequenceSample(\n",
      "                ids=input_.ids,\n",
      "                seqlens=dict(packed_input_ids=input_.seqlens[\"packed_prompts\"]),\n",
      "                keys=[\"packed_input_ids\"],\n",
      "                dtypes=dict(packed_input_ids=torch.long),\n",
      "                trailing_shapes=dict(packed_input_ids=()),\n",
      "                data=dict(packed_input_ids=input_.data[\"packed_prompts\"]),\n",
      "            )\n",
      "\n",
      "        res = module.generate(\n",
      "            input_=grouped_input,\n",
      "            tokenizer=model.tokenizer,\n",
      "            gconfig=self.gconfig,\n",
      "            mb_spec=mb_spec,\n",
      "        )\n",
      "        if res is None or res[0] is None:\n",
      "            return None\n",
      "\n",
      "        gen_tokens, logprobs, _ = res\n",
      "\n",
      "        pad_token_id = model.tokenizer.pad_token_id\n",
      "        eos_token_id = model.tokenizer.eos_token_id\n",
      "        seq_no_eos_mask = (gen_tokens[:, -1] != eos_token_id).logical_and(\n",
      "            gen_tokens[:, -1] != pad_token_id\n",
      "        )\n",
      "        # We also want gen_lengths to include the eos token, where the reward model outputs a score for this sequence.\n",
      "        gen_lengths = (gen_tokens != pad_token_id).logical_and(\n",
      "            gen_tokens != eos_token_id\n",
      "        ).sum(dim=-1) + 1\n",
      "        gen_lengths = gen_lengths.clip(max=gen_tokens.shape[-1])\n",
      "        input_seq_lens = [\n",
      "            x for x in input_.seqlens[\"packed_prompts\"] for _ in range(self.group_size)\n",
      "        ]\n",
      "        input_token_ids = torch.cat(new_input_ids)\n",
      "\n",
      "        if self.generation_size is not None and self.generation_size > self.group_size:\n",
      "\n",
      "            # best of k\n",
      "            query_ids = [\n",
      "                query_id for query_id in input_.ids for _ in range(self.generation_size)\n",
      "            ]\n",
      "            scores = get_score(new_input_ids, gen_tokens, query_ids, model.tokenizer)\n",
      "            input_ids_topk, gen_tokens_topk, logprobs_topk, gen_lengths_topk = (\n",
      "                [],\n",
      "                [],\n",
      "                [],\n",
      "                [],\n",
      "            )\n",
      "            for data_idx in range(0, len(gen_tokens), self.generation_size):\n",
      "                topk_idx = topk(\n",
      "                    scores[data_idx : data_idx + self.generation_size],\n",
      "                    gen_lengths[data_idx : data_idx + self.generation_size],\n",
      "                    self.group_size,\n",
      "                )\n",
      "                topk_idx = [data_idx + x for x in topk_idx]\n",
      "                gen_tokens_topk += gen_tokens[topk_idx]\n",
      "                logprobs_topk += logprobs[topk_idx]\n",
      "                gen_lengths_topk += gen_lengths[topk_idx]\n",
      "                input_ids_topk += [new_input_ids[x] for x in topk_idx]\n",
      "\n",
      "            input_token_ids = torch.cat(input_ids_topk)\n",
      "\n",
      "            gen_tokens = torch.stack(gen_tokens_topk)\n",
      "            logprobs = torch.stack(logprobs_topk)\n",
      "            gen_lengths = torch.stack(gen_lengths_topk)\n",
      "            seq_no_eos_mask = (gen_tokens[:, -1] != eos_token_id).logical_and(\n",
      "                gen_tokens[:, -1] != pad_token_id\n",
      "            )\n",
      "\n",
      "        (\n",
      "            packed_input_ids,\n",
      "            packed_logprobs,\n",
      "            _,\n",
      "            seq_lengths,\n",
      "            prompt_mask,\n",
      "        ) = concat_prompt_to_generation_output(\n",
      "            packed_prompts=input_token_ids,\n",
      "            prompt_lengths=torch.tensor(flat2d(input_seq_lens)).to(model.device),\n",
      "            gen_tokens=gen_tokens,\n",
      "            logprobs=logprobs,\n",
      "            logits_mask=None,\n",
      "            gen_lengths=gen_lengths,\n",
      "        )\n",
      "\n",
      "        # Partition generated data into groups.\n",
      "        seqlens = [\n",
      "            seq_lengths[i * self.group_size : (i + 1) * self.group_size]\n",
      "            .cpu()\n",
      "            .numpy()\n",
      "            .tolist()\n",
      "            for i in range(input_.bs)\n",
      "        ]\n",
      "\n",
      "        data = dict(\n",
      "            seq_no_eos_mask=seq_no_eos_mask,\n",
      "            packed_input_ids=packed_input_ids,\n",
      "            packed_logprobs=packed_logprobs,\n",
      "            prompt_mask=prompt_mask,\n",
      "        )\n",
      "\n",
      "        res = SequenceSample(\n",
      "            keys=[\n",
      "                \"packed_input_ids\",\n",
      "                \"prompt_mask\",\n",
      "                \"packed_logprobs\",\n",
      "                \"seq_no_eos_mask\",\n",
      "            ],\n",
      "            trailing_shapes=dict(\n",
      "                packed_input_ids=(),\n",
      "                prompt_mask=(),\n",
      "                packed_logprobs=(),\n",
      "                seq_no_eos_mask=(),\n",
      "            ),\n",
      "            dtypes=dict(\n",
      "                packed_input_ids=torch.long,\n",
      "                prompt_mask=torch.bool,\n",
      "                packed_logprobs=torch.float,\n",
      "                seq_no_eos_mask=torch.bool,\n",
      "            ),\n",
      "            seqlens=dict(\n",
      "                packed_input_ids=seqlens,\n",
      "                packed_logprobs=[[x - 1 for x in slens] for slens in seqlens],\n",
      "                prompt_mask=seqlens,\n",
      "                seq_no_eos_mask=[[1] * self.group_size for _ in seqlens],\n",
      "            ),\n",
      "            data=data,\n",
      "            ids=input_.ids,\n",
      "            prompt_mask=prompt_mask,\n",
      "        )\n",
      "\n",
      "        return res\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample:\n",
      "        module = model.module\n",
      "        module.eval()\n",
      "\n",
      "        # This post_hook will gather log probabilities in mini-batches,\n",
      "        # reducing peak memory usage.\n",
      "        def calc_logprobs(logits, input_):\n",
      "            logits /= self.gconfig.temperature\n",
      "\n",
      "            input_lens = torch.tensor(input_.seqlens[\"packed_input_ids\"]).view(-1)\n",
      "            cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "\n",
      "            logprobs = gather_packed_shifted_log_probs(\n",
      "                logits, cu_seqlens, input_.data[\"packed_input_ids\"]\n",
      "            )\n",
      "            return logprobs\n",
      "\n",
      "        #### Start Change ####\n",
      "        # input_flattend = SequenceSample.from_default(\n",
      "        #     ids=list(range(input_.bs * self.group_size)),\n",
      "        #     seqlens=flat2d(input_.seqlens[\"packed_input_ids\"]),\n",
      "        #     data=dict(packed_input_ids=input_.data[\"packed_input_ids\"]),\n",
      "        # )\n",
      "        all_seqlens_flat = flat2d(input_.seqlens[\"packed_input_ids\"])\n",
      "        num_total_turns = len(all_seqlens_flat)\n",
      "        \n",
      "        # Directly construct the flattened SequenceSample, avoiding from_default\n",
      "        input_flattend = SequenceSample(\n",
      "            keys={\"packed_input_ids\"},\n",
      "            ids=list(range(num_total_turns)),\n",
      "            seqlens={\"packed_input_ids\": [[l] for l in all_seqlens_flat]},\n",
      "            trailing_shapes={\"packed_input_ids\": input_.trailing_shapes[\"packed_input_ids\"]},\n",
      "            dtypes={\"packed_input_ids\": input_.dtypes[\"packed_input_ids\"]},\n",
      "            data={\"packed_input_ids\": input_.data[\"packed_input_ids\"]},\n",
      "        )\n",
      "\n",
      "\n",
      "        #### End Change ####\n",
      "        # add posthook to avoid storing full logits\n",
      "        logprobs = module.forward(\n",
      "            input_=input_flattend,\n",
      "            post_hook=calc_logprobs,\n",
      "            output_seqlens=[\n",
      "                [x - 1 for x in slens]\n",
      "                for slens in input_flattend.seqlens[\"packed_input_ids\"]\n",
      "            ],\n",
      "            mb_spec=mb_spec,\n",
      "        )\n",
      "\n",
      "        res = SequenceSample(\n",
      "            keys=[\"logprobs\"],\n",
      "            ids=input_.ids,\n",
      "            dtypes=dict(logprobs=model.module.dtype),\n",
      "            trailing_shapes=dict(logprobs=()),\n",
      "            data=dict(logprobs=logprobs),\n",
      "            seqlens=dict(\n",
      "                logprobs=[\n",
      "                    [x - 1 for x in slen] for slen in input_.seqlens[\"packed_input_ids\"]\n",
      "                ]\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        return res\n",
      "\n",
      "    def train_step(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> Dict | List[Dict]:\n",
      "        module = model.module\n",
      "        # We call module.eval() because dropout causes the computation of incorrect of log probs.\n",
      "        module.eval()\n",
      "\n",
      "        prompt_mask = input_.data[\"prompt_mask\"]\n",
      "        input_lens = torch.tensor(\n",
      "            flat2d(input_.seqlens[\"packed_input_ids\"]), device=model.device\n",
      "        )\n",
      "        cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "        prompt_lens = []\n",
      "        for s, e in zip(cu_seqlens[:-1], cu_seqlens[1:]):\n",
      "            prompt_lens.append(prompt_mask[s:e].sum())\n",
      "        prompt_lens = torch.tensor(prompt_lens, device=model.device)\n",
      "        reward_score = input_.data[\"rewards\"].float()\n",
      "        task_ids = input_.data[\"task_ids\"]\n",
      "        task_ids = task_ids.repeat(self.group_size, 1).transpose(0, 1).reshape(-1)\n",
      "\n",
      "        if \"dense_rewards\" in input_.data:\n",
      "            dense_reward_score = input_.data[\"dense_rewards\"].float()\n",
      "        if not self.disable_value:\n",
      "            values = input_.data[\"values\"].float()\n",
      "        else:\n",
      "            values = torch.zeros_like(\n",
      "                input_.data[\"packed_input_ids\"], dtype=torch.float32\n",
      "            )\n",
      "        seq_no_eos_mask = input_.data[\"seq_no_eos_mask\"]\n",
      "        if self.kl_adapter.value == 0:\n",
      "            ref_logp: torch.FloatTensor = reward_score.new_zeros(\n",
      "                int(input_lens.sum()) - len(input_lens)\n",
      "            )\n",
      "        else:\n",
      "            ref_logp: torch.FloatTensor = input_.data[\"packed_ref_logprobs\"].float()\n",
      "        old_logp: torch.FloatTensor = input_.data[\"packed_logprobs\"].float()\n",
      "\n",
      "        if not self.disable_value:\n",
      "            if self.value_norm:\n",
      "                denormalized_values = self.rms.denormalize(values)\n",
      "            else:\n",
      "                denormalized_values = values\n",
      "        else:\n",
      "            denormalized_values = values\n",
      "\n",
      "        for i in range(seq_no_eos_mask.shape[0]):\n",
      "            if not seq_no_eos_mask[i]:\n",
      "                # Set value at the EOS token to be zero.\n",
      "                denormalized_values[cu_seqlens[i + 1] - 1] = 0.0\n",
      "                values[cu_seqlens[i + 1] - 1] = 0.0\n",
      "\n",
      "        # Shift the loss mask by one token for each packed sequences.\n",
      "        short1cu_seqlens = cu_seqlens.clone()\n",
      "        short1cu_seqlens[1:] -= torch.ones_like(cu_seqlens[1:]).cumsum(0)\n",
      "        loss_mask = prompt_mask.logical_not()\n",
      "\n",
      "        if self.mask_too_long:\n",
      "            for i in range(seq_no_eos_mask.shape[0]):\n",
      "                if seq_no_eos_mask[i]:\n",
      "                    loss_mask[cu_seqlens[i] : cu_seqlens[i + 1]] = False\n",
      "\n",
      "        shift_one_indices = torch.cat(\n",
      "            [\n",
      "                torch.arange(\n",
      "                    cu_seqlens[i] + 1,\n",
      "                    cu_seqlens[i + 1],\n",
      "                    dtype=torch.long,\n",
      "                    device=cu_seqlens.device,\n",
      "                )\n",
      "                for i in range(cu_seqlens.shape[0] - 1)\n",
      "            ]\n",
      "        )\n",
      "        loss_mask = loss_mask[shift_one_indices]\n",
      "\n",
      "        # Apply the mask to log probabilities.\n",
      "        ref_logp *= loss_mask\n",
      "        old_logp *= loss_mask\n",
      "\n",
      "        # Compute rewards and GAEs.\n",
      "        if self.use_dense_reward:\n",
      "            kl_rewards, rewards = ppo_functional.get_packed_reward_dense(\n",
      "                kl_ctl=self.kl_adapter.value,\n",
      "                clip_reward_value=self.max_reward_clip,\n",
      "                log_probs=old_logp,\n",
      "                ref_log_probs=ref_logp,\n",
      "                dense_reward_score=dense_reward_score,\n",
      "                short1cu_seqlens=short1cu_seqlens,\n",
      "                seq_no_eos_mask=seq_no_eos_mask,\n",
      "                reward_delta=self.reward_delta,\n",
      "            )\n",
      "        else:\n",
      "            kl_rewards, rewards = ppo_functional.get_packed_rewards(\n",
      "                kl_ctl=self.kl_adapter.value,\n",
      "                clip_reward_value=self.max_reward_clip,\n",
      "                log_probs=old_logp,\n",
      "                ref_log_probs=ref_logp,\n",
      "                reward_score=(reward_score),\n",
      "                short1cu_seqlens=short1cu_seqlens,\n",
      "                seq_no_eos_mask=seq_no_eos_mask,\n",
      "                mask_no_eos_with_zero=self.mask_no_eos_with_zero,\n",
      "            )\n",
      "        advantages, returns = ppo_functional.get_packed_advantages_and_returns(\n",
      "            gamma=self.discount,\n",
      "            lam=self.gae_lambda,\n",
      "            values=(\n",
      "                denormalized_values\n",
      "                if not self.disable_value\n",
      "                else denormalized_values.new_zeros(denormalized_values.shape)\n",
      "            ),\n",
      "            rewards=rewards,\n",
      "            short1cu_seqlens=short1cu_seqlens,\n",
      "            seq_no_eos_mask=seq_no_eos_mask,\n",
      "        )\n",
      "\n",
      "        # Optionally perform normalization.\n",
      "        if self.value_norm:\n",
      "            self.rms.update(returns, mask=loss_mask)\n",
      "        if self.adv_norm:\n",
      "            if self.group_adv_norm == False:\n",
      "                advantages = masked_normalization(advantages, loss_mask)\n",
      "            else:\n",
      "                logger.info(f\"adv_shape: {advantages.shape}\")\n",
      "                logger.info(f\"prompt_mask_shape: {prompt_mask.shape}\")\n",
      "                n_samples = len(cu_seqlens) - 1\n",
      "                assert n_samples % self.group_size == 0\n",
      "                adv_list = []\n",
      "                for i in range(0, n_samples, self.group_size):\n",
      "                    for j in range(1, self.group_size):\n",
      "                        assert (\n",
      "                            prompt_mask[cu_seqlens[i] : cu_seqlens[i + 1]].sum()\n",
      "                            == prompt_mask[\n",
      "                                cu_seqlens[i + j] : cu_seqlens[i + j + 1]\n",
      "                            ].sum()\n",
      "                        )\n",
      "                    adv_list.append(\n",
      "                        masked_normalization(\n",
      "                            advantages[\n",
      "                                short1cu_seqlens[i] : short1cu_seqlens[\n",
      "                                    i + self.group_size\n",
      "                                ]\n",
      "                            ],\n",
      "                            loss_mask[\n",
      "                                short1cu_seqlens[i] : short1cu_seqlens[\n",
      "                                    i + self.group_size\n",
      "                                ]\n",
      "                            ],\n",
      "                            all_reduce=False,\n",
      "                        )\n",
      "                    )\n",
      "\n",
      "                advantages = torch.cat(adv_list, 0)\n",
      "\n",
      "        # Prepare data to be splitted into mini-batches.\n",
      "        flat_data = dict(\n",
      "            advantages=advantages,\n",
      "            old_logp=old_logp,\n",
      "            ppo_loss_mask=loss_mask,\n",
      "            packed_input_ids=input_.data[\"packed_input_ids\"],\n",
      "            kl_rewards=kl_rewards,\n",
      "        )\n",
      "        use_prox_logp = \"proximal_logprobs\" in input_.data\n",
      "        if use_prox_logp:\n",
      "            flat_data[\"prox_logp\"] = input_.data[\"proximal_logprobs\"].float()\n",
      "\n",
      "\n",
      "        #### Start Change ####\n",
      "        flat_input = SequenceSample.from_default(\n",
      "            ids=list(range(input_.bs * self.group_size)),\n",
      "            data=flat_data,\n",
      "            seqlens=[int(x) for x in input_lens.cpu().numpy().tolist()],\n",
      "        )\n",
      "\n",
      "\n",
      "\n",
      "        #### End Change ####\n",
      "\n",
      "        if self.use_dense_reward:\n",
      "            dense_reward_score = dense_reward_score[shift_one_indices]\n",
      "\n",
      "        ### Logging code starts. ###\n",
      "        all_stats = []\n",
      "        with stats_tracker.scope(\"ppo_actor\"):\n",
      "            assert (\n",
      "                task_ids.shape == reward_score.shape\n",
      "            ), f\"task_ids ({task_ids.shape}) and reward_score ({reward_score.shape}) must have the same shape\"\n",
      "\n",
      "            task_denominators = {\n",
      "                f\"{task}_n_seqs\": (task_ids == idx).bool()\n",
      "                for idx, task in enumerate(RL_TASKS)\n",
      "            }\n",
      "\n",
      "            result_denominators = {\n",
      "                \"correct_n_seqs\": (reward_score > 0).bool(),\n",
      "                \"incorrect_n_seqs\": (reward_score <= 0).bool(),\n",
      "            }\n",
      "\n",
      "            global_denominators = dict(\n",
      "                n_seqs=torch.ones_like(reward_score, dtype=torch.bool),\n",
      "                n_tokens=torch.ones_like(prompt_mask, dtype=torch.bool),\n",
      "                n_valid_tokens=loss_mask.bool(),\n",
      "                **task_denominators,\n",
      "                **result_denominators,\n",
      "            )\n",
      "            stats_tracker.denominator(**global_denominators)\n",
      "\n",
      "            for task in RL_TASKS:\n",
      "                stats_tracker.stat(\n",
      "                    **{f\"{task}_reward\": reward_score}, denominator=f\"{task}_n_seqs\"\n",
      "                )\n",
      "\n",
      "            stats_tracker.stat(\n",
      "                correct_seq_len=input_lens.float(), denominator=\"correct_n_seqs\"\n",
      "            )\n",
      "            stats_tracker.stat(\n",
      "                incorrect_seq_len=input_lens.float(), denominator=\"incorrect_n_seqs\"\n",
      "            )\n",
      "\n",
      "            stats = dict(\n",
      "                advantages=advantages,\n",
      "                kl_rewards=kl_rewards,\n",
      "                final_reward=rewards,\n",
      "            )\n",
      "            if self.use_dense_reward:\n",
      "                stats[\"dense_reward\"] = dense_reward_score\n",
      "            stats_tracker.stat(**stats, denominator=\"n_valid_tokens\")\n",
      "\n",
      "            seq_stats = dict(\n",
      "                no_eos_ratios=seq_no_eos_mask.float(),\n",
      "                task_reward=reward_score,\n",
      "                prompt_len=prompt_lens.float(),\n",
      "                seq_len=input_lens.float(),\n",
      "            )\n",
      "            if \"version_start\" in input_.data:\n",
      "                seq_stats[\"head_offpolicyness\"] = (\n",
      "                    model.version.global_step - input_.data[\"version_start\"]\n",
      "                ).float()\n",
      "            if \"version_end\" in input_.data:\n",
      "                seq_stats[\"tail_offpolicyness\"] = (\n",
      "                    model.version.global_step - input_.data[\"version_end\"]\n",
      "                ).float()\n",
      "            stats_tracker.stat(\n",
      "                **seq_stats,\n",
      "                denominator=\"n_seqs\",\n",
      "            )\n",
      "            scalars = dict(\n",
      "                disable_value=self.disable_value,\n",
      "                mask_no_eos_with_zero=self.mask_no_eos_with_zero,\n",
      "                eps_clip=self.eps_clip,\n",
      "                use_prox_logp=use_prox_logp,\n",
      "            )\n",
      "            if self.c_clip is not None:\n",
      "                scalars[\"c_clip\"] = self.c_clip\n",
      "                scalars[\"use_dual_clip\"] = 1\n",
      "            else:\n",
      "                scalars[\"use_dual_clip\"] = 0\n",
      "            if self.behav_imp_weight_cap is not None:\n",
      "                scalars[\"behav_imp_weight_cap\"] = self.behav_imp_weight_cap\n",
      "            stats_tracker.scalar(**scalars)\n",
      "\n",
      "            global_stats = stats_tracker.export()\n",
      "            for k in global_denominators:\n",
      "                global_stats.pop(f\"ppo_actor/{k}\")\n",
      "\n",
      "            # Run mini-batched PPO training!\n",
      "            def _loss_fn(logits, input_):\n",
      "                return _ppo_actor_loss_from_model_outputs(\n",
      "                    logits,\n",
      "                    input_,\n",
      "                    kl_adapter=self.kl_adapter,\n",
      "                    eps_clip=self.eps_clip,\n",
      "                    early_stop_imp_ratio=self.early_stop_imp_ratio,\n",
      "                    early_stop_kl=self.early_stop_kl,\n",
      "                    c_clip=self.c_clip,\n",
      "                    behav_imp_weight_cap=self.behav_imp_weight_cap,\n",
      "                    temperature=self.gconfig.temperature,\n",
      "                )\n",
      "\n",
      "            for reuse in range(self.sample_reuse):\n",
      "                # NOTE: We split PPO minibatches in terms of #seqs instead of #tokens.\n",
      "                flat_input = SequenceSample.shuffled(flat_input)\n",
      "                bs = flat_input.bs\n",
      "                sizes = [0 for _ in range(self.n_minibatches)]\n",
      "                for idx in range(bs):\n",
      "                    sizes[idx % self.n_minibatches] += 1\n",
      "                spec = SequenceSplitSpec(sizes=sizes)\n",
      "                datas = flat_input.split_with_spec(spec)\n",
      "                logger.info(\n",
      "                    f\"PPO minibatch split (size {self.n_minibatches}): \"\n",
      "                    f\"#seqs: {[s.bs for s in datas]}, \"\n",
      "                    f\"#tokens: {[sum([sum(lens) for lens in s.seqlens[s._get_split_key()]]) for s in datas]}\"\n",
      "                )\n",
      "                for mb_i, data in enumerate(datas):\n",
      "                    train_stat = module.train_batch(\n",
      "                        input_=data,\n",
      "                        mb_spec=mb_spec,\n",
      "                        version_steps=model.version.global_step,\n",
      "                        loss_fn=_loss_fn,\n",
      "                        loss_weight_fn=lambda x: x.data[\n",
      "                            \"ppo_loss_mask\"\n",
      "                        ].count_nonzero(),\n",
      "                        token_normalize_scope=self.token_normalize_scope,\n",
      "                    )\n",
      "                    stats_tracker.scalar(**train_stat)\n",
      "                    all_stats.append(stats_tracker.export())\n",
      "\n",
      "        model.inc_version()\n",
      "        all_stats[0].update(global_stats)\n",
      "\n",
      "        return all_stats\n",
      "\n",
      "    # Mock methods for profiling only.\n",
      "    def _mock_inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        dataset_input: SequenceSample,\n",
      "    ) -> SequenceSample:\n",
      "        prompt_lens = flat2d(dataset_input.seqlens[\"packed_prompts\"])\n",
      "        seqlens = [x + self.gconfig.max_new_tokens for x in prompt_lens]\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        mconfig = module.config\n",
      "        packed_input_ids = torch.randint(\n",
      "            0,\n",
      "            mconfig.vocab_size,\n",
      "            (sum(seqlens),),\n",
      "            dtype=torch.long,\n",
      "            device=model.device,\n",
      "        )\n",
      "\n",
      "        return SequenceSample.from_default(\n",
      "            seqlens=seqlens,\n",
      "            ids=dataset_input.ids,\n",
      "            data=dict(packed_input_ids=packed_input_ids),\n",
      "        )\n",
      "\n",
      "    # Mock methods for profiling only.\n",
      "    def _mock_train_step(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        dataset_input: SequenceSample,\n",
      "    ) -> Dict:\n",
      "        prompt_lens = flat2d(dataset_input.seqlens[\"packed_prompts\"])\n",
      "        bs = len(prompt_lens)\n",
      "        seqlens = [x + self.gconfig.max_new_tokens for x in prompt_lens]\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        mconfig = module.config\n",
      "        mdtype = module.dtype\n",
      "        short1_seqlens = [x - 1 for x in seqlens]\n",
      "\n",
      "        packed_logprobs = torch.randn(\n",
      "            (sum(short1_seqlens),), dtype=mdtype, device=model.device\n",
      "        )\n",
      "        packed_ref_logprobs = torch.randn_like(packed_logprobs)\n",
      "        prompt_mask = torch.zeros(\n",
      "            (sum(seqlens),), dtype=torch.bool, device=model.device\n",
      "        )\n",
      "        packed_input_ids = torch.randint(\n",
      "            0,\n",
      "            mconfig.vocab_size,\n",
      "            (sum(seqlens),),\n",
      "            dtype=torch.long,\n",
      "            device=model.device,\n",
      "        )\n",
      "        rewards = torch.randn(bs, dtype=mdtype, device=model.device)\n",
      "        seq_no_eos_mask = torch.randint(\n",
      "            0, 2, (bs,), dtype=torch.bool, device=model.device\n",
      "        )\n",
      "        values = torch.randn(\n",
      "            (sum(seqlens),),\n",
      "            dtype=mdtype,\n",
      "            device=model.device,\n",
      "        )\n",
      "\n",
      "        return SequenceSample.from_default(\n",
      "            seqlens=seqlens,\n",
      "            ids=dataset_input.ids,\n",
      "            data=dict(\n",
      "                packed_logprobs=packed_logprobs,\n",
      "                packed_ref_logprobs=packed_ref_logprobs,\n",
      "                prompt_mask=prompt_mask,\n",
      "                packed_input_ids=packed_input_ids,\n",
      "                rewards=rewards,\n",
      "                seq_no_eos_mask=seq_no_eos_mask,\n",
      "                values=values,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "\n",
      "def _ppo_critic_loss_from_model_outputs(\n",
      "    new_values: torch.FloatTensor,\n",
      "    input_: SequenceSample,\n",
      "    value_eps_clip: float,\n",
      "    kl_adapter: ppo_functional.KLController,\n",
      "    rms=None,\n",
      ") -> torch.Tensor:\n",
      "\n",
      "    cu_seqlens = (\n",
      "        torch.nn.functional.pad(\n",
      "            torch.tensor(flat2d(input_.seqlens[\"packed_input_ids\"])).cumsum(0),\n",
      "            (1, 0),\n",
      "        )\n",
      "        .int()\n",
      "        .to(new_values.device)\n",
      "    )\n",
      "    ppo_loss_mask = input_.data[\"ppo_loss_mask\"]\n",
      "    returns = input_.data[\"returns\"]\n",
      "    values = input_.data[\"values\"]\n",
      "    kl_rewards = input_.data[\"kl_rewards\"]\n",
      "\n",
      "    leave_one_indices = torch.cat(\n",
      "        [\n",
      "            torch.arange(\n",
      "                cu_seqlens[i],\n",
      "                cu_seqlens[i + 1] - 1,\n",
      "                dtype=torch.long,\n",
      "                device=cu_seqlens.device,\n",
      "            )\n",
      "            for i in range(cu_seqlens.shape[0] - 1)\n",
      "        ]\n",
      "    )\n",
      "    new_values = new_values[leave_one_indices].view(-1).float()\n",
      "    values = values[leave_one_indices].view(-1).float()\n",
      "\n",
      "    loss, loss_stat = ppo_functional.critic_loss_fn(\n",
      "        value=new_values,\n",
      "        old_value=values,\n",
      "        target_value=returns,\n",
      "        value_eps_clip=value_eps_clip,\n",
      "        loss_mask=ppo_loss_mask,\n",
      "    )\n",
      "\n",
      "    if rms is not None:\n",
      "        denormalized_values = rms.denormalize(new_values)\n",
      "    else:\n",
      "        denormalized_values = new_values\n",
      "\n",
      "    # Logging.\n",
      "    stats_tracker.denominator(n_valid_tokens=ppo_loss_mask.bool())\n",
      "    stats_tracker.stat(\n",
      "        value_loss=loss_stat[\"loss\"],\n",
      "        clip_ratio=loss_stat[\"clip_mask\"].float(),\n",
      "        denormalized_values=denormalized_values.detach().float(),\n",
      "        denominator=\"n_valid_tokens\",\n",
      "    )\n",
      "\n",
      "    # Update KL coefficient to be consistent with actor.\n",
      "    mean_ref_kl = (kl_rewards.detach().float() * ppo_loss_mask).sum()\n",
      "    dist.all_reduce(mean_ref_kl, group=constants.data_parallel_group())\n",
      "    _n_valid_tokens = ppo_loss_mask.count_nonzero().clone()\n",
      "    dist.all_reduce(_n_valid_tokens, group=constants.data_parallel_group())\n",
      "    mean_ref_kl /= _n_valid_tokens\n",
      "    kl_adapter.update(mean_ref_kl, n_steps=cu_seqlens.shape[0] - 1)\n",
      "\n",
      "    return loss\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PPOCriticInterface(model_api.ModelInterface):\n",
      "    n_minibatches: int = 4\n",
      "    enable_save: bool = True\n",
      "    kl_ctl: float = 0.1\n",
      "    discount: float = 1.0\n",
      "    gae_lambda: float = 0.95\n",
      "    value_eps_clip: float = 0.2\n",
      "    max_reward_clip: float = 5.0\n",
      "    adaptive_kl_ctl: bool = False\n",
      "    adaptive_kl_target: Optional[float] = 6\n",
      "    adaptive_kl_horizon: Optional[float] = 10000\n",
      "    value_norm: bool = False\n",
      "    value_norm_type: str = dataclasses.field(\n",
      "        metadata={\"choices\": [\"exp\", \"ma\"]}, default=\"exp\"\n",
      "    )\n",
      "    value_norm_beta: float = 0.99995\n",
      "    value_norm_eps: float = 1e-5\n",
      "    disable_value: bool = False\n",
      "\n",
      "    group_size: int = 1\n",
      "    mask_no_eos_with_zero: bool = False\n",
      "    mask_too_long: bool = False\n",
      "    use_dense_reward: bool = False\n",
      "    reward_delta: bool = True\n",
      "    token_normalize_scope: Literal[\"global\", \"dp\"] = \"global\"\n",
      "\n",
      "    sample_reuse: int = 1\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.adaptive_kl_ctl:\n",
      "            assert self.adaptive_kl_target is not None\n",
      "            assert self.adaptive_kl_horizon is not None\n",
      "            self.kl_adapter = ppo_functional.AdaptiveKLController(\n",
      "                self.kl_ctl, self.adaptive_kl_target, self.adaptive_kl_horizon\n",
      "            )\n",
      "        else:\n",
      "            self.kl_adapter = ppo_functional.FixedKLController(self.kl_ctl)\n",
      "        if self.value_norm:\n",
      "            from realhf.impl.model.modules import (\n",
      "                ExponentialRunningMeanStd,\n",
      "                MovingAverageRunningMeanStd,\n",
      "            )\n",
      "\n",
      "            if self.value_norm_type == \"exp\":\n",
      "                self.rms = ExponentialRunningMeanStd(\n",
      "                    beta=self.value_norm_beta, epsilon=self.value_norm_eps\n",
      "                )\n",
      "            elif self.value_norm_type == \"ma\":\n",
      "                self.rms = MovingAverageRunningMeanStd()\n",
      "            else:\n",
      "                raise ValueError(f\"Unknown value_norm_type {self.value_norm_type}\")\n",
      "        self.kl_ctl = None\n",
      "\n",
      "    def save(self, model: model_api.Model, save_dir: str):\n",
      "        if not self.enable_save:\n",
      "            return\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        module.save_to_hf(\n",
      "            tokenizer=model.tokenizer,\n",
      "            save_dir=save_dir,\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> SequenceSample:\n",
      "        assert model.module.module.config.is_critic\n",
      "        module = model.module\n",
      "        module.eval()\n",
      "\n",
      "\n",
      "        #### Start Change ####\n",
      "        # input_flattend = SequenceSample.from_default(\n",
      "        #     ids=list(range(input_.bs * self.group_size)),\n",
      "        #     seqlens=flat2d(input_.seqlens[\"packed_input_ids\"]),\n",
      "        #     data=dict(packed_input_ids=input_.data[\"packed_input_ids\"]),\n",
      "        # )\n",
      "        all_seqlens_flat = flat2d(input_.seqlens[\"packed_input_ids\"])\n",
      "        num_total_turns = len(all_seqlens_flat)\n",
      "\n",
      "        # Directly construct the flattened SequenceSample, avoiding from_default\n",
      "        input_flattend = SequenceSample(\n",
      "            keys={\"packed_input_ids\"},\n",
      "            ids=list(range(num_total_turns)),\n",
      "            seqlens={\"packed_input_ids\": [[l] for l in all_seqlens_flat]},\n",
      "            trailing_shapes={\"packed_input_ids\": input_.trailing_shapes[\"packed_input_ids\"]},\n",
      "            dtypes={\"packed_input_ids\": input_.dtypes[\"packed_input_ids\"]},\n",
      "            data={\"packed_input_ids\": input_.data[\"packed_input_ids\"]},\n",
      "        )\n",
      "        #### End Change ####\n",
      "\n",
      "\n",
      "        if self.disable_value:\n",
      "            scores = input_.data[\"packed_input_ids\"].new_zeros(dtype=module.dtype)\n",
      "        else:\n",
      "            scores = module.forward(input_=input_flattend, mb_spec=mb_spec)\n",
      "\n",
      "        if scores is None:\n",
      "            return None\n",
      "        scores = scores.view(-1)\n",
      "        # res = SequenceSample.from_default(\n",
      "        #     ids=input_.ids,\n",
      "        #     data=dict(values=scores),\n",
      "        #     seqlens=input_.seqlens[\"packed_input_ids\"],\n",
      "        # )\n",
      "        res = SequenceSample(\n",
      "            keys=[\"values\"],\n",
      "            ids=input_.ids,\n",
      "            dtypes=dict(values=module.dtype),\n",
      "            trailing_shapes=dict(values=()),\n",
      "            data=dict(values=scores),\n",
      "            seqlens=dict(values=input_.seqlens[\"packed_input_ids\"]),\n",
      "        )\n",
      "\n",
      "        return res\n",
      "\n",
      "    def train_step(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "    ) -> Dict | List[Dict]:\n",
      "        assert model.module.module.config.is_critic\n",
      "\n",
      "        if self.disable_value:\n",
      "            return dict()\n",
      "\n",
      "        module = model.module\n",
      "        tokenizer = model.tokenizer\n",
      "        # We call module.eval() because dropout causes the computation of incorrect of log probs.\n",
      "        module.eval()\n",
      "\n",
      "        prompt_mask = input_.data[\"prompt_mask\"]\n",
      "        input_lens = torch.tensor(\n",
      "            flat2d(input_.seqlens[\"packed_input_ids\"]), device=model.device\n",
      "        )\n",
      "        cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "        reward_score = input_.data[\"rewards\"].float()\n",
      "        if \"dense_rewards\" in input_.data:\n",
      "            dense_reward_score = input_.data[\"dense_rewards\"].float()\n",
      "        values = input_.data[\"values\"].float()\n",
      "        seq_no_eos_mask = input_.data[\"seq_no_eos_mask\"]\n",
      "        if self.kl_adapter.value == 0:\n",
      "            ref_logp: torch.FloatTensor = reward_score.new_zeros(\n",
      "                int(input_lens.sum()) - len(input_lens)\n",
      "            )\n",
      "        else:\n",
      "            ref_logp: torch.FloatTensor = input_.data[\"packed_ref_logprobs\"].float()\n",
      "        old_logp: torch.FloatTensor = input_.data[\"packed_logprobs\"].float()\n",
      "\n",
      "        if self.value_norm:\n",
      "            denormalized_values = self.rms.denormalize(values)\n",
      "        else:\n",
      "            denormalized_values = values\n",
      "\n",
      "        for i in range(seq_no_eos_mask.shape[0]):\n",
      "            if not seq_no_eos_mask[i]:\n",
      "                # Set value at the EOS token to be zero.\n",
      "                denormalized_values[cu_seqlens[i + 1] - 1] = 0.0\n",
      "                values[cu_seqlens[i + 1] - 1] = 0.0\n",
      "\n",
      "        # Shift the loss mask by one token for each packed sequences.\n",
      "        input_lens = cu_seqlens[1:] - cu_seqlens[:-1]\n",
      "        short1cu_seqlens = cu_seqlens.clone()\n",
      "        short1cu_seqlens[1:] -= torch.ones_like(cu_seqlens[1:]).cumsum(0)\n",
      "        loss_mask = prompt_mask.logical_not()\n",
      "\n",
      "        if self.mask_too_long:\n",
      "            for i in range(seq_no_eos_mask.shape[0]):\n",
      "                if seq_no_eos_mask[i]:\n",
      "                    loss_mask[cu_seqlens[i] : cu_seqlens[i + 1]] = False\n",
      "\n",
      "        shift_one_indices = torch.cat(\n",
      "            [\n",
      "                torch.arange(\n",
      "                    cu_seqlens[i] + 1,\n",
      "                    cu_seqlens[i + 1],\n",
      "                    dtype=torch.long,\n",
      "                    device=cu_seqlens.device,\n",
      "                )\n",
      "                for i in range(cu_seqlens.shape[0] - 1)\n",
      "            ]\n",
      "        )\n",
      "        loss_mask = loss_mask[shift_one_indices]\n",
      "\n",
      "        # Apply the mask to log probabilities.\n",
      "        ref_logp *= loss_mask\n",
      "        old_logp *= loss_mask\n",
      "\n",
      "        # Compute rewards and GAEs.\n",
      "        if self.use_dense_reward:\n",
      "            kl_rewards, rewards = ppo_functional.get_packed_reward_dense(\n",
      "                kl_ctl=self.kl_adapter.value,\n",
      "                clip_reward_value=self.max_reward_clip,\n",
      "                log_probs=old_logp,\n",
      "                ref_log_probs=ref_logp,\n",
      "                dense_reward_score=dense_reward_score,\n",
      "                short1cu_seqlens=short1cu_seqlens,\n",
      "                seq_no_eos_mask=seq_no_eos_mask,\n",
      "                reward_delta=self.reward_delta,\n",
      "            )\n",
      "        else:\n",
      "            kl_rewards, rewards = ppo_functional.get_packed_rewards(\n",
      "                kl_ctl=self.kl_adapter.value,\n",
      "                clip_reward_value=self.max_reward_clip,\n",
      "                log_probs=old_logp,\n",
      "                ref_log_probs=ref_logp,\n",
      "                reward_score=(reward_score),\n",
      "                short1cu_seqlens=short1cu_seqlens,\n",
      "                seq_no_eos_mask=seq_no_eos_mask,\n",
      "                mask_no_eos_with_zero=self.mask_no_eos_with_zero,\n",
      "            )\n",
      "        _, returns = ppo_functional.get_packed_advantages_and_returns(\n",
      "            gamma=self.discount,\n",
      "            lam=self.gae_lambda,\n",
      "            values=denormalized_values,\n",
      "            rewards=rewards,\n",
      "            short1cu_seqlens=short1cu_seqlens,\n",
      "            seq_no_eos_mask=seq_no_eos_mask,\n",
      "        )\n",
      "\n",
      "        # Optionally perform normalization.\n",
      "        if self.value_norm:\n",
      "            self.rms.update(returns, mask=loss_mask)\n",
      "            normalized_returns = self.rms.normalize(returns)\n",
      "        else:\n",
      "            normalized_returns = returns\n",
      "\n",
      "\n",
      "        #### Start Change ####\n",
      "        # Prepare data to be splitted into mini-batches.\n",
      "        flat_input = SequenceSample.from_default(\n",
      "            ids=list(range(input_.bs * self.group_size)),\n",
      "            data=dict(\n",
      "                returns=normalized_returns,\n",
      "                values=values,\n",
      "                ppo_loss_mask=loss_mask,\n",
      "                packed_input_ids=input_.data[\"packed_input_ids\"],\n",
      "                kl_rewards=kl_rewards,\n",
      "            ),\n",
      "            seqlens=[int(x) for x in input_lens.cpu().numpy().tolist()],\n",
      "        )\n",
      "\n",
      "        #### \n",
      "\n",
      "        # Logging.\n",
      "        with stats_tracker.scope(\"ppo_critic\"):\n",
      "            stats_tracker.denominator(n_valid_tokens=loss_mask)\n",
      "            stats_tracker.stat(returns=returns, denominator=\"n_valid_tokens\")\n",
      "\n",
      "            def _loss_fn(out, inp):\n",
      "                return _ppo_critic_loss_from_model_outputs(\n",
      "                    out,\n",
      "                    inp,\n",
      "                    value_eps_clip=self.value_eps_clip,\n",
      "                    kl_adapter=self.kl_adapter,\n",
      "                    rms=None if not self.value_norm else self.rms,\n",
      "                )\n",
      "\n",
      "            # Run mini-batched PPO training!\n",
      "            for reuse in range(self.sample_reuse):\n",
      "                with stats_tracker.scope(f\"reuse{reuse}\"):\n",
      "                    # NOTE: We split PPO minibatches in terms of #seqs instead of #tokens.\n",
      "                    flat_input = SequenceSample.shuffled(flat_input)\n",
      "                    bs = flat_input.bs\n",
      "                    sizes = [0 for _ in range(self.n_minibatches)]\n",
      "                    for idx in range(bs):\n",
      "                        sizes[idx % self.n_minibatches] += 1\n",
      "                    spec = SequenceSplitSpec(sizes=sizes)\n",
      "                    datas = flat_input.split_with_spec(spec)\n",
      "                    logger.info(\n",
      "                        f\"PPO minibatch split (size {self.n_minibatches}): \"\n",
      "                        f\"#seqs: {[s.bs for s in datas]}, \"\n",
      "                        f\"#tokens: {[sum([sum(lens) for lens in s.seqlens[s._get_split_key()]]) for s in datas]}\"\n",
      "                    )\n",
      "                    for mb_i, data in enumerate(datas):\n",
      "                        with stats_tracker.scope(f\"mb{mb_i}\"):\n",
      "                            stats = module.train_batch(\n",
      "                                input_=data,\n",
      "                                mb_spec=mb_spec,\n",
      "                                version_steps=model.version.global_step,\n",
      "                                loss_fn=_loss_fn,\n",
      "                                loss_weight_fn=lambda x: x.data[\n",
      "                                    \"ppo_loss_mask\"\n",
      "                                ].count_nonzero(),\n",
      "                                token_normalize_scope=self.token_normalize_scope,\n",
      "                            )\n",
      "                            stats_tracker.scalar(**stats)\n",
      "\n",
      "        model.inc_version()\n",
      "\n",
      "        return stats_tracker.export()\n",
      "\n",
      "    # Mock methods for profiling only.\n",
      "    def _mock_inference(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        dataset_input: SequenceSample,\n",
      "    ) -> SequenceSample:\n",
      "        seqlens = flat2d(dataset_input.seqlens[\"packed_prompts\"])\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        mconfig = module.config\n",
      "        packed_input_ids = torch.randint(\n",
      "            0,\n",
      "            mconfig.vocab_size,\n",
      "            (sum(seqlens),),\n",
      "            dtype=torch.long,\n",
      "            device=model.device,\n",
      "        )\n",
      "\n",
      "        return SequenceSample.from_default(\n",
      "            seqlens=seqlens,\n",
      "            ids=dataset_input.ids,\n",
      "            data=dict(packed_input_ids=packed_input_ids),\n",
      "        )\n",
      "\n",
      "    # Mock methods for profiling only.\n",
      "    def _mock_train_step(\n",
      "        self,\n",
      "        model: model_api.Model,\n",
      "        dataset_input: SequenceSample,\n",
      "    ) -> Dict:\n",
      "        seqlens = flat2d(dataset_input.seqlens[\"packed_prompts\"])\n",
      "        bs = len(seqlens)\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            module = module.module\n",
      "        mconfig = module.config\n",
      "        mdtype = module.dtype\n",
      "        short1_seqlens = [x - 1 for x in seqlens]\n",
      "\n",
      "        packed_logprobs = torch.randn(\n",
      "            (sum(short1_seqlens),), dtype=mdtype, device=model.device\n",
      "        )\n",
      "        packed_ref_logprobs = torch.randn_like(packed_logprobs)\n",
      "        prompt_mask = torch.zeros(\n",
      "            (sum(seqlens),), dtype=torch.bool, device=model.device\n",
      "        )\n",
      "        packed_input_ids = torch.randint(\n",
      "            0,\n",
      "            mconfig.vocab_size,\n",
      "            (sum(seqlens),),\n",
      "            dtype=torch.long,\n",
      "            device=model.device,\n",
      "        )\n",
      "        rewards = torch.randn(bs, dtype=mdtype, device=model.device)\n",
      "        seq_no_eos_mask = torch.randint(\n",
      "            0, 2, (bs,), dtype=torch.bool, device=model.device\n",
      "        )\n",
      "        values = torch.randn(\n",
      "            (sum(seqlens),),\n",
      "            dtype=mdtype,\n",
      "            device=model.device,\n",
      "        )\n",
      "\n",
      "        return SequenceSample.from_default(\n",
      "            seqlens=seqlens,\n",
      "            ids=dataset_input.ids,\n",
      "            data=dict(\n",
      "                packed_logprobs=packed_logprobs,\n",
      "                packed_ref_logprobs=packed_ref_logprobs,\n",
      "                prompt_mask=prompt_mask,\n",
      "                packed_input_ids=packed_input_ids,\n",
      "                rewards=rewards,\n",
      "                seq_no_eos_mask=seq_no_eos_mask,\n",
      "                values=values,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "\n",
      "model_api.register_interface(\"ppo_actor\", PPOActorInterface)\n",
      "model_api.register_interface(\"ppo_critic\", PPOCriticInterface)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/nn/real_llm_api.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import contextlib\n",
      "import dataclasses\n",
      "import functools\n",
      "import os\n",
      "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.distributed\n",
      "import torch.nn as nn\n",
      "import torch.utils.checkpoint\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core import model_api\n",
      "from realhf.api.core.config import ModelName\n",
      "from realhf.base import constants, logging, topology\n",
      "from realhf.base.monitor import CUDATimeMarkType, cuda_tmark, cuda_tmarked\n",
      "from realhf.impl.model.comm.global_comm import NCCLProcessGroupInfo\n",
      "from realhf.impl.model.comm.param_realloc import (\n",
      "    ReparallelizeReceiverStep,\n",
      "    ReparallelizeSenderStep,\n",
      "    ReparallelizeTraget,\n",
      "    _derive_reparallelize_comm_plan,\n",
      "    is_trainable,\n",
      ")\n",
      "from realhf.impl.model.nn.flatten_param import set_intervals, slice_intervals\n",
      "from realhf.impl.model.utils.padding import pad_input, unpad_input\n",
      "\n",
      "from .flatten_param import build_param_spec, map_param_to_contigous_memory\n",
      "from .real_llm_base import (\n",
      "    OutputHead,\n",
      "    ParallelActorHead,\n",
      "    PipeCacheData,\n",
      "    PipeTransferData,\n",
      "    ReaLModelBlock,\n",
      "    SequenceParallelCriticHead,\n",
      "    VocabPositionEmbedding,\n",
      ")\n",
      "from .real_llm_generate import generate\n",
      "from .real_llm_parallel import partition_pipeline_layers\n",
      "\n",
      "logger = logging.getLogger(\"ReaLModel Interface\")\n",
      "\n",
      "\n",
      "def chunked_bcast(x, src, group, chunk_size_bytes=1024 * 1024**2):\n",
      "    assert len(x.shape) == 1\n",
      "    n_chunks = (x.numel() * x.dtype.itemsize + chunk_size_bytes - 1) // chunk_size_bytes\n",
      "    chunk_size = chunk_size_bytes // x.dtype.itemsize\n",
      "    for i in range(n_chunks):\n",
      "        if isinstance(x, torch.Tensor):\n",
      "            torch.distributed.broadcast(\n",
      "                x[i * chunk_size : (i + 1) * chunk_size], src=src, group=group\n",
      "            )\n",
      "        else:\n",
      "            assert isinstance(x, list) and len(x) == n_chunks\n",
      "            torch.distributed.broadcast(x[i], src=src, group=group)\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class DuckModelOutput:\n",
      "    logits: Optional[Union[List[torch.Tensor], torch.Tensor]] = None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class DuckGenerationOutput:\n",
      "    sequences: torch.Tensor\n",
      "    scores: Optional[torch.Tensor] = None\n",
      "    logits_mask: Optional[torch.Tensor] = None\n",
      "\n",
      "\n",
      "def _sync_embedding_and_output_weights(layers: nn.ModuleList):\n",
      "    pp_size = constants.pipe_parallel_world_size()\n",
      "    pp_rank = constants.pipe_parallel_rank()\n",
      "    if pp_size == 1:\n",
      "        old_head_w = layers[-1].weight.data\n",
      "        layers[-1].weight = layers[0].wte.weight\n",
      "        del old_head_w\n",
      "        layers[0].wte.weight.zero_out_wgrad = True\n",
      "        return\n",
      "\n",
      "    if pp_rank != 0 and pp_rank != pp_size - 1:\n",
      "        return\n",
      "\n",
      "    if pp_rank == 0:\n",
      "        weight = layers[0].wte.weight\n",
      "        weight.shared_embedding = True\n",
      "    else:\n",
      "        weight = layers[-1].weight\n",
      "        weight.data.fill_(0.0)\n",
      "        # To make Megatron happy\n",
      "        weight.shared = True\n",
      "        weight.shared_embedding = True\n",
      "\n",
      "    group = constants.grid().embedding_proc_group\n",
      "    torch.distributed.all_reduce(weight.data, group=group)\n",
      "\n",
      "\n",
      "class ReaLModel(nn.Module):\n",
      "    \"\"\"The transformer model used in ReaL.\n",
      "\n",
      "    This model supports 3D parallelism, offloaded inference,\n",
      "    and parameter reallocation. It is usually more efficient\n",
      "    than HuggingFace implementations.\n",
      "\n",
      "    During construction, model parameters are not instantiated\n",
      "    immediately because the model may be redistributed.\n",
      "    The method ``instantiate`` should be called before using\n",
      "    model parameters, e.g., forward or state dict.\n",
      "\n",
      "    :param config: The model configuration.\n",
      "    :type config: model_api.ReaLModelConfig\n",
      "    :param dtype: The data type of the model.\n",
      "    :type dtype: Optional[torch.dtype], optional\n",
      "    :param device: The device of the model.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: model_api.ReaLModelConfig,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "        hf_model_family: Optional[str] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        if dtype is None:\n",
      "            dtype = torch.float16\n",
      "        self.config = config\n",
      "        self.dtype = dtype\n",
      "        self.device = device\n",
      "\n",
      "        # The main attribute of the model: layers,\n",
      "        # including the embedding layer, decoder layers, and the output head.\n",
      "        self.layer_mapping = partition_pipeline_layers(\n",
      "            config,\n",
      "            constants.pipe_parallel_world_size(),\n",
      "        )\n",
      "        self.layer_idx_start = self.layer_mapping[constants.pipe_parallel_rank()][0]\n",
      "        self.layer_idx_end = self.layer_mapping[constants.pipe_parallel_rank()][1]\n",
      "        self.num_stages = constants.pipe_parallel_world_size()\n",
      "\n",
      "        self.layers = nn.ModuleList()\n",
      "\n",
      "        # The model is lazily instantiated due to parameter reallocation.\n",
      "        # For models that will be redistributed, we instantiate replica 0\n",
      "        # and do not instantiate other replicas.\n",
      "        self._instantiated = False\n",
      "        self._instantiation_hooks = []\n",
      "\n",
      "        # Attributes used for parameter reallocation.\n",
      "        self._reparallelize_targets: Dict[\n",
      "            Tuple[ModelName, ModelName], ReparallelizeTraget\n",
      "        ] = {}\n",
      "\n",
      "        # Attributes used for offload.\n",
      "        self._offload_buffer = None\n",
      "        self._offloaded = False\n",
      "\n",
      "        # Attributes used for flattening parameters.\n",
      "        self.head_param_point_to_embedding = (\n",
      "            self.config.tied_embedding\n",
      "            and not self.config.is_critic\n",
      "            and constants.pipe_parallel_world_size() == 1\n",
      "        )\n",
      "        self._param_spec, self._param_size = build_param_spec(\n",
      "            list(range(self.layer_idx_start, self.layer_idx_end)),\n",
      "            self.config,\n",
      "            tp_size=constants.tensor_parallel_world_size(),\n",
      "            pp_size=constants.pipe_parallel_world_size(),\n",
      "            dp_size=constants.data_parallel_world_size(),\n",
      "            head_param_point_to_embedding=self.head_param_point_to_embedding,\n",
      "        )\n",
      "        self.contiguous_param = None\n",
      "\n",
      "        self.hf_model_family = hf_model_family\n",
      "\n",
      "    def save_to_hf(self, tokenizer, save_dir):\n",
      "        return getattr(self, f\"to_{self.hf_model_family}\")(tokenizer, save_dir)\n",
      "\n",
      "    def load_from_hf(self, load_dir, init_critic_from_actor):\n",
      "        return getattr(self, f\"from_{self.hf_model_family}\")(\n",
      "            load_dir, init_critic_from_actor\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def pre_process(self):\n",
      "        # A workaround to make Megatron-LM backend happy.\n",
      "        if constants.pipe_parallel_rank() == 0:\n",
      "            return self.layers[0]\n",
      "        elif constants.pipe_parallel_rank() == constants.pipe_parallel_world_size() - 1:\n",
      "            return self.layers[-1]\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def post_process(self):\n",
      "        # A workaround to make Megatron-LM backend happy.\n",
      "        if constants.pipe_parallel_rank() == constants.pipe_parallel_world_size() - 1:\n",
      "            return self.layers[-1]\n",
      "        return None\n",
      "\n",
      "    def shared_embedding_or_output_weight(self) -> None | torch.Tensor:\n",
      "        # NOTE: Use this name in consistent with Megatron-LM.\n",
      "        if not self.config.tied_embedding or self.config.is_critic:\n",
      "            return None\n",
      "        if constants.is_first_pipe_stage():\n",
      "            return self.layers[0].wte.weight\n",
      "        elif constants.is_last_pipe_stage():\n",
      "            return self.layers[-1].weight\n",
      "        return None\n",
      "\n",
      "    def instantiate(self):\n",
      "        \"\"\"Instantiate the model parameters.\n",
      "\n",
      "        Note that users can append hooks to this method to do more\n",
      "        processing, such as loading from HuggingFace models.\n",
      "        \"\"\"\n",
      "        assert not self._instantiated or self.contiguous_param.numel() == 0\n",
      "        layers = []\n",
      "        for idx in range(self.layer_idx_start, self.layer_idx_end):\n",
      "            layers.append(self._build_layer(idx, self.config))\n",
      "        self.layers = nn.ModuleList(layers)\n",
      "\n",
      "        if self.config.tied_embedding and not self.config.is_critic:\n",
      "            _sync_embedding_and_output_weights(self.layers)\n",
      "\n",
      "        self.contiguous_param = torch.empty(\n",
      "            self._param_size, dtype=self.dtype, device=self.device\n",
      "        )\n",
      "        map_param_to_contigous_memory(\n",
      "            self.layers,\n",
      "            self.config,\n",
      "            self.head_param_point_to_embedding,\n",
      "            self._param_spec,\n",
      "            self.contiguous_param,\n",
      "            self.layer_idx_start,\n",
      "            allocate_only=False,\n",
      "        )\n",
      "\n",
      "        for h in self._instantiation_hooks:\n",
      "            h()\n",
      "\n",
      "        self._instantiated = True\n",
      "        self._instantiation_hooks = []\n",
      "\n",
      "    @property\n",
      "    def num_layers(self):\n",
      "        \"\"\"Return the number of embedding or transformer layers in this\n",
      "        pipeline stage.\"\"\"\n",
      "        return self.layer_idx_end - self.layer_idx_start\n",
      "\n",
      "    @property\n",
      "    def is_critic(self):\n",
      "        return self.config.is_critic\n",
      "\n",
      "    def _build_layer(self, idx: int, config: model_api.ReaLModelConfig) -> nn.Module:\n",
      "        dtype = self.dtype\n",
      "        device = self.device\n",
      "        if idx == 0:\n",
      "            l = VocabPositionEmbedding(config, dtype=dtype, device=device)\n",
      "        elif idx == config.n_layers + 1:\n",
      "            l = self._build_output_head(config)\n",
      "        else:\n",
      "            l = ReaLModelBlock(\n",
      "                config=config,\n",
      "                layer_index=idx - 1,\n",
      "                output_layernorm=(idx == config.n_layers),\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "        return l\n",
      "\n",
      "    def _build_output_head(self, config: model_api.ReaLModelConfig) -> nn.Module:\n",
      "        dtype = self.dtype\n",
      "        device = self.device\n",
      "        if config.is_critic and constants.sequence_parallel():\n",
      "            l = SequenceParallelCriticHead(\n",
      "                config.hidden_dim,\n",
      "                1,\n",
      "                bias=False,\n",
      "                device=device,\n",
      "                dtype=dtype,\n",
      "            )\n",
      "        elif not config.is_critic and constants.tensor_parallel_world_size() > 1:\n",
      "            l = ParallelActorHead(\n",
      "                config.hidden_dim,\n",
      "                config.vocab_size,\n",
      "                norm_head=self.config.norm_head,\n",
      "                norm_softmax=self.config.norm_softmax,\n",
      "                bias=False,\n",
      "                gradient_accumulation_fusion=constants.gradient_accumulation_fusion(),\n",
      "                device=device,\n",
      "                dtype=dtype,\n",
      "            )\n",
      "        else:\n",
      "            l = OutputHead(\n",
      "                config.hidden_dim,\n",
      "                1 if config.is_critic else config.vocab_size,\n",
      "                bias=False,\n",
      "                norm_head=self.config.norm_head,\n",
      "                norm_softmax=self.config.norm_softmax,\n",
      "                device=device,\n",
      "                dtype=dtype,\n",
      "            )\n",
      "        return l\n",
      "\n",
      "    def async_offload(self):\n",
      "        \"\"\"Trigger offload asynchronously.\"\"\"\n",
      "        if not constants.use_cuda():\n",
      "            return\n",
      "        assert not self._offloaded\n",
      "        assert self._instantiated\n",
      "        if self._offload_buffer is None:\n",
      "            self._offload_buffer = torch.empty_like(\n",
      "                self.contiguous_param,\n",
      "                dtype=self.dtype,\n",
      "                device=\"cpu\",\n",
      "                pin_memory=True,\n",
      "            )\n",
      "        else:\n",
      "            assert self._offload_buffer.shape == self.contiguous_param.shape\n",
      "        dummy_tensor = torch.tensor((), device=self.device, dtype=self.dtype)\n",
      "        self._offload_stream = torch.cuda.Stream()\n",
      "        self._offload_event = torch.cuda.Event()\n",
      "        self.contiguous_param = None\n",
      "        for i, l in enumerate(self.layers):\n",
      "            layer_idx = self.layer_idx_start + i\n",
      "            with torch.cuda.stream(self._offload_stream):\n",
      "                for k, p in l.named_parameters():\n",
      "                    spec = self._param_spec[f\"{layer_idx}.{k}\"]\n",
      "                    if (\n",
      "                        self.head_param_point_to_embedding\n",
      "                        and layer_idx == self.config.n_layers + 1\n",
      "                    ):\n",
      "                        continue\n",
      "                    self._offload_buffer[spec.start_idx : spec.end_idx].copy_(\n",
      "                        p.data.view(-1), non_blocking=True\n",
      "                    )\n",
      "                    p.data = dummy_tensor\n",
      "        self._offload_event.record(self._offload_stream)\n",
      "        self._offloaded = True\n",
      "\n",
      "    def wait_for_offload(self):\n",
      "        \"\"\"Wait for offload to finish.\"\"\"\n",
      "        if not constants.use_cuda():\n",
      "            return\n",
      "        assert self._offloaded\n",
      "        torch.cuda.current_stream().wait_event(self._offload_event)\n",
      "\n",
      "    def __overlapped_load_forward(\n",
      "        self, x: PipeTransferData, ys: List[PipeCacheData]\n",
      "    ) -> Tuple[PipeTransferData, List[PipeCacheData]]:\n",
      "        assert len(ys) == self.num_layers\n",
      "        raw_pp_input = x.pp_input\n",
      "        self.contiguous_param = torch.empty(\n",
      "            self._param_size, dtype=self.dtype, device=self.device\n",
      "        )\n",
      "        map_param_to_contigous_memory(\n",
      "            self.layers,\n",
      "            self.config,\n",
      "            self.head_param_point_to_embedding,\n",
      "            self._param_spec,\n",
      "            self.contiguous_param,\n",
      "            self.layer_idx_start,\n",
      "            allocate_only=True,\n",
      "        )\n",
      "        self.wait_for_offload()\n",
      "\n",
      "        stream = torch.cuda.Stream()\n",
      "        events: List[torch.cuda.Event] = [\n",
      "            torch.cuda.Event() for _ in range(self.num_layers)\n",
      "        ]\n",
      "        with torch.cuda.stream(stream):\n",
      "            for layer_idx, y, l, e in zip(\n",
      "                range(self.layer_idx_start, self.layer_idx_end),\n",
      "                ys,\n",
      "                self.layers,\n",
      "                events,\n",
      "            ):\n",
      "                # NOTE: although we can do more fine-grained overlapping, the overhead that can be\n",
      "                # reduced is very small (~50ms), which is unnecessary for now.\n",
      "                for k, v in l.named_parameters():\n",
      "                    spec = self._param_spec[f\"{layer_idx}.{k}\"]\n",
      "                    v.data.copy_(\n",
      "                        self._offload_buffer[spec.start_idx : spec.end_idx].view(\n",
      "                            spec.shape\n",
      "                        ),\n",
      "                        non_blocking=True,\n",
      "                    )\n",
      "                e: torch.cuda.Event\n",
      "                e.record(stream)\n",
      "\n",
      "        for layer_idx, y, l, e in zip(\n",
      "            range(self.layer_idx_start, self.layer_idx_end),\n",
      "            ys,\n",
      "            self.layers,\n",
      "            events,\n",
      "        ):\n",
      "            torch.cuda.default_stream().wait_event(e)\n",
      "            x = l(x, y)\n",
      "            x.pp_input = x.pp_output\n",
      "        self._offloaded = False\n",
      "        x.pp_input = raw_pp_input\n",
      "        return x, ys\n",
      "\n",
      "    def __forward(\n",
      "        self, x: PipeTransferData, ys: List[PipeCacheData]\n",
      "    ) -> Tuple[PipeTransferData, List[PipeCacheData]]:\n",
      "        layers = self.layers\n",
      "        assert len(ys) == len(layers), (len(ys), len(layers))\n",
      "        raw_pp_input = x.pp_input\n",
      "        for i, (layer, y) in enumerate(zip(layers, ys)):\n",
      "            x = layer(x, y)\n",
      "            x.pp_input = x.pp_output\n",
      "        # Finally, pp_input is the input of this pipeline stage (maybe across several layers),\n",
      "        # pp_output is the output of this pipeline stage.\n",
      "        # In the first stage, pp_input is None.\n",
      "        x.pp_input = raw_pp_input\n",
      "        return x, ys\n",
      "\n",
      "    def forward(\n",
      "        self, x: PipeTransferData, ys: List[PipeCacheData]\n",
      "    ) -> Tuple[PipeTransferData, List[PipeCacheData]]:\n",
      "        if x.max_seqlen is not None and not isinstance(x.max_seqlen, int):\n",
      "            x.max_seqlen = int(x.max_seqlen)\n",
      "        if x.cu_seqlens is not None and not isinstance(x.cu_seqlens, torch.IntTensor):\n",
      "            x.cu_seqlens = x.cu_seqlens.int()\n",
      "\n",
      "        # Copy input tensor to a pinned buffer.\n",
      "        tp_size = constants.tensor_parallel_world_size()\n",
      "        batch_length = None\n",
      "        if ys[0].packed_input_ids is not None:\n",
      "            batch_length = ys[0].packed_input_ids.shape[0]\n",
      "        if x.pp_input is not None:\n",
      "            batch_length = x.pp_input.shape[0]\n",
      "        assert batch_length is not None\n",
      "        padded_batch_length = (batch_length + tp_size - 1) // tp_size * tp_size\n",
      "        pad_size = padded_batch_length - batch_length\n",
      "\n",
      "        if (\n",
      "            constants.sequence_parallel()\n",
      "            and pad_size > 0\n",
      "            and ys[0].packed_input_ids is not None\n",
      "        ):\n",
      "            _cu_seqlens = x.cu_seqlens\n",
      "            _max_seqlen = x.max_seqlen\n",
      "            _input_ids = ys[0].packed_input_ids\n",
      "            _pp_input = x.pp_input\n",
      "\n",
      "            x.cu_seqlens = torch.nn.functional.pad(\n",
      "                x.cu_seqlens, (0, 1), value=padded_batch_length\n",
      "            )\n",
      "            x.max_seqlen = max(x.max_seqlen, padded_batch_length - batch_length)\n",
      "            if ys[0].packed_input_ids is not None:\n",
      "                input_ids_buf = torch.zeros(\n",
      "                    (padded_batch_length,),\n",
      "                    dtype=torch.long,\n",
      "                    device=self.device,\n",
      "                )\n",
      "                input_ids_buf[:batch_length] = ys[0].packed_input_ids\n",
      "                ys[0].packed_input_ids = input_ids_buf\n",
      "\n",
      "            if x.pp_input is not None:\n",
      "                pp_input_buf = torch.zeros(\n",
      "                    (padded_batch_length, *x.pp_input.shape[1:]),\n",
      "                    dtype=x.pp_input.dtype,\n",
      "                    device=self.device,\n",
      "                )\n",
      "                pp_input_buf[:batch_length] = x.pp_input\n",
      "                x.pp_input = pp_input_buf\n",
      "\n",
      "        tmark_type = CUDATimeMarkType.forward\n",
      "        with cuda_tmarked(\"fwd\", tmark_type):\n",
      "            # Main forward calls.\n",
      "            if not self._offloaded:\n",
      "                x, ys = self.__forward(x, ys)\n",
      "            else:\n",
      "                x, ys = self.__overlapped_load_forward(x, ys)\n",
      "\n",
      "        # Resume from padding.\n",
      "        if (\n",
      "            constants.sequence_parallel()\n",
      "            and pad_size > 0\n",
      "            and ys[0].packed_input_ids is not None\n",
      "        ):\n",
      "            x.pp_output = x.pp_output[:-pad_size]\n",
      "\n",
      "            x.pp_input = _pp_input\n",
      "            ys[0].packed_input_ids = _input_ids\n",
      "            x.cu_seqlens = _cu_seqlens\n",
      "            x.max_seqlen = _max_seqlen\n",
      "\n",
      "            if x.store_kv_cache:\n",
      "                for y in ys:\n",
      "                    if y.k_cache is not None:\n",
      "                        y.k_cache = y.k_cache[:-pad_size]\n",
      "                    if y.v_cache is not None:\n",
      "                        y.v_cache = y.v_cache[:-pad_size]\n",
      "\n",
      "        # Release the memory used for TP gathering.\n",
      "        constants.clear_global_memory_buffer()\n",
      "        return x, ys\n",
      "\n",
      "    def _forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor,\n",
      "        cu_seqlens: torch.IntTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        hidden_states: Optional[torch.Tensor],\n",
      "        k_caches: Optional[List[torch.Tensor]],\n",
      "        v_caches: Optional[List[torch.Tensor]],\n",
      "        cache_seqlens: Optional[torch.IntTensor],\n",
      "        max_seqlen: Optional[int],\n",
      "    ):\n",
      "        if k_caches is None:\n",
      "            assert v_caches is None\n",
      "            assert cache_seqlens is None\n",
      "            k_caches = [None] * self.num_layers\n",
      "            v_caches = [None] * self.num_layers\n",
      "\n",
      "        h = hidden_states\n",
      "        for idx, l in enumerate(self.layers):\n",
      "            if isinstance(l, VocabPositionEmbedding):\n",
      "                h = l._forward(input_ids, position_ids)\n",
      "            elif isinstance(l, ReaLModelBlock):\n",
      "                h, _, _ = l._forward(\n",
      "                    h,\n",
      "                    cu_seqlens=cu_seqlens,\n",
      "                    k_cache=k_caches[idx],\n",
      "                    v_cache=v_caches[idx],\n",
      "                    cache_seqlens=cache_seqlens,\n",
      "                    max_seqlen=max_seqlen,\n",
      "                )\n",
      "            elif isinstance(\n",
      "                l,\n",
      "                (\n",
      "                    OutputHead,\n",
      "                    SequenceParallelCriticHead,\n",
      "                    ParallelActorHead,\n",
      "                ),\n",
      "            ):\n",
      "                h = l._forward(h)\n",
      "            else:\n",
      "                raise NotImplementedError(f\"Unsupported layer type {type(l)}\")\n",
      "\n",
      "        return h\n",
      "\n",
      "    def state_dict(self, *args, **kwargs):\n",
      "        \"\"\"Map layer indices to global layer indices.\"\"\"\n",
      "        state_dict = self.layers.state_dict(*args, **kwargs)\n",
      "        new_state_dict = {}\n",
      "        for k, v in state_dict.items():\n",
      "            k = k.lstrip(\"module.\").lstrip(\"layers.\")\n",
      "            local_idx = int(k.split(\".\")[0])\n",
      "            name = k.split(\".\", 1)[1]\n",
      "            new_state_dict[f\"{local_idx + self.layer_idx_start}.{name}\"] = v\n",
      "        return new_state_dict\n",
      "\n",
      "    def load_state_dict(self, state_dict, strict: bool = True, assign: bool = False):\n",
      "        new_state_dict = {}\n",
      "        for k, v in state_dict.items():\n",
      "            name = k.split(\".\", 1)[1]\n",
      "            global_idx = int(k.split(\".\")[0])\n",
      "            new_state_dict[f\"layers.{global_idx - self.layer_idx_start}.{name}\"] = v\n",
      "        return super().load_state_dict(\n",
      "            new_state_dict,\n",
      "            strict=strict,\n",
      "            assign=assign,\n",
      "        )\n",
      "\n",
      "    def build_reparallelization_plan(\n",
      "        self,\n",
      "        from_model_name: ModelName,\n",
      "        to_model_name: ModelName,\n",
      "        from_topo: topology.ProcessTopology,\n",
      "        to_topo: topology.ProcessTopology,\n",
      "        to_model_config: model_api.ReaLModelConfig,\n",
      "        pg_info: NCCLProcessGroupInfo,\n",
      "        from_model_config: None | model_api.ReaLModelConfig = None,\n",
      "    ):\n",
      "        if from_model_config is None:\n",
      "            from_model_config = self.config\n",
      "        to_layer_mapping = partition_pipeline_layers(\n",
      "            to_model_config,\n",
      "            to_topo.get_dim(\"pipe\"),\n",
      "        )\n",
      "        to_layers_handle_dict = {}\n",
      "        to_layer_indices = []\n",
      "        if constants.has_model_name(to_model_name):\n",
      "            with constants.model_scope(to_model_name):\n",
      "                to_pp_rank = constants.pipe_parallel_rank()\n",
      "                to_layer_indices = list(\n",
      "                    range(\n",
      "                        to_layer_mapping[to_pp_rank][0],\n",
      "                        to_layer_mapping[to_pp_rank][1],\n",
      "                    )\n",
      "                )\n",
      "                for _to_layer_idx in to_layer_indices:\n",
      "                    l = self._build_layer(_to_layer_idx, to_model_config)\n",
      "                    for v in l.parameters():\n",
      "                        v.data = torch.tensor((), dtype=self.dtype, device=self.device)\n",
      "                    to_layers_handle_dict[_to_layer_idx] = l\n",
      "        to_model_head_param_point_to_embedding = (\n",
      "            to_model_config.tied_embedding\n",
      "            and not to_model_config.is_critic\n",
      "            and to_topo.get_dim(\"pipe\") == 1\n",
      "        )\n",
      "        to_param_spec, to_param_size = build_param_spec(\n",
      "            to_layer_indices,\n",
      "            to_model_config,\n",
      "            tp_size=to_topo.get_dim(\"tensor\"),\n",
      "            dp_size=to_topo.get_dim(\"data\"),\n",
      "            pp_size=to_topo.get_dim(\"pipe\"),\n",
      "            head_param_point_to_embedding=to_model_head_param_point_to_embedding,\n",
      "        )\n",
      "        if len(to_layer_indices) > 0:\n",
      "            to_layer_idx_start = min(to_layer_indices)\n",
      "            to_layer_idx_end = max(to_layer_indices) + 1\n",
      "        else:\n",
      "            to_layer_idx_start = to_layer_idx_end = -1\n",
      "        to_layers_handle = nn.ModuleList(\n",
      "            [to_layers_handle_dict[i] for i in to_layer_indices]\n",
      "        )\n",
      "\n",
      "        comm_plan = _derive_reparallelize_comm_plan(\n",
      "            from_model_name=from_model_name,\n",
      "            to_model_name=to_model_name,\n",
      "            from_topo=from_topo,\n",
      "            to_topo=to_topo,\n",
      "            from_model_config=from_model_config,\n",
      "            to_model_config=to_model_config,\n",
      "            pg_info=pg_info,\n",
      "            dtype=self.dtype,\n",
      "        )\n",
      "        rtgt = ReparallelizeTraget(\n",
      "            comm_plan=comm_plan,\n",
      "            to_param_spec=to_param_spec,\n",
      "            to_param_size=to_param_size,\n",
      "            to_layers_handle=to_layers_handle,\n",
      "            to_layer_start_idx=to_layer_idx_start,\n",
      "            to_layer_end_idx=to_layer_idx_end,\n",
      "        )\n",
      "        self._reparallelize_targets[(from_model_name, to_model_name)] = rtgt\n",
      "\n",
      "    # FIXME: we can get topo given model name from constants\n",
      "    @cuda_tmark(\"param_realloc\", CUDATimeMarkType.mem_layout)\n",
      "    def build_reparallelized_layers_async(\n",
      "        self,\n",
      "        from_model_name: ModelName,\n",
      "        to_model_name: ModelName,\n",
      "        from_topo: topology.ProcessTopology,\n",
      "        to_topo: topology.ProcessTopology,\n",
      "        to_model_config: model_api.ReaLModelConfig,\n",
      "        pg_info: NCCLProcessGroupInfo,\n",
      "    ) -> Tuple[nn.ModuleList, torch.Tensor, torch.Tensor]:\n",
      "        \"\"\"Trigger the parameter realloaction from the source model to the\n",
      "        target model.\"\"\"\n",
      "\n",
      "        assert not (is_trainable(from_model_name) and is_trainable(to_model_name))\n",
      "        assert is_trainable(from_model_name) or is_trainable(to_model_name)\n",
      "\n",
      "        if (from_model_name, to_model_name) not in self._reparallelize_targets:\n",
      "            self.build_reparallelization_plan(\n",
      "                from_model_name,\n",
      "                to_model_name,\n",
      "                from_topo,\n",
      "                to_topo,\n",
      "                to_model_config,\n",
      "                pg_info,\n",
      "            )\n",
      "        rtgt = self._reparallelize_targets[(from_model_name, to_model_name)]\n",
      "\n",
      "        # Since the default implementation of PyTorch optimizers holds\n",
      "        # the reference of trainable parameters, we cannot deallocate\n",
      "        # them even after parameter reallocation. Therefore, there is no\n",
      "        # need to release and re-allocate the trainable parameters back-and-forth.\n",
      "        # We simply store the layer handles and fetch them when converting back.\n",
      "        with constants.model_scope(from_model_name):\n",
      "            from_model_ranks = constants.parallelism_group_ranks()\n",
      "        if not is_trainable(from_model_name):\n",
      "            if torch.distributed.get_rank() in from_model_ranks:\n",
      "                dummy_tensor = torch.tensor((), dtype=self.dtype, device=self.device)\n",
      "                for p in self.layers.parameters():\n",
      "                    p.data = dummy_tensor\n",
      "                self.contiguous_param = dummy_tensor\n",
      "            return None, None, 0.0\n",
      "\n",
      "        # The following tensor holds the contiguous memory of incoming parameters\n",
      "        # If this process is not a receiver, to_param_size is 0 and it's an empty tensor.\n",
      "        to_contiguous_param = torch.zeros(\n",
      "            rtgt.to_param_size,\n",
      "            dtype=self.dtype,\n",
      "            device=constants.current_device(),\n",
      "        )\n",
      "        to_model_head_param_point_to_embedding = (\n",
      "            to_model_config.tied_embedding\n",
      "            and not to_model_config.is_critic\n",
      "            and to_topo.get_dim(\"pipe\") == 1\n",
      "        )\n",
      "        map_param_to_contigous_memory(\n",
      "            rtgt.to_layers_handle,\n",
      "            to_model_config,\n",
      "            to_model_head_param_point_to_embedding,\n",
      "            rtgt.to_param_spec,\n",
      "            to_contiguous_param,\n",
      "            rtgt.to_layer_start_idx,\n",
      "            allocate_only=True,\n",
      "        )\n",
      "\n",
      "        # Allocate tensors in advance to reduce overhead.\n",
      "        recv_buf_specs = []\n",
      "        send_buf_specs = []\n",
      "        comm_volume = torch.zeros(\n",
      "            (), dtype=torch.long, device=constants.current_device()\n",
      "        )\n",
      "        for step in rtgt.comm_plan:\n",
      "            if (\n",
      "                isinstance(step, ReparallelizeReceiverStep)\n",
      "                and step.rank == torch.distributed.get_rank()\n",
      "            ):\n",
      "                if step.rank == step.src:\n",
      "                    # TODO: we can develop a kernel to directly move the\n",
      "                    # memory without creating an intermediate buffer.\n",
      "                    buf = slice_intervals(\n",
      "                        self.contiguous_param,\n",
      "                        step.sender_param_intervals_cpu,\n",
      "                        intervals_cuda=step.sender_param_intervals_cuda,\n",
      "                        max_interval_size=step.sender_max_interval_size,\n",
      "                        output_size=step.param_size,\n",
      "                    )\n",
      "                else:\n",
      "                    buf = torch.zeros(\n",
      "                        step.param_size,\n",
      "                        dtype=step.param_dtype,\n",
      "                        device=constants.current_device(),\n",
      "                    )\n",
      "                    comm_volume += buf.numel()\n",
      "\n",
      "                recv_buf_specs.append(\n",
      "                    dict(\n",
      "                        src=buf,\n",
      "                        dst=to_contiguous_param,\n",
      "                        intervals_cpu=step.receiver_param_intervals_cpu,\n",
      "                        intervals_cuda=step.receiver_param_intervals_cuda,\n",
      "                        max_interval_size=step.receiver_max_interval_size,\n",
      "                    )\n",
      "                )\n",
      "\n",
      "            if (\n",
      "                isinstance(step, ReparallelizeSenderStep)\n",
      "                and step.rank == torch.distributed.get_rank()\n",
      "            ):\n",
      "                if step.group is not None:\n",
      "                    buf = slice_intervals(\n",
      "                        self.contiguous_param,\n",
      "                        step.param_intervals_cpu,\n",
      "                        intervals_cuda=step.param_intervals_cuda,\n",
      "                        max_interval_size=step.max_interval_size,\n",
      "                        output_size=step.param_size,\n",
      "                    )\n",
      "                    send_buf_specs.append(buf)\n",
      "\n",
      "        # Run boradcast!\n",
      "        recv_buf_cnt = 0\n",
      "        recv_events = []\n",
      "        for step in rtgt.comm_plan:\n",
      "            if constants.use_cuda():\n",
      "                s = torch.cuda.Stream()\n",
      "                ctx = torch.cuda.stream(s)\n",
      "            else:\n",
      "                ctx = contextlib.nullcontext()\n",
      "            with ctx:\n",
      "                if (\n",
      "                    isinstance(step, ReparallelizeReceiverStep)\n",
      "                    and step.rank == torch.distributed.get_rank()\n",
      "                ):\n",
      "                    if constants.use_cuda():\n",
      "                        e = torch.cuda.Event()\n",
      "                    else:\n",
      "                        e = None\n",
      "                    if step.rank != step.src:\n",
      "                        buf = recv_buf_specs[recv_buf_cnt][\"src\"]\n",
      "                        chunked_bcast(buf, src=step.src, group=step.group)\n",
      "                    if constants.use_cuda():\n",
      "                        e.record(s)\n",
      "                    recv_events.append(e)\n",
      "                    recv_buf_cnt += 1\n",
      "\n",
      "                if (\n",
      "                    isinstance(step, ReparallelizeSenderStep)\n",
      "                    and step.rank == torch.distributed.get_rank()\n",
      "                ):\n",
      "                    if step.group is not None:\n",
      "                        buf = send_buf_specs.pop(0)\n",
      "                        chunked_bcast(buf, src=step.rank, group=step.group)\n",
      "\n",
      "        # Post-processing.\n",
      "        assert len(send_buf_specs) == 0, len(send_buf_specs)\n",
      "        assert recv_buf_cnt == len(recv_buf_specs), (\n",
      "            len(recv_buf_specs),\n",
      "            recv_buf_cnt,\n",
      "        )\n",
      "        # assert len(state_dict) == 0\n",
      "        assert len(recv_events) == len(recv_buf_specs)\n",
      "        for e, x in zip(recv_events, recv_buf_specs):\n",
      "            if constants.use_cuda():\n",
      "                torch.cuda.current_stream().wait_event(e)\n",
      "            set_intervals(**x)\n",
      "\n",
      "        return rtgt.to_layers_handle, to_contiguous_param, comm_volume\n",
      "\n",
      "    def patch_reparallelization(self, x, eta):\n",
      "        if eta == 1.0:\n",
      "            self.layers, self.contiguous_param = x\n",
      "        else:\n",
      "            new_layers, new_param = x\n",
      "            self.contiguous_param = eta * new_param + (1 - eta) * self.contiguous_param\n",
      "            map_param_to_contigous_memory(\n",
      "                self.layers,\n",
      "                self.config,\n",
      "                self.head_param_point_to_embedding,\n",
      "                param_spec=self._param_spec,\n",
      "                contiguous_param=self.contiguous_param,\n",
      "                layer_idx_offset=self.layer_idx_start,\n",
      "                allocate_only=False,\n",
      "            )\n",
      "            dummy_tensor = torch.tensor((), dtype=self.dtype, device=self.device)\n",
      "            for p in new_layers.parameters():\n",
      "                p.data = dummy_tensor\n",
      "        assert self.layers is not None\n",
      "        assert self.contiguous_param is not None\n",
      "        assert self.contiguous_param.shape[0] > 0\n",
      "        for l in self.layers:\n",
      "            for p in l.parameters():\n",
      "                p.requires_grad_()\n",
      "\n",
      "\n",
      "# a helper function to make real_model look like huggingface model\n",
      "def generate_helper(\n",
      "    self: ReaLModel,\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "    input_ids: Optional[torch.Tensor] = None,\n",
      "    attention_mask: Optional[torch.Tensor] = None,\n",
      "    packed_input_ids: Optional[torch.Tensor] = None,\n",
      "    cu_seqlens: Optional[torch.Tensor] = None,\n",
      "    max_seqlen: Optional[int] = None,\n",
      "    gconfig: model_api.GenerationHyperparameters = dataclasses.field(\n",
      "        default_factory=model_api.GenerationHyperparameters\n",
      "    ),\n",
      ") -> DuckGenerationOutput:\n",
      "    assert (packed_input_ids is None) == (cu_seqlens is None) == (max_seqlen is None)\n",
      "    if attention_mask is None and input_ids is not None:\n",
      "        attention_mask = torch.ones_like(input_ids)\n",
      "    if packed_input_ids is None and attention_mask is not None:\n",
      "        packed_input_ids, _, cu_seqlens, max_seqlen = unpad_input(\n",
      "            input_ids, attention_mask\n",
      "        )\n",
      "    current_forward = self.forward\n",
      "    self.forward = functools.partial(ReaLModel.forward, self)\n",
      "    seq, scores, mask, _, _ = generate(\n",
      "        model=self,\n",
      "        tokenizer=tokenizer,\n",
      "        packed_input_ids=packed_input_ids,\n",
      "        cu_seqlens=cu_seqlens,\n",
      "        max_seqlen=max_seqlen,\n",
      "        gconfig=gconfig,\n",
      "    )\n",
      "    self.forward = current_forward\n",
      "    return DuckGenerationOutput(seq, scores, mask)\n",
      "\n",
      "\n",
      "# a helper function to make real_model look like huggingface model\n",
      "def forward_helper(\n",
      "    self: ReaLModel,\n",
      "    input_ids: Optional[torch.Tensor] = None,\n",
      "    attention_mask: Optional[torch.Tensor] = None,\n",
      "    packed_input_ids: Optional[torch.Tensor] = None,\n",
      "    cu_seqlens: Optional[torch.Tensor] = None,\n",
      "    max_seqlen: Optional[int] = None,\n",
      ") -> DuckModelOutput:\n",
      "    assert (packed_input_ids is None) == (cu_seqlens is None) == (max_seqlen is None)\n",
      "    build_packed = False\n",
      "    if attention_mask is None and input_ids is not None:\n",
      "        attention_mask = torch.ones_like(input_ids)\n",
      "    if packed_input_ids is None and attention_mask is not None:\n",
      "        build_packed = True\n",
      "        packed_input_ids, indices, cu_seqlens, max_seqlen = unpad_input(\n",
      "            input_ids, attention_mask\n",
      "        )\n",
      "        batch_size, seqlen = input_ids.shape[:2]\n",
      "    x = PipeTransferData(cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
      "    ys = [PipeCacheData(packed_input_ids=packed_input_ids)] + [\n",
      "        PipeCacheData() for _ in range(self.config.n_layers + 1)\n",
      "    ]\n",
      "    scores = ReaLModel.forward(self, x, ys)[0].pp_output\n",
      "    if build_packed:\n",
      "        scores = pad_input(scores, indices, batch_size, seqlen)\n",
      "    return DuckModelOutput(logits=scores)\n",
      "\n",
      "\n",
      "def add_helper_functions(m: ReaLModel):\n",
      "    m.forward = functools.partial(forward_helper, m)\n",
      "    m.generate = functools.partial(generate_helper, m)\n",
      "    return m\n",
      "\n",
      "\n",
      "def make_real_model(\n",
      "    name: ModelName,\n",
      "    device: torch.device,\n",
      "    model_path: str,\n",
      "    is_critic: bool,\n",
      "    init_from_scratch: bool,\n",
      "    init_critic_from_actor: bool,\n",
      "    dtype: Optional[str] = None,\n",
      "    hf_model_family: Optional[str] = None,\n",
      ") -> model_api.Model:\n",
      "    if dtype == \"fp16\" or dtype == None:\n",
      "        dtype = torch.float16\n",
      "    elif dtype == \"bf16\":\n",
      "        dtype = torch.bfloat16\n",
      "    elif dtype == \"fp32\":\n",
      "        dtype = torch.float32\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Unsupported dtype {dtype}\")\n",
      "\n",
      "    tokenizer = model_api.load_hf_tokenizer(model_path)\n",
      "    mconfig = getattr(ReaLModel, f\"config_from_{hf_model_family}\")(\n",
      "        model_path=model_path,\n",
      "        is_critic=is_critic or init_critic_from_actor,\n",
      "    )\n",
      "    m = ReaLModel(mconfig, dtype=dtype, device=device, hf_model_family=hf_model_family)\n",
      "\n",
      "    if not init_from_scratch:\n",
      "        m._instantiation_hooks.append(\n",
      "            lambda: getattr(m, f\"from_{hf_model_family}\")(\n",
      "                load_dir=model_path, init_critic_from_actor=init_critic_from_actor\n",
      "            )\n",
      "        )\n",
      "\n",
      "    if constants.pipe_parallel_world_size() == 1:\n",
      "        m = add_helper_functions(m)\n",
      "    return model_api.Model(name, m, tokenizer, device, dtype=dtype)\n",
      "\n",
      "\n",
      "model_api.register_model(\"real_model\", make_real_model)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/nn/real_llm_base.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import contextlib\n",
      "import dataclasses\n",
      "import functools\n",
      "import itertools\n",
      "import json\n",
      "import os\n",
      "from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.utils.checkpoint\n",
      "import transformers\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "import realhf.impl.model.parallelism.tensor_parallel.mappings as tensor_parallel\n",
      "from realhf.api.core import model_api\n",
      "from realhf.impl.model.modules import (\n",
      "    CausalSelfAttentionLayer,\n",
      "    GemmaRMSNorm,\n",
      "    LayerNormMLP,\n",
      "    LayerNormMoELayer,\n",
      "    LlamaLayerNormMLP,\n",
      "    LlamaRMSNorm,\n",
      "    OffsetParallelPositionalEmbedding,\n",
      "    OffsetPositionalEmbedding,\n",
      ")\n",
      "from realhf.impl.model.parallelism.tensor_parallel.modules import (\n",
      "    ColumnParallelLinear,\n",
      "    ParallelEmbedding,\n",
      "    gather_from_sequence_parallel_region,\n",
      "    gather_from_tensor_model_parallel_region,\n",
      "    parallel_lm_logits,\n",
      "    scatter_to_tensor_model_parallel_region,\n",
      ")\n",
      "from realhf.impl.model.utils.functional import compute_varlen_position_indices\n",
      "\n",
      "logger = logging.getLogger(\"ReaLModelBase\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PipeTransferData:\n",
      "    \"\"\"Data structure for transferring data between stages.\n",
      "\n",
      "    Each pipeline stage has exactly one PipeTransferData as the input and the output,\n",
      "    no matter how many layers are in this stage.\n",
      "\n",
      "    Attributes:\n",
      "        pp_input: The input to the current stage. Usually hidden states\n",
      "            with shape [bs, seq_len, hidden_dim].\n",
      "        pp_output: The output of the current stage, also the input to the next stage.\n",
      "            Usually hidden states with shape [bs, seq_len, hidden_dim].\n",
      "        cu_seqlens: The cumulative sequence lengths of packed input_ids.\n",
      "            Used by flash_attn_varlen_func. Will not be used during generation.\n",
      "            It's configuration-like data that must be transfered from the first stage\n",
      "            to the last. Shape [bs + 1].\n",
      "        max_seqlen: The maximum sequence length of packed input_ids.\n",
      "            Used by flash_attn_varlen_func. Will not be used during generation.\n",
      "            It's configuration-like data that must be transfered from the first stage\n",
      "            to the last.\n",
      "        store_kv_cache: Whether to store the key and value cache for generation.\n",
      "        attention_mask: The attention mask of the input, the same as huggingface transformers.\n",
      "            Used by torch_attn_func to examine the outputs of PyTorch attention and flash\n",
      "            attention are the same. Only for debugging. Shape [bs, seq_len].\n",
      "    \"\"\"\n",
      "\n",
      "    pp_input: torch.Tensor = None\n",
      "    pp_output: torch.Tensor = None\n",
      "\n",
      "    # The followings are \"configuration\"-like data that should be passed across all stages.\n",
      "    cu_seqlens: torch.Tensor = None\n",
      "    max_seqlen: int = None\n",
      "    store_kv_cache: bool = False\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PipeCacheData:\n",
      "    \"\"\"Data structure for caching data locally that will not be trasferred.\n",
      "\n",
      "    Each layer has exactly one PipeCacheData as the input.\n",
      "    If a pipeline stage has multiple layers, a list of PipeCacheData should be passed\n",
      "    as the input. The cached tensors will be changed in-place.\n",
      "\n",
      "    Attributes:\n",
      "        input_ids: The input token ids. Used only at the first stage.\n",
      "            Can be packed with shape [total_seq_len] or unpacked with shape [bs, seq].\n",
      "        prompt_mask: Prompt mask used\n",
      "        position_ids: Input position IDs. Can be resolved automatically in most cases.\n",
      "            Used only at the first stage. The same shape as input_ids.\n",
      "            If None, will be resolved automatically.\n",
      "        k_cache: Key cache used for generation, shape [bs, max_seq, n_kv_heads, head_dim].\n",
      "            Note that this is the cache for a specific layer, not for all layers.\n",
      "        v_cache: Value cache used for generation, shape [bs, max_seq, n_kv_heads, head_dim].\n",
      "            Note that this is the cache for a specific layer, not for all layers.\n",
      "        cache_seqlens: The sequence lengths of the cached tokens. Used for generation. Shape [bs].\n",
      "    \"\"\"\n",
      "\n",
      "    # Only cached in the first stage.\n",
      "    packed_input_ids: torch.Tensor = None\n",
      "    packed_position_ids: torch.Tensor = None\n",
      "    # Cached in each transformer layer.\n",
      "    k_cache: torch.Tensor = None\n",
      "    v_cache: torch.Tensor = None\n",
      "    cache_seqlens: torch.Tensor = None\n",
      "\n",
      "\n",
      "class ReaLModelBlock(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: model_api.ReaLModelConfig,\n",
      "        layer_index: int,\n",
      "        output_layernorm: bool = False,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        if dtype is None:\n",
      "            dtype = torch.float16\n",
      "        self.config = config\n",
      "        self.layer_index = layer_index\n",
      "        self.attn = CausalSelfAttentionLayer(\n",
      "            hidden_dim=config.hidden_dim,\n",
      "            n_kv_heads=config.n_kv_heads,\n",
      "            n_q_heads=config.n_q_heads,\n",
      "            head_dim=config.head_dim,\n",
      "            resid_pdrop=config.resid_pdrop,\n",
      "            attn_pdrop=config.attn_pdrop,\n",
      "            layer_index=layer_index,\n",
      "            layer_norm_epsilon=config.layer_norm_epsilon,\n",
      "            scale_attn_by_inverse_layer_idx=config.scale_attn_by_inverse_layer_idx,\n",
      "            scale_attn_weights=config.scale_attn_weights,\n",
      "            layer_norm_type=config.layer_norm_type,\n",
      "            use_attention_bias=config.use_attention_bias,\n",
      "            use_attn_proj_bias=config.use_attn_proj_bias,\n",
      "            do_layernorm_before=config.do_layernorm_before,\n",
      "            qk_layernorm=config.qk_layernorm,\n",
      "            apply_rotary=config.apply_rotary,\n",
      "            rotary_base=config.rotary_base,\n",
      "            rotary_interleaved=config.rotary_interleaved,\n",
      "            rotary_scaling=config.rotary_scaling,\n",
      "            rotary_scaling_type=config.rotary_scaling_type,\n",
      "            rotary_special_impl=config.rotary_special_impl,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        if config.mlp_type is None:\n",
      "            self.mlp = LayerNormMLP(\n",
      "                hidden_dim=config.hidden_dim,\n",
      "                intermediate_dim=config.intermediate_dim,\n",
      "                resid_pdrop=config.resid_pdrop,\n",
      "                use_bias=config.use_mlp_bias,\n",
      "                do_layernorm_before=config.do_layernorm_before,\n",
      "                activation_function=config.activation_function,\n",
      "                layer_norm_epsilon=config.layer_norm_epsilon,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "        elif config.mlp_type == \"llama\":\n",
      "            self.mlp = LlamaLayerNormMLP(\n",
      "                hidden_dim=config.hidden_dim,\n",
      "                intermediate_dim=config.intermediate_dim,\n",
      "                activation_function=config.activation_function,\n",
      "                layer_norm_epsilon=config.layer_norm_epsilon,\n",
      "                layer_norm_type=config.layer_norm_type,\n",
      "                use_bias=config.use_mlp_bias,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "        elif config.mlp_type == \"moe\":\n",
      "            self.mlp = LayerNormMoELayer(\n",
      "                config=config,\n",
      "                layer_idx=layer_index,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "        else:\n",
      "            raise NotImplementedError(f\"Unknown MLP type: {config.mlp_type}\")\n",
      "\n",
      "        self.output_layernorm = output_layernorm\n",
      "        if output_layernorm:\n",
      "            if config.layer_norm_type is None:\n",
      "                layer_norm_fn = nn.LayerNorm\n",
      "            elif config.layer_norm_type == \"rms\":\n",
      "                layer_norm_fn = LlamaRMSNorm\n",
      "            elif config.layer_norm_type == \"gemma\":\n",
      "                layer_norm_fn = GemmaRMSNorm\n",
      "            self.ln_f = layer_norm_fn(\n",
      "                config.hidden_dim,\n",
      "                eps=config.layer_norm_epsilon,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "\n",
      "    def forward(self, x: PipeTransferData, y: PipeCacheData) -> PipeTransferData:\n",
      "        pp_input = x.pp_input\n",
      "        cu_seqlens = x.cu_seqlens\n",
      "        k_cache = y.k_cache\n",
      "        v_cache = y.v_cache\n",
      "        cache_seqlens = y.cache_seqlens\n",
      "        max_seqlen = x.max_seqlen\n",
      "        if constants.gradient_checkpointing():\n",
      "            pp_output, k, v = torch.utils.checkpoint.checkpoint(\n",
      "                self._forward,\n",
      "                pp_input,\n",
      "                cu_seqlens,\n",
      "                k_cache,\n",
      "                v_cache,\n",
      "                cache_seqlens,\n",
      "                max_seqlen,\n",
      "                use_reentrant=True,\n",
      "            )\n",
      "        else:\n",
      "            pp_output, k, v = self._forward(\n",
      "                pp_input,\n",
      "                cu_seqlens,\n",
      "                k_cache,\n",
      "                v_cache,\n",
      "                cache_seqlens,\n",
      "                max_seqlen,\n",
      "            )\n",
      "\n",
      "        x.pp_output = pp_output\n",
      "        if x.store_kv_cache:\n",
      "            if y.k_cache is None:\n",
      "                y.k_cache = k.detach()\n",
      "            if y.v_cache is None:\n",
      "                y.v_cache = v.detach()\n",
      "            if y.cache_seqlens is None and x.cu_seqlens is not None:\n",
      "                y.cache_seqlens = x.cu_seqlens[1:] - x.cu_seqlens[:-1]\n",
      "        return x\n",
      "\n",
      "    def _forward(\n",
      "        self,\n",
      "        pp_input: torch.Tensor,\n",
      "        cu_seqlens: torch.Tensor,\n",
      "        k_cache: Optional[torch.Tensor],\n",
      "        v_cache: Optional[torch.Tensor],\n",
      "        cache_seqlens: Optional[torch.Tensor],\n",
      "        max_seqlen: int,\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "        h = pp_input\n",
      "        attn_out, k, v = self.attn(\n",
      "            hidden_states=h,\n",
      "            cu_seqlens=cu_seqlens,\n",
      "            max_seqlen=max_seqlen,\n",
      "            k_cache=k_cache,\n",
      "            v_cache=v_cache,\n",
      "            cache_seqlens=cache_seqlens,\n",
      "        )\n",
      "        h = h + attn_out\n",
      "\n",
      "        # For opt-350m\n",
      "        if not self.config.do_layernorm_before:\n",
      "            h = self.attn.c_attn.ln(h)\n",
      "\n",
      "        h = self.mlp(h) + h\n",
      "\n",
      "        # For opt-350m\n",
      "        if not self.config.do_layernorm_before:\n",
      "            h = self.mlp.ln(h)\n",
      "\n",
      "        if self.output_layernorm:\n",
      "            h = self.ln_f(h)\n",
      "        return h, k, v\n",
      "\n",
      "\n",
      "class VocabPositionEmbedding(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: model_api.ReaLModelConfig,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.n_positions = config.n_positions\n",
      "        self.hidden_dim = config.hidden_dim\n",
      "\n",
      "        tensor_parallel = constants.tensor_parallel_world_size() > 1\n",
      "        if tensor_parallel:\n",
      "            embed_cls = ParallelEmbedding\n",
      "        else:\n",
      "            embed_cls = nn.Embedding\n",
      "\n",
      "        self.wte = embed_cls(\n",
      "            config.vocab_size, config.hidden_dim, dtype=dtype, device=device\n",
      "        )\n",
      "\n",
      "        self.apply_abs_pos_embed = not config.apply_rotary\n",
      "        if self.apply_abs_pos_embed:\n",
      "            p_embed_cls = (\n",
      "                OffsetParallelPositionalEmbedding\n",
      "                if tensor_parallel\n",
      "                else OffsetPositionalEmbedding\n",
      "            )\n",
      "            self.wpe = p_embed_cls(\n",
      "                config.n_positions,\n",
      "                config.hidden_dim,\n",
      "                offset=config.abs_position_embedding_offset,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "\n",
      "        self.normalize_embed = config.normalize_embed\n",
      "        self.embed_drop = nn.Dropout(config.embd_pdrop)\n",
      "\n",
      "    def forward(self, x: PipeTransferData, y: PipeCacheData) -> PipeTransferData:\n",
      "        # Set position ids.\n",
      "        # if y.packed_position_ids is not None:\n",
      "        #     raise ValueError(\"In our use cases, position_ids must be None.\")\n",
      "        y.packed_position_ids = compute_varlen_position_indices(\n",
      "            total_seqlen=y.packed_input_ids.shape[0],\n",
      "            cu_seqlens=x.cu_seqlens,\n",
      "            seqlen_offsets=y.cache_seqlens,\n",
      "        )\n",
      "        if x.max_seqlen > self.n_positions:\n",
      "            raise ValueError(\n",
      "                f\"max_seqlen ({x.max_seqlen}) must be <= n_positions ({self.n_positions}).\"\n",
      "            )\n",
      "        assert y.packed_position_ids.shape == y.packed_input_ids.shape, (\n",
      "            y.packed_position_ids.shape,\n",
      "            y.packed_input_ids.shape,\n",
      "            x.cu_seqlens,\n",
      "        )\n",
      "\n",
      "        x.pp_output = self._forward(y.packed_input_ids, y.packed_position_ids)\n",
      "        return x\n",
      "\n",
      "    def _forward(\n",
      "        self, input_ids: torch.LongTensor, position_ids: torch.LongTensor\n",
      "    ) -> torch.Tensor:\n",
      "        inputs_embeds = self.wte(input_ids)\n",
      "        if self.apply_abs_pos_embed:\n",
      "            inputs_embeds = inputs_embeds + self.wpe(position_ids)\n",
      "        if self.normalize_embed:\n",
      "            normalizer = torch.tensor(self.hidden_dim**0.5, dtype=inputs_embeds.dtype)\n",
      "            inputs_embeds = inputs_embeds * normalizer\n",
      "        if constants.sequence_parallel():\n",
      "            inputs_embeds = tensor_parallel.scatter_to_sequence_parallel_region(\n",
      "                inputs_embeds\n",
      "            )\n",
      "            # `scatter_to_sequence_parallel_region` returns a view, which prevents\n",
      "            # the original tensor from being garbage collected. Clone to facilitate GC.\n",
      "            # Has a small runtime cost (~0.5%).\n",
      "            inputs_embeds = inputs_embeds.clone()\n",
      "            # with tensor_parallel.get_cuda_rng_tracker().fork():\n",
      "            x = self.embed_drop(inputs_embeds)\n",
      "        else:\n",
      "            x = self.embed_drop(inputs_embeds)\n",
      "        return x\n",
      "\n",
      "\n",
      "class OutputHead(nn.Linear):\n",
      "    def __init__(\n",
      "        self, *args, norm_head: bool = False, norm_softmax: bool = False, **kwargs\n",
      "    ):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self._norm_head = norm_head\n",
      "        self._norm_softmax = norm_softmax\n",
      "\n",
      "    def forward(self, x: PipeTransferData, y: PipeCacheData) -> PipeTransferData:\n",
      "        x.pp_output = self._forward(x.pp_input)\n",
      "        return x\n",
      "\n",
      "    def _forward(self, x: torch.Tensor):\n",
      "        if self._norm_head and self.out_features != 1:\n",
      "            unnormed_head = nn.functional.linear(\n",
      "                torch.eye(\n",
      "                    self.in_features, dtype=self.weight.dtype, device=self.weight.device\n",
      "                ),\n",
      "                self.weight,\n",
      "            ).transpose(1, 0)\n",
      "            head_norm = unnormed_head.norm(dim=0, keepdim=True, p=2)\n",
      "            normed_head = unnormed_head / (head_norm + 1e-7)\n",
      "            logits = nn.functional.linear(x, normed_head, None)\n",
      "        else:\n",
      "            logits = super().forward(x)\n",
      "\n",
      "        if self._norm_softmax and self.out_features != 1:\n",
      "            logits = logits / (torch.std(logits, dim=-1, keepdim=True) + 1e-6)\n",
      "        return logits\n",
      "\n",
      "\n",
      "class SequenceParallelCriticHead(nn.Linear):\n",
      "\n",
      "    def forward(self, x: PipeTransferData, y: PipeCacheData) -> PipeTransferData:\n",
      "        all_gather_buffer = tensor_parallel.gather_from_sequence_parallel_region(\n",
      "            x.pp_input\n",
      "        )\n",
      "        x.pp_output = nn.functional.linear(all_gather_buffer, self.weight, self.bias)\n",
      "        return x\n",
      "\n",
      "    def _forward(self, x: torch.Tensor):\n",
      "        x = tensor_parallel.gather_from_sequence_parallel_region(x)\n",
      "        return super().forward(x)\n",
      "\n",
      "\n",
      "class ParallelActorHead(ColumnParallelLinear):\n",
      "\n",
      "    def __init__(\n",
      "        self, *args, norm_head: bool = False, norm_softmax: bool = False, **kwargs\n",
      "    ):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self._norm_head = norm_head\n",
      "        self._norm_softmax = norm_softmax\n",
      "\n",
      "    def forward(self, x: PipeTransferData, y: PipeCacheData) -> PipeTransferData:\n",
      "        x.pp_output = self._forward(x.pp_input)\n",
      "        return x\n",
      "\n",
      "    def _forward(self, x: torch.Tensor):\n",
      "        weight = self.weight\n",
      "        if self._norm_head:\n",
      "            from realhf.impl.model.parallelism.tensor_parallel.mappings import (\n",
      "                gather_from_sequence_parallel_region,\n",
      "            )\n",
      "\n",
      "            # HACK: This is a terrible implementation for Bailing's head norming,\n",
      "            # because we basically eliminates TP for the LM head.\n",
      "            whole_weight = gather_from_sequence_parallel_region(self.weight)\n",
      "            unnormed_head = nn.functional.linear(\n",
      "                torch.eye(\n",
      "                    self.input_size, dtype=self.weight.dtype, device=self.weight.device\n",
      "                ),\n",
      "                whole_weight,\n",
      "            ).transpose(1, 0)\n",
      "            head_norm = unnormed_head.norm(dim=0, keepdim=True, p=2)\n",
      "            normed_head = unnormed_head / (head_norm + 1e-7)\n",
      "            weight = tensor_parallel.scatter_to_sequence_parallel_region(normed_head)\n",
      "\n",
      "        output = parallel_lm_logits(\n",
      "            x,\n",
      "            weight,\n",
      "            parallel_output=True,\n",
      "            gradient_accumulation_fusion=self.gradient_accumulation_fusion,\n",
      "            bias=self.bias,\n",
      "        )\n",
      "\n",
      "        if self._norm_softmax:\n",
      "            whole_output = gather_from_tensor_model_parallel_region(output)\n",
      "            whole_output = whole_output / (\n",
      "                torch.std(whole_output, dim=-1, keepdim=True) + 1e-6\n",
      "            )\n",
      "            output = scatter_to_tensor_model_parallel_region(whole_output)\n",
      "\n",
      "        return output\n",
      "\n",
      "\n",
      "class ReaLModelParamKeys:\n",
      "    \"\"\"The keys of parameters in ReaLModel, used for parameter reallocation.\n",
      "\n",
      "    **IMPORTANT**: The returned keys are **ordered**. They should have\n",
      "    the same order as we iterate layer indices and call\n",
      "    layer.state_dict().\n",
      "    \"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def embed(config: model_api.ReaLModelConfig) -> int:\n",
      "        keys = [\"0.wte.weight\"]\n",
      "        if not config.apply_rotary:\n",
      "            keys += [\"0.wpe.weight\"]\n",
      "        return keys\n",
      "\n",
      "    @staticmethod\n",
      "    def tblock(config: model_api.ReaLModelConfig, idx: int) -> List[str]:\n",
      "        # NOTE: `idx`` is the index of transformer blocks,\n",
      "        # i.e, 0 for the first block or the second layer of the transformer.\n",
      "        # NOTE: The order matters, we should not change the order of keys.\n",
      "        keys = [f\"{idx + 1}.attn.c_attn.ln.weight\"]\n",
      "        if config.layer_norm_type is None:\n",
      "            keys += [f\"{idx + 1}.attn.c_attn.ln.bias\"]\n",
      "        keys += [f\"{idx + 1}.attn.c_attn.q_attn.weight\"]\n",
      "        if config.use_attention_bias:\n",
      "            keys += [f\"{idx + 1}.attn.c_attn.q_attn.bias\"]\n",
      "        keys += [f\"{idx + 1}.attn.c_attn.k_attn.weight\"]\n",
      "        if config.use_attention_bias:\n",
      "            keys += [f\"{idx + 1}.attn.c_attn.k_attn.bias\"]\n",
      "        keys += [f\"{idx + 1}.attn.c_attn.v_attn.weight\"]\n",
      "        if config.use_attention_bias:\n",
      "            keys += [f\"{idx + 1}.attn.c_attn.v_attn.bias\"]\n",
      "        keys += [f\"{idx + 1}.attn.c_proj.weight\"]\n",
      "        if config.use_attn_proj_bias:\n",
      "            keys += [f\"{idx + 1}.attn.c_proj.bias\"]\n",
      "        if config.qk_layernorm:\n",
      "            keys += [f\"{idx + 1}.attn.q_ln.weight\"]\n",
      "            keys += [f\"{idx + 1}.attn.k_ln.weight\"]\n",
      "            if config.layer_norm_type is None:\n",
      "                keys += [f\"{idx + 1}.attn.q_ln.bias\"]\n",
      "                keys += [f\"{idx + 1}.attn.k_ln.bias\"]\n",
      "        keys += [f\"{idx + 1}.mlp.ln.weight\"]\n",
      "        if config.layer_norm_type is None:\n",
      "            keys += [f\"{idx + 1}.mlp.ln.bias\"]\n",
      "\n",
      "        if config.mlp_type is None:\n",
      "            if config.use_mlp_bias:\n",
      "                keys += [\n",
      "                    f\"{idx + 1}.mlp.c_fc.weight\",\n",
      "                    f\"{idx + 1}.mlp.c_fc.bias\",\n",
      "                    f\"{idx + 1}.mlp.c_proj.weight\",\n",
      "                    f\"{idx + 1}.mlp.c_proj.bias\",\n",
      "                ]\n",
      "            else:\n",
      "                keys += [\n",
      "                    f\"{idx + 1}.mlp.c_fc.weight\",\n",
      "                    f\"{idx + 1}.mlp.c_proj.weight\",\n",
      "                ]\n",
      "        elif config.mlp_type == \"llama\":\n",
      "            weights_key = [\n",
      "                f\"{idx + 1}.mlp.gate_proj.weight\",\n",
      "                f\"{idx + 1}.mlp.up_proj.weight\",\n",
      "                f\"{idx + 1}.mlp.down_proj.weight\",\n",
      "            ]\n",
      "            if config.use_mlp_bias:\n",
      "                keys += list(\n",
      "                    itertools.chain.from_iterable(\n",
      "                        zip(\n",
      "                            weights_key,\n",
      "                            [\n",
      "                                f\"{idx + 1}.mlp.gate_proj.bias\",\n",
      "                                f\"{idx + 1}.mlp.up_proj.bias\",\n",
      "                                f\"{idx + 1}.mlp.down_proj.bias\",\n",
      "                            ],\n",
      "                        )\n",
      "                    )\n",
      "                )\n",
      "            else:\n",
      "                keys += weights_key\n",
      "        elif config.mlp_type == \"moe\":\n",
      "            num_experts = config.moe.num_experts\n",
      "            keys += [\n",
      "                f\"{idx + 1}.mlp.router.weight\",\n",
      "            ]\n",
      "            for j in range(num_experts):\n",
      "                weights_key = [\n",
      "                    f\"{idx + 1}.mlp.experts.local_experts.{j}.gate_proj.weight\",\n",
      "                    f\"{idx + 1}.mlp.experts.local_experts.{j}.up_proj.weight\",\n",
      "                    f\"{idx + 1}.mlp.experts.local_experts.{j}.down_proj.weight\",\n",
      "                ]\n",
      "                if config.use_mlp_bias:\n",
      "                    keys += list(\n",
      "                        itertools.chain.from_iterable(\n",
      "                            zip(\n",
      "                                weights_key,\n",
      "                                [\n",
      "                                    f\"{idx + 1}.mlp.experts.local_experts.{j}.gate_proj.bias\",\n",
      "                                    f\"{idx + 1}.mlp.experts.local_experts.{j}.up_proj.bias\",\n",
      "                                    f\"{idx + 1}.mlp.experts.local_experts.{j}.down_proj.bias\",\n",
      "                                ],\n",
      "                            )\n",
      "                        )\n",
      "                    )\n",
      "                else:\n",
      "                    keys += weights_key\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "        if idx == config.n_layers - 1:\n",
      "            keys += [f\"{idx + 1}.ln_f.weight\"]\n",
      "            if config.layer_norm_type is None:\n",
      "                keys += [f\"{idx + 1}.ln_f.bias\"]\n",
      "        return keys\n",
      "\n",
      "    @staticmethod\n",
      "    def head(config: model_api.ReaLModelConfig) -> List[str]:\n",
      "        return [f\"{config.n_layers + 1}.weight\"]\n",
      "\n",
      "\n",
      "def keys_from_layer_indices(\n",
      "    config: model_api.ReaLModelConfig, layer_indices: List[int]\n",
      ") -> List[str]:\n",
      "    # assert _is_integer_list_contiguous(layer_indices)\n",
      "    sd_keys = []\n",
      "    for layer_idx in sorted(layer_indices):\n",
      "        if layer_idx == 0:\n",
      "            sd_keys += ReaLModelParamKeys.embed(config)\n",
      "        elif layer_idx == config.n_layers + 1:\n",
      "            sd_keys += ReaLModelParamKeys.head(config)\n",
      "        else:\n",
      "            sd_keys += ReaLModelParamKeys.tblock(config, layer_idx - 1)\n",
      "    return sd_keys\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/nn/flatten_param.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import contextlib\n",
      "import dataclasses\n",
      "import math\n",
      "import os\n",
      "import subprocess\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.utils.cpp_extension as torch_cpp_ext\n",
      "from packaging.version import Version, parse\n",
      "\n",
      "import realhf\n",
      "from realhf.api.core import model_api\n",
      "from realhf.api.core.config import ModelName\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "from .real_llm_base import ReaLModelParamKeys\n",
      "from .real_llm_parallel import (\n",
      "    get_real_model_param_shape,\n",
      "    intervals_partition_fn,\n",
      "    shape_partition_fn,\n",
      "    tp_partition_key,\n",
      ")\n",
      "\n",
      "try:\n",
      "    from realhf._C.interval_op import merge_intervals\n",
      "except ImportError:\n",
      "    merge_intervals = None\n",
      "logger = logging.getLogger(\"FlattenParam\")\n",
      "\n",
      "_FLAT_PARAM_INDICES_CACHE = {}\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class ContiguousParamSpec:\n",
      "    start_idx: int\n",
      "    end_idx: int\n",
      "    shape: torch.Size\n",
      "\n",
      "\n",
      "def _is_integer_list_contiguous(l: List[int]) -> bool:\n",
      "    return np.all(np.array(l) == np.arange(len(l)) + l[0])\n",
      "\n",
      "\n",
      "def _are_intervals_contiguous(l: List[Tuple[int, int]]) -> bool:\n",
      "    l = sorted(l, key=lambda x: x[0])\n",
      "    res = True\n",
      "    for i in range(len(l) - 1):\n",
      "        res &= l[i][1] == l[i + 1][0]\n",
      "    return res\n",
      "\n",
      "\n",
      "def recursive_getattr(obj, attr_string):\n",
      "    attrs = attr_string.split(\".\")\n",
      "    for attr in attrs:\n",
      "        obj = getattr(obj, attr)\n",
      "    return obj\n",
      "\n",
      "\n",
      "def _slice_intervals_py(src: torch.Tensor, intervals: List[Tuple[int, int]]):\n",
      "    # Drop-in replacement for the C++ implementation.\n",
      "    assert len(src.shape) == 1\n",
      "    assert all([x[0] >= 0 for x in intervals])\n",
      "    assert all([x[1] <= src.shape[0] for x in intervals])\n",
      "    N = len(intervals)\n",
      "    slices = []\n",
      "    for i, j in intervals:\n",
      "        slices.append(src[i:j])\n",
      "    return torch.cat(slices, dim=0)\n",
      "\n",
      "\n",
      "def _set_intervals_py(\n",
      "    src: torch.Tensor,\n",
      "    dst: torch.Tensor,\n",
      "    intervals: List[Tuple[int, int]],\n",
      "):\n",
      "    # Drop-in replacement for the C++ implementation.\n",
      "    assert len(dst.shape) == len(src.shape) == 1\n",
      "    offset = 0\n",
      "    for i, j in intervals:\n",
      "        assert i >= 0\n",
      "        assert j <= dst.shape[0], (j, dst.shape[0])\n",
      "        dst[i:j] = src[offset : offset + j - i]\n",
      "        offset += j - i\n",
      "    assert offset == src.shape[0]\n",
      "\n",
      "\n",
      "_SLICE_INTERVAL_EXT_WARNED = False\n",
      "_SET_INTERVAL_EXT_WARNED = False\n",
      "\n",
      "\n",
      "def slice_intervals(\n",
      "    src: torch.Tensor,\n",
      "    intervals_cpu: List[Tuple[int, int]] = None,\n",
      "    intervals_cuda: torch.Tensor = None,\n",
      "    output_size: int = None,\n",
      "    max_interval_size: Optional[int] = None,\n",
      "):\n",
      "    if intervals_cpu is not None:\n",
      "        N = len(intervals_cpu)\n",
      "    else:\n",
      "        N = intervals_cuda.size(0)\n",
      "    if not constants.use_cuda() or N < 1024:\n",
      "        # NOTE: The CUDA implementation will launch a thread for each interval,\n",
      "        # which has a negative effect when the number of intervals is small.\n",
      "        return _slice_intervals_py(src, intervals_cpu)\n",
      "    try:\n",
      "        from realhf._C.interval_op_cuda import (\n",
      "            slice_intervals_bf16,\n",
      "            slice_intervals_fp16,\n",
      "            slice_intervals_fp32,\n",
      "        )\n",
      "\n",
      "        if src.dtype == torch.float32:\n",
      "            return slice_intervals_fp32(\n",
      "                src, intervals_cuda, output_size, max_interval_size\n",
      "            )\n",
      "        elif src.dtype == torch.float16:\n",
      "            return slice_intervals_fp16(\n",
      "                src, intervals_cuda, output_size, max_interval_size\n",
      "            )\n",
      "        elif src.dtype == torch.bfloat16:\n",
      "            return slice_intervals_bf16(\n",
      "                src, intervals_cuda, output_size, max_interval_size\n",
      "            )\n",
      "        else:\n",
      "            raise NotImplementedError(src.dtype)\n",
      "    except ImportError:\n",
      "        global _SLICE_INTERVAL_EXT_WARNED\n",
      "        if not _SLICE_INTERVAL_EXT_WARNED:\n",
      "            _SLICE_INTERVAL_EXT_WARNED = True\n",
      "            logger.warning(\n",
      "                f\"The `slice_interval` extension not found. \"\n",
      "                \"Fallback to python, which can be very slow. \"\n",
      "                \"You should re-install the package with REAL_CUDA=1 or \"\n",
      "                \"set REAL_PARAM_REALLOC_OPT_LEVEL=1.\"\n",
      "            )\n",
      "        return _slice_intervals_py(src, intervals_cpu)\n",
      "\n",
      "\n",
      "def set_intervals(\n",
      "    src: torch.Tensor,\n",
      "    dst: torch.Tensor,\n",
      "    intervals_cpu: List[Tuple[int, int]] = None,\n",
      "    intervals_cuda: torch.Tensor = None,\n",
      "    max_interval_size: Optional[int] = None,\n",
      "):\n",
      "    if intervals_cpu is not None:\n",
      "        N = len(intervals_cpu)\n",
      "    else:\n",
      "        N = intervals_cuda.size(0)\n",
      "    if not constants.use_cuda() or N < 512 or not (src.is_cuda and dst.is_cuda):\n",
      "        # NOTE: The CUDA implementation will launch a thread for each interval,\n",
      "        # which has a negative effect when the number of intervals is small.\n",
      "        return _set_intervals_py(src, dst, intervals_cpu)\n",
      "    try:\n",
      "        from realhf._C.interval_op_cuda import (\n",
      "            set_intervals_bf16,\n",
      "            set_intervals_fp16,\n",
      "            set_intervals_fp32,\n",
      "        )\n",
      "\n",
      "        if src.dtype == torch.float32:\n",
      "            return set_intervals_fp32(src, dst, intervals_cuda, max_interval_size)\n",
      "        elif src.dtype == torch.float16:\n",
      "            return set_intervals_fp16(src, dst, intervals_cuda, max_interval_size)\n",
      "        elif src.dtype == torch.bfloat16:\n",
      "            return set_intervals_bf16(src, dst, intervals_cuda, max_interval_size)\n",
      "        else:\n",
      "            raise NotImplementedError(src.dtype)\n",
      "    except ImportError:\n",
      "        global _SET_INTERVAL_EXT_WARNED\n",
      "        if not _SET_INTERVAL_EXT_WARNED:\n",
      "            _SET_INTERVAL_EXT_WARNED = True\n",
      "            logger.warning(\n",
      "                f\"The `set_interval` extension not found. \"\n",
      "                \"Fallback to python, which can be very slow. \"\n",
      "                \"You should re-install the package with REAL_CUDA=1 or \"\n",
      "                \"set REAL_PARAM_REALLOC_OPT_LEVEL=1.\"\n",
      "            )\n",
      "        return _set_intervals_py(src, dst, intervals_cpu)\n",
      "\n",
      "\n",
      "def param_size_from_keys(\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    src_tp_size: int,\n",
      "    sd_keys: List[str],\n",
      "    src2dst_tp_size: int,\n",
      "    src2dst_tp_rank: int,\n",
      "    head_param_point_to_embedding: bool,\n",
      ") -> Tuple[List[int], int]:\n",
      "    param_size = 0\n",
      "    for k in sd_keys:\n",
      "        if (\n",
      "            head_param_point_to_embedding\n",
      "            and k == f\"{config.n_layers + 1}.weight\"\n",
      "            and \"0.wte.weight\" in sd_keys\n",
      "        ):\n",
      "            continue\n",
      "        new_shape = tp_partition_key(\n",
      "            k,\n",
      "            get_real_model_param_shape(k, config, src_tp_size),\n",
      "            src2dst_tp_rank,\n",
      "            src2dst_tp_size,\n",
      "            config,\n",
      "            partition_fn=shape_partition_fn,\n",
      "        )\n",
      "        param_size += int(np.prod(new_shape))\n",
      "    return param_size\n",
      "\n",
      "\n",
      "def build_param_spec(\n",
      "    layer_indices: List[int],\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    dp_size: int,\n",
      "    tp_size: int,\n",
      "    pp_size: int,\n",
      "    head_param_point_to_embedding: bool,\n",
      "    bucket_size: int = 40000000,\n",
      ") -> Tuple[Dict[str, ContiguousParamSpec], int]:\n",
      "    # TODO: omit parameters that do not require gradient?\n",
      "    # TODO: allow different dtypes for different buckets\n",
      "    if len(layer_indices) == 0:\n",
      "        return {}, 0\n",
      "\n",
      "    disable_bucketing = 0 not in layer_indices\n",
      "\n",
      "    sd_keys = []\n",
      "    for layer_idx in sorted(layer_indices):\n",
      "        if layer_idx == 0:\n",
      "            sd_keys += ReaLModelParamKeys.embed(config)\n",
      "        elif layer_idx == config.n_layers + 1:\n",
      "            sd_keys += ReaLModelParamKeys.head(config)\n",
      "        else:\n",
      "            sd_keys += ReaLModelParamKeys.tblock(config, layer_idx - 1)\n",
      "\n",
      "    # In the reverse order as backpropagation, consistent with Megatron.\n",
      "    sd_keys = list(reversed(sd_keys))\n",
      "\n",
      "    data_start_index = 0\n",
      "    bucket_data_start_index = data_start_index\n",
      "    bucket_params = set()\n",
      "\n",
      "    def _requires_new_allreduce_bucket(k):\n",
      "        if pp_size == 1:\n",
      "            return False\n",
      "        if config.is_critic:\n",
      "            return False\n",
      "        if not config.tied_embedding:\n",
      "            return False\n",
      "        return k == f\"{config.n_layers + 1}.weight\" or k == \"0.wte.weight\"\n",
      "\n",
      "    def _pad_to_multiple(x, m):\n",
      "        return int(math.ceil(x / m)) * m\n",
      "\n",
      "    def _create_fake_bucket(data_end_index) -> int:\n",
      "        nonlocal bucket_data_start_index, bucket_params\n",
      "        data_end_index = _pad_to_multiple(data_end_index, dp_size)\n",
      "        # Update bucket metadata.\n",
      "        bucket_data_start_index = data_end_index\n",
      "        # Re-set bucket_params and increment bucket_id for next bucket.\n",
      "        bucket_params = set()\n",
      "        # Return the potentially padded data_end_index.\n",
      "        return data_end_index\n",
      "\n",
      "    param_spec = {}\n",
      "    for k in sd_keys:\n",
      "        if head_param_point_to_embedding and k == f\"{config.n_layers + 1}.weight\":\n",
      "            continue\n",
      "\n",
      "        shape = get_real_model_param_shape(k, config, tp_size)\n",
      "        numel = int(np.prod(shape))\n",
      "        data_end_index = data_start_index + numel\n",
      "\n",
      "        if _requires_new_allreduce_bucket(k) and len(bucket_params) > 0:\n",
      "            _create_fake_bucket(data_start_index)\n",
      "\n",
      "        param_spec[k] = ContiguousParamSpec(\n",
      "            data_start_index,\n",
      "            data_end_index,\n",
      "            shape,\n",
      "        )\n",
      "        bucket_params.add(k)\n",
      "        if (\n",
      "            not disable_bucketing\n",
      "            and (data_end_index - bucket_data_start_index) >= bucket_size\n",
      "        ) or _requires_new_allreduce_bucket(k):\n",
      "            data_end_index = _create_fake_bucket(data_end_index)\n",
      "\n",
      "        data_start_index = data_end_index\n",
      "\n",
      "    if len(bucket_params) > 0:\n",
      "        data_end_index = _create_fake_bucket(data_end_index)\n",
      "\n",
      "    if head_param_point_to_embedding and f\"{config.n_layers + 1}.weight\" in sd_keys:\n",
      "        param_spec[f\"{config.n_layers + 1}.weight\"] = param_spec[\"0.wte.weight\"]\n",
      "    return param_spec, data_end_index\n",
      "\n",
      "\n",
      "def param_intervals_from_keys(\n",
      "    model_name: ModelName,\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    head_param_point_to_embedding: bool,\n",
      "    param_spec: Dict[str, ContiguousParamSpec],\n",
      "    tp_size: int,\n",
      "    sd_keys: List[str],\n",
      "    portion_size: int,\n",
      "    portion_rank: int,\n",
      ") -> List[int]:\n",
      "    param_size = param_size_from_keys(\n",
      "        config=config,\n",
      "        src_tp_size=tp_size,\n",
      "        sd_keys=sd_keys,\n",
      "        src2dst_tp_size=portion_size,\n",
      "        src2dst_tp_rank=portion_rank,\n",
      "        head_param_point_to_embedding=head_param_point_to_embedding,\n",
      "    )\n",
      "\n",
      "    interval_size = 0\n",
      "    intervals = []\n",
      "    for k in sd_keys:\n",
      "        if (\n",
      "            head_param_point_to_embedding\n",
      "            and k == f\"{config.n_layers + 1}.weight\"\n",
      "            and \"0.wte.weight\" in sd_keys\n",
      "        ):\n",
      "            continue\n",
      "        if (\n",
      "            model_name,\n",
      "            k.split(\".\", 1)[1],\n",
      "            tp_size,\n",
      "            portion_rank,\n",
      "            portion_size,\n",
      "        ) not in _FLAT_PARAM_INDICES_CACHE:\n",
      "            zero_start_intervals = tp_partition_key(\n",
      "                k,\n",
      "                get_real_model_param_shape(k, config, tp_size),\n",
      "                portion_rank,\n",
      "                portion_size,\n",
      "                config,\n",
      "                partition_fn=intervals_partition_fn,\n",
      "            )\n",
      "            _FLAT_PARAM_INDICES_CACHE[\n",
      "                (\n",
      "                    model_name,\n",
      "                    k.split(\".\", 1)[1],\n",
      "                    tp_size,\n",
      "                    portion_rank,\n",
      "                    portion_size,\n",
      "                )\n",
      "            ] = zero_start_intervals\n",
      "        else:\n",
      "            zero_start_intervals = _FLAT_PARAM_INDICES_CACHE[\n",
      "                (\n",
      "                    model_name,\n",
      "                    k.split(\".\", 1)[1],\n",
      "                    tp_size,\n",
      "                    portion_rank,\n",
      "                    portion_size,\n",
      "                )\n",
      "            ]\n",
      "        intervals += (zero_start_intervals + param_spec[k].start_idx).tolist()\n",
      "        interval_size += sum(zero_start_intervals[:, 1] - zero_start_intervals[:, 0])\n",
      "    # assert len(set([x[0] for x in intervals])) == len(intervals)\n",
      "    assert interval_size == param_size, (interval_size, param_size)\n",
      "    if merge_intervals is not None:\n",
      "        intervals = merge_intervals(intervals)\n",
      "    return intervals\n",
      "\n",
      "\n",
      "def map_param_to_contigous_memory(\n",
      "    layers: torch.nn.ModuleList,\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    head_param_point_to_embedding: bool,\n",
      "    param_spec: Dict[str, ContiguousParamSpec],\n",
      "    contiguous_param: torch.Tensor,\n",
      "    layer_idx_offset: int,\n",
      "    allocate_only: bool,\n",
      "):\n",
      "    for local_layer_idx, l in enumerate(layers):\n",
      "        layer_idx = local_layer_idx + layer_idx_offset\n",
      "        for k, v in l.named_parameters():\n",
      "            spec = param_spec[f\"{layer_idx}.{k}\"]\n",
      "            old_param_data = v.data\n",
      "            target = contiguous_param[spec.start_idx : spec.end_idx].view(spec.shape)\n",
      "            if not allocate_only:\n",
      "                target.copy_(old_param_data)\n",
      "            else:\n",
      "                if not (\n",
      "                    head_param_point_to_embedding and layer_idx == config.n_layers + 1\n",
      "                ):\n",
      "                    assert old_param_data.shape == torch.Size([0]), (\n",
      "                        old_param_data.shape,\n",
      "                        spec.shape,\n",
      "                        f\"{layer_idx}.{k}\",\n",
      "                    )\n",
      "            recursive_getattr(l, k).data = target\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/nn/real_llm_parallel.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import *\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "from realhf.api.core import model_api\n",
      "from realhf.base import constants, datapack, logging\n",
      "from realhf.impl.model.nn.real_llm_base import ReaLModelParamKeys\n",
      "\n",
      "logger = logging.getLogger(\"ReaL parallel\")\n",
      "\n",
      "# keys used to identify modules\n",
      "EMBEDDING_KEYS = [\".wte\", \".wpe\"]  # dim=0 no bias\n",
      "COLUMN_LINEAR_KEYS = [\n",
      "    \".attn.c_attn.q_attn\",\n",
      "    \".attn.c_attn.k_attn\",\n",
      "    \".attn.c_attn.v_attn\",\n",
      "    \".mlp.c_fc\",\n",
      "    \".gate_proj\",\n",
      "    \".up_proj\",\n",
      "]  # dim=0 + partition bias\n",
      "ROW_LINEAR_KEYS = [\n",
      "    \".attn.c_proj\",\n",
      "    \".down_proj\",\n",
      "    \".mlp.c_proj\",\n",
      "]  # dim=1 + no partition bias\n",
      "\n",
      "if constants.use_te_impl():\n",
      "    COLUMN_LINEAR_KEYS = [\n",
      "        \".attn.c_attn.q_attn\",\n",
      "        \".attn.c_attn.k_attn\",\n",
      "        \".attn.c_attn.v_attn\",\n",
      "        \".mlp.c_fc\",\n",
      "        \".mlp.fc1_weight\",\n",
      "    ]  # dim=0 + partition bias\n",
      "    ROW_LINEAR_KEYS = [\".attn.c_proj\", \".mlp.fc2_weight\"]\n",
      "\n",
      "\n",
      "def tensor_slice_partition_fn(\n",
      "    tensor: torch.Tensor,\n",
      "    tp_rank: Optional[int],\n",
      "    tp_world_size: int,\n",
      "    dim: Optional[int],\n",
      ") -> Union[List[torch.Tensor], torch.Tensor]:\n",
      "    \"\"\"Partition a tensor by slicing along a dimension for tensor-model\n",
      "    parallelism.\"\"\"\n",
      "    if dim is None:\n",
      "        splits = [tensor for _ in range(tp_world_size)]\n",
      "    else:\n",
      "        assert tensor.shape[dim] % tp_world_size == 0\n",
      "        splits = torch.split(tensor, tensor.shape[dim] // tp_world_size, dim=dim)\n",
      "    if tp_rank is None:\n",
      "        return [s.contiguous() for s in splits]\n",
      "    else:\n",
      "        return splits[tp_rank].contiguous()\n",
      "\n",
      "\n",
      "def intervals_partition_fn(\n",
      "    shape: torch.Size,\n",
      "    tp_rank: Optional[int],\n",
      "    tp_world_size: int,\n",
      "    dim: Optional[int],\n",
      ") -> Union[List[torch.Tensor], torch.Tensor]:\n",
      "    \"\"\"Get the intervals of a MP-partitioned tensor in the flatten view.\n",
      "\n",
      "    For example, if a tensor of shape (2, 4) is partitioned along the\n",
      "    second dimension into 2 parts, then the intervals of the first part\n",
      "    are [(0, 2), (2, 4)].\n",
      "\n",
      "    Used by parameter reallocation. Return a numpy array of shape [N,\n",
      "    2], where N is the number of intervals.\n",
      "    \"\"\"\n",
      "    assert tp_rank is not None\n",
      "    param_size = int(np.prod(shape))\n",
      "    if dim is None:\n",
      "        return np.array([(0, param_size)], dtype=np.int64)\n",
      "\n",
      "    if dim < 0:\n",
      "        dim = len(shape) + dim\n",
      "    assert shape[dim] % tp_world_size == 0\n",
      "\n",
      "    if len(shape) == 1:\n",
      "        assert dim == 0\n",
      "        partition_size = shape[0] // tp_world_size\n",
      "        return np.array(\n",
      "            [(partition_size * tp_rank, partition_size * (tp_rank + 1))],\n",
      "            dtype=np.int64,\n",
      "        )\n",
      "    else:\n",
      "        assert len(shape) == 2, shape\n",
      "        if dim == 0:\n",
      "            row_start = tp_rank * shape[0] // tp_world_size\n",
      "            row_end = (tp_rank + 1) * shape[0] // tp_world_size\n",
      "            return np.array(\n",
      "                [(row_start * shape[1], row_end * shape[1])], dtype=np.int64\n",
      "            )\n",
      "        else:\n",
      "            assert dim == 1\n",
      "            col_start = tp_rank * shape[1] // tp_world_size\n",
      "            col_end = (tp_rank + 1) * shape[1] // tp_world_size\n",
      "            return np.arange(shape[0], dtype=np.int64)[:, None] * shape[1] + np.array(\n",
      "                [(col_start, col_end)], dtype=np.int64\n",
      "            )\n",
      "\n",
      "\n",
      "def shape_partition_fn(\n",
      "    shape: torch.Size,\n",
      "    tp_rank: Optional[int],\n",
      "    tp_world_size: int,\n",
      "    dim: Optional[int],\n",
      "):\n",
      "    \"\"\"Get the partitioned shape of a tensor for tensor-model parallelism.\"\"\"\n",
      "    if dim is None:\n",
      "        splits = [shape for _ in range(tp_world_size)]\n",
      "    else:\n",
      "        if dim < 0:\n",
      "            dim = len(shape) + dim\n",
      "        assert shape[dim] % tp_world_size == 0\n",
      "        splits = [\n",
      "            (*shape[:dim], shape[dim] // tp_world_size, *shape[dim + 1 :])\n",
      "            for _ in range(tp_world_size)\n",
      "        ]\n",
      "    if tp_rank is None:\n",
      "        return [s for s in splits]\n",
      "    else:\n",
      "        return splits[tp_rank]\n",
      "\n",
      "\n",
      "def tp_partition_key(\n",
      "    key: str,\n",
      "    tensor_or_shape: torch.Tensor | torch.Size,\n",
      "    tp_rank: Optional[int],\n",
      "    tp_size: Optional[int],\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    partition_fn: Callable[\n",
      "        [torch.Tensor, Optional[int], int, Optional[int]],\n",
      "        Union[List[torch.Tensor], torch.Tensor],\n",
      "    ] = tensor_slice_partition_fn,\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"Run the partition functor on the tensor or shape based on the key.\n",
      "\n",
      "    The key determines the partitioning strategy, e.g., whether to\n",
      "    perform partition and along which dimension.\n",
      "    \"\"\"\n",
      "\n",
      "    if any([ek in key for ek in EMBEDDING_KEYS]):\n",
      "        assert \"weight\" in key\n",
      "        return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=0)\n",
      "    elif key == f\"{config.n_layers + 1}.weight\":  # output head\n",
      "        if (\n",
      "            isinstance(tensor_or_shape, torch.Tensor) and tensor_or_shape.shape[0] == 1\n",
      "        ) or (\n",
      "            not isinstance(tensor_or_shape, torch.Tensor) and tensor_or_shape[0] == 1\n",
      "        ):\n",
      "            assert config.is_critic\n",
      "            return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=None)\n",
      "        else:\n",
      "            return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=0)\n",
      "    elif any([ck in key for ck in COLUMN_LINEAR_KEYS]):\n",
      "        if (\n",
      "            (\"k_attn\" in key) or (\"v_attn\" in key)\n",
      "        ) and config.n_kv_heads % tp_size != 0:\n",
      "            return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=None)\n",
      "        # partition both weight and bias\n",
      "        return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=0)\n",
      "    elif any([rk in key for rk in ROW_LINEAR_KEYS]):\n",
      "        # only partition weight\n",
      "        if \"weight\" in key:\n",
      "            return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=1)\n",
      "        else:\n",
      "            assert \"bias\" in key, key\n",
      "            return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=None)\n",
      "    else:\n",
      "        return partition_fn(tensor_or_shape, tp_rank, tp_size, dim=None)\n",
      "\n",
      "\n",
      "def tp_partition_real_model_state_dict(\n",
      "    state_dict: Dict[str, torch.Tensor],\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    tp_size: int,\n",
      "    tp_rank: Optional[int] = None,\n",
      ") -> Union[Dict, List[Dict]]:\n",
      "    \"\"\"A helper function to partition a state dict using `tp_partition_key`.\"\"\"\n",
      "    if tp_size == 1:\n",
      "        if tp_rank is None:\n",
      "            return [state_dict]\n",
      "        else:\n",
      "            return state_dict\n",
      "\n",
      "    new_state_dict = {}\n",
      "    for k, v in state_dict.items():\n",
      "        new_state_dict[k] = tp_partition_key(k, v, tp_rank, tp_size, config)\n",
      "\n",
      "    if tp_rank is None:\n",
      "        return [\n",
      "            {k: v[tp_rank] for k, v in new_state_dict.items()}\n",
      "            for tp_rank in range(tp_size)\n",
      "        ]\n",
      "    else:\n",
      "        return new_state_dict\n",
      "\n",
      "\n",
      "def get_real_model_param_shape(\n",
      "    k: str, config: model_api.ReaLModelConfig, tp_size: int\n",
      ") -> Tuple:\n",
      "    if \"wte.weight\" in k:\n",
      "        assert config.vocab_size % tp_size == 0\n",
      "        return (config.vocab_size // tp_size, config.hidden_dim)\n",
      "    elif \"wpe.weight\" in k:\n",
      "        assert config.n_positions % tp_size == 0\n",
      "        if (config.n_positions + config.abs_position_embedding_offset) % tp_size != 0:\n",
      "            raise ValueError(\n",
      "                f\"The dimenstion of position embedding \"\n",
      "                f\"({config.n_positions} + offset {config.abs_position_embedding_offset}) \"\n",
      "                f\"is not divisible by tp_size ({tp_size}). \"\n",
      "                \"Models like this (e.g. OPT-350m) inherently do not support tensor parallelism.\"\n",
      "            )\n",
      "        return (\n",
      "            (config.n_positions + config.abs_position_embedding_offset) // tp_size,\n",
      "            config.hidden_dim,\n",
      "        )\n",
      "    elif \".ln.\" in k or \".ln_f.\" in k:\n",
      "        return (config.hidden_dim,)\n",
      "    elif \".q_ln.\" in k or \".k_ln.\" in k:\n",
      "        return (config.head_dim,)\n",
      "    elif k == f\"{config.n_layers + 1}.weight\":  # output head\n",
      "        if config.is_critic:\n",
      "            return (1, config.hidden_dim)\n",
      "        elif tp_size > 1:\n",
      "            assert config.vocab_size % tp_size == 0\n",
      "            return (config.vocab_size // tp_size, config.hidden_dim)\n",
      "        else:\n",
      "            return (config.vocab_size, config.hidden_dim)\n",
      "    elif any([ck in k for ck in COLUMN_LINEAR_KEYS]):\n",
      "        if \"k_attn\" in k or \"v_attn\" in k:\n",
      "            if \"weight\" in k:\n",
      "                if config.n_kv_heads % tp_size == 0:\n",
      "                    return (\n",
      "                        config.head_dim * config.n_kv_heads // tp_size,\n",
      "                        config.hidden_dim,\n",
      "                    )\n",
      "                else:\n",
      "                    return (\n",
      "                        config.head_dim * config.n_kv_heads,\n",
      "                        config.hidden_dim,\n",
      "                    )\n",
      "            else:\n",
      "                assert \"bias\" in k\n",
      "                if config.n_kv_heads % tp_size == 0:\n",
      "                    return (config.head_dim * config.n_kv_heads // tp_size,)\n",
      "                else:\n",
      "                    return (config.head_dim * config.n_kv_heads,)\n",
      "        if \"mlp\" in k:\n",
      "            if \"weight\" in k:\n",
      "                return (config.intermediate_dim // tp_size, config.hidden_dim)\n",
      "            else:\n",
      "                assert \"bias\" in k\n",
      "                return (config.intermediate_dim // tp_size,)\n",
      "        if \"weight\" in k:\n",
      "            assert config.n_q_heads % tp_size == 0\n",
      "            return (config.n_q_heads * config.head_dim // tp_size, config.hidden_dim)\n",
      "        else:\n",
      "            assert \"bias\" in k\n",
      "            return (config.n_q_heads * config.head_dim // tp_size,)\n",
      "    elif any([rk in k for rk in ROW_LINEAR_KEYS]):\n",
      "        if \"mlp\" in k and \"weight\" in k:\n",
      "            return (config.hidden_dim, config.intermediate_dim // tp_size)\n",
      "        elif \"attn\" in k and \"weight\" in k:\n",
      "            return (config.hidden_dim, config.n_q_heads * config.head_dim // tp_size)\n",
      "        elif \"bias\" in k:\n",
      "            return (config.hidden_dim,)\n",
      "        else:\n",
      "            raise NotImplementedError(f\"unkown shape of key {k}.\")\n",
      "    elif \".mlp.router\" in k:\n",
      "        # mp does not partition router weights\n",
      "        return (config.moe.num_experts, config.hidden_dim)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"unkown shape of key {k}.\")\n",
      "\n",
      "\n",
      "def tp_merge_key(\n",
      "    k: str,\n",
      "    tensors: List[torch.Tensor],\n",
      "    config: model_api.ReaLModelConfig,\n",
      ") -> torch.Tensor:\n",
      "    if any([ek in k for ek in EMBEDDING_KEYS]) and \"weight\" in k:\n",
      "        return torch.cat(tensors, dim=0)\n",
      "    elif k == f\"{config.n_layers + 1}.weight\" and not config.is_critic:\n",
      "        return torch.cat(tensors, dim=0)\n",
      "    elif any([ck in k for ck in COLUMN_LINEAR_KEYS]):\n",
      "        return torch.cat(tensors, dim=0)\n",
      "    elif any([rk in k for rk in ROW_LINEAR_KEYS]) and \"weight\" in k:\n",
      "        return torch.cat(tensors, dim=1)\n",
      "    else:\n",
      "        return tensors[0]\n",
      "\n",
      "\n",
      "def tp_merge_real_model_state_dict(\n",
      "    state_dicts: List[Dict[str, torch.Tensor]],\n",
      "    config: model_api.ReaLModelConfig,\n",
      ") -> Dict:\n",
      "    tp_size = len(state_dicts)\n",
      "    if tp_size == 1:\n",
      "        return state_dicts[0]\n",
      "\n",
      "    new_state_dict = {}\n",
      "    for k in state_dicts[0].keys():\n",
      "        new_state_dict[k] = tp_merge_key(k, [sd[k] for sd in state_dicts], config)\n",
      "\n",
      "    return new_state_dict\n",
      "\n",
      "\n",
      "class ReaLModelParamCount:\n",
      "    \"\"\"Paramter count, used for partitioning pipeline stages.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def _derive_count_from_keys(\n",
      "        keys: List[str], config: model_api.ReaLModelConfig, tp_size: int\n",
      "    ) -> int:\n",
      "        count = 0\n",
      "        for k in keys:\n",
      "            count += np.prod(get_real_model_param_shape(k, config, tp_size))\n",
      "        return int(count)\n",
      "\n",
      "    @staticmethod\n",
      "    def embed(config: model_api.ReaLModelConfig, tp_size: int) -> int:\n",
      "        return ReaLModelParamCount._derive_count_from_keys(\n",
      "            ReaLModelParamKeys.embed(config), config, tp_size\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def tblock(config: model_api.ReaLModelConfig, idx: int, tp_size: int) -> int:\n",
      "        return ReaLModelParamCount._derive_count_from_keys(\n",
      "            ReaLModelParamKeys.tblock(config, idx), config, tp_size\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def head(config: model_api.ReaLModelConfig, tp_size: int) -> int:\n",
      "        return ReaLModelParamCount._derive_count_from_keys(\n",
      "            ReaLModelParamKeys.head(config), config, tp_size\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    def total(config: model_api.ReaLModelConfig, idx: int, tp_size: int) -> int:\n",
      "        return (\n",
      "            config.n_layers * ReaLModelParamCount.tblock(config, idx, tp_size)\n",
      "            + ReaLModelParamCount.head(config, tp_size)\n",
      "            + ReaLModelParamCount.embed(config, tp_size)\n",
      "        )\n",
      "\n",
      "\n",
      "def partition_pipeline_layers(\n",
      "    config: model_api.ReaLModelConfig,\n",
      "    num_stages: int,\n",
      "    method: str = \"parameters\",\n",
      ") -> Dict[int, Tuple[int, int]]:\n",
      "    # Ignoring tp_size in param count because tensor parallel equally partitions parameters.\n",
      "    # It is irrelevant to how we partition pipeline stages.\n",
      "    param_counts = (\n",
      "        [ReaLModelParamCount.embed(config, 1)]\n",
      "        + [ReaLModelParamCount.tblock(config, i, 1) for i in range(config.n_layers)]\n",
      "        + [ReaLModelParamCount.head(config, 1)]\n",
      "    )\n",
      "\n",
      "    parts = None\n",
      "    if method == \"uniform\":\n",
      "        # Each stage gets a simple uniform number of layers.\n",
      "        from deepspeed.runtime import utils as ds_utils\n",
      "\n",
      "        parts = ds_utils.partition_uniform(\n",
      "            num_items=config.n_layers + 2, num_parts=num_stages\n",
      "        )\n",
      "    elif method == \"parameters\":\n",
      "        # Partition according to the parameter count.\n",
      "        param_counts = np.array(param_counts)\n",
      "        parts = datapack.partition_balanced(param_counts, k=num_stages)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Partitioning method {method} not implemented.\")\n",
      "\n",
      "    stage_to_layer_idx = {}\n",
      "    for stage in range(num_stages):\n",
      "        start = parts[stage]\n",
      "        stop = parts[stage + 1]\n",
      "        stage_to_layer_idx[stage] = (start, stop)\n",
      "    return stage_to_layer_idx\n",
      "\n",
      "\n",
      "def pipeline_repartition_strategy(\n",
      "    layer_mapping1: Dict[int, List[int]],\n",
      "    layer_mapping2: Dict[int, List[int]],\n",
      "):\n",
      "    assert set(sum(layer_mapping1.values(), [])) == set(\n",
      "        sum(layer_mapping2.values(), [])\n",
      "    )\n",
      "    assert all(isinstance(i, int) for i in layer_mapping1)\n",
      "    assert all(isinstance(i, int) for i in layer_mapping2)\n",
      "\n",
      "    layer_mapping1 = dict(sorted(layer_mapping1.items()))\n",
      "    layer_mapping2 = dict(sorted(layer_mapping2.items()))\n",
      "\n",
      "    layer_map: Dict[Tuple[int, int], List[int]] = {}\n",
      "    for pp_rank2, layer_indices2 in layer_mapping2.items():\n",
      "        for pp_rank1, layer_indices1 in layer_mapping1.items():\n",
      "            layer_map[(pp_rank1, pp_rank2)] = sorted(\n",
      "                list(set(layer_indices1).intersection(set(layer_indices2)))\n",
      "            )\n",
      "\n",
      "    return layer_map\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/nn/real_llm_generate.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import gc\n",
      "import itertools\n",
      "import queue\n",
      "from typing import TYPE_CHECKING, Callable, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.distributed\n",
      "import torch.utils.checkpoint\n",
      "import transformers\n",
      "\n",
      "import realhf.impl.model.utils.cuda_graph as cuda_graph\n",
      "from realhf.api.core.model_api import GenerationHyperparameters, ReaLModelConfig\n",
      "from realhf.base import constants, logging\n",
      "from realhf.impl.model.nn.real_llm_base import PipeCacheData, PipeTransferData\n",
      "from realhf.impl.model.utils.functional import mask_eos_token\n",
      "from realhf.impl.model.utils.logits_warper import top_k_top_p_logits\n",
      "from realhf.impl.model.utils.padding import index_first_axis, unpad_input\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from .real_llm_api import ReaLModel\n",
      "\n",
      "logger = logging.getLogger(\"ReaLModel Generation\")\n",
      "\n",
      "\n",
      "def genstep(\n",
      "    next_token_logits: torch.Tensor,\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "    unfinished_sequences: torch.Tensor,\n",
      "    generated_idx: Union[torch.IntTensor, int],\n",
      "    gconfig: GenerationHyperparameters,\n",
      ") -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], bool, torch.Tensor]:\n",
      "    \"\"\"Advance generation by one step given logits.\n",
      "\n",
      "    Args:\n",
      "        next_token_logits (torch.Tensor): Shape [bs, vocab_size].\n",
      "        tokenizer (transformers.PreTrainedTokenizerFast): .\n",
      "        unfinished_sequences (torch.Tensor): Bool tensor indicator of whether a sequence is finished.\n",
      "            Shape [bs].\n",
      "        generated_idx (int): The token index to be generated.\n",
      "        gconfig (GenerationHyperparameters): .\n",
      "\n",
      "    Returns:\n",
      "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, bool, torch.Tensor]:\n",
      "        A tuple of\n",
      "            next_tokens: Shape [bs].\n",
      "            logprob: The log probability of selected tokens. May be re-normalized\n",
      "                according to the mask machanism. Shape [bs].\n",
      "            logits_mask: The mask of logits. Shape [bs, vocab_size].\n",
      "            terminate: Whether the generation should be terminated.\n",
      "            unfinished_sequences: Bool tensor indicator of whether a sequence is finished.\n",
      "                Shape [bs].\n",
      "    \"\"\"\n",
      "    if constants.tensor_parallel_world_size() > 1:\n",
      "        from realhf.impl.model.parallelism.tensor_parallel.mappings import (\n",
      "            gather_from_tensor_model_parallel_region,\n",
      "        )\n",
      "\n",
      "        next_token_logits = gather_from_tensor_model_parallel_region(next_token_logits)\n",
      "\n",
      "    unfinished_sequences = unfinished_sequences.bool()\n",
      "    next_token_logits = next_token_logits.float()\n",
      "    if isinstance(generated_idx, int):\n",
      "        if generated_idx < gconfig.min_new_tokens:\n",
      "            next_token_logits = mask_eos_token(\n",
      "                next_token_logits, eos_token_id=tokenizer.eos_token_id\n",
      "            )\n",
      "    else:\n",
      "        assert isinstance(generated_idx, torch.Tensor)\n",
      "        if (generated_idx < gconfig.min_new_tokens).any():\n",
      "            _batch_indices = (generated_idx < gconfig.min_new_tokens).unsqueeze(1)\n",
      "            _vocab_indices = _batch_indices.new_zeros((1, next_token_logits.shape[1]))\n",
      "            if tokenizer.eos_token_id is not None:\n",
      "                _vocab_indices[:, tokenizer.eos_token_id] = 1\n",
      "            next_token_logits.masked_fill_(\n",
      "                _batch_indices * _vocab_indices,\n",
      "                torch.finfo(next_token_logits.dtype).min,\n",
      "            )\n",
      "\n",
      "    if not gconfig.greedy:\n",
      "        next_token_logits /= gconfig.temperature\n",
      "        next_token_logits = top_k_top_p_logits(\n",
      "            next_token_logits,\n",
      "            top_k=gconfig.top_k,\n",
      "            top_p=gconfig.top_p,\n",
      "            inplace=True,\n",
      "            ordered=False,\n",
      "        )\n",
      "\n",
      "    distrb = torch.distributions.Categorical(logits=next_token_logits)\n",
      "    next_tokens = distrb.mode if gconfig.greedy else distrb.sample()\n",
      "    logprob = distrb.log_prob(next_tokens)\n",
      "\n",
      "    if constants.tensor_parallel_world_size() > 1:\n",
      "        if constants.tensor_parallel_rank() > 0:\n",
      "            logprob[:] = 0\n",
      "            next_tokens[:] = 0\n",
      "        handle = torch.distributed.all_reduce(\n",
      "            logprob,\n",
      "            torch.distributed.ReduceOp.SUM,\n",
      "            async_op=True,\n",
      "            group=constants.tensor_parallel_group(),\n",
      "        )\n",
      "        torch.distributed.all_reduce(\n",
      "            next_tokens,\n",
      "            torch.distributed.ReduceOp.SUM,\n",
      "            group=constants.tensor_parallel_group(),\n",
      "        )\n",
      "\n",
      "    if tokenizer.eos_token_id is not None:\n",
      "        if tokenizer.pad_token_id is None:\n",
      "            raise ValueError(\n",
      "                \"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\"\n",
      "            )\n",
      "        next_tokens.masked_fill_(\n",
      "            unfinished_sequences.logical_not(), tokenizer.pad_token_id\n",
      "        )\n",
      "        # next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
      "    # unfinished_sequences = next_tokens.ne(tokenizer.eos_token_id).long() * unfinished_sequences\n",
      "    unfinished_sequences.logical_and_(next_tokens.ne(tokenizer.eos_token_id))\n",
      "\n",
      "    # terminate check\n",
      "    if isinstance(generated_idx, int):\n",
      "        terminate = (generated_idx >= gconfig.max_new_tokens - 1) or (\n",
      "            unfinished_sequences.max() == 0\n",
      "        )\n",
      "    else:\n",
      "        unfinished_sequences.logical_and_(generated_idx < gconfig.max_new_tokens - 1)\n",
      "        terminate = unfinished_sequences.max() == 0\n",
      "\n",
      "    if gconfig.force_no_logits_mask:\n",
      "        logits_mask = None\n",
      "    else:\n",
      "        logits_mask = next_token_logits == torch.finfo(next_token_logits.dtype).min\n",
      "        if not logits_mask.any():\n",
      "            logits_mask = None\n",
      "\n",
      "    if constants.tensor_parallel_world_size() > 1:\n",
      "        handle.wait()\n",
      "\n",
      "    return next_tokens, logprob, logits_mask, terminate, unfinished_sequences\n",
      "\n",
      "\n",
      "def prepare_generate_inputs(\n",
      "    module: \"ReaLModel\",\n",
      "    gconfig: GenerationHyperparameters,\n",
      "    x: PipeTransferData,\n",
      "    ys: List[PipeCacheData],\n",
      "    cuda_graph_name: str,\n",
      "):\n",
      "    if gconfig.top_p >= 1 and gconfig.top_k >= module.config.vocab_size:\n",
      "        gconfig.force_no_logits_mask = True\n",
      "\n",
      "    cu_seqlens = x.cu_seqlens\n",
      "    input_lens = cu_seqlens[1:] - cu_seqlens[:-1]\n",
      "    # assert constants.pipe_parallel_world_size() >= 2\n",
      "    layer_indices = range(module.layer_idx_start, module.layer_idx_end)\n",
      "    min_layer_index = module.layer_idx_start\n",
      "    bs = input_lens.shape[0]\n",
      "    cache_seqlens = input_lens.clone().to(dtype=torch.int32)\n",
      "\n",
      "    block_ys = ys\n",
      "    assert len(layer_indices) == len(block_ys), (len(block_ys), layer_indices)\n",
      "    if constants.is_first_pipe_stage():\n",
      "        layer_indices = layer_indices[1:]\n",
      "        ys[0].cache_seqlens = cache_seqlens\n",
      "        block_ys = ys[1:]\n",
      "    if constants.is_last_pipe_stage():\n",
      "        layer_indices = layer_indices[:-1]\n",
      "        ys[-1].cache_seqlens = cache_seqlens\n",
      "        block_ys = block_ys[:-1]\n",
      "\n",
      "    for y, layer_idx in zip(block_ys, layer_indices):\n",
      "        assert (\n",
      "            y.k_cache is not None\n",
      "            and y.v_cache is not None\n",
      "            and y.cache_seqlens is not None\n",
      "        ), (y.k_cache is None, y.v_cache is None, y.cache_seqlens is None)\n",
      "        kvcache_seqlen = max(\n",
      "            constants.max_prompt_len() + gconfig.max_new_tokens,\n",
      "            module.config.hidden_dim // module.config.head_dim + 10,\n",
      "        )\n",
      "        k_cache_handle = cuda_graph.input_buffer_handle(cuda_graph_name, \"k_caches\")\n",
      "        v_cache_handle = cuda_graph.input_buffer_handle(cuda_graph_name, \"v_caches\")\n",
      "        if k_cache_handle is not None and v_cache_handle is not None:\n",
      "            k_cache = k_cache_handle[layer_idx - min_layer_index][:bs, :kvcache_seqlen]\n",
      "            v_cache = v_cache_handle[layer_idx - min_layer_index][:bs, :kvcache_seqlen]\n",
      "        else:\n",
      "            k_cache = torch.zeros(\n",
      "                (bs, kvcache_seqlen, *y.k_cache.shape[1:]),\n",
      "                dtype=y.k_cache.dtype,\n",
      "                device=y.k_cache.device,\n",
      "            )\n",
      "            v_cache = torch.zeros(\n",
      "                (bs, kvcache_seqlen, *y.v_cache.shape[1:]),\n",
      "                dtype=y.v_cache.dtype,\n",
      "                device=y.v_cache.device,\n",
      "            )\n",
      "        indices = (\n",
      "            torch.arange(kvcache_seqlen, device=module.device, dtype=torch.long)[\n",
      "                None, :\n",
      "            ]\n",
      "            < input_lens[:, None]\n",
      "        )\n",
      "        k_cache[indices] = y.k_cache\n",
      "        v_cache[indices] = y.v_cache\n",
      "        y.k_cache = k_cache\n",
      "        y.v_cache = v_cache\n",
      "        y.cache_seqlens = cache_seqlens\n",
      "\n",
      "    return x, ys\n",
      "\n",
      "\n",
      "def maybe_capture_cudagraph(\n",
      "    module: \"ReaLModel\",\n",
      "    x: PipeTransferData,\n",
      "    ys: List[PipeCacheData],\n",
      "    cuda_graph_name: str,\n",
      "    force_recapture: bool,\n",
      "):\n",
      "    bs = x.cu_seqlens.shape[0] - 1\n",
      "    cache_seqlens = ys[0].cache_seqlens\n",
      "    input_buffers = dict(\n",
      "        input_ids=torch.ones((bs,), dtype=torch.long, device=\"cuda\"),\n",
      "        position_ids=cache_seqlens.clone(),\n",
      "        k_caches=[y.k_cache for y in ys],\n",
      "        v_caches=[y.v_cache for y in ys],\n",
      "        cache_seqlens=cache_seqlens.clone(),\n",
      "        max_seqlen=x.max_seqlen,\n",
      "        cu_seqlens=x.cu_seqlens.clone(),\n",
      "        hidden_states=(\n",
      "            torch.zeros(\n",
      "                (bs, x.pp_input.shape[1]), dtype=x.pp_input.dtype, device=\"cuda\"\n",
      "            )\n",
      "            if x.pp_input is not None\n",
      "            else None\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    graph, input_buffers, output_buffers = cuda_graph.capture_func(\n",
      "        cuda_graph_name,\n",
      "        module._forward,\n",
      "        input_buffers,\n",
      "        force_recapture=force_recapture,\n",
      "        no_grad=True,\n",
      "    )\n",
      "\n",
      "    return graph, input_buffers, output_buffers\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def generate(\n",
      "    model: \"ReaLModel\",\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "    packed_input_ids: Optional[torch.LongTensor] = None,\n",
      "    cu_seqlens: Optional[torch.LongTensor] = None,\n",
      "    max_seqlen: Optional[int] = None,\n",
      "    gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "        default_factory=GenerationHyperparameters\n",
      "    ),\n",
      ") -> Tuple[\n",
      "    torch.Tensor,\n",
      "    torch.Tensor,\n",
      "    torch.Tensor,\n",
      "    List[PipeCacheData],\n",
      "    Optional[torch.Tensor],\n",
      "]:\n",
      "    \"\"\"Generete a sequence with a ReaLModel.\"\"\"\n",
      "    bs = cu_seqlens.shape[0] - 1\n",
      "    device = model.device\n",
      "    mconfig: ReaLModelConfig = model.config\n",
      "\n",
      "    terminate = False\n",
      "    generated_idx = 0\n",
      "    unfinished_sequences = torch.ones(bs, dtype=torch.long, device=device)\n",
      "\n",
      "    gen_token_ph = []\n",
      "    gen_logprob_ph = []\n",
      "    gen_logits_mask_ph = []\n",
      "\n",
      "    prompt_logits = None\n",
      "    # Prepare inputs for generation iterations\n",
      "\n",
      "    # Input_ids may have different lengths, we should first pack them into a large batch\n",
      "    # to use varlen flash attention, then record kv caches for the following inferences.\n",
      "    max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n",
      "    if constants.max_prompt_len() < max_seqlen:\n",
      "        raise RuntimeError(\n",
      "            f\"Input sequence length {max_seqlen} is larger than the maximum sequence length \"\n",
      "            f\"supported by the model {constants.max_prompt_len()}.\"\n",
      "        )\n",
      "    x = PipeTransferData(\n",
      "        cu_seqlens=cu_seqlens, max_seqlen=max_seqlen, store_kv_cache=True\n",
      "    )\n",
      "    # one embedding layer, n_layers transformer block, one output layer\n",
      "    ys = [PipeCacheData(packed_input_ids=packed_input_ids)] + [\n",
      "        PipeCacheData() for _ in range(mconfig.n_layers + 1)\n",
      "    ]\n",
      "\n",
      "    # Model forward will set k/v cache in PipeCacheData.\n",
      "    prompt_logits = model(x, ys)[0].pp_output\n",
      "    logits = prompt_logits[cu_seqlens[1:] - 1]\n",
      "\n",
      "    # Next, we will generate the next token after prompts.\n",
      "    # cache_seqlens is exactly the lengths of prompts.\n",
      "    # We perform a genstep outside the loop due to a historical reason.\n",
      "    next_tokens, logprob, logits_mask, terminate, unfinished_sequences = genstep(\n",
      "        logits, tokenizer, unfinished_sequences, generated_idx, gconfig\n",
      "    )\n",
      "    del logits\n",
      "\n",
      "    gen_token_ph.append(next_tokens)\n",
      "    gen_logprob_ph.append(logprob)\n",
      "    gen_logits_mask_ph.append(logits_mask)\n",
      "    generated_idx += 1\n",
      "\n",
      "    cuda_graph_name = \"decoding\"\n",
      "    x, ys = prepare_generate_inputs(model, gconfig, x, ys, cuda_graph_name)\n",
      "    graph = None\n",
      "    if gconfig.use_cuda_graph:\n",
      "        graph, input_buffers, output_buffers = maybe_capture_cudagraph(\n",
      "            model,\n",
      "            x,\n",
      "            ys,\n",
      "            cuda_graph_name,\n",
      "            force_recapture=gconfig.force_cudagraph_recapture,\n",
      "        )\n",
      "\n",
      "    # The main loop.\n",
      "    while not terminate:\n",
      "        # the next round of inference\n",
      "        if graph is not None:\n",
      "            input_buffers[\"input_ids\"][:bs].copy_(next_tokens, non_blocking=True)\n",
      "            input_buffers[\"position_ids\"][:bs].copy_(\n",
      "                ys[0].cache_seqlens, non_blocking=True\n",
      "            )\n",
      "            input_buffers[\"cache_seqlens\"][:bs].copy_(\n",
      "                ys[0].cache_seqlens, non_blocking=True\n",
      "            )\n",
      "            # K/v cache will be changed in-place with flash attention.\n",
      "            graph.replay()\n",
      "            logits = output_buffers[\"output\"][:bs].squeeze(1)\n",
      "        else:\n",
      "            ys[0].packed_input_ids = next_tokens\n",
      "            ys[0].packed_position_ids = None\n",
      "            x.cu_seqlens = torch.arange(bs + 1, dtype=torch.int32, device=device)\n",
      "            x.max_seqlen = 1\n",
      "            # K/v cache will be changed in-place with flash attention.\n",
      "            logits = model(x, ys)[0].pp_output.squeeze(dim=1)\n",
      "\n",
      "        ys[\n",
      "            0\n",
      "        ].cache_seqlens += (\n",
      "            1  # The global handle. This will increase all handles in ys by 1.\n",
      "        )\n",
      "        next_tokens, logprob, logits_mask, terminate, unfinished_sequences = genstep(\n",
      "            logits, tokenizer, unfinished_sequences, generated_idx, gconfig\n",
      "        )\n",
      "        gen_token_ph.append(next_tokens)\n",
      "        gen_logprob_ph.append(logprob)\n",
      "        gen_logits_mask_ph.append(logits_mask)\n",
      "        generated_idx += 1\n",
      "\n",
      "    gen_tokens, log_probs, logits_mask = _gather_gen_output_from_list(\n",
      "        gen_token_ph, gen_logprob_ph, gen_logits_mask_ph\n",
      "    )\n",
      "    if gconfig.use_cuda_graph and gconfig.force_cudagraph_recapture:\n",
      "        cuda_graph.destroy(cuda_graph_name)\n",
      "\n",
      "    return gen_tokens, log_probs, logits_mask, ys[1:-1], prompt_logits\n",
      "\n",
      "\n",
      "def _gather_gen_output_from_list(\n",
      "    gen_token_ph: List[torch.LongTensor],\n",
      "    gen_logprob_ph: List[torch.FloatTensor],\n",
      "    gen_logits_mask_ph: List[torch.BoolTensor],\n",
      ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "    \"\"\"Stack over the sequence dimension given a list of single-token\n",
      "    tensors.\"\"\"\n",
      "    gen_tokens = torch.stack(gen_token_ph, -1)  # [bs, seqlen]\n",
      "    log_probs = torch.stack(gen_logprob_ph, -1)  # [bs, seqlen]\n",
      "    if all([m is None for m in gen_logits_mask_ph]):\n",
      "        logits_mask = None\n",
      "    else:\n",
      "        mm = next(m for m in gen_logits_mask_ph if m is not None)\n",
      "        gen_logits_mask_ph = [\n",
      "            torch.ones_like(mm) if m is None else m for m in gen_logits_mask_ph\n",
      "        ]\n",
      "        logits_mask = torch.stack(gen_logits_mask_ph, 1)  # [bs, seqlen, vocab_size]\n",
      "    return gen_tokens, log_probs, logits_mask\n",
      "\n",
      "\n",
      "def _gather_minibatch_gen_outputs(\n",
      "    all_gen_tokens: List[torch.LongTensor],\n",
      "    all_log_probs: List[torch.FloatTensor],\n",
      "    all_logits_mask: List[torch.BoolTensor],\n",
      "    pad_token_id: int,\n",
      ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "    \"\"\"Concate over the batch dimension given multiple [bs, seqlen] mini-batch\n",
      "    tensors.\n",
      "\n",
      "    Since different minibatches may have different generated lengths, we\n",
      "    should pad them to the same length.\n",
      "    \"\"\"\n",
      "    gen_tokens_lengths = [t.shape[-1] for t in all_gen_tokens]\n",
      "    max_gen_tokens_length = max(gen_tokens_lengths)\n",
      "\n",
      "    padded_gen_tokens = []\n",
      "    padded_log_probs = []\n",
      "    padded_logits_mask = []\n",
      "\n",
      "    n_mbs = len(all_gen_tokens)\n",
      "    for i in range(n_mbs):\n",
      "        assert all_gen_tokens[i].shape == all_log_probs[i].shape\n",
      "        gen_len = all_gen_tokens[i].shape[-1]\n",
      "\n",
      "        gen_token = all_gen_tokens[i]\n",
      "        log_probs = all_log_probs[i]\n",
      "        logits_mask = all_logits_mask[i]\n",
      "\n",
      "        if gen_len < max_gen_tokens_length:\n",
      "            pad_size = max_gen_tokens_length - gen_len\n",
      "            gen_token = torch.nn.functional.pad(\n",
      "                gen_token, (0, pad_size), value=pad_token_id\n",
      "            )\n",
      "            log_probs = torch.nn.functional.pad(log_probs, (0, pad_size))\n",
      "            if logits_mask is not None:\n",
      "                logits_mask = torch.nn.functional.pad(\n",
      "                    logits_mask,\n",
      "                    (0, 0, 0, pad_size),\n",
      "                    mode=\"constant\",\n",
      "                    value=1,\n",
      "                )\n",
      "\n",
      "        padded_gen_tokens.append(gen_token)\n",
      "        padded_log_probs.append(log_probs)\n",
      "        padded_logits_mask.append(logits_mask)\n",
      "\n",
      "    gen_tokens = torch.cat(padded_gen_tokens, 0)\n",
      "    log_probs = torch.cat(padded_log_probs, 0)\n",
      "    if all([m is None for m in padded_logits_mask]):\n",
      "        logits_mask = None\n",
      "    else:\n",
      "        mm = next(m for m in padded_logits_mask if m is not None)\n",
      "        padded_logits_mask = [\n",
      "            torch.ones_like(mm) if m is None else m for m in padded_logits_mask\n",
      "        ]\n",
      "        logits_mask = torch.cat(padded_logits_mask, 0)  # [bs, seqlen, vocab_size]\n",
      "\n",
      "    return (gen_tokens, log_probs, logits_mask)\n",
      "\n",
      "\n",
      "def concat_prompt_to_generation_output(\n",
      "    packed_prompts: torch.LongTensor,\n",
      "    prompt_lengths: torch.IntTensor,\n",
      "    gen_tokens: torch.LongTensor,\n",
      "    logprobs: torch.FloatTensor,\n",
      "    logits_mask: torch.BoolTensor | None,\n",
      "    gen_lengths: torch.IntTensor,\n",
      ") -> Tuple[\n",
      "    torch.LongTensor,\n",
      "    torch.FloatTensor,\n",
      "    torch.BoolTensor,\n",
      "    torch.IntTensor,\n",
      "    torch.BoolTensor,\n",
      "]:\n",
      "    device = packed_prompts.device\n",
      "\n",
      "    prompts_list, prompt_log_probs_list, prompt_logits_mask_list = [], [], []\n",
      "    gen_tokens_list, gen_log_probs_list, gen_logits_mask_list = [], [], []\n",
      "\n",
      "    bs = prompt_lengths.shape[0]\n",
      "    prompt_cu_seqlens = torch.nn.functional.pad(prompt_lengths.cumsum(0), (1, 0))\n",
      "    for i in range(bs):\n",
      "        prompt_len, gen_len = prompt_lengths[i].item(), gen_lengths[i].item()\n",
      "\n",
      "        # log_probs is one-step shorter than token sequences.\n",
      "        prompts_list.append(\n",
      "            packed_prompts[prompt_cu_seqlens[i] : prompt_cu_seqlens[i + 1]]\n",
      "        )\n",
      "        prompt_log_probs_list.append(logprobs.new_zeros(prompt_len - 1))\n",
      "        if logits_mask is not None:\n",
      "            prompt_logits_mask_list.append(\n",
      "                logits_mask.new_ones((prompt_len - 1, logits_mask.shape[-1]))\n",
      "            )\n",
      "\n",
      "        # Generated tokens are right-padded.\n",
      "        gen_tokens_list.append(gen_tokens[i, :gen_len])\n",
      "        gen_log_probs_list.append(logprobs[i, :gen_len])\n",
      "        if logits_mask is not None:\n",
      "            gen_logits_mask_list.append(\n",
      "                torch.cat(\n",
      "                    [\n",
      "                        logits_mask[i, :gen_len],\n",
      "                        logits_mask.new_ones(1, logits_mask.shape[-1]),\n",
      "                    ]\n",
      "                )\n",
      "            )\n",
      "\n",
      "    seq = torch.cat(\n",
      "        list(itertools.chain.from_iterable(zip(prompts_list, gen_tokens_list)))\n",
      "    )\n",
      "    seq_lengths = prompt_lengths + gen_lengths\n",
      "    packed_logprobs = torch.cat(\n",
      "        list(\n",
      "            itertools.chain.from_iterable(\n",
      "                zip(prompt_log_probs_list, gen_log_probs_list)\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    assert seq.shape[0] == packed_logprobs.shape[0] + bs, (\n",
      "        seq.shape,\n",
      "        packed_logprobs.shape,\n",
      "        bs,\n",
      "    )\n",
      "    packed_logits_mask = None\n",
      "    if gen_logits_mask_list:\n",
      "        packed_logits_mask = torch.cat(\n",
      "            list(\n",
      "                itertools.chain.from_iterable(\n",
      "                    zip(prompt_logits_mask_list, gen_logits_mask_list)\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "\n",
      "    prompt_mask = zip(\n",
      "        [torch.ones(plen, dtype=torch.bool, device=device) for plen in prompt_lengths],\n",
      "        [torch.zeros(glen, dtype=torch.bool, device=device) for glen in gen_lengths],\n",
      "    )\n",
      "    prompt_mask = torch.cat(list(itertools.chain.from_iterable(prompt_mask)))\n",
      "\n",
      "    return (seq, packed_logprobs, packed_logits_mask, seq_lengths, prompt_mask)\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def vanilla_packed_generate(\n",
      "    model: \"ReaLModel\",\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "    input_ids: torch.Tensor,\n",
      "    attention_mask: Optional[torch.Tensor] = None,\n",
      "    gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "        default_factory=GenerationHyperparameters\n",
      "    ),\n",
      ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "    \"\"\"Only used for debugging.\"\"\"\n",
      "    mconfig: ReaLModelConfig = model.config\n",
      "\n",
      "    terminate = False\n",
      "    generated_idx = 0\n",
      "    unfinished_sequences = torch.ones(\n",
      "        input_ids.shape[0], dtype=torch.long, device=input_ids.device\n",
      "    )\n",
      "\n",
      "    gen_token_ph = []\n",
      "    gen_logprob_ph = []\n",
      "    gen_logits_mask_ph = []\n",
      "\n",
      "    # The main loop.\n",
      "    while not terminate:\n",
      "        packed_input_ids, _, cu_seqlens, max_seqlen = unpad_input(\n",
      "            input_ids, attention_mask\n",
      "        )\n",
      "        x = PipeTransferData(cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
      "        # one embedding layer, n_layers transformer block, one output layer\n",
      "        ys = [PipeCacheData(packed_input_ids=packed_input_ids)] + [\n",
      "            PipeCacheData() for _ in range(mconfig.n_layers + 1)\n",
      "        ]\n",
      "        # Model forward will set k/v cache in PipeCacheData.\n",
      "        logits = model(x, ys).pp_output\n",
      "        logits = logits[cu_seqlens[1:] - 1]\n",
      "        # Next, we will generate the next token after prompts.\n",
      "        # cache_seqlens is exactly the lengths of prompts.\n",
      "        next_tokens, logprob, logits_mask, terminate, unfinished_sequences = genstep(\n",
      "            logits, tokenizer, unfinished_sequences, generated_idx, gconfig\n",
      "        )\n",
      "        gen_token_ph.append(next_tokens)\n",
      "        gen_logprob_ph.append(logprob)\n",
      "        gen_logits_mask_ph.append(logits_mask)\n",
      "        generated_idx += 1\n",
      "\n",
      "        input_ids = torch.cat([input_ids, next_tokens.unsqueeze(-1)], 1)\n",
      "        am = torch.logical_and(\n",
      "            next_tokens.unsqueeze(-1).not_equal(tokenizer.eos_token_id),\n",
      "            next_tokens.unsqueeze(-1).not_equal(tokenizer.pad_token_id),\n",
      "        )\n",
      "        attention_mask = torch.cat([attention_mask, am], 1)\n",
      "\n",
      "    gen_tokens = torch.stack(gen_token_ph, -1)\n",
      "    log_probs = torch.stack(gen_logprob_ph, -1)\n",
      "    if all([m is None for m in gen_logits_mask_ph]):\n",
      "        logits_mask = None\n",
      "    else:\n",
      "        mm = next(m for m in gen_logits_mask_ph if m is not None)\n",
      "        gen_logits_mask_ph = [\n",
      "            torch.ones_like(mm) if m is None else m for m in gen_logits_mask_ph\n",
      "        ]\n",
      "        logits_mask = torch.stack(gen_logits_mask_ph, -2)\n",
      "\n",
      "    return gen_tokens, log_probs, logits_mask\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def vanilla_cpu_generate(\n",
      "    model: \"ReaLModel\",\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "    input_ids: torch.Tensor,\n",
      "    attention_mask: Optional[torch.Tensor] = None,\n",
      "    gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "        default_factory=GenerationHyperparameters\n",
      "    ),\n",
      ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "    \"\"\"Only used for debugging.\"\"\"\n",
      "    mconfig: ReaLModelConfig = model.config\n",
      "    assert str(input_ids.device) == \"cpu\"\n",
      "\n",
      "    terminate = False\n",
      "    generated_idx = 0\n",
      "    unfinished_sequences = torch.ones(\n",
      "        input_ids.shape[0], dtype=torch.long, device=input_ids.device\n",
      "    )\n",
      "\n",
      "    gen_token_ph = []\n",
      "    gen_logprob_ph = []\n",
      "    gen_logits_mask_ph = []\n",
      "\n",
      "    # The main loop.\n",
      "    while not terminate:\n",
      "        x = PipeTransferData(attention_mask=attention_mask)\n",
      "        # one embedding layer, n_layers transformer block, one output layer\n",
      "        ys = [PipeCacheData(packed_input_ids=input_ids)] + [\n",
      "            PipeCacheData() for _ in range(mconfig.n_layers + 1)\n",
      "        ]\n",
      "        # Model forward will set k/v cache in PipeCacheData.\n",
      "        logits = model(x, ys).pp_output[:, -1, :]\n",
      "        # Next, we will generate the next token after prompts.\n",
      "        # cache_seqlens is exactly the lengths of prompts.\n",
      "        next_tokens, logprob, logits_mask, terminate, unfinished_sequences = genstep(\n",
      "            logits, tokenizer, unfinished_sequences, generated_idx, gconfig\n",
      "        )\n",
      "        gen_token_ph.append(next_tokens)\n",
      "        gen_logprob_ph.append(logprob)\n",
      "        gen_logits_mask_ph.append(logits_mask)\n",
      "        generated_idx += 1\n",
      "\n",
      "        input_ids = torch.cat([input_ids, next_tokens.unsqueeze(-1)], 1)\n",
      "        am = torch.logical_and(\n",
      "            next_tokens.unsqueeze(-1).not_equal(tokenizer.eos_token_id),\n",
      "            next_tokens.unsqueeze(-1).not_equal(tokenizer.pad_token_id),\n",
      "        )\n",
      "        attention_mask = torch.cat([attention_mask, am], 1)\n",
      "\n",
      "    gen_tokens = torch.stack(gen_token_ph, -1)\n",
      "    log_probs = torch.stack(gen_logprob_ph, -1)\n",
      "    if all([m is None for m in gen_logits_mask_ph]):\n",
      "        logits_mask = None\n",
      "    else:\n",
      "        mm = next(m for m in gen_logits_mask_ph if m is not None)\n",
      "        gen_logits_mask_ph = [\n",
      "            torch.ones_like(mm) if m is None else m for m in gen_logits_mask_ph\n",
      "        ]\n",
      "        logits_mask = torch.stack(gen_logits_mask_ph, -2)\n",
      "\n",
      "    return gen_tokens, log_probs, logits_mask\n",
      "\n",
      "\n",
      "class InflightBatchingGenerator:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        inqueue: queue.Queue,\n",
      "        outqueue: queue.Queue,\n",
      "        model: \"ReaLModel\",\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: GenerationHyperparameters,\n",
      "        batch_size: int,\n",
      "        max_prompt_len: int,\n",
      "    ):\n",
      "        self.inqueue = inqueue\n",
      "        self.outqueue = outqueue\n",
      "\n",
      "        self.model = model\n",
      "        self.mconfig = mconfig = model.config\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "        self.gconfig = gconfig\n",
      "        self.batch_size = batch_size\n",
      "        self.max_prompt_len = max_prompt_len\n",
      "\n",
      "        kvcache_seqlen = max(\n",
      "            max_prompt_len + gconfig.max_new_tokens,\n",
      "            mconfig.hidden_dim // mconfig.head_dim + 10,\n",
      "        )\n",
      "        _p = next(self.model.parameters())\n",
      "        dtype, device = _p.dtype, _p.device\n",
      "\n",
      "        # Cache\n",
      "        self.k_caches = [\n",
      "            torch.zeros(\n",
      "                (\n",
      "                    batch_size,\n",
      "                    kvcache_seqlen,\n",
      "                    mconfig.n_kv_heads,\n",
      "                    mconfig.head_dim,\n",
      "                ),\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "            for _ in range(self.mconfig.n_layers)\n",
      "        ]\n",
      "        self.v_caches = [\n",
      "            torch.zeros(\n",
      "                (\n",
      "                    batch_size,\n",
      "                    kvcache_seqlen,\n",
      "                    mconfig.n_kv_heads,\n",
      "                    mconfig.head_dim,\n",
      "                ),\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "            for _ in range(self.mconfig.n_layers)\n",
      "        ]\n",
      "        self.cache_seqlens = torch.zeros(\n",
      "            (batch_size,), dtype=torch.int32, device=device\n",
      "        )\n",
      "\n",
      "        # Input buffers\n",
      "        self.input_buf = torch.zeros(\n",
      "            (batch_size, max_prompt_len), dtype=torch.long, device=device\n",
      "        )\n",
      "        self.input_buf_lens = torch.zeros(\n",
      "            (batch_size,), dtype=torch.int32, device=device\n",
      "        )\n",
      "\n",
      "        # Save prompts for output\n",
      "        self.prompt_tokens = [None for _ in range(batch_size)]\n",
      "\n",
      "        # Generation state\n",
      "        self.generate_idx = torch.zeros((batch_size,), dtype=torch.int32, device=device)\n",
      "        self.unfinished_sequences = torch.zeros(\n",
      "            (batch_size,), dtype=torch.float32, device=device\n",
      "        )\n",
      "\n",
      "        self.ys = (\n",
      "            [\n",
      "                PipeCacheData(\n",
      "                    cache_seqlens=self.cache_seqlens,\n",
      "                )\n",
      "            ]\n",
      "            + [\n",
      "                PipeCacheData(k_cache=k, v_cache=v, cache_seqlens=self.cache_seqlens)\n",
      "                for k, v in zip(self.k_caches, self.v_caches)\n",
      "            ]\n",
      "            + [PipeCacheData()]\n",
      "        )\n",
      "\n",
      "        # output buffers\n",
      "        self.output_tokens_buf = [[] for _ in range(batch_size)]\n",
      "        self.output_logprob_buf = [[] for _ in range(batch_size)]\n",
      "        self.output_logits_mask = [[] for _ in range(batch_size)]\n",
      "\n",
      "    def _get_non_eos_logits(self) -> torch.FloatTensor:\n",
      "        self.ys[0].packed_position_ids = None\n",
      "        self.ys[0].packed_input_ids = self.input_buf[:, :1]\n",
      "        logits = self.model(PipeTransferData(), self.ys).pp_output.squeeze(dim=1)\n",
      "\n",
      "        self.cache_seqlens += 1\n",
      "        return logits.float()\n",
      "\n",
      "    def _get_inflight_logits(self) -> torch.FloatTensor:\n",
      "        finished_sequences = self.unfinished_sequences.logical_not()\n",
      "        assert finished_sequences.any()\n",
      "\n",
      "        finish_indices = finished_sequences.nonzero().squeeze(-1).tolist()\n",
      "\n",
      "        # pop out finished sequences and clear corresponding buffers\n",
      "        for i in finish_indices:\n",
      "            prompt_tokens = self.prompt_tokens[i]\n",
      "\n",
      "            # Used to skip the first call.\n",
      "            if prompt_tokens is not None:\n",
      "                gen_tokens = torch.stack(self.output_tokens_buf[i])\n",
      "                gen_logp = torch.stack(self.output_logprob_buf[i])\n",
      "                if all([m is None for m in self.output_logits_mask[i]]):\n",
      "                    gen_logits_mask = None\n",
      "                else:\n",
      "                    mm = next(m for m in self.output_logits_mask[i] if m is not None)\n",
      "                    gen_logits_mask = [\n",
      "                        torch.ones_like(mm) if m is None else m\n",
      "                        for m in self.output_logits_mask[i]\n",
      "                    ]\n",
      "                    gen_logits_mask = torch.stack(gen_logits_mask, -2)\n",
      "\n",
      "                res = dict(\n",
      "                    prompt=prompt_tokens,\n",
      "                    gen=gen_tokens,\n",
      "                    logp=gen_logp,\n",
      "                    logits_mask=gen_logits_mask,\n",
      "                )\n",
      "                try:\n",
      "                    self.outqueue.put_nowait(res)\n",
      "                except queue.Full as e:\n",
      "                    raise RuntimeError(\n",
      "                        \"Output queue is full. Please set a larger queue size.\"\n",
      "                    ) from e\n",
      "\n",
      "            # clear cache\n",
      "            self.cache_seqlens[i] = 0\n",
      "\n",
      "            # clear input buffers and prompts\n",
      "            self.input_buf[i] = 0\n",
      "            self.input_buf_lens[i] = 0\n",
      "            self.prompt_tokens[i] = None\n",
      "\n",
      "            # clear generation state\n",
      "            self.generate_idx[i] = 0\n",
      "            self.unfinished_sequences[i] = 1\n",
      "\n",
      "            self.output_logits_mask[i] = []\n",
      "            self.output_tokens_buf[i] = []\n",
      "            self.output_logprob_buf[i] = []\n",
      "\n",
      "        # build packed input ids with variable lengths for the next-step inference\n",
      "        for i in range(self.batch_size):\n",
      "            if i in finish_indices:\n",
      "                try:\n",
      "                    prompt = self.inqueue.get_nowait()\n",
      "                    self.prompt_tokens[i] = prompt\n",
      "                    self.input_buf[i, : prompt.shape[0]] = prompt\n",
      "                    self.input_buf_lens[i] = prompt.shape[0]\n",
      "                except queue.Empty as e:\n",
      "                    raise RuntimeError(\n",
      "                        \"Input queue is empty. This should not happen.\"\n",
      "                    ) from e\n",
      "\n",
      "        input_lens = self.input_buf_lens\n",
      "        valid_input_mask = torch.arange(\n",
      "            self.max_prompt_len, device=self.input_buf.device, dtype=torch.int32\n",
      "        ).unsqueeze(0) < input_lens.unsqueeze(-1)\n",
      "        indices = torch.nonzero(valid_input_mask.flatten(), as_tuple=False).flatten()\n",
      "        packed_input_ids = self.input_buf.flatten()[indices]\n",
      "        max_seqlen = int(max(input_lens))\n",
      "        cu_seqlens = torch.nn.functional.pad(\n",
      "            input_lens.cumsum(0), (1, 0), value=0\n",
      "        ).int()\n",
      "\n",
      "        x = PipeTransferData(cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
      "        self.ys[0].packed_position_ids = None\n",
      "        self.ys[0].packed_input_ids = packed_input_ids\n",
      "        logits = self.model(x, self.ys).pp_output\n",
      "        logits = index_first_axis(logits, (cu_seqlens[1:] - 1).long())\n",
      "\n",
      "        self.cache_seqlens += input_lens\n",
      "\n",
      "        return logits.float()\n",
      "\n",
      "    def advance_one_genstep(self):\n",
      "        if self.unfinished_sequences.logical_not().any():\n",
      "            logits = self._get_inflight_logits()\n",
      "        else:\n",
      "            logits = self._get_non_eos_logits()\n",
      "\n",
      "        next_tokens, logprob, logits_mask, _, self.unfinished_sequences = genstep(\n",
      "            logits,\n",
      "            self.tokenizer,\n",
      "            self.unfinished_sequences,\n",
      "            self.generate_idx,\n",
      "            self.gconfig,\n",
      "        )\n",
      "\n",
      "        for i in range(self.batch_size):\n",
      "            self.output_tokens_buf[i].append(next_tokens[i].long())\n",
      "            self.output_logprob_buf[i].append(logprob[i].float())\n",
      "            if logits_mask is not None:\n",
      "                self.output_logits_mask[i].append(logits_mask[i].bool())\n",
      "            else:\n",
      "                self.output_logits_mask[i].append(None)\n",
      "\n",
      "        self.generate_idx += 1\n",
      "        self.input_buf[:, 0] = next_tokens\n",
      "\n",
      "    def step_for(self, n: int):\n",
      "        for _ in range(n):\n",
      "            self.advance_one_genstep()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/random.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "# Parts of the code here are adapted from PyTorch\n",
      "# repo: https://github.com/pytorch/pytorch\n",
      "\n",
      "import contextlib\n",
      "\n",
      "import torch\n",
      "from torch import _C\n",
      "from torch.cuda import _lazy_call\n",
      "from torch.cuda import device as device_ctx_manager\n",
      "from torch.utils.checkpoint import detach_variable\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "from realhf.impl.model.parallelism.tensor_parallel.utils import (\n",
      "    divide,\n",
      "    gather_split_1d_tensor,\n",
      "    safely_set_viewless_tensor_data,\n",
      "    set_tensor_model_parallel_attributes,\n",
      "    split_tensor_into_1d_equal_chunks,\n",
      ")\n",
      "\n",
      "# Default name for the model parallel rng tracker.\n",
      "_MODEL_PARALLEL_RNG_TRACKER_NAME = \"model-parallel-rng\"\n",
      "_EXPERT_PARALLEL_RNG_TRACKER_NAME = \"expert-parallel-rng\"\n",
      "_DATA_PARALLEL_RNG_TRACKER_NAME = \"data-parallel-rng\"\n",
      "\n",
      "\n",
      "def _set_cuda_rng_state(new_state, device=-1):\n",
      "    \"\"\"Sets the random number generator state of the current GPU.\n",
      "\n",
      "    Argumentss:\n",
      "        new_state (torch.ByteTensor): The desired state\n",
      "    This function is adapted from PyTorch repo (torch.cuda.set_rng_state)\n",
      "    with a single change: the input state is not cloned. Cloning caused\n",
      "    major performance issues for +4 GPU cases.\n",
      "    \"\"\"\n",
      "    if hasattr(_C, \"_cuda_setRNGState\") and callable(_C._cuda_setRNGState):\n",
      "        # older PyTorch\n",
      "        def cb():\n",
      "            with device_ctx_manager(device):\n",
      "                _C._cuda_setRNGState(new_state)\n",
      "\n",
      "    else:\n",
      "        # newer PyTorch\n",
      "        if device == -1:\n",
      "            device = torch.device(\"cuda\")\n",
      "        elif isinstance(device, str):\n",
      "            device = torch.device(device)\n",
      "        elif isinstance(device, int):\n",
      "            device = torch.device(\"cuda\", device)\n",
      "\n",
      "        def cb():\n",
      "            idx = device.index\n",
      "            if idx is None:\n",
      "                idx = constants.current_device()\n",
      "            default_generator = torch.cuda.default_generators[idx]\n",
      "            default_generator.set_state(new_state)\n",
      "\n",
      "    _lazy_call(cb)\n",
      "\n",
      "\n",
      "def get_expert_parallel_rng_tracker_name():\n",
      "    global _EXPERT_PARALLEL_RNG_TRACKER_NAME\n",
      "    return _EXPERT_PARALLEL_RNG_TRACKER_NAME\n",
      "\n",
      "\n",
      "def get_data_parallel_rng_tracker_name():\n",
      "    global _DATA_PARALLEL_RNG_TRACKER_NAME\n",
      "    return _DATA_PARALLEL_RNG_TRACKER_NAME\n",
      "\n",
      "\n",
      "class CudaRNGStatesTracker:\n",
      "    \"\"\"Tracker for the cuda RNG states.\n",
      "\n",
      "    Using the `add` method, a cuda rng state is initialized based on\n",
      "    the input `seed` and is assigned to `name`. Later, by forking the\n",
      "    rng state, we can perform operations and return to our starting\n",
      "    cuda state.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        # Map from a string name to the cuda rng state.\n",
      "        self.states_ = {}\n",
      "        # Seeds are just for book keeping and ensure no seed is set twice.\n",
      "        self.seeds_ = set()\n",
      "\n",
      "    def reset(self):\n",
      "        \"\"\"Set to the initial state (no tracker).\"\"\"\n",
      "        self.states_ = {}\n",
      "        self.seeds_ = set()\n",
      "\n",
      "    def get_states(self):\n",
      "        \"\"\"Get rng states.\n",
      "\n",
      "        Copy the dictionary so we have direct pointers to the states,\n",
      "        not just a pointer to the dictionary.\n",
      "        \"\"\"\n",
      "        states = {}\n",
      "        for name in self.states_:\n",
      "            states[name] = self.states_[name]\n",
      "        return states\n",
      "\n",
      "    def set_states(self, states):\n",
      "        \"\"\"Set the rng states.\n",
      "\n",
      "        For efficiency purposes, we do not check the size of seed for\n",
      "        compatibility.\n",
      "        \"\"\"\n",
      "        self.states_ = states\n",
      "\n",
      "    def add(self, name, seed):\n",
      "        \"\"\"Track the rng state.\"\"\"\n",
      "        # Check seed is not already used.\n",
      "        if seed in self.seeds_:\n",
      "            raise Exception(\"seed {} already exists\".format(seed))\n",
      "        self.seeds_.add(seed)\n",
      "        # Check that state is not already defined.\n",
      "        if name in self.states_:\n",
      "            raise Exception(\"cuda rng state {} already exists\".format(name))\n",
      "        # Get the current rng state.\n",
      "        orig_rng_state = torch.cuda.get_rng_state()\n",
      "        # Set the new state and store it.\n",
      "        torch.cuda.manual_seed(seed)\n",
      "        self.states_[name] = torch.cuda.get_rng_state()\n",
      "        # Reset rng state to what it was.\n",
      "        _set_cuda_rng_state(orig_rng_state)\n",
      "\n",
      "    @contextlib.contextmanager\n",
      "    def fork(self, name=_MODEL_PARALLEL_RNG_TRACKER_NAME):\n",
      "        \"\"\"Fork the cuda rng state, perform operations, and exit with the\n",
      "        original state.\"\"\"\n",
      "        # Check if we have added the state\n",
      "        if name not in self.states_:\n",
      "            raise Exception(\"cuda rng state {} is not added\".format(name))\n",
      "        # Store current rng state.\n",
      "        orig_cuda_rng_state = torch.cuda.get_rng_state()\n",
      "        # Set rng state to the desired one\n",
      "        _set_cuda_rng_state(self.states_[name])\n",
      "        # Do the stuff we wanted to do.\n",
      "        try:\n",
      "            yield\n",
      "        finally:\n",
      "            # Update the current rng state for later use.\n",
      "            self.states_[name] = torch.cuda.get_rng_state()\n",
      "            # And set the state to the original state we started with.\n",
      "            _set_cuda_rng_state(orig_cuda_rng_state)\n",
      "\n",
      "\n",
      "# RNG tracker object.\n",
      "_CUDA_RNG_STATE_TRACKER = CudaRNGStatesTracker()\n",
      "\n",
      "\n",
      "def get_cuda_rng_tracker():\n",
      "    \"\"\"Get cuda rng tracker.\"\"\"\n",
      "    return _CUDA_RNG_STATE_TRACKER\n",
      "\n",
      "\n",
      "def model_parallel_cuda_manual_seed(seed):\n",
      "    \"\"\"Initialize model parallel cuda seed.\n",
      "\n",
      "    This function should be called after the model parallel is\n",
      "    initialized. Also, no torch.cuda.manual_seed should be called\n",
      "    after this function. Basically, this is replacement for that\n",
      "    function.\n",
      "    Two set of RNG states are tracked:\n",
      "    default state: This is for data parallelism and is the same among a set of model parallel GPUs but different across different model paralle groups. This is used for example for dropout in the non-tensor-model-parallel regions.\n",
      "    tensor-model-parallel state: This state is different among a set of model parallel GPUs, but the same across data parallel groups. This is used for example for dropout in model parallel regions.\n",
      "    \"\"\"\n",
      "    # 2718 is just for fun and any POSITIVE value will work.\n",
      "    tensor_parallel_rank = constants.tensor_parallel_rank()\n",
      "    expert_parallel_rank = 0\n",
      "    offset = seed + 2718\n",
      "    tensor_model_parallel_seed = offset + tensor_parallel_rank\n",
      "    # Data parallel gets the original seed.\n",
      "    data_parallel_seed = seed\n",
      "\n",
      "    _CUDA_RNG_STATE_TRACKER.reset()\n",
      "    # Set the default state.\n",
      "    torch.cuda.manual_seed(data_parallel_seed)\n",
      "    _CUDA_RNG_STATE_TRACKER.add(_DATA_PARALLEL_RNG_TRACKER_NAME, data_parallel_seed)\n",
      "\n",
      "    # and model parallel state.\n",
      "    _CUDA_RNG_STATE_TRACKER.add(\n",
      "        _MODEL_PARALLEL_RNG_TRACKER_NAME, tensor_model_parallel_seed\n",
      "    )\n",
      "\n",
      "    expert_parallel_seed = (\n",
      "        seed + 1024 + 100 * expert_parallel_rank + tensor_parallel_rank\n",
      "    )\n",
      "    _CUDA_RNG_STATE_TRACKER.add(_EXPERT_PARALLEL_RNG_TRACKER_NAME, expert_parallel_seed)\n",
      "\n",
      "\n",
      "class CheckpointFunction(torch.autograd.Function):\n",
      "    \"\"\"Checkpoint Function.\n",
      "\n",
      "    This function is adapted from torch.utils.checkpoint with two main changes:\n",
      "    1) torch.cuda.set_rng_state is replaced with `_set_cuda_rng_state`\n",
      "    2) the states in the model parallel tracker are also properly tracked/set/reset.\n",
      "    \"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, run_function, distribute_saved_activations, *args):\n",
      "        ctx.run_function = run_function\n",
      "        ctx.distribute_saved_activations = distribute_saved_activations\n",
      "\n",
      "        # Copy the rng states.\n",
      "        ctx.fwd_cpu_rng_state = torch.get_rng_state()\n",
      "        ctx.fwd_cuda_rng_state = torch.cuda.get_rng_state()\n",
      "        ctx.fwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\n",
      "\n",
      "        with torch.no_grad():\n",
      "            outputs = run_function(*args)\n",
      "\n",
      "        # Divide hidden states across model parallel group and only keep\n",
      "        # the chunk corresponding to the current rank.\n",
      "        if distribute_saved_activations:\n",
      "            ctx.input_0_shape = args[0].data.shape\n",
      "            safely_set_viewless_tensor_data(\n",
      "                args[0],\n",
      "                split_tensor_into_1d_equal_chunks(args[0].data, new_buffer=True),\n",
      "            )\n",
      "\n",
      "        # Store everything.\n",
      "        ctx.save_for_backward(*args)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, *args):\n",
      "        if not torch.autograd._is_checkpoint_valid():\n",
      "            raise RuntimeError(\n",
      "                \"Checkpointing is not compatible with .grad(), \"\n",
      "                \"please use .backward() if possible\"\n",
      "            )\n",
      "        inputs = ctx.saved_tensors\n",
      "        if ctx.distribute_saved_activations:\n",
      "            safely_set_viewless_tensor_data(\n",
      "                inputs[0],\n",
      "                gather_split_1d_tensor(inputs[0].data).view(ctx.input_0_shape),\n",
      "            )\n",
      "\n",
      "        # Store the current states.\n",
      "        bwd_cpu_rng_state = torch.get_rng_state()\n",
      "        bwd_cuda_rng_state = torch.cuda.get_rng_state()\n",
      "        bwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\n",
      "\n",
      "        # Set the states to what it used to be before the forward pass.\n",
      "        torch.set_rng_state(ctx.fwd_cpu_rng_state)\n",
      "        _set_cuda_rng_state(ctx.fwd_cuda_rng_state)\n",
      "        get_cuda_rng_tracker().set_states(ctx.fwd_cuda_rng_state_tracker)\n",
      "\n",
      "        # Compute the forward pass.\n",
      "        detached_inputs = detach_variable(inputs)\n",
      "        with torch.enable_grad():\n",
      "            outputs = ctx.run_function(*detached_inputs)\n",
      "\n",
      "        # Set the states back to what it was at the start of this function.\n",
      "        torch.set_rng_state(bwd_cpu_rng_state)\n",
      "        _set_cuda_rng_state(bwd_cuda_rng_state)\n",
      "        get_cuda_rng_tracker().set_states(bwd_cuda_rng_state_tracker)\n",
      "\n",
      "        if isinstance(outputs, torch.Tensor):\n",
      "            outputs = (outputs,)\n",
      "\n",
      "        # filter out non tensor outputs for backward pass\n",
      "        outputs, args = zip(\n",
      "            *filter(lambda x: torch.is_tensor(x[0]), zip(outputs, args))\n",
      "        )\n",
      "        torch.autograd.backward(outputs, args)\n",
      "        grads = tuple(\n",
      "            inp.grad if isinstance(inp, torch.Tensor) else inp\n",
      "            for inp in detached_inputs\n",
      "        )\n",
      "        return (None, None) + grads\n",
      "\n",
      "\n",
      "def checkpoint(function, distribute_saved_activations, *args):\n",
      "    \"\"\"Checkpoint a model or part of the model.\n",
      "\n",
      "    This has been directly copied from torch.utils.checkpoint.\n",
      "    \"\"\"\n",
      "    return CheckpointFunction.apply(function, distribute_saved_activations, *args)\n",
      "\n",
      "\n",
      "def _initialize_affine_weight_gpu(\n",
      "    weight,\n",
      "    init_method,\n",
      "    partition_dim,\n",
      "    stride=1,\n",
      "):\n",
      "    \"\"\"Initialize affine weight for model parallel on GPU.\"\"\"\n",
      "    set_tensor_model_parallel_attributes(\n",
      "        tensor=weight, is_parallel=True, dim=partition_dim, stride=stride\n",
      "    )\n",
      "    init_method(weight)\n",
      "\n",
      "\n",
      "def _initialize_affine_weight_cpu(\n",
      "    weight,\n",
      "    output_size,\n",
      "    input_size,\n",
      "    per_partition_size,\n",
      "    partition_dim,\n",
      "    init_method,\n",
      "    stride=1,\n",
      "    return_master_weight=False,\n",
      "    *,\n",
      "    params_dtype=torch.float32,\n",
      "):\n",
      "    \"\"\"Initialize affine weight for model parallel.\n",
      "\n",
      "    Build the master weight on all processes and scatter the relevant\n",
      "    chunk.\n",
      "    \"\"\"\n",
      "\n",
      "    set_tensor_model_parallel_attributes(\n",
      "        tensor=weight, is_parallel=True, dim=partition_dim, stride=stride\n",
      "    )\n",
      "\n",
      "    # Initialize master weight\n",
      "    master_weight = torch.empty(\n",
      "        output_size, input_size, dtype=torch.float, requires_grad=False\n",
      "    )\n",
      "    init_method(master_weight)\n",
      "    master_weight = master_weight.to(dtype=params_dtype)\n",
      "\n",
      "    # Split and copy\n",
      "    per_partition_per_stride_size = divide(per_partition_size, stride)\n",
      "    weight_list = torch.split(\n",
      "        master_weight, per_partition_per_stride_size, dim=partition_dim\n",
      "    )\n",
      "    rank = constants.tensor_parallel_rank()\n",
      "    world_size = constants.tensor_parallel_world_size()\n",
      "    my_weight_list = weight_list[rank::world_size]\n",
      "\n",
      "    with torch.no_grad():\n",
      "        torch.cat(my_weight_list, dim=partition_dim, out=weight)\n",
      "    if return_master_weight:\n",
      "        return master_weight\n",
      "    return None\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/dpo_functional.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import Tuple\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "def dpo_loss(\n",
      "    pi_logps: torch.Tensor,\n",
      "    ref_logps: torch.Tensor,\n",
      "    beta: float,\n",
      "):\n",
      "    assert len(pi_logps.shape) == 1 and pi_logps.shape[0] % 2 == 0, (\n",
      "        pi_logps.shape,\n",
      "        ref_logps.shape,\n",
      "    )\n",
      "    assert len(ref_logps.shape) == 1 and ref_logps.shape[0] % 2 == 0, (\n",
      "        pi_logps.shape,\n",
      "        ref_logps.shape,\n",
      "    )\n",
      "    pi_logps = pi_logps.view(-1, 2)\n",
      "    ref_logps = ref_logps.view(-1, 2)\n",
      "    pi_yw_logps, pi_yl_logps = pi_logps[:, 0], pi_logps[:, 1]\n",
      "    ref_yw_logps, ref_yl_logps = ref_logps[:, 0], ref_logps[:, 1]\n",
      "    pi_logratios = pi_yw_logps - pi_yl_logps\n",
      "    ref_logratios = ref_yw_logps - ref_yl_logps\n",
      "    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios)).mean()\n",
      "    pos_score = beta * (pi_yw_logps - ref_yw_logps).detach().sum()\n",
      "    neg_score = beta * (pi_yl_logps - ref_yl_logps).detach().sum()\n",
      "    kl = -(pi_logps - ref_logps).detach().sum()\n",
      "    return losses, pos_score, neg_score, kl\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/padding.py ====\n",
      "\n",
      "# Modified from flash-attention under BSD-3 license.\n",
      "# Copyright (c) 2023, Tri Dao.\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from einops import rearrange, repeat\n",
      "\n",
      "from realhf.base import constants\n",
      "\n",
      "\n",
      "class IndexFirstAxis(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input, indices):\n",
      "        ctx.save_for_backward(indices)\n",
      "        assert input.ndim >= 2\n",
      "        ctx.first_axis_dim, other_shape = input.shape[0], input.shape[1:]\n",
      "        second_dim = other_shape.numel()\n",
      "        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n",
      "        # return input[indices]\n",
      "        return torch.gather(\n",
      "            rearrange(input, \"b ... -> b (...)\"),\n",
      "            0,\n",
      "            repeat(indices, \"z -> z d\", d=second_dim),\n",
      "        ).reshape(-1, *other_shape)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        (indices,) = ctx.saved_tensors\n",
      "        assert grad_output.ndim >= 2\n",
      "        other_shape = grad_output.shape[1:]\n",
      "        grad_output = rearrange(grad_output, \"b ... -> b (...)\")\n",
      "        grad_input = torch.zeros(\n",
      "            [ctx.first_axis_dim, grad_output.shape[1]],\n",
      "            device=grad_output.device,\n",
      "            dtype=grad_output.dtype,\n",
      "        )\n",
      "        # TD [2022-03-04] For some reason torch.scatter is a bit faster than indexing.\n",
      "        # grad_input[indices] = grad_output\n",
      "        grad_input.scatter_(\n",
      "            0, repeat(indices, \"z -> z d\", d=grad_output.shape[1]), grad_output\n",
      "        )\n",
      "        return grad_input.reshape(ctx.first_axis_dim, *other_shape), None\n",
      "\n",
      "\n",
      "def index_first_axis(x: torch.Tensor, indices: torch.LongTensor):\n",
      "    if len(x.shape) == 1:\n",
      "        return x[indices]\n",
      "    else:\n",
      "        return IndexFirstAxis.apply(x, indices)\n",
      "\n",
      "\n",
      "class IndexPutFirstAxis(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, values, indices, first_axis_dim):\n",
      "        ctx.save_for_backward(indices)\n",
      "        assert indices.ndim == 1\n",
      "        assert values.ndim >= 2\n",
      "        output = torch.zeros(\n",
      "            first_axis_dim,\n",
      "            *values.shape[1:],\n",
      "            device=values.device,\n",
      "            dtype=values.dtype,\n",
      "        )\n",
      "        # TD [2022-03-04] For some reason torch.scatter is a bit faster than indexing.\n",
      "        output[indices] = values\n",
      "        # output.scatter_(0, repeat(indices, \"z -> z d\", d=values.shape[1]), values)\n",
      "        return output\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        (indices,) = ctx.saved_tensors\n",
      "        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n",
      "        grad_values = grad_output[indices]\n",
      "        # grad_values = torch.gather(grad_output, 0, repeat(indices, \"z -> z d\", d=grad_output.shape[1]))\n",
      "        return grad_values, None, None\n",
      "\n",
      "\n",
      "def index_put_first_axis(\n",
      "    values: torch.Tensor, indices: torch.LongTensor, first_axis_dim: int\n",
      "):\n",
      "    if len(values.shape) == 1:\n",
      "        output = torch.zeros(first_axis_dim, device=values.device, dtype=values.dtype)\n",
      "        output[indices] = values\n",
      "        return output\n",
      "    else:\n",
      "        return IndexPutFirstAxis.apply(values, indices, first_axis_dim)\n",
      "\n",
      "\n",
      "class IndexFirstAxisResidual(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input, indices):\n",
      "        ctx.save_for_backward(indices)\n",
      "        assert input.ndim >= 2\n",
      "        ctx.first_axis_dim, other_shape = input.shape[0], input.shape[1:]\n",
      "        second_dim = other_shape.numel()\n",
      "        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n",
      "        output = input[indices]\n",
      "        # We don't want to reshape input (b ... -> b (...)) since it could change the channel_last\n",
      "        # memory format to channel_first. In other words, input might not be contiguous.\n",
      "        # If we don't detach, Pytorch complains about output being a view and is being modified inplace\n",
      "        return output, input.detach()\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output, grad_residual):\n",
      "        (indices,) = ctx.saved_tensors\n",
      "        assert grad_output.ndim >= 2\n",
      "        other_shape = grad_output.shape[1:]\n",
      "        assert grad_residual.shape[1:] == other_shape\n",
      "        grad_input = grad_residual\n",
      "        # grad_input[indices] += grad_output\n",
      "        indices = indices.reshape(indices.shape[0], *((1,) * (grad_output.ndim - 1)))\n",
      "        indices = indices.expand_as(grad_output)\n",
      "        grad_input.scatter_add_(0, indices, grad_output)\n",
      "        return grad_input.reshape(ctx.first_axis_dim, *other_shape), None\n",
      "\n",
      "\n",
      "index_first_axis_residual = IndexFirstAxisResidual.apply\n",
      "\n",
      "\n",
      "def unpad_input(hidden_states, attention_mask):\n",
      "    \"\"\"\n",
      "    Arguments:\n",
      "        hidden_states: (batch, seqlen, ...)\n",
      "        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n",
      "    Return:\n",
      "        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n",
      "        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n",
      "        max_seqlen_in_batch: int\n",
      "    \"\"\"\n",
      "    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n",
      "    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n",
      "    max_seqlen_in_batch = seqlens_in_batch.max().item()\n",
      "    cu_seqlens = F.pad(\n",
      "        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0)\n",
      "    )\n",
      "    # TD [2022-03-04] We don't want to index with a bool mask, because Pytorch will expand the\n",
      "    # bool mask, then call nonzero to get the indices, then index with those. The indices is @dim\n",
      "    # times larger than it needs to be, wasting memory. It's faster and more memory-efficient to\n",
      "    # index with integer indices. Moreover, torch's index is a bit slower than it needs to be,\n",
      "    # so we write custom forward and backward to make it a bit faster.\n",
      "    return (\n",
      "        index_first_axis(rearrange(hidden_states, \"b s ... -> (b s) ...\"), indices),\n",
      "        indices,\n",
      "        cu_seqlens,\n",
      "        max_seqlen_in_batch,\n",
      "    )\n",
      "\n",
      "\n",
      "def unpad_input_for_concatenated_sequences(hidden_states, attention_mask_in_length):\n",
      "    \"\"\"\n",
      "    Supports concatenating short samples in one sequence.\n",
      "    The attention_mask_in_length is utilized to mask other short samples.\n",
      "    It helps efficient training of variant lengths-based samples\n",
      "    (e.g., the supervised fine-tuning task in large language model).\n",
      "    The motivation for this function is explained\n",
      "    [here](https://github.com/Dao-AILab/flash-attention/issues/432#issuecomment-1668822286).\n",
      "\n",
      "    For example, if batch = 3 and seqlen = 6, the attention_mask_in_length is:\n",
      "        ```\n",
      "        [\n",
      "          [2, 3, 0, 0, 0, 0],\n",
      "          [3, 2, 0, 0, 0, 0],\n",
      "          [6, 0, 0, 0, 0, 0]\n",
      "        ]\n",
      "        ```\n",
      "    , which refers to the 3D-attention mask:\n",
      "        ```\n",
      "        [\n",
      "          [\n",
      "            [1, 0, 0, 0, 0, 0],\n",
      "            [1, 1, 0, 0, 0, 0],\n",
      "            [0, 0, 1, 0, 0, 0],\n",
      "            [0, 0, 1, 1, 0, 0],\n",
      "            [0, 0, 1, 1, 1, 0],\n",
      "            [0, 0, 0, 0, 0, 1]\n",
      "          ],\n",
      "          [\n",
      "            [1, 0, 0, 0, 0, 0],\n",
      "            [1, 1, 0, 0, 0, 0],\n",
      "            [1, 1, 1, 0, 0, 0],\n",
      "            [0, 0, 0, 1, 0, 0],\n",
      "            [0, 0, 0, 1, 1, 0],\n",
      "            [0, 0, 0, 0, 0, 1]\n",
      "          ],\n",
      "          [\n",
      "            [1, 0, 0, 0, 0, 0],\n",
      "            [1, 1, 0, 0, 0, 0],\n",
      "            [1, 1, 1, 0, 0, 0],\n",
      "            [1, 1, 1, 1, 0, 0],\n",
      "            [1, 1, 1, 1, 1, 0],\n",
      "            [1, 1, 1, 1, 1, 1]\n",
      "          ]\n",
      "        ]\n",
      "        ```.\n",
      "\n",
      "    Arguments:\n",
      "        hidden_states: (batch, seqlen, ...)\n",
      "        attention_mask_in_length: (batch, seqlen), int, a nonzero number (e.g., 1, 2, 3, etc.) means length of concatenated sequence in b-th batch, and 0 means none.\n",
      "    Return:\n",
      "        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n",
      "        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n",
      "        max_seqlen_in_batch: int\n",
      "    \"\"\"\n",
      "    length = attention_mask_in_length.sum(dim=-1)\n",
      "    seqlen = attention_mask_in_length.size(-1)\n",
      "    attention_mask_2d = torch.arange(\n",
      "        seqlen, device=length.device, dtype=length.dtype\n",
      "    ).expand(len(length), seqlen) < length.unsqueeze(1)\n",
      "    real_indices_idx = torch.nonzero(\n",
      "        attention_mask_in_length.flatten(), as_tuple=False\n",
      "    ).flatten()\n",
      "    seqlens_in_batch = attention_mask_in_length.flatten()[real_indices_idx]\n",
      "    indices = torch.nonzero(attention_mask_2d.flatten(), as_tuple=False).flatten()\n",
      "    max_seqlen_in_batch = seqlens_in_batch.max().item()\n",
      "    cu_seqlens = F.pad(\n",
      "        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0)\n",
      "    )\n",
      "    # TD [2022-03-04] We don't want to index with a bool mask, because Pytorch will expand the\n",
      "    # bool mask, then call nonzero to get the indices, then index with those. The indices is @dim\n",
      "    # times larger than it needs to be, wasting memory. It's faster and more memory-efficient to\n",
      "    # index with integer indices. Moreover, torch's index is a bit slower than it needs to be,\n",
      "    # so we write custom forward and backward to make it a bit faster.\n",
      "    return (\n",
      "        index_first_axis(rearrange(hidden_states, \"b s ... -> (b s) ...\"), indices),\n",
      "        indices,\n",
      "        cu_seqlens,\n",
      "        max_seqlen_in_batch,\n",
      "    )\n",
      "\n",
      "\n",
      "def pad_input(hidden_states, indices, batch, seqlen):\n",
      "    \"\"\"\n",
      "    Arguments:\n",
      "        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n",
      "        indices: (total_nnz)\n",
      "    Return:\n",
      "        hidden_states: (batch, seqlen, ...)\n",
      "    \"\"\"\n",
      "    dim = hidden_states.shape[-1]\n",
      "    # output = torch.zeros((batch * seqlen), dim, device=hidden_states.device, dtype=hidden_states.dtype)\n",
      "    # output[indices] = hidden_states\n",
      "    output = index_put_first_axis(hidden_states, indices, batch * seqlen)\n",
      "    return rearrange(output, \"(b s) ... -> b s ...\", b=batch)\n",
      "\n",
      "\n",
      "def pad_sequence_parallel_input(\n",
      "    packed_input_ids: torch.Tensor, cu_seqlens: torch.Tensor, max_seqlen: int\n",
      "):\n",
      "    \"\"\"Sequence parallel requires packed_input_ids has a shape of 1 dimension\n",
      "    [total_seq_len], and total_seq_len should be divisible by\n",
      "    tensor_parallel_world_size. This function is used to pad packed_input_ids to\n",
      "    suitable length with an empty sequence, and return new packed_input_ids,\n",
      "    cu_seqlens and max_seqlen.\n",
      "\n",
      "    Args:\n",
      "        packed_input_ids (torch.Tensor): unpadded packed_input_ids\n",
      "        cu_seqlens (torch.Tensor): unpadded cu_seqlens\n",
      "        max_seqlen (int): unpadded max_seqlen\n",
      "\n",
      "    Returns:\n",
      "        (torch.Tensor, torch.Tensor, int, int): padded (packed_input_ids, cu_seqlens, max_seqlen, pad_size)\n",
      "    \"\"\"\n",
      "    tp_world_size = constants.tensor_parallel_world_size()\n",
      "    pad_size = 0\n",
      "    if len(packed_input_ids) % tp_world_size != 0:\n",
      "        pad_size = tp_world_size - len(packed_input_ids) % tp_world_size\n",
      "        packed_input_ids = torch.nn.functional.pad(\n",
      "            packed_input_ids, (0, pad_size), value=1\n",
      "        )\n",
      "        cu_seqlens = torch.nn.functional.pad(\n",
      "            cu_seqlens, (0, 1), value=len(packed_input_ids)\n",
      "        )\n",
      "        max_seqlen = max_seqlen if pad_size < max_seqlen else pad_size\n",
      "    return packed_input_ids, cu_seqlens, max_seqlen, pad_size\n",
      "\n",
      "\n",
      "def pad_sequence_parallel_generate_input(\n",
      "    packed_input_ids: torch.Tensor, cu_seqlens: torch.Tensor, max_seqlen: int\n",
      "):\n",
      "    \"\"\"Only for pipeline generate input when model+seq parallel is enabled. To\n",
      "    make sure inputs for seq parallel model have a shape with first dimension\n",
      "    divisible by tensor_parallel_world_size, the packed_input_ids should have\n",
      "    length divisible by tensor_parallel_world_size, and contains number of\n",
      "    sequences divisible by tensor_parallel_world_size.\n",
      "\n",
      "    Args:\n",
      "        packed_input_ids (torch.Tensor): unpadded packed_input_ids\n",
      "        cu_seqlens (torch.Tensor): unpadded cu_seqlens\n",
      "        max_seqlen (int): unpadded max_seqlen\n",
      "\n",
      "    Returns:\n",
      "        (torch.Tensor, torch.Tensor, int, int, int): padded (packed_input_ids, cu_seqlens, max_seqlen, pad_size, pad_seq_size)\n",
      "    \"\"\"\n",
      "    tp_world_size = constants.tensor_parallel_world_size()\n",
      "    pad_size, pad_seq_size = 0, 0\n",
      "    if (\n",
      "        len(packed_input_ids) % tp_world_size != 0\n",
      "        or (len(cu_seqlens) - 1) % tp_world_size != 0\n",
      "    ):\n",
      "        pad_size = tp_world_size - len(packed_input_ids) % tp_world_size\n",
      "        pad_seq_size = tp_world_size - (len(cu_seqlens) - 1) % tp_world_size\n",
      "        if pad_size < pad_seq_size:\n",
      "            pad_size += tp_world_size\n",
      "        pad_cu_seqlens = torch.tensor(list(range(1, pad_seq_size)) + [pad_size]) + len(\n",
      "            packed_input_ids\n",
      "        )\n",
      "        pad_cu_seqlens = pad_cu_seqlens.to(\n",
      "            dtype=cu_seqlens.dtype, device=cu_seqlens.device\n",
      "        )\n",
      "        packed_input_ids = torch.nn.functional.pad(\n",
      "            packed_input_ids, (0, pad_size), value=1\n",
      "        )\n",
      "        cu_seqlens = torch.cat([cu_seqlens, pad_cu_seqlens], dim=0)\n",
      "        max_seqlen = (\n",
      "            max_seqlen\n",
      "            if (pad_size - pad_seq_size + 1) < max_seqlen\n",
      "            else (pad_size - pad_seq_size + 1)\n",
      "        )\n",
      "    return packed_input_ids, cu_seqlens, max_seqlen, pad_size, pad_seq_size\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/moe.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "import math\n",
      "from typing import Any, Optional\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "\n",
      "def switch_load_balancing_loss_func(\n",
      "    probs: torch.Tensor,\n",
      "    tokens_per_expert: torch.Tensor,\n",
      "    topk: int,\n",
      "    moe_aux_loss_coeff: float,\n",
      "    sequence_partition_group=None,\n",
      "):\n",
      "    \"\"\"Calculate the auxiliary loss for load balancing.\n",
      "    Refer to the Switch Transformer paper (https://arxiv.org/abs/2101.03961) for details.\n",
      "\n",
      "    Args:\n",
      "        probs (torch.Tensor): Softmax probabilities output by the router for each token. [num_tokens, num_experts]\n",
      "        tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert. [num_experts]\n",
      "        topk (int): The number of experts selected for each token.\n",
      "        moe_aux_loss_coeff (float): The coefficient for the auxiliary loss.\n",
      "        sequence_partition_group (optional): The parallel group over which the sequence is partitioned. If None, no partitioning is applied. Defaults to None.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: The auxiliary loss for load balancing.\n",
      "    \"\"\"\n",
      "    num_sub_sequence = 1\n",
      "\n",
      "    # If the sequence is partitioned by certain parallelism strategies like Sequence Parallelism or Context Parallelism, compute the gradient of the auxiliary loss with respect to the full sequence.\n",
      "    if sequence_partition_group is not None:\n",
      "        # We can keep `aggregated_probs_per_expert` local since we don't need the gradient for `tokens_per_expert`, saving one allreduce operation for `aggregated_probs_per_expert`.\n",
      "        # NOTE: Since the auxiliary loss is computed on the local `aggregated_probs_per_expert`, it requires scaling by `dist.world_size(sequence_partition_group)` when printing the loss.\n",
      "        num_sub_sequence = dist.get_world_size(sequence_partition_group)\n",
      "        dist.all_reduce(tokens_per_expert, group=sequence_partition_group)\n",
      "\n",
      "    num_tokens = probs.shape[0] * topk * num_sub_sequence\n",
      "    num_experts = probs.shape[1]\n",
      "\n",
      "    # The formula of aux_loss: aux_loss = sum((probs_per_expert/num_tokens) * (tokens_per_expert/num_tokens)) * num_experts * moe_aux_loss_coeff.\n",
      "    # This can be simplified to fuse the division and multiplication operations.\n",
      "    aggregated_probs_per_expert = probs.sum(dim=0)\n",
      "    aux_loss = torch.sum(aggregated_probs_per_expert * tokens_per_expert) * (\n",
      "        num_experts * moe_aux_loss_coeff / (num_tokens * num_tokens)\n",
      "    )\n",
      "    return aux_loss\n",
      "\n",
      "\n",
      "def z_loss_func(logits, z_loss_coeff):\n",
      "    \"\"\"Encourages the router's logits to remain small to enhance stability.\n",
      "    Please refer to the ST-MoE paper (https://arxiv.org/pdf/2202.08906.pdf) for details.\n",
      "\n",
      "    Args:\n",
      "        logits (torch.Tensor): The logits of the router.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: The logits after applying the z-loss.\n",
      "    \"\"\"\n",
      "\n",
      "    z_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff\n",
      "    return z_loss\n",
      "\n",
      "\n",
      "def sinkhorn(cost: torch.Tensor, tol: float = 0.0001):\n",
      "    \"\"\"Sinkhorn based MoE routing function.\"\"\"\n",
      "    cost = torch.exp(cost)\n",
      "    d0 = torch.ones(cost.size(0), device=cost.device, dtype=cost.dtype)\n",
      "    d1 = torch.ones(cost.size(1), device=cost.device, dtype=cost.dtype)\n",
      "\n",
      "    eps = 0.00000001\n",
      "    error = 1e9\n",
      "    d1_old = d1\n",
      "    while error > tol:\n",
      "        d0 = (1 / d0.size(0)) * 1 / (torch.sum(d1 * cost, 1) + eps)\n",
      "        d1 = (1 / d1.size(0)) * 1 / (torch.sum(d0.unsqueeze(1) * cost, 0) + eps)\n",
      "        error = torch.mean(torch.abs(d1_old - d1))\n",
      "        d1_old = d1\n",
      "    return d1 * cost * d0.unsqueeze(1)\n",
      "\n",
      "\n",
      "def get_capacity(\n",
      "    num_tokens: int, num_experts: int, capacity_factor: float, min_capacity=None\n",
      "):\n",
      "    \"\"\"Calculate the capacity of each expert.\n",
      "\n",
      "    Args:\n",
      "        num_tokens (int): num of the input tokens.\n",
      "        num_experts (int): num of the experts.\n",
      "        capacity_factor (float): Capacity factor.\n",
      "        min_capacity (int, optional): Minimum capacity. Defaults to None.\n",
      "\n",
      "    Returns:\n",
      "        Tensor: Capacity of each expert.\n",
      "    \"\"\"\n",
      "    capacity = math.ceil((num_tokens / num_experts) * capacity_factor)\n",
      "    if min_capacity is not None and capacity < min_capacity:\n",
      "        capacity = min_capacity\n",
      "    return capacity\n",
      "\n",
      "\n",
      "def custom_histc(input: torch.Tensor, bins: int, min: Any, max: Any):\n",
      "    \"\"\"A CPU compatible version of torch.histc.\"\"\"\n",
      "    if input.is_cpu:\n",
      "        out = torch.zeros(bins, dtype=torch.long)\n",
      "        bin_width = (max - min) / bins\n",
      "\n",
      "        # Iterate over the input tensor and increment the appropriate bin\n",
      "        for value in input.flatten():\n",
      "            if min <= value < max:\n",
      "                bin_index = int((value - min) / bin_width)\n",
      "                out[bin_index] += 1\n",
      "            elif value == max:\n",
      "                out[bins - 1] += 1\n",
      "        return out\n",
      "    else:\n",
      "        return torch.histc(input, bins=bins, min=min, max=max)\n",
      "\n",
      "\n",
      "class MoEAuxLossAutoScaler(torch.autograd.Function):\n",
      "    \"\"\"An AutoScaler that compute and scales the grad for auxiliary loss.\"\"\"\n",
      "\n",
      "    main_loss_backward_scale: torch.Tensor = torch.tensor(1.0)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, output: torch.Tensor, aux_loss: torch.Tensor):\n",
      "        \"\"\"Preserve the aux_loss by storing it in the context to avoid garbage\n",
      "        collection.\n",
      "\n",
      "        Args:\n",
      "            output (torch.Tensor): The output tensor.\n",
      "            aux_loss (torch.Tensor): The auxiliary loss tensor.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: The output tensor.\n",
      "        \"\"\"\n",
      "        ctx.save_for_backward(aux_loss)\n",
      "        return output\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output: torch.Tensor):\n",
      "        \"\"\"Compute and scale the gradient for auxiliary loss..\n",
      "\n",
      "        Args:\n",
      "            grad_output (torch.Tensor): The gradient of the output.\n",
      "\n",
      "        Returns:\n",
      "            Tuple[torch.Tensor, torch.Tensor]: The gradient of the output, scaled auxiliary loss gradient.\n",
      "        \"\"\"\n",
      "        (aux_loss,) = ctx.saved_tensors\n",
      "        aux_loss_backward_scale = MoEAuxLossAutoScaler.main_loss_backward_scale\n",
      "        scaled_aux_loss_grad = torch.ones_like(aux_loss) * aux_loss_backward_scale\n",
      "        return grad_output, scaled_aux_loss_grad\n",
      "\n",
      "    @staticmethod\n",
      "    def set_loss_scale(scale: torch.Tensor):\n",
      "        \"\"\"Set the scale of the aux loss.\n",
      "\n",
      "        Args:\n",
      "            scale (torch.Tensor): The scale value to set. Please ensure that the scale passed in matches the scale of the main_loss.\n",
      "        \"\"\"\n",
      "        MoEAuxLossAutoScaler.main_loss_backward_scale = scale\n",
      "\n",
      "\n",
      "def permute(tokens, indices, num_out_tokens: int = None, padded_mode: bool = False):\n",
      "    \"\"\"Permute the tokens based on the indices. Token with the same index will be grouped together.\n",
      "       The input indices shape is [tokens, top_k], it indicates which experts were selected by each token separately.\n",
      "    Args:\n",
      "        tokens (torch.Tensor): The input token tensor.\n",
      "        indices (torch.Tensor): The token to expert indices tensor, should have a shape of [num_tokens] or [num_tokens, topk].\n",
      "        num_out_tokens (int, optional): The effective output token count, when enabling the capacity factor, should equal the number of tokens not dropped. By default, set to None, meaning no tokens are dropped.\n",
      "        padded_mode (bool, optional): If True, indicating the indices are padded to [num_expert, capacity] to denote selected tokens per expert. Defaults to False.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: The permuted tensor.\n",
      "        torch.Tensor: The sorted_indices corresponding permuted tensor.\n",
      "    \"\"\"\n",
      "    if padded_mode:\n",
      "        return permute_with_padded_tokens(tokens, indices)\n",
      "\n",
      "    if indices.dim() == 1:\n",
      "        topk = 1\n",
      "    else:\n",
      "        topk = indices.size(1)\n",
      "\n",
      "    flatten_indices = indices.view(-1)\n",
      "    sorted_indices = torch.argsort(flatten_indices, stable=True)\n",
      "    if num_out_tokens is not None:\n",
      "        sorted_indices = sorted_indices[:num_out_tokens]\n",
      "    permuted_tokens = tokens.index_select(0, sorted_indices // topk)\n",
      "    return permuted_tokens, sorted_indices\n",
      "\n",
      "\n",
      "def unpermute(\n",
      "    permuted_tokens: torch.Tensor,\n",
      "    sorted_indices: torch.Tensor,\n",
      "    probs: torch.Tensor = None,\n",
      "    padded_mode: bool = False,\n",
      "    restore_shape: torch.Size = None,\n",
      "):\n",
      "    \"\"\"Unpermute a tensor of permuted tokens based on sorted indices, and\n",
      "    optionally merge the tokens with their corresponding probabilities.\n",
      "\n",
      "    Args:\n",
      "        permuted_tokens (torch.Tensor): The tensor of permuted tokens to be unpermuted.\n",
      "        sorted_indices (torch.Tensor): The tensor of sorted indices used to unpermute the tokens.\n",
      "        probs (torch.Tensor, optional): The tensor of probabilities corresponding to the permuted tokens. If provided, the unpermuted tokens will be merged with their respective probabilities.\n",
      "        padded_mode (bool, optional): If True, indicating the indices are padded to [num_expert, capacity] to denote selected tokens per expert. Defaults to False.\n",
      "        restore_shape (torch.Size, optional): The input shape before permutation, only used in padding mode. Defaults to None.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: The unpermuted tokens, optionally merged with probabilities.\n",
      "    \"\"\"\n",
      "    if padded_mode:\n",
      "        return unpermute_with_padded_tokens(\n",
      "            permuted_tokens, sorted_indices, probs, restore_shape=restore_shape\n",
      "        )\n",
      "\n",
      "    assert sorted_indices.numel() == permuted_tokens.size(0)\n",
      "    if probs is not None:\n",
      "        # Unpermute and merge the tokens with their probabilities\n",
      "        num_unpermuted_tokens = probs.numel()\n",
      "        topk = probs.size(1)\n",
      "    else:\n",
      "        # Unpermute the tokens without merge\n",
      "        num_unpermuted_tokens = permuted_tokens.size(0)\n",
      "        topk = 1\n",
      "\n",
      "    unpermuted_tokens = torch.zeros(\n",
      "        [num_unpermuted_tokens, permuted_tokens.shape[-1]],\n",
      "        dtype=permuted_tokens.dtype,\n",
      "        device=permuted_tokens.device,\n",
      "    )\n",
      "    unpermuted_tokens.index_copy_(0, sorted_indices, permuted_tokens)\n",
      "    unpermuted_tokens = unpermuted_tokens.reshape(-1, topk, permuted_tokens.size(-1))\n",
      "    if probs is not None:\n",
      "        unpermuted_tokens = unpermuted_tokens * probs.unsqueeze(-1)\n",
      "    unpermuted_tokens = unpermuted_tokens.sum(dim=1)\n",
      "\n",
      "    return unpermuted_tokens\n",
      "\n",
      "\n",
      "def permute_with_padded_tokens(tokens, indices):\n",
      "    \"\"\"Permute the tokens based on the indices, only used in padding mode.\n",
      "       The input indices shape is [num_expert, capacity], it indicates which tokens were selected by each expert separately.\n",
      "    Args:\n",
      "        tokens (torch.Tensor): The input token tensor.\n",
      "        indices (torch.Tensor): A tensor with shape [num_expert, capacity], indicating the selected tokens for each expert.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: The permuted tensor.\n",
      "        torch.Tensor: The sorted_indices corresponding permuted tensor.\n",
      "    \"\"\"\n",
      "    permuted_tokens = tokens.index_select(dim=0, index=indices.view(-1))\n",
      "\n",
      "    return permuted_tokens, indices\n",
      "\n",
      "\n",
      "def unpermute_with_padded_tokens(\n",
      "    permuted_tokens: torch.Tensor,\n",
      "    indices: torch.Tensor,\n",
      "    probs: torch.Tensor,\n",
      "    restore_shape: torch.Size,\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"Unpermutes a padded permuted tokens based on sorted indices and merges\n",
      "    the tokens with their corresponding probabilities.\n",
      "\n",
      "    This function takes a tensor of permuted tokens and reorders them according to the provided indices. It also combines the tokens with their associated probabilities.\n",
      "\n",
      "    Parameters:\n",
      "        permuted_tokens (torch.Tensor): A 2D tensor containing permuted tokens.\n",
      "        indices (torch.Tensor): A tensor with shape [num_expert, capacity], indicating the selected tokens for each expert.\n",
      "        probs (torch.Tensor): A tensor with the same shape as indices, containing probabilities corresponding to each token.\n",
      "        restore_shape (torch.Size): The target shape for the unpermuted tokens tensor.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: A tensor of unpermuted tokens, merged with their probabilities.\n",
      "    \"\"\"\n",
      "    # Ensure permuted_tokens is 2D\n",
      "    assert permuted_tokens.dim() == 2, f\"Got {permuted_tokens.dim()}D.\"\n",
      "\n",
      "    # Reshape and expand probabilities and indices to match permuted_tokens\n",
      "    probs = probs.view(-1).unsqueeze(-1)\n",
      "    indices = indices.view(-1, 1).expand(-1, permuted_tokens.shape[1])\n",
      "    assert (\n",
      "        permuted_tokens.shape == indices.shape\n",
      "    ), \"Shape mismatch between permuted_tokens and indices.\"\n",
      "\n",
      "    # Combine tokens with their probabilities\n",
      "    combined_output = probs * permuted_tokens\n",
      "\n",
      "    # Prepare a tensor of zeros with the desired output shape\n",
      "    empty_tokens = torch.zeros(\n",
      "        restore_shape,\n",
      "        dtype=combined_output.dtype,\n",
      "        device=combined_output.device,\n",
      "        requires_grad=True,\n",
      "    )\n",
      "\n",
      "    # Scatter the combined tokens back to their original positions\n",
      "    unpermuted_tokens = torch.scatter_add(empty_tokens, 0, indices, combined_output)\n",
      "\n",
      "    return unpermuted_tokens\n",
      "\n",
      "\n",
      "def topk_softmax_with_capacity(\n",
      "    logits: torch.Tensor,\n",
      "    topk: int,\n",
      "    capacity_factor: Optional[float] = None,\n",
      "    pad_to_capacity: bool = False,\n",
      "    drop_policy: str = \"probs\",\n",
      "):\n",
      "    \"\"\"Apply capacity and padding to the top-k selection.\n",
      "    Args:\n",
      "        logits (torch.Tensor): Logits tensor.\n",
      "        topk (int): The number of experts to select for each token.\n",
      "        capacity_factor (int): The capacity factor of each expert. Will drop tokens if the number of tokens exceeds the capacity.\n",
      "        pad_to_capacity (bool): Whether to need padding in token drop mode.\n",
      "        drop_policy (str): The policy to drop tokens. Can be either \"prob\" or \"position\". If \"prob\", the tokens with the lowest probabilities will be dropped. If \"position\", tokens at the end of each batch will be dropped.\n",
      "\n",
      "    Returns:\n",
      "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Probs, indices and tokens_per_expert tensor.\n",
      "\n",
      "        (1) If there's no token padding, the shape of probs and indices is [tokens, top_k], indicating the selected experts for each token.\n",
      "        (2) If there's token padding, the shape of probs and indices is [num_expert, capacity], indicating the tokens selected for each expert.\n",
      "    \"\"\"\n",
      "    assert (\n",
      "        logits.dim() == 2\n",
      "    ), f\"Expected 2D logits [num_tokens, num_experts], got {logits.dim()}.\"\n",
      "    num_tokens = logits.shape[0]\n",
      "    num_experts = logits.shape[1]\n",
      "\n",
      "    scores, top_indices = torch.topk(logits, k=topk, dim=1)\n",
      "    probs = torch.softmax(scores, dim=-1, dtype=torch.float32).type_as(logits)\n",
      "\n",
      "    if capacity_factor is None:\n",
      "        # TopK without capacity\n",
      "        tokens_per_expert = custom_histc(\n",
      "            top_indices, bins=num_experts, min=0, max=num_experts\n",
      "        )\n",
      "        return probs, top_indices, tokens_per_expert\n",
      "    else:\n",
      "        # TopK with capacity\n",
      "        expert_capacity = get_capacity(\n",
      "            num_tokens=num_tokens * topk,\n",
      "            num_experts=num_experts,\n",
      "            capacity_factor=capacity_factor,\n",
      "        )\n",
      "        # TopK selection, Maskout unused experts\n",
      "        topk_masked_gates = torch.zeros_like(logits).scatter(1, top_indices, probs)\n",
      "        topk_mask = torch.zeros_like(logits).scatter(1, top_indices, 1)\n",
      "\n",
      "        # Maskout exceeded tokens\n",
      "        if drop_policy == \"probs\":\n",
      "            capacity_probs, capacity_indices = torch.topk(\n",
      "                topk_masked_gates, k=expert_capacity, dim=0, sorted=False\n",
      "            )\n",
      "            capacity_mask = torch.zeros_like(logits).scatter(0, capacity_indices, 1)\n",
      "        elif drop_policy == \"position\":\n",
      "            _, capacity_indices = torch.topk(\n",
      "                topk_mask, k=expert_capacity, dim=0, sorted=False\n",
      "            )\n",
      "            capacity_mask = torch.zeros_like(logits).scatter(0, capacity_indices, 1)\n",
      "            capacity_probs = torch.gather(topk_masked_gates, 0, capacity_indices)\n",
      "        else:\n",
      "            raise ValueError(f\"Invalid drop_policy: {drop_policy}\")\n",
      "\n",
      "        if pad_to_capacity:\n",
      "            final_probs, final_indices = (\n",
      "                capacity_probs.T.contiguous(),\n",
      "                capacity_indices.T.contiguous(),\n",
      "            )\n",
      "            tokens_per_expert_before_capacity = topk_mask.sum(dim=0)\n",
      "        else:\n",
      "            # Get exceed mask and maskout exceeded probs and indices\n",
      "            final_mask = torch.logical_and(topk_mask, capacity_mask)\n",
      "            drop_mask = torch.logical_not(final_mask)\n",
      "            exceed_mask = torch.gather(drop_mask, 1, top_indices)\n",
      "            final_probs = probs * torch.logical_not(exceed_mask)\n",
      "            final_indices = top_indices.clone().masked_fill_(\n",
      "                exceed_mask, torch.iinfo(torch.long).max\n",
      "            )\n",
      "            tokens_per_expert_before_capacity = topk_mask.sum(dim=0)\n",
      "        return final_probs, final_indices, tokens_per_expert_before_capacity\n",
      "\n",
      "\n",
      "# logging related\n",
      "aux_loss_names = [\"load_balancing_loss\", \"z_loss\"]\n",
      "\n",
      "\n",
      "def update_aux_losses_tracker(\n",
      "    name: str, loss: torch.Tensor, layer_number: int, num_layers: int\n",
      "):\n",
      "    \"\"\"Save the auxiliary loss for logging.\n",
      "\n",
      "    Args:\n",
      "        name (str): The name of the loss.\n",
      "        loss (torch.Tensor): The loss tensor.\n",
      "        layer_number (int): Layer index of the loss.\n",
      "        num_layers (int): The number of total layers.\n",
      "    \"\"\"\n",
      "    from realhf.base.stats_tracker import MOE_AUX_LOSSES\n",
      "\n",
      "    assert name in aux_loss_names, f\"Invalid aux loss name: {name}.\"\n",
      "    losses = MOE_AUX_LOSSES.get(name, None)\n",
      "    if losses is None:\n",
      "        losses = torch.zeros(num_layers, device=loss.device)\n",
      "    losses[layer_number] += loss.detach()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/logits_warper.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import functools\n",
      "from typing import List, Optional, Tuple\n",
      "\n",
      "import torch\n",
      "\n",
      "\n",
      "class LogitsWarper:\n",
      "    \"\"\"Abstract base class for all logit processors that can be applied during\n",
      "    generation.\n",
      "\n",
      "    Cloned from huggingface transformers/src/transformers/generation/logits_process.py,\n",
      "    except that we can optionally change the logits inplace.\n",
      "    \"\"\"\n",
      "\n",
      "    def __call__(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor,\n",
      "        logits: torch.FloatTensor,\n",
      "        inplace: bool = False,\n",
      "    ) -> torch.FloatTensor:\n",
      "        raise NotImplementedError(\n",
      "            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TemperatureLogitsWarper(LogitsWarper):\n",
      "    temperature: float\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.temperature > 1.0 or self.temperature < 0.0:\n",
      "            raise ValueError(\"temperature has to be between 0 and 1\")\n",
      "\n",
      "    def __call__(\n",
      "        self,\n",
      "        _,\n",
      "        logits: torch.FloatTensor,\n",
      "        inplace: bool = False,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
      "        if inplace:\n",
      "            logits.div_(self.temperature)\n",
      "        else:\n",
      "            logits = logits / self.temperature\n",
      "        return logits / self.temperature\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TopPLogitsWarper(LogitsWarper):\n",
      "    top_p: float\n",
      "    filter_value: float = -float(\"Inf\")\n",
      "    min_tokens_to_keep: int = 1\n",
      "\n",
      "    def __post_init__(self):\n",
      "        self.top_p = top_p = float(self.top_p)\n",
      "        if top_p < 0 or top_p > 1.0:\n",
      "            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n",
      "        if not isinstance(self.min_tokens_to_keep, int) or (\n",
      "            self.min_tokens_to_keep < 1\n",
      "        ):\n",
      "            raise ValueError(\n",
      "                f\"`min_tokens_to_keep` has to be a positive integer, \"\n",
      "                f\"but is {self.min_tokens_to_keep}\"\n",
      "            )\n",
      "\n",
      "    def __call__(\n",
      "        self,\n",
      "        _,\n",
      "        logits: torch.FloatTensor,\n",
      "        inplace: bool = False,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
      "        sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n",
      "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "\n",
      "        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
      "        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)\n",
      "        # Keep at least min_tokens_to_keep\n",
      "        sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n",
      "\n",
      "        # scatter sorted tensors to original indexing\n",
      "        indices_to_remove = sorted_indices_to_remove.scatter(\n",
      "            -1, sorted_indices, sorted_indices_to_remove\n",
      "        )\n",
      "        self.filter_value = torch.finfo(logits.dtype).min\n",
      "        if inplace:\n",
      "            logits.masked_fill_(indices_to_remove, self.filter_value)\n",
      "        else:\n",
      "            logits = logits.masked_fill(indices_to_remove, self.filter_value)\n",
      "        return logits\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class TopKLogitsWarper(LogitsWarper):\n",
      "    top_k: int\n",
      "    filter_value: float = -float(\"Inf\")\n",
      "    min_tokens_to_keep: int = 1\n",
      "\n",
      "    def __post_init__(self):\n",
      "        top_k = self.top_k\n",
      "        min_tokens_to_keep = self.min_tokens_to_keep\n",
      "        self.top_k = max(top_k, min_tokens_to_keep)\n",
      "        if not isinstance(top_k, int) or top_k <= 0:\n",
      "            raise ValueError(\n",
      "                f\"`top_k` has to be a strictly positive integer, but is {top_k}\"\n",
      "            )\n",
      "\n",
      "    def __call__(\n",
      "        self,\n",
      "        _,\n",
      "        logits: torch.FloatTensor,\n",
      "        inplace: bool = False,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
      "        top_k = min(self.top_k, logits.size(-1))  # Safety check\n",
      "        # Remove all tokens with a probability less than the last token of the top-k\n",
      "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
      "        self.filter_value = torch.finfo(logits.dtype).min\n",
      "        if inplace:\n",
      "            logits.masked_fill_(indices_to_remove, self.filter_value)\n",
      "        else:\n",
      "            logits = logits.masked_fill(indices_to_remove, self.filter_value)\n",
      "        return logits\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class EpsilonLogitsWarper(LogitsWarper):\n",
      "    epsilon: float\n",
      "    filter_value: float = -float(\"Inf\")\n",
      "    min_tokens_to_keep: int = 1\n",
      "\n",
      "    def __post_init__(self):\n",
      "        self.epsilon = epsilon = float(self.epsilon)\n",
      "        epsilon = float(epsilon)\n",
      "        if epsilon <= 0 or epsilon >= 1:\n",
      "            raise ValueError(\n",
      "                f\"`eta_cutoff` has to be a float > 0 and < 1, but is {epsilon}\"\n",
      "            )\n",
      "\n",
      "        self.min_tokens_to_keep = min_tokens_to_keep = int(self.min_tokens_to_keep)\n",
      "        if min_tokens_to_keep < 1:\n",
      "            raise ValueError(\n",
      "                f\"`min_tokens_to_keep` has to be a strictly positive integer, but is {min_tokens_to_keep}\"\n",
      "            )\n",
      "\n",
      "    def __call__(\n",
      "        self,\n",
      "        _,\n",
      "        logits: torch.FloatTensor,\n",
      "        inplace: bool = False,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
      "        # Calculate the adaptive cutoff\n",
      "        probabilities = logits.softmax(dim=-1)\n",
      "        entropy = torch.distributions.Categorical(logits).entropy()\n",
      "        eta = torch.min(self.epsilon, torch.sqrt(self.epsilon) * torch.exp(-entropy))[\n",
      "            ..., None\n",
      "        ]\n",
      "        indices_to_remove = probabilities < eta\n",
      "\n",
      "        # Keep the words with the 'min_tokens_to_keep'-highest probabilities\n",
      "        top_k = min(self.min_tokens_to_keep, logits.size(-1))  # Safety check\n",
      "        indices_to_remove = indices_to_remove & (\n",
      "            logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
      "        )\n",
      "\n",
      "        self.filter_value = torch.finfo(logits.dtype).min\n",
      "        if inplace:\n",
      "            logits.masked_fill_(indices_to_remove, self.filter_value)\n",
      "        else:\n",
      "            logits = logits.masked_fill(indices_to_remove, self.filter_value)\n",
      "        return logits\n",
      "\n",
      "\n",
      "def chained_logits_wraper(xs: List[LogitsWarper], inplace: bool = False):\n",
      "\n",
      "    def foo(\n",
      "        input_ids: torch.LongTensor,\n",
      "        logits: torch.FloatTensor,\n",
      "    ) -> torch.FloatTensor:\n",
      "        for x in xs:\n",
      "            logits = x(input_ids, logits, inplace)\n",
      "        return logits\n",
      "\n",
      "    return foo\n",
      "\n",
      "\n",
      "def unioned_logits_wraper(xs: List[LogitsWarper], inplace: bool = False):\n",
      "\n",
      "    def foo(\n",
      "        input_ids: torch.LongTensor,\n",
      "        logits: torch.FloatTensor,\n",
      "    ) -> torch.FloatTensor:\n",
      "        processed_logits = [x(input_ids, logits, inplace=False) for x in xs]\n",
      "        masks = [logits != pl for pl in processed_logits]\n",
      "        mask = functools.reduce(torch.logical_or, masks)\n",
      "        if inplace:\n",
      "            logits.masked_fill_(mask, torch.finfo(logits.dtype).min)\n",
      "        else:\n",
      "            logits = logits.masked_fill(mask, torch.finfo(logits.dtype).min)\n",
      "        return logits\n",
      "\n",
      "    return foo\n",
      "\n",
      "\n",
      "def top_k_top_p_logits(\n",
      "    logits: torch.Tensor,\n",
      "    top_k=0,\n",
      "    top_p=1.0,\n",
      "    inplace: bool = False,\n",
      "    ordered: bool = False,\n",
      ") -> torch.FloatTensor:\n",
      "    if top_p == 1.0 and top_k >= logits.shape[-1]:\n",
      "        return logits\n",
      "    if top_p == 1.0:\n",
      "        return TopKLogitsWarper(top_k=top_k)(None, logits, inplace=inplace)\n",
      "    if top_k >= logits.shape[-1]:\n",
      "        return TopPLogitsWarper(top_p=top_p)(None, logits, inplace=inplace)\n",
      "    warper_fn = unioned_logits_wraper if not ordered else chained_logits_wraper\n",
      "    p = warper_fn(\n",
      "        [TopKLogitsWarper(top_k=top_k), TopPLogitsWarper(top_p=top_p)],\n",
      "        inplace=inplace,\n",
      "    )\n",
      "    return p(None, logits)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/ppo_functional.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import functools\n",
      "from typing import Dict, Optional, Tuple\n",
      "\n",
      "import torch\n",
      "import torch.distributed\n",
      "\n",
      "from realhf.base import pkg_version\n",
      "\n",
      "\n",
      "class KLController:\n",
      "\n",
      "    def __init__(self, kl_coef):\n",
      "        self.value = kl_coef\n",
      "\n",
      "    def update(self):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "class AdaptiveKLController(KLController):\n",
      "    \"\"\"\n",
      "    Adaptive KL controller described in the paper:\n",
      "    https://arxiv.org/pdf/1909.08593.pdf\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, init_kl_coef, target, horizon):\n",
      "        self.value = init_kl_coef\n",
      "        self.target = target\n",
      "        self.horizon = horizon\n",
      "\n",
      "    def update(self, current, n_steps):\n",
      "        target = self.target\n",
      "        proportional_error = torch.clip(current / target - 1, -0.2, 0.2)\n",
      "        mult = 1 + proportional_error * n_steps / self.horizon\n",
      "        self.value = self.value * mult\n",
      "\n",
      "\n",
      "class FixedKLController(KLController):\n",
      "    \"\"\"Fixed KL controller.\"\"\"\n",
      "\n",
      "    def __init__(self, kl_coef):\n",
      "        self.value = kl_coef\n",
      "\n",
      "    def update(self, current, n_steps):\n",
      "        pass\n",
      "\n",
      "\n",
      "def actor_loss_fn(\n",
      "    logprobs: torch.FloatTensor,\n",
      "    old_logprobs: torch.FloatTensor,\n",
      "    advantages: torch.FloatTensor,\n",
      "    eps_clip: float,\n",
      "    loss_mask: Optional[torch.BoolTensor] = None,\n",
      "    c_clip: Optional[float] = None,\n",
      "    proximal_logprobs: Optional[torch.FloatTensor] = None,\n",
      "    behav_imp_weight_cap: Optional[torch.FloatTensor] = None,\n",
      ") -> Tuple[torch.Tensor, Dict]:\n",
      "    \"\"\"Compute PPO actor loss function.\n",
      "\n",
      "    There is no shape requirements for the inputs, but they must have the same shape.\n",
      "    Either [bs, max_seqlen] for batch padded inputs or [tot_seqlen] for padded inputs.\n",
      "\n",
      "    Args:\n",
      "        logprobs (torch.FloatTensor): Log probabilities of actions.\n",
      "        old_logprobs (torch.FloatTensor): Old log probabilities of actions.\n",
      "        advantages (torch.FloatTensor): GAE (normalized) advantages.\n",
      "        eps_clip (float): Clip ratio of PPO.\n",
      "        c_clip (float | None): The dual clip factor.\n",
      "            Check https://arxiv.org/pdf/1912.09729 for details.\n",
      "        loss_mask (Optional[torch.BoolTensor], optional): Mask for loss computation.\n",
      "            1 if valid else 0. Defaults to None.\n",
      "\n",
      "    Returns:\n",
      "        Tuple[torch.Tensor, Dict]: Scalar loss and statistics.\n",
      "    \"\"\"\n",
      "    assert logprobs.dtype == torch.float32\n",
      "    assert old_logprobs.dtype == torch.float32\n",
      "    assert advantages.dtype == torch.float32\n",
      "\n",
      "    # clone inference tensors\n",
      "    if old_logprobs.is_inference():\n",
      "        old_logprobs = old_logprobs.clone()\n",
      "    if advantages.is_inference():\n",
      "        advantages = advantages.clone()\n",
      "    if proximal_logprobs is not None:\n",
      "        assert proximal_logprobs.dtype == torch.float32\n",
      "        if proximal_logprobs.is_inference():\n",
      "            proximal_logprobs = proximal_logprobs.clone()\n",
      "        denorm_logprobs = proximal_logprobs\n",
      "    else:\n",
      "        denorm_logprobs = old_logprobs\n",
      "\n",
      "    # create mask\n",
      "    if loss_mask is None:\n",
      "        loss_mask = torch.ones_like(logprobs, dtype=torch.bool)\n",
      "    loss_mask: torch.BoolTensor\n",
      "\n",
      "    loss_mask_count = loss_mask.count_nonzero() or 1\n",
      "    # For numerical stability.\n",
      "    ratio = torch.where(loss_mask, torch.exp(logprobs - denorm_logprobs), 0)\n",
      "\n",
      "    clipped_ratio = torch.clamp(ratio, 1.0 - eps_clip, 1.0 + eps_clip)\n",
      "    pg_loss1 = -advantages * ratio\n",
      "    pg_loss2 = -advantages * clipped_ratio\n",
      "    clip_mask = pg_loss1.detach() < pg_loss2.detach()\n",
      "\n",
      "    pg_loss = torch.max(pg_loss1, pg_loss2)\n",
      "    if c_clip is not None:\n",
      "        assert c_clip > 1.0, c_clip\n",
      "        pg_loss3 = torch.sign(advantages) * c_clip * advantages\n",
      "        dual_clip_mask = pg_loss3.detach() < pg_loss.detach()\n",
      "        pg_loss = torch.min(pg_loss, pg_loss3)\n",
      "    else:\n",
      "        dual_clip_mask = torch.zeros_like(clip_mask)\n",
      "    if proximal_logprobs is not None:\n",
      "        behav_kl = proximal_logprobs - old_logprobs\n",
      "        behav_imp_weight = behav_kl.exp()\n",
      "        if behav_imp_weight_cap is not None:\n",
      "            behav_mask = (behav_imp_weight <= behav_imp_weight_cap).logical_and(\n",
      "                loss_mask\n",
      "            )\n",
      "        else:\n",
      "            behav_mask = loss_mask\n",
      "        behav_kl = torch.where(behav_mask, behav_kl, 0.0)\n",
      "        behav_imp_weight = torch.where(behav_mask, behav_imp_weight, 0.0)\n",
      "        pg_loss = pg_loss * behav_imp_weight\n",
      "\n",
      "    logging_loss = pg_loss.detach()\n",
      "    pg_loss = torch.where(loss_mask, pg_loss, 0).sum() / loss_mask_count\n",
      "\n",
      "    clip_mask.logical_and_(loss_mask)\n",
      "    dual_clip_mask.logical_and_(loss_mask)\n",
      "    # Remain torch.CudaTensor here for all-reduce after train step.\n",
      "    stat = dict(\n",
      "        loss=logging_loss,\n",
      "        importance_weight=ratio.detach(),\n",
      "        approx_kl=(logprobs - denorm_logprobs).detach(),\n",
      "        clip_mask=clip_mask,\n",
      "        dual_clip_mask=dual_clip_mask,\n",
      "    )\n",
      "    if proximal_logprobs is not None:\n",
      "        stat[\"behave_imp_weight\"] = behav_imp_weight\n",
      "        stat[\"behave_approx_kl\"] = behav_kl\n",
      "        stat[\"behave_mask\"] = behav_mask\n",
      "\n",
      "    return pg_loss, stat\n",
      "\n",
      "\n",
      "def _huber_loss(x: torch.Tensor, y: torch.Tensor, delta: float):\n",
      "    diff = torch.abs(x - y)\n",
      "    return torch.where(diff < delta, 0.5 * diff**2, delta * (diff - 0.5 * delta))\n",
      "\n",
      "\n",
      "def _mse_loss(x: torch.Tensor, y: torch.Tensor):\n",
      "    return 0.5 * (x - y) ** 2\n",
      "\n",
      "\n",
      "def critic_loss_fn(\n",
      "    value: torch.FloatTensor,\n",
      "    old_value: torch.FloatTensor,\n",
      "    target_value: torch.FloatTensor,\n",
      "    value_eps_clip: float,\n",
      "    loss_mask: Optional[torch.FloatTensor] = None,\n",
      "    loss_fn_type: str = \"mse\",\n",
      ") -> Tuple[torch.Tensor, Dict]:\n",
      "    \"\"\"Compute PPO critic loss function given padded batch inputs.\n",
      "\n",
      "    There is no shape requirements for the inputs, but they must have the same shape.\n",
      "    Either [bs, max_seqlen] for batch padded inputs or [tot_seqlen] for padded inputs.\n",
      "\n",
      "    Args:\n",
      "        value (torch.FloatTensor): Values. The position of the final token is not included.\n",
      "            (The whole generated sequence is not a state.)\n",
      "        old_value (torch.FloatTensor): Old values.\n",
      "        target_value (torch.FloatTensor): Returns computed by GAE.\n",
      "        value_eps_clip (float): Clip ratio.\n",
      "        loss_mask (Optional[torch.FloatTensor], optional): Mask for loss computation.\n",
      "            1 if valid else 0. Defaults to None.\n",
      "        loss_fn_type (str, optional): Type of loss function. Defaults to 'huber'.\n",
      "\n",
      "    Returns:\n",
      "        Tuple[torch.Tensor, Dict]: Scalar loss and statistics.\n",
      "    \"\"\"\n",
      "    assert value.dtype == torch.float32\n",
      "    assert old_value.dtype == torch.float32\n",
      "    assert target_value.dtype == torch.float32\n",
      "\n",
      "    if loss_fn_type == \"huber\":\n",
      "        loss_fn = functools.partial(_huber_loss, delta=10.0)\n",
      "    elif loss_fn_type == \"mse\":\n",
      "        loss_fn = _mse_loss\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Unknown loss fn type: {loss_fn_type}\")\n",
      "\n",
      "    if target_value.is_inference():\n",
      "        target_value = target_value.clone()  # clone a inference tensor\n",
      "\n",
      "    value_loss_original = loss_fn(value, target_value)\n",
      "\n",
      "    value_clipped = old_value + (value - old_value).clamp(\n",
      "        -value_eps_clip, value_eps_clip\n",
      "    )\n",
      "\n",
      "    value_loss_clipped = loss_fn(value_clipped, target_value)\n",
      "\n",
      "    value_loss = torch.max(value_loss_original, value_loss_clipped)\n",
      "\n",
      "    with torch.no_grad():\n",
      "        clip_mask = value_loss_clipped.detach() > value_loss_original.detach()\n",
      "        if loss_mask is not None:\n",
      "            clip_mask.logical_and_(loss_mask)\n",
      "\n",
      "        stat = dict(clip_mask=clip_mask, loss=value_loss.detach())\n",
      "\n",
      "    if loss_mask is not None:\n",
      "        value_loss = (\n",
      "            torch.where(loss_mask, value_loss, 0).sum() / loss_mask.count_nonzero()\n",
      "        )\n",
      "    else:\n",
      "        value_loss = value_loss.mean()\n",
      "\n",
      "    return value_loss, stat\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def get_packed_rewards(\n",
      "    kl_ctl: float,\n",
      "    clip_reward_value: float,\n",
      "    log_probs: torch.FloatTensor,\n",
      "    ref_log_probs: torch.FloatTensor,\n",
      "    reward_score: torch.FloatTensor,\n",
      "    short1cu_seqlens: torch.IntTensor,\n",
      "    seq_no_eos_mask: torch.BoolTensor,\n",
      "    mask_no_eos_with_zero: bool = False,\n",
      ") -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
      "    # Here log_probs/ref_log_probs is one-step shorter than packed_input_ids (the last step is removed),\n",
      "    # so the log_probs at the EOS token is not included in this tensor.\n",
      "    # We directly add reward scores of each sequence onto the final token of each sequence.\n",
      "    tot_rewards = -kl_ctl * (log_probs - ref_log_probs)\n",
      "    kl_rewards = tot_rewards.clone()\n",
      "    reward_score = reward_score.clip(-clip_reward_value, clip_reward_value)\n",
      "\n",
      "    if mask_no_eos_with_zero:\n",
      "        tot_rewards[short1cu_seqlens[1:] - 1] += torch.where(\n",
      "            seq_no_eos_mask, 0, reward_score\n",
      "        )\n",
      "    else:\n",
      "        tot_rewards[short1cu_seqlens[1:] - 1] += reward_score\n",
      "\n",
      "    return kl_rewards, tot_rewards\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def get_packed_reward_dense(\n",
      "    kl_ctl: float,\n",
      "    clip_reward_value: float,\n",
      "    log_probs: torch.FloatTensor,\n",
      "    ref_log_probs: torch.FloatTensor,\n",
      "    dense_reward_score: torch.FloatTensor,\n",
      "    short1cu_seqlens: torch.IntTensor,\n",
      "    seq_no_eos_mask: torch.FloatTensor,\n",
      "    reward_delta: bool,\n",
      "):\n",
      "    tot_rewards = -kl_ctl * (log_probs - ref_log_probs)\n",
      "    kl_rewards = tot_rewards.clone()\n",
      "    dense_reward_score = dense_reward_score.clip(-clip_reward_value, clip_reward_value)\n",
      "    offset = 0\n",
      "    offset2 = 0\n",
      "    for seq_len in short1cu_seqlens[1:] - short1cu_seqlens[:-1]:\n",
      "        try:\n",
      "            if reward_delta == True:\n",
      "                tot_rewards[offset : offset + seq_len] += (\n",
      "                    dense_reward_score[offset2 + 1 : offset2 + seq_len + 1]\n",
      "                    - dense_reward_score[offset2 : offset2 + seq_len]\n",
      "                )\n",
      "            else:\n",
      "                tot_rewards[offset : offset + seq_len] += dense_reward_score[\n",
      "                    offset2 + 1 : offset2 + seq_len + 1\n",
      "                ]\n",
      "        except:\n",
      "            raise RuntimeError(\n",
      "                f\"seq_len: {seq_len}, offset: {offset}, offset2: {offset2}, {tot_rewards.shape}, {dense_reward_score.shape}\"\n",
      "            )\n",
      "        offset += seq_len\n",
      "        offset2 += seq_len + 1\n",
      "    return kl_rewards, tot_rewards\n",
      "\n",
      "\n",
      "def pygae1d_nolp_misalign(\n",
      "    rewards: torch.FloatTensor,\n",
      "    values: torch.FloatTensor,\n",
      "    cu_seqlens_: torch.IntTensor,\n",
      "    bootstrap: torch.FloatTensor,\n",
      "    gamma: float,\n",
      "    lam: float,\n",
      ") -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
      "    cu_seqlens = cu_seqlens_.clone()\n",
      "    cu_seqlens[1:] += torch.ones_like(cu_seqlens_[1:]).cumsum(0)\n",
      "\n",
      "    bs = cu_seqlens_.shape[0] - 1\n",
      "    assert values.shape[0] == rewards.shape[0] + bs\n",
      "    advantages_reversed = []\n",
      "    returns_reversed = []\n",
      "    for i in reversed(range(bs)):\n",
      "        v_offset = cu_seqlens[i]\n",
      "        r_offset, r_end = cu_seqlens_[i], cu_seqlens_[i + 1]\n",
      "        assert cu_seqlens[i + 1] - v_offset - 1 == r_end - r_offset\n",
      "        lastgaelam = 0\n",
      "        for t in reversed(range(r_end - r_offset)):\n",
      "            nextvalues = values[v_offset + t + 1]\n",
      "            if t == r_end - r_offset - 1:\n",
      "                nextvalues *= bootstrap[i]\n",
      "            delta = rewards[r_offset + t] + gamma * nextvalues - values[v_offset + t]\n",
      "            lastgaelam = delta + gamma * lam * lastgaelam\n",
      "            advantages_reversed.append(lastgaelam)\n",
      "            returns_reversed.append(lastgaelam + values[v_offset + t])\n",
      "\n",
      "    advantages = torch.stack(advantages_reversed[::-1])\n",
      "    returns = torch.stack(returns_reversed[::-1])\n",
      "    return advantages, returns\n",
      "\n",
      "\n",
      "def cugae1d_nolp_misalign_func(\n",
      "    rewards: torch.FloatTensor,\n",
      "    values: torch.FloatTensor,\n",
      "    cu_seqlens: torch.IntTensor,\n",
      "    truncate: torch.BoolTensor,\n",
      "    gamma: float,\n",
      "    lam: float,\n",
      ") -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
      "    \"\"\"Compute GAE over a batch of packed sequences with different lengths.\n",
      "\n",
      "    This function assumes that rewards and values are packed into an 1D tensor.\n",
      "    Values are longer than rewards by the number of sequences in rewards because of bootstrapping.\n",
      "    cu_seqlens marks the bounary of sequences in rewards.\n",
      "\n",
      "    The final step of each sequence is *NOT* overlapped with the first step of the next sequence,\n",
      "    and rewards/values do not have the same length, so this function is suffixed with\n",
      "    \"nolp\" (non-overlap) and \"misalign\".\n",
      "\n",
      "    Args:\n",
      "        rewards (torch.FloatTensor): Shape [total_seqlen], rewards across sequences.\n",
      "        values (torch.FloatTensor): Shape [total_seqlen + batch_size], values across sequences.\n",
      "            Values are bootstrapped, so it's longer than rewards.\n",
      "        cu_seqlens (torch.IntTensor): Marker of sequence boundaries in rewards,\n",
      "            e.g., [0, s1, s1+s2, ..., total_seqlen]. It should starts with 0 and ends with total_seqlen.\n",
      "        truncate (torch.BoolTensor): Whether each sequence is truncated because of exceeding max length.\n",
      "            If truncate, the next value of the last step will be bootstraped, otherwise 0.\n",
      "        gamma (float): Discount factor.\n",
      "        lam (float): GAE discount factor.\n",
      "\n",
      "    Returns:\n",
      "        Tuple[torch.FloatTensor, torch.FloatTensor]: Advantages and returns (value targets).\n",
      "            Both have the same shape as rewards.\n",
      "    \"\"\"\n",
      "    if pkg_version.is_available(\"cugae\"):\n",
      "        from cugae import cugae1d_nolp_misalign_func as gae_1d_nolp_misalign\n",
      "    else:\n",
      "        from realhf._C.cugae import gae_1d_nolp_misalign\n",
      "\n",
      "    assert len(rewards.shape) == len(values.shape) == len(cu_seqlens.shape) == 1\n",
      "    assert cu_seqlens[0] == 0 and cu_seqlens[-1] == rewards.shape[0]\n",
      "    return gae_1d_nolp_misalign(rewards, values, cu_seqlens, truncate, gamma, lam)\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def get_packed_advantages_and_returns(\n",
      "    gamma: float,\n",
      "    lam: float,\n",
      "    values: torch.FloatTensor,\n",
      "    rewards: torch.FloatTensor,\n",
      "    short1cu_seqlens: torch.IntTensor,\n",
      "    seq_no_eos_mask: torch.FloatTensor,\n",
      ") -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
      "    if rewards.get_device() == -1:\n",
      "        return pygae1d_nolp_misalign(\n",
      "            rewards, values, short1cu_seqlens, seq_no_eos_mask, gamma, lam\n",
      "        )\n",
      "    try:\n",
      "        return cugae1d_nolp_misalign_func(\n",
      "            rewards,\n",
      "            values,\n",
      "            short1cu_seqlens.int(),\n",
      "            seq_no_eos_mask.bool(),\n",
      "            gamma,\n",
      "            lam,\n",
      "        )\n",
      "    except ModuleNotFoundError:\n",
      "        return pygae1d_nolp_misalign(\n",
      "            rewards, values, short1cu_seqlens, seq_no_eos_mask, gamma, lam\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/cuda_graph.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import gc\n",
      "import time\n",
      "from collections import defaultdict\n",
      "from contextlib import contextmanager, nullcontext\n",
      "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "\n",
      "logger = logging.getLogger(\"CUDAGraph\")\n",
      "\n",
      "CUDA_GRAPH_STORAGE: Dict[str, torch.cuda.CUDAGraph] = dict()\n",
      "CUDA_GRAPH_INPUT_BUFFER: Dict[str, Dict[str, torch.Tensor]] = dict()\n",
      "CUDA_GRAPH_OUTPUT_BUFFER: Dict[str, Dict[str, torch.Tensor]] = dict()\n",
      "CUDA_GRAPH_FIRST_CAPTURE: Dict[str, bool] = defaultdict(lambda: True)\n",
      "CUDA_GRAPH_DESTROYED: Dict[str, bool] = defaultdict(lambda: False)\n",
      "\n",
      "\n",
      "@contextmanager\n",
      "def capture_context(graph: torch.cuda.CUDAGraph):\n",
      "    # NOTE: We use lower level API in pytorch CUDAGraph instead of `torch.cuda.graph`\n",
      "    # context. This is because in `torch.cuda.graph`, `torch.cuda.empty_cache()` and\n",
      "    # `gc.collect()` are unnecessarily called everytime the context is entered.\n",
      "    # This will introduce large overhead.\n",
      "    graph.capture_begin()\n",
      "    yield\n",
      "    graph.capture_end()\n",
      "\n",
      "\n",
      "@contextmanager\n",
      "def outer_capture_context(stream=None, no_grad=False):\n",
      "    \"\"\"Context wrapped outside warmup and CUDAGraph capture context:\n",
      "    1. Alter stream from default.\n",
      "    2. Apply torch.no_grad context if required\n",
      "    \"\"\"\n",
      "    if stream is None:\n",
      "        stream = torch.cuda.Stream()\n",
      "    maybe_no_grad = nullcontext() if not no_grad else torch.no_grad()\n",
      "    with torch.cuda.stream(stream), maybe_no_grad:\n",
      "        yield\n",
      "\n",
      "\n",
      "def reinitialize_input_buffer(cuda_graph_name, new_buf):\n",
      "    global CUDA_GRAPH_INPUT_BUFFER\n",
      "    assert (\n",
      "        cuda_graph_name in CUDA_GRAPH_INPUT_BUFFER\n",
      "    ), f\"CUDAGraph {cuda_graph_name} does not exist.\"\n",
      "\n",
      "    buf = CUDA_GRAPH_INPUT_BUFFER[cuda_graph_name]\n",
      "    for k, v in buf.items():\n",
      "        if torch.is_tensor(v):\n",
      "            v.copy_(new_buf[k])\n",
      "        elif isinstance(v, list):\n",
      "            for i, vv in enumerate(v):\n",
      "                if torch.is_tensor(vv):\n",
      "                    vv.copy_(new_buf[k][i])\n",
      "        else:\n",
      "            buf[k] = new_buf[k]\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def capture_func(\n",
      "    name: str,\n",
      "    func: Callable,\n",
      "    input_buffer: Dict[str, Any],\n",
      "    force_recapture: bool = False,\n",
      "    no_grad: bool = False,\n",
      ") -> Tuple[Any, Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n",
      "    \"\"\"Capture a function with cuda graph, store the graph and input/output\n",
      "    buffers by name. The input/output metadata should match the inputs and\n",
      "    outputs of function.\n",
      "\n",
      "    This function uses pytorch original CUDAGraph implementation\n",
      "\n",
      "    :param name: The identifier of the CUDAGraph to be captured/reused.\n",
      "    :type name: str\n",
      "    :param func: The function to be captured.\n",
      "    :type func: Callable\n",
      "    :param input_buffer: The input buffer of the function.\n",
      "    :type input_buffer: Dict[str, Any]\n",
      "    :param force_recapture: Whether to force recapture the function.\n",
      "    :type force_recapture: bool\n",
      "    :param no_grad: Whether to run the function in no_grad context.\n",
      "    :type no_grad: bool\n",
      "    \"\"\"\n",
      "    global CUDA_GRAPH_STORAGE\n",
      "    global CUDA_GRAPH_INPUT_BUFFER\n",
      "    global CUDA_GRAPH_OUTPUT_BUFFER\n",
      "    global CUDA_GRAPH_FIRST_CAPTURE\n",
      "    global CUDA_GRAPH_DESTROYED\n",
      "\n",
      "    if not force_recapture and not CUDA_GRAPH_FIRST_CAPTURE[name]:\n",
      "        assert name in CUDA_GRAPH_STORAGE\n",
      "        assert name in CUDA_GRAPH_INPUT_BUFFER\n",
      "        assert name in CUDA_GRAPH_OUTPUT_BUFFER\n",
      "        reinitialize_input_buffer(name, input_buffer)\n",
      "        return (\n",
      "            CUDA_GRAPH_STORAGE[name],\n",
      "            CUDA_GRAPH_INPUT_BUFFER[name],\n",
      "            CUDA_GRAPH_OUTPUT_BUFFER[name],\n",
      "        )\n",
      "\n",
      "    stream = torch.cuda.Stream()\n",
      "    st = time.monotonic()\n",
      "    logger.debug(f\"Rank {dist.get_rank()}: Capturing CUDA graph for {name}\")\n",
      "    first_capture = CUDA_GRAPH_FIRST_CAPTURE[name]\n",
      "\n",
      "    with outer_capture_context(stream, no_grad):\n",
      "        if first_capture:\n",
      "            func(**input_buffer)  # warmup\n",
      "            logger.debug(\n",
      "                f\"before clear cache before capture \"\n",
      "                f\"mem allocated: {torch.cuda.memory_allocated()/1024/1024:.4f} MB\"\n",
      "                f\"mem reserved: {torch.cuda.memory_reserved()/1024/1024:.4f} MB\"\n",
      "            )\n",
      "            torch.cuda.synchronize()\n",
      "            gc.collect()\n",
      "            torch.cuda.empty_cache()\n",
      "            logger.debug(\n",
      "                f\"after clear cache after capture \"\n",
      "                f\"mem allocated: {torch.cuda.memory_allocated()/1024/1024:.4f} MB\"\n",
      "                f\"mem reserved: {torch.cuda.memory_reserved()/1024/1024:.4f} MB\"\n",
      "            )\n",
      "\n",
      "        graph = torch.cuda.CUDAGraph()\n",
      "\n",
      "        with capture_context(graph):\n",
      "            output = func(**input_buffer)\n",
      "\n",
      "    logger.debug(\n",
      "        f\"Rank {dist.get_rank()}: Capturing CUDA graph {name} \"\n",
      "        f\"takes {time.monotonic() - st:.4f} seconds\"\n",
      "    )\n",
      "\n",
      "    assert torch.is_tensor(output)\n",
      "    output_buffer = dict(output=output)\n",
      "\n",
      "    CUDA_GRAPH_STORAGE[name] = graph\n",
      "    CUDA_GRAPH_INPUT_BUFFER[name] = input_buffer\n",
      "    CUDA_GRAPH_OUTPUT_BUFFER[name] = output_buffer\n",
      "    CUDA_GRAPH_FIRST_CAPTURE[name] = False\n",
      "    CUDA_GRAPH_DESTROYED[name] = False\n",
      "    return graph, input_buffer, output_buffer\n",
      "\n",
      "\n",
      "def input_buffer_handle(graph_name: str, tensor_name: str):\n",
      "    if graph_name not in CUDA_GRAPH_INPUT_BUFFER:\n",
      "        return None\n",
      "    if CUDA_GRAPH_DESTROYED[graph_name]:\n",
      "        return None\n",
      "    if tensor_name not in CUDA_GRAPH_INPUT_BUFFER[graph_name]:\n",
      "        raise ValueError(\n",
      "            f\"Tensor {tensor_name} not found in input buffer of graph {graph_name}, \"\n",
      "            f\"Existing keys = {CUDA_GRAPH_INPUT_BUFFER[graph_name].keys()}\"\n",
      "        )\n",
      "    return CUDA_GRAPH_INPUT_BUFFER[graph_name][tensor_name]\n",
      "\n",
      "\n",
      "def output_buffer_handle(graph_name: str, tensor_name: str):\n",
      "    if graph_name not in CUDA_GRAPH_OUTPUT_BUFFER:\n",
      "        return None\n",
      "    if CUDA_GRAPH_DESTROYED[graph_name]:\n",
      "        return None\n",
      "    if tensor_name not in CUDA_GRAPH_OUTPUT_BUFFER[graph_name]:\n",
      "        raise ValueError(\n",
      "            f\"Tensor {tensor_name} not found in output buffer of graph {graph_name}, \"\n",
      "            f\"existing keys = {CUDA_GRAPH_OUTPUT_BUFFER[graph_name].keys()}\"\n",
      "        )\n",
      "    return CUDA_GRAPH_OUTPUT_BUFFER[graph_name][tensor_name]\n",
      "\n",
      "\n",
      "def get_graph(name: str):\n",
      "    return CUDA_GRAPH_STORAGE.get(name, None)\n",
      "\n",
      "\n",
      "def destroy(name):\n",
      "    # NOTE: This function should only be used when the graph is not needed\n",
      "    # or the graph is to be recaptured. It will free memory occupied by pinned input/output buffers\n",
      "    # and destroy CUDAGraph object.\n",
      "\n",
      "    global CUDA_GRAPH_STORAGE\n",
      "    global CUDA_GRAPH_INPUT_BUFFER\n",
      "    global CUDA_GRAPH_OUTPUT_BUFFER\n",
      "    global CUDA_GRAPH_FIRST_CAPTURE\n",
      "    global CUDA_GRAPH_DESTROYED\n",
      "\n",
      "    assert (\n",
      "        name in CUDA_GRAPH_STORAGE and not CUDA_GRAPH_FIRST_CAPTURE[name]\n",
      "    ), f\"CUDAGraph {name} should be created before destroy.\"\n",
      "    assert CUDA_GRAPH_DESTROYED[name] is False, f\"CUDAGraph {name} already destroyed.\"\n",
      "\n",
      "    CUDA_GRAPH_STORAGE[name].reset()\n",
      "    CUDA_GRAPH_STORAGE[name] = None\n",
      "    CUDA_GRAPH_INPUT_BUFFER[name] = None\n",
      "    CUDA_GRAPH_OUTPUT_BUFFER[name] = None\n",
      "    CUDA_GRAPH_DESTROYED[name] = True\n",
      "\n",
      "\n",
      "def destroy_all():\n",
      "    global CUDA_GRAPH_STORAGE\n",
      "    global CUDA_GRAPH_INPUT_BUFFER\n",
      "    global CUDA_GRAPH_OUTPUT_BUFFER\n",
      "    global CUDA_GRAPH_FIRST_CAPTURE\n",
      "    global CUDA_GRAPH_DESTROYED\n",
      "\n",
      "    for name in CUDA_GRAPH_STORAGE:\n",
      "        if CUDA_GRAPH_DESTROYED[name]:\n",
      "            continue\n",
      "        CUDA_GRAPH_STORAGE[name].reset()\n",
      "        CUDA_GRAPH_STORAGE[name] = None\n",
      "        CUDA_GRAPH_INPUT_BUFFER[name] = None\n",
      "        CUDA_GRAPH_OUTPUT_BUFFER[name] = None\n",
      "        CUDA_GRAPH_DESTROYED[name] = True\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/utils/functional.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import Optional, Tuple\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import transformers\n",
      "\n",
      "from realhf.base import constants, logging\n",
      "from realhf.impl.model.utils.padding import pad_input, unpad_input\n",
      "\n",
      "logger = logging.getLogger(\"Modeling Functional Utils\")\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def upcast_masked_softmax(\n",
      "    x: torch.Tensor,\n",
      "    mask: torch.Tensor,\n",
      "    mask_value: torch.Tensor,\n",
      "    scale: float,\n",
      "    softmax_dtype: torch.dtype,\n",
      "):\n",
      "    input_dtype = x.dtype\n",
      "    x = x.to(softmax_dtype) * scale\n",
      "    x = torch.where(mask, x, mask_value)\n",
      "    x = torch.nn.functional.softmax(x, dim=-1).to(input_dtype)\n",
      "    return x\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def upcast_softmax(x: torch.Tensor, scale: float, softmax_dtype: torch.dtype):\n",
      "    input_dtype = x.dtype\n",
      "    x = x.to(softmax_dtype) * scale\n",
      "    x = torch.nn.functional.softmax(x, dim=-1).to(input_dtype)\n",
      "    return x\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def masked_softmax(x: torch.Tensor, mask: torch.Tensor, mask_value: torch.Tensor):\n",
      "    x = torch.where(mask, x, mask_value)\n",
      "    x = torch.nn.functional.softmax(x, dim=-1)\n",
      "    return x\n",
      "\n",
      "\n",
      "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
      "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
      "    total_seqlen, n_kv_heads, head_dim = x.shape\n",
      "    if n_rep == 1:\n",
      "        return x\n",
      "    return (\n",
      "        x[:, :, None, :]\n",
      "        .expand(total_seqlen, n_kv_heads, n_rep, head_dim)\n",
      "        .reshape(total_seqlen, n_kv_heads * n_rep, head_dim)\n",
      "    )\n",
      "\n",
      "\n",
      "def mask_eos_token(\n",
      "    logits: torch.Tensor,\n",
      "    eos_token_id: Optional[int] = None,\n",
      ") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
      "    # for min_new_tokens\n",
      "    if eos_token_id is not None:\n",
      "        logits[..., eos_token_id] = torch.finfo(logits.dtype).min\n",
      "    return logits\n",
      "\n",
      "\n",
      "def gather_shifted_log_probs(\n",
      "    logits: torch.FloatTensor, labels: torch.LongTensor\n",
      ") -> torch.FloatTensor:\n",
      "    \"\"\"Gather log probs of shifted labels from logits.\n",
      "\n",
      "    Args:\n",
      "        logits (torch.FloatTensor): Non-shifted logits with shape [bs, seqlen].\n",
      "            The final value at [:, seqlen -1] is not used.\n",
      "        labels (torch.LongTensor): Non-shifted labels/input_ids with shape [bs, seqlen].\n",
      "            The first value at [:, 0] has no corresponding log prob.\n",
      "\n",
      "    Returns:\n",
      "        torch.FloatTensor: Shifted log probability with shape [bs, seqlen -1].\n",
      "    \"\"\"\n",
      "    logits = logits[:, :-1]\n",
      "    labels = labels[:, 1:]\n",
      "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
      "    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(-1))\n",
      "    return log_probs_labels.squeeze(-1)\n",
      "\n",
      "\n",
      "def build_shift_one_indices(\n",
      "    x: torch.HalfTensor, cu_seqlens: torch.IntTensor\n",
      ") -> torch.IntTensor:\n",
      "    \"\"\"Build indices for shifting labels/input_ids one step to the left.\n",
      "\n",
      "    Equivalent to:\n",
      "    ```\n",
      "    shift_one_indices = torch.cat([\n",
      "        torch.arange(cu_seqlens[i] + 1, cu_seqlens[i + 1], dtype=torch.long, device=cu_seqlens.device)\n",
      "        for i in range(cu_seqlens.shape[0] - 1)\n",
      "    ])\n",
      "    ```\n",
      "    but the above implementaion will implicitly convert a tensor (cu_seqlens[i]) to an integer,\n",
      "    which will cause a cuda device sync and slow down performance.\n",
      "\n",
      "    Args:\n",
      "        x (torch.HalfTensor): Shape [total_seqlen]. This tensor is required to get\n",
      "            total_seqlen from its shape. Computing total_seqlen from cu_seqlens will implicitly cause\n",
      "            a cuda device sync.\n",
      "        cu_seqlens (torch.IntTensor): Shape [bs + 1]. Indices marking the start\n",
      "            and end of each sequences.\n",
      "\n",
      "    Returns:\n",
      "        torch.IntTensor: Shape [tot_seqlen - bs]. Indices for shifting labels/input_ids\n",
      "            one step to the left.\n",
      "    \"\"\"\n",
      "    total_seqlen = x.shape[0]\n",
      "    bs = cu_seqlens.shape[0] - 1\n",
      "    short1lens = cu_seqlens[1:] - cu_seqlens[:-1] - 1\n",
      "    short1cu_seqlens = torch.nn.functional.pad(short1lens.cumsum(0), (1, 0), value=0)\n",
      "    indexing_t = torch.arange(\n",
      "        total_seqlen - bs, dtype=torch.long, device=cu_seqlens.device\n",
      "    )\n",
      "    return indexing_t + (\n",
      "        indexing_t.unsqueeze(0) >= short1cu_seqlens[:-1].unsqueeze(1)\n",
      "    ).sum(0)\n",
      "\n",
      "\n",
      "def build_leave_one_indices(\n",
      "    x: torch.HalfTensor, cu_seqlens: torch.IntTensor\n",
      ") -> torch.IntTensor:\n",
      "    \"\"\"Build indices for leaving one token out at the end of each sequence.\n",
      "\n",
      "    Equivalent to:\n",
      "    ```\n",
      "    leave_one_indices = torch.cat([\n",
      "        torch.arange(cu_seqlens[i], cu_seqlens[i + 1] - 1, dtype=torch.long, device=cu_seqlens.device)\n",
      "        for i in range(cu_seqlens.shape[0] - 1)\n",
      "    ])\n",
      "    ```\n",
      "    but the above implementaion will implicitly convert a tensor (cu_seqlens[i]) to an integer,\n",
      "    which will cause a cuda device sync and slow down performance.\n",
      "\n",
      "    Args:\n",
      "        x (torch.HalfTensor): Shape [total_seqlen]. This tensor is required to get\n",
      "            total_seqlen from its shape. Computing total_seqlen from cu_seqlens will implicitly cause\n",
      "            a cuda device sync.\n",
      "        cu_seqlens (torch.IntTensor): Shape [bs + 1]. Indices marking the start\n",
      "            and end of each sequences.\n",
      "\n",
      "    Returns:\n",
      "        torch.IntTensor: Shape [tot_seqlen - bs]. Indices for shifting labels/input_ids\n",
      "            one step to the left.\n",
      "    \"\"\"\n",
      "    total_seqlen = x.shape[0]\n",
      "    bs = cu_seqlens.shape[0] - 1\n",
      "    short1lens = cu_seqlens[1:] - cu_seqlens[:-1] - 1\n",
      "    short1cu_seqlens = torch.nn.functional.pad(short1lens.cumsum(0), (1, 0), value=0)\n",
      "    indexing_t = torch.arange(\n",
      "        total_seqlen - bs, dtype=torch.long, device=cu_seqlens.device\n",
      "    )\n",
      "    return (\n",
      "        indexing_t\n",
      "        + (indexing_t.unsqueeze(0) >= short1cu_seqlens[:-1].unsqueeze(1)).sum(0)\n",
      "        - 1\n",
      "    )\n",
      "\n",
      "\n",
      "def _gather_logprobs(\n",
      "    logits: torch.Tensor,\n",
      "    labels: torch.Tensor,\n",
      "):\n",
      "    \"\"\"Gather log probs from logits and labels.\n",
      "\n",
      "    Args:\n",
      "        logits (torch.FloatTensor): Shape [tot_seqlen]. The final value at the end of\n",
      "            each sequence is not used.\n",
      "        labels (torch.LongTensor): Labels or input_ids with shape [tot_seqlen].\n",
      "            The first value at the beginning of each sequence has no corresponding log prob.\n",
      "\n",
      "    Returns:\n",
      "        torch.FloatTensor: Log probability with shape [tot_seqlen - #seqs].\n",
      "    \"\"\"\n",
      "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
      "    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
      "    return log_probs_labels\n",
      "\n",
      "\n",
      "_gather_logprobs_compiled = None\n",
      "\n",
      "\n",
      "def gather_logprobs(\n",
      "    logits: torch.Tensor,\n",
      "    labels: torch.Tensor,\n",
      "):\n",
      "    from realhf.base import cluster\n",
      "\n",
      "    if cluster.spec.name == \"wa180\":\n",
      "        # torch.compile doesn't work on PPU\n",
      "        return _gather_logprobs(logits, labels)\n",
      "    global _gather_logprobs_compiled\n",
      "    if _gather_logprobs_compiled is None:\n",
      "        _gather_logprobs_compiled = torch.compile(_gather_logprobs)\n",
      "    return _gather_logprobs_compiled(logits, labels)\n",
      "\n",
      "\n",
      "def gather_packed_shifted_log_probs(\n",
      "    logits: torch.FloatTensor,\n",
      "    cu_seqlens: torch.Tensor,\n",
      "    labels: torch.LongTensor,\n",
      ") -> torch.FloatTensor:\n",
      "    \"\"\"Gather log probs from packed input_ids and logits.\n",
      "\n",
      "    Args:\n",
      "        logits (torch.FloatTensor): Shape [tot_seqlen]. The final value at the end of\n",
      "            each sequence is not used.\n",
      "        cu_seqlens (torch.Tensor): Shape [#seqs + 1]. Indices marking the start\n",
      "            and end of each sequence.\n",
      "        labels (torch.LongTensor): Labels or input_ids with shape [tot_seqlen].\n",
      "            The first value at the beginning of each sequence has no corresponding log prob.\n",
      "\n",
      "    Returns:\n",
      "        torch.FloatTensor: Log probability with shape [tot_seqlen - #seqs].\n",
      "    \"\"\"\n",
      "    labels = torch.nn.functional.pad(labels[1:], (0, 1), value=0)\n",
      "    leave_one_indices = build_leave_one_indices(logits, cu_seqlens)\n",
      "    if constants.tensor_parallel_world_size() > 1:\n",
      "        # NOTE: logprobs is freaking sensitive to input_ids. If the input sequence is a natural sequence, everything will be fine.\n",
      "        # However, if we input random token IDs, parallel cross entropy can produce VERY different results than the normal\n",
      "        # torch.gather based version (e.g., the maximum absolute different can reach ~50).\n",
      "        from realhf.impl.model.parallelism.tensor_parallel.modules import (\n",
      "            vocab_parallel_cross_entropy,\n",
      "        )\n",
      "\n",
      "        logprobs = -vocab_parallel_cross_entropy(logits, labels)[leave_one_indices]\n",
      "        return logprobs\n",
      "    logits_shape = logits.shape\n",
      "    # shift_one_indices = torch.cat([\n",
      "    #     torch.arange(cu_seqlens[i] + 1 , cu_seqlens[i + 1], dtype=torch.long, device=cu_seqlens.device)\n",
      "    #     for i in range(cu_seqlens.shape[0] - 1)\n",
      "    # ])\n",
      "    # shift labels one step to the left and pad it to match the shape of logits\n",
      "    log_probs_labels = gather_logprobs(logits, labels)\n",
      "    log_probs_labels = log_probs_labels[leave_one_indices]\n",
      "    assert log_probs_labels.shape[0] == logits_shape[0] - cu_seqlens.shape[0] + 1, (\n",
      "        log_probs_labels.shape,\n",
      "        logits_shape,\n",
      "        cu_seqlens.shape,\n",
      "        cu_seqlens,\n",
      "        # shift_one_indices,\n",
      "    )\n",
      "    return log_probs_labels\n",
      "\n",
      "\n",
      "def apply_logits_mask(logits: torch.HalfTensor, mask: torch.BoolTensor):\n",
      "    assert (\n",
      "        mask.shape[-1] == logits.shape[-1] * constants.tensor_parallel_world_size()\n",
      "    ), (\n",
      "        constants.tensor_parallel_world_size(),\n",
      "        logits.shape,\n",
      "        mask.shape,\n",
      "    )\n",
      "    parallel_vocab_size = logits.shape[-1]\n",
      "    tp_rank = constants.tensor_parallel_rank()\n",
      "    mask = mask[:, tp_rank * parallel_vocab_size : (tp_rank + 1) * parallel_vocab_size]\n",
      "    logits.masked_fill_(mask, torch.finfo(logits.dtype).min)\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "def masked_normalization(\n",
      "    x: torch.Tensor,\n",
      "    mask: Optional[torch.BoolTensor] = None,\n",
      "    dim=None,\n",
      "    inplace=False,\n",
      "    unbiased=False,\n",
      "    eps=1e-5,\n",
      "    high_precision=True,\n",
      "    all_reduce=True,\n",
      "):\n",
      "    \"\"\"Normalize x with a mask. Typically used in advantage normalization.\n",
      "\n",
      "    Args:\n",
      "        x (torch.Tensor):\n",
      "            Tensor to be normalized.\n",
      "        mask (torch.Tensor, optional):\n",
      "            A mask with the same shape as x. Defaults to None.\n",
      "        dim (int or tuple of ints, optional):\n",
      "            Dimensions to be normalized. Defaults to None.\n",
      "        inplace (bool, optional):\n",
      "            Whether to perform in-place operation. Defaults to False.\n",
      "        eps (torch.Tensor, optional):\n",
      "            Minimal denominator. Defaults to 1e-5.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor:\n",
      "            Normalized x, with the same shape as x.\n",
      "    \"\"\"\n",
      "    dtype = torch.float64 if high_precision else torch.float32\n",
      "    x = x.to(dtype)\n",
      "    if not inplace:\n",
      "        x = x.clone()\n",
      "    if dim is None:\n",
      "        dim = tuple(range(len(x.shape)))\n",
      "    if mask is None:\n",
      "        factor = torch.tensor(\n",
      "            np.prod([x.shape[d] for d in dim]), dtype=dtype, device=x.device\n",
      "        )\n",
      "    else:\n",
      "        mask = mask.to(dtype)\n",
      "        assert len(mask.shape) == len(x.shape), (mask.shape, x.shape, dim)\n",
      "        for i in range(len(x.shape)):\n",
      "            if i in dim:\n",
      "                assert mask.shape[i] == x.shape[i], (mask.shape, x.shape, dim)\n",
      "            else:\n",
      "                assert mask.shape[i] == 1, (mask.shape, x.shape, dim)\n",
      "        x = x * mask\n",
      "        factor = mask.sum(dim, keepdim=True)\n",
      "    x_sum = x.sum(dim=dim, keepdim=True)\n",
      "    x_sum_sq = x.square().sum(dim=dim, keepdim=True)\n",
      "    if dist.is_initialized() and all_reduce:\n",
      "        dist.all_reduce(\n",
      "            factor, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "        )\n",
      "        dist.all_reduce(\n",
      "            x_sum, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "        )\n",
      "        dist.all_reduce(\n",
      "            x_sum_sq,\n",
      "            op=dist.ReduceOp.SUM,\n",
      "            group=constants.data_parallel_group(),\n",
      "        )\n",
      "    mean = x_sum / factor\n",
      "    meansq = x_sum_sq / factor\n",
      "    var = meansq - mean**2\n",
      "    if unbiased:\n",
      "        var *= factor / (factor - 1)\n",
      "    return ((x - mean) / (var.sqrt() + eps)).float()\n",
      "\n",
      "\n",
      "def get_eos_indices(\n",
      "    input_ids: torch.LongTensor,\n",
      "    tokenizer: transformers.PreTrainedTokenizerFast,\n",
      ") -> Tuple[torch.LongTensor, torch.FloatTensor]:\n",
      "    if torch.any(input_ids[:, 0] == tokenizer.eos_token_id):\n",
      "        indices = (input_ids[:, 0] == tokenizer.eos_token_id).nonzero().flatten()\n",
      "        bad_input_ids = input_ids[indices]\n",
      "        bad_strs = tokenizer.batch_decode(\n",
      "            bad_input_ids,\n",
      "            skip_special_tokens=True,\n",
      "            clean_up_tokenization_spaces=True,\n",
      "        )\n",
      "        raise RuntimeError(\n",
      "            f\"Generated sequence terminates unexpectedly early: {bad_strs}\"\n",
      "        )\n",
      "    seq_len = input_ids.shape[1]\n",
      "    eos_mask = (input_ids == tokenizer.eos_token_id).float()\n",
      "    seq_no_eos_mask = (eos_mask.sum(1) == 0).float()\n",
      "    eos_indices = eos_mask.argmax(1)\n",
      "    eos_indices = (\n",
      "        eos_indices * (1 - seq_no_eos_mask) + seq_no_eos_mask * (seq_len - 1)\n",
      "    ).long()\n",
      "    return eos_indices, seq_no_eos_mask\n",
      "\n",
      "\n",
      "def torch_attn_func(\n",
      "    q: torch.Tensor,\n",
      "    k: torch.Tensor,\n",
      "    v: torch.Tensor,\n",
      "    causal: bool,\n",
      "    cu_seqlens_q: torch.IntTensor,\n",
      "    max_seqlen_q: int,\n",
      "    cu_seqlens_k: torch.IntTensor,\n",
      "    max_seqlen_k: int,\n",
      "    dropout_p: float,\n",
      "    softmax_scale: float,\n",
      "    upcast_unscale: float = 1.0,\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"PyTorch implementation of the attention function with a flash-attn-like\n",
      "    realhf.api.\n",
      "\n",
      "    We use this function to compare the output of our model and huggingface models.\n",
      "    Flash-attn/float16/CUDAkernels will all more or less suffer from float point errors.\n",
      "    We call this function with float32 and CPU to get the \"ground truth\" output.\n",
      "\n",
      "    Args:\n",
      "        q (torch.Tensor): Shape [total_seqlen, #q, head_dim].\n",
      "        k (torch.Tensor): Shape [total_seqlen, #kv, head_dim].\n",
      "        v (torch.Tensor): Shape [total_seqlen, #kv, head_dim].\n",
      "        causal (bool): .\n",
      "        dropout_p (float): .\n",
      "        softmax_scale (float): .\n",
      "        upcast_unscale (float, optional): Scale factor when upcastin attention scores.\n",
      "            Defaults to 1.0.\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Attention score. Shape [bs, seqlen, #q, head_dim].\n",
      "    \"\"\"\n",
      "    nq = q.shape[-2]\n",
      "    nkv = k.shape[-2]\n",
      "    n_rep = q.shape[-2] // k.shape[-2]\n",
      "    bsz = cu_seqlens_q.shape[0] - 1\n",
      "    # repeat k/v heads if n_kv_heads < n_heads\n",
      "    k = repeat_kv(k, n_rep)  # (total_seqlen, nq, head_dim)\n",
      "    v = repeat_kv(v, n_rep)  # (total_seqlen, nq, head_dim)\n",
      "\n",
      "    input_lens_k = cu_seqlens_k[1:] - cu_seqlens_k[:-1]\n",
      "    attention_mask_k = torch.arange(\n",
      "        max_seqlen_k, dtype=torch.long, device=\"cpu\"\n",
      "    ).unsqueeze(0) < input_lens_k.unsqueeze(1)\n",
      "    _, _pad_indices_k, _, _ = unpad_input(attention_mask_k, attention_mask_k)\n",
      "\n",
      "    input_lens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]\n",
      "    attention_mask_q = torch.arange(\n",
      "        max_seqlen_q, dtype=torch.long, device=\"cpu\"\n",
      "    ).unsqueeze(0) < input_lens_q.unsqueeze(1)\n",
      "    _, _pad_indices_q, _, _ = unpad_input(attention_mask_q, attention_mask_q)\n",
      "\n",
      "    q = pad_input(q, _pad_indices_q, bsz, max_seqlen_q)\n",
      "    k = pad_input(k, _pad_indices_k, bsz, max_seqlen_k)\n",
      "    v = pad_input(v, _pad_indices_k, bsz, max_seqlen_k)\n",
      "\n",
      "    q = q.transpose(1, 2)  # (bs, nq, seqlen, head_dim)\n",
      "    k = k.transpose(1, 2)\n",
      "    v = v.transpose(1, 2)\n",
      "    scores = torch.matmul(q, k.transpose(2, 3)) * softmax_scale\n",
      "\n",
      "    mask = (\n",
      "        attention_mask_k.unsqueeze(1).unsqueeze(1).repeat(1, nq, max_seqlen_q, 1)\n",
      "    )  # [bs, nq, seqlen, seqlen]\n",
      "    if causal:\n",
      "        _ms = max(max_seqlen_q, max_seqlen_k)\n",
      "        causal_mask = torch.tril(\n",
      "            torch.ones(_ms, _ms, device=q.device, dtype=torch.bool)\n",
      "        )[-max_seqlen_q:, -max_seqlen_k:]\n",
      "        mask = mask & causal_mask\n",
      "\n",
      "    # if mask_softmax:\n",
      "    scores = upcast_masked_softmax(\n",
      "        scores,\n",
      "        mask,\n",
      "        mask_value=torch.full(\n",
      "            [],\n",
      "            torch.finfo(torch.float32).min,\n",
      "            device=scores.device,\n",
      "            dtype=torch.float32,\n",
      "        ),\n",
      "        scale=upcast_unscale,\n",
      "        softmax_dtype=torch.float32,\n",
      "    )\n",
      "    # else:\n",
      "    #     scores = upcast_softmax(scores, scale=upcast_unscale, softmax_dtype=torch.float32)\n",
      "    scores = torch.nn.functional.dropout(scores, p=dropout_p)\n",
      "    scores = scores.to(q.dtype)\n",
      "    output = torch.matmul(scores, v)  # (bs, nq, seqlen, head_dim)\n",
      "    output = output.transpose(1, 2).contiguous()\n",
      "\n",
      "    output = unpad_input(output, attention_mask_q)[0]\n",
      "    return output\n",
      "\n",
      "\n",
      "def rotate_half(x: torch.HalfTensor, interleaved: bool = False):\n",
      "    if not interleaved:\n",
      "        x1, x2 = x.chunk(2, dim=-1)\n",
      "        return torch.cat((-x2, x1), dim=-1)\n",
      "    else:\n",
      "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
      "        # return rearrange(torch.stack((-x2, x1), dim=-1), \"... d two -> ... (d two)\", two=2)\n",
      "        return torch.stack((-x2, x1), dim=-1).flatten(start_dim=-2)\n",
      "\n",
      "\n",
      "@torch.no_grad()\n",
      "@torch.jit.script\n",
      "def compute_varlen_position_indices(\n",
      "    total_seqlen: int,\n",
      "    cu_seqlens: torch.IntTensor,\n",
      "    seqlen_offsets: Optional[torch.IntTensor] = None,\n",
      ") -> torch.IntTensor:\n",
      "    indexing_t = torch.arange(\n",
      "        total_seqlen, dtype=torch.long, device=cu_seqlens.device\n",
      "    ).unsqueeze_(0)\n",
      "    indexing_t = (cu_seqlens[:-1].unsqueeze(1) <= indexing_t) & (\n",
      "        indexing_t < cu_seqlens[1:].unsqueeze(1)\n",
      "    )\n",
      "    indices = indexing_t.cumsum(1) - 1\n",
      "    if seqlen_offsets is not None:\n",
      "        indices += seqlen_offsets.unsqueeze(1)\n",
      "    return torch.where(indexing_t, indices, 0).sum(0)\n",
      "\n",
      "\n",
      "# @torch.jit.script\n",
      "def apply_rotary_varlen(\n",
      "    x: torch.HalfTensor,\n",
      "    cos: torch.HalfTensor,\n",
      "    sin: torch.HalfTensor,\n",
      "    cu_seqlens: torch.IntTensor,\n",
      "    interleaved: bool,\n",
      "    seqlen_offsets: Optional[torch.IntTensor] = None,\n",
      "    rotary_indices: Optional[torch.LongTensor] = None,\n",
      "    special_impl: Optional[str] = None,\n",
      ") -> Tuple[torch.HalfTensor, torch.LongTensor]:\n",
      "    if rotary_indices is None:\n",
      "        rotary_indices = compute_varlen_position_indices(\n",
      "            x.shape[0], cu_seqlens, seqlen_offsets\n",
      "        )\n",
      "\n",
      "    cos = cos[rotary_indices]\n",
      "    sin = sin[rotary_indices]\n",
      "    if special_impl == \"bailing\":\n",
      "        return x * cos[:, None, :] + rotate_half(x, interleaved) * sin[:, None, :]\n",
      "\n",
      "    assert special_impl is None\n",
      "    ro_dim = cos.shape[-1] * 2\n",
      "    assert ro_dim <= x.shape[-1], (x.shape, cos.shape)\n",
      "    if not interleaved:\n",
      "        cos = cos[:, None, None, :].repeat(1, 1, 2, 1).flatten(start_dim=-2)\n",
      "        sin = sin[:, None, None, :].repeat(1, 1, 2, 1).flatten(start_dim=-2)\n",
      "    else:\n",
      "        cos = cos[:, None, :, None].repeat(1, 1, 1, 2).flatten(start_dim=-2)\n",
      "        sin = sin[:, None, :, None].repeat(1, 1, 1, 2).flatten(start_dim=-2)\n",
      "\n",
      "    # cos = repeat(cos, \"... d -> ... 1 (2 d)\" if not interleaved else \"... d -> ... 1 (d 2)\")\n",
      "    # sin = repeat(sin, \"... d -> ... 1 (2 d)\" if not interleaved else \"... d -> ... 1 (d 2)\")\n",
      "    return torch.cat(\n",
      "        [\n",
      "            x[..., :ro_dim] * cos + rotate_half(x[..., :ro_dim], interleaved) * sin,\n",
      "            x[..., ro_dim:],\n",
      "        ],\n",
      "        dim=-1,\n",
      "    )\n",
      "\n",
      "\n",
      "def apply_rotary(\n",
      "    x: torch.HalfTensor,\n",
      "    cos: torch.HalfTensor,\n",
      "    sin: torch.HalfTensor,\n",
      "    interleaved: bool = False,\n",
      "):\n",
      "    \"\"\"\n",
      "    x: (batch_size, seqlen, nheads, headdim)\n",
      "    cos, sin: (seqlen, rotary_dim / 2) or (batch_size, seqlen, rotary_dim / 2)\n",
      "    \"\"\"\n",
      "    ro_dim = cos.shape[-1] * 2\n",
      "    assert ro_dim <= x.shape[-1]\n",
      "    if not interleaved:\n",
      "        cos = cos[:, None, None, :].repeat(1, 1, 2, 1).flatten(start_dim=-2)\n",
      "        sin = sin[:, None, None, :].repeat(1, 1, 2, 1).flatten(start_dim=-2)\n",
      "    else:\n",
      "        cos = cos[:, None, :, None].repeat(1, 1, 1, 2).flatten(start_dim=-2)\n",
      "        sin = sin[:, None, :, None].repeat(1, 1, 1, 2).flatten(start_dim=-2)\n",
      "    return torch.cat(\n",
      "        [\n",
      "            x[..., :ro_dim] * cos + rotate_half(x[..., :ro_dim], interleaved) * sin,\n",
      "            x[..., ro_dim:],\n",
      "        ],\n",
      "        dim=-1,\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/rotary.py ====\n",
      "\n",
      "# Modified from flash-attention under BSD-3 license.\n",
      "# Copyright (c) 2023, Tri Dao.\n",
      "\n",
      "import math\n",
      "from typing import Literal, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "from einops import rearrange, repeat\n",
      "\n",
      "try:\n",
      "    from flash_attn.ops.triton.rotary import apply_rotary\n",
      "except ModuleNotFoundError:\n",
      "    pass\n",
      "\n",
      "\n",
      "class ApplyRotaryEmb(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(\n",
      "        ctx,\n",
      "        x,\n",
      "        cos,\n",
      "        sin,\n",
      "        interleaved=False,\n",
      "        inplace=False,\n",
      "        seqlen_offsets: Union[int, torch.Tensor] = 0,\n",
      "        cu_seqlens: Optional[torch.Tensor] = None,\n",
      "        max_seqlen: Optional[int] = None,\n",
      "    ):\n",
      "        out = apply_rotary(\n",
      "            x,\n",
      "            cos,\n",
      "            sin,\n",
      "            seqlen_offsets=seqlen_offsets,\n",
      "            cu_seqlens=cu_seqlens,\n",
      "            max_seqlen=max_seqlen,\n",
      "            interleaved=interleaved,\n",
      "            inplace=inplace,\n",
      "        )\n",
      "        if isinstance(seqlen_offsets, int):\n",
      "            ctx.save_for_backward(\n",
      "                cos, sin, cu_seqlens\n",
      "            )  # Can't save int with save_for_backward\n",
      "            ctx.seqlen_offsets = seqlen_offsets\n",
      "        else:\n",
      "            ctx.save_for_backward(cos, sin, cu_seqlens, seqlen_offsets)\n",
      "            ctx.seqlen_offsets = None\n",
      "        ctx.interleaved = interleaved\n",
      "        ctx.inplace = inplace\n",
      "        ctx.max_seqlen = max_seqlen\n",
      "        return out if not inplace else x\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, do):\n",
      "        seqlen_offsets = ctx.seqlen_offsets\n",
      "        if seqlen_offsets is None:\n",
      "            cos, sin, cu_seqlens, seqlen_offsets = ctx.saved_tensors\n",
      "        else:\n",
      "            cos, sin, cu_seqlens = ctx.saved_tensors\n",
      "        # TD [2023-09-02]: For some reason Triton (2.0.0.post1) errors with\n",
      "        # \"[CUDA]: invalid device context\", and cloning makes it work. Idk why. Triton 2.1.0 works.\n",
      "        if not ctx.interleaved and not ctx.inplace:\n",
      "            do = do.clone()\n",
      "        dx = apply_rotary(\n",
      "            do,\n",
      "            cos,\n",
      "            sin,\n",
      "            seqlen_offsets=seqlen_offsets,\n",
      "            cu_seqlens=cu_seqlens,\n",
      "            max_seqlen=ctx.max_seqlen,\n",
      "            interleaved=ctx.interleaved,\n",
      "            inplace=ctx.inplace,\n",
      "            conjugate=True,\n",
      "        )\n",
      "        return dx, None, None, None, None, None, None, None\n",
      "\n",
      "\n",
      "def apply_rotary_emb(\n",
      "    x,\n",
      "    cos,\n",
      "    sin,\n",
      "    interleaved=False,\n",
      "    inplace=False,\n",
      "    seqlen_offsets: Union[int, torch.Tensor] = 0,\n",
      "    cu_seqlens: Optional[torch.Tensor] = None,\n",
      "    max_seqlen: Optional[int] = None,\n",
      "):\n",
      "    \"\"\"\n",
      "    Arguments:\n",
      "        x: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\n",
      "            else (total_seqlen, nheads, headdim)\n",
      "        cos, sin: (seqlen_rotary, rotary_dim / 2)\n",
      "        interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n",
      "            of 1st half and 2nd half (GPT-NeoX style).\n",
      "        inplace: if True, apply rotary embedding in-place.\n",
      "        seqlen_offsets: (batch_size,) or int. Each sequence in x is shifted by this amount.\n",
      "            Most commonly used in inference when we have KV cache.\n",
      "        cu_seqlens: (batch + 1,) or None\n",
      "        max_seqlen: int\n",
      "    Return:\n",
      "        out: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\n",
      "            else (total_seqlen, nheads, headdim)\n",
      "    rotary_dim must be <= headdim\n",
      "    Apply rotary embedding to the first rotary_dim of x.\n",
      "    \"\"\"\n",
      "    return ApplyRotaryEmb.apply(\n",
      "        x,\n",
      "        cos,\n",
      "        sin,\n",
      "        interleaved,\n",
      "        inplace,\n",
      "        seqlen_offsets,\n",
      "        cu_seqlens,\n",
      "        max_seqlen,\n",
      "    )\n",
      "\n",
      "\n",
      "# For backward compatibility\n",
      "apply_rotary_emb_func = apply_rotary_emb\n",
      "\n",
      "\n",
      "class RotaryEmbedding(torch.nn.Module):\n",
      "    \"\"\"The rotary position embeddings from RoFormer_ (Su et. al). A crucial\n",
      "    insight from the method is that the query and keys are transformed by\n",
      "    rotation matrices which depend on the relative positions.\n",
      "\n",
      "    Other implementations are available in the Rotary Transformer repo_ and in\n",
      "    GPT-NeoX_, GPT-NeoX was an inspiration\n",
      "\n",
      "    .. _RoFormer: https://arxiv.org/abs/2104.09864\n",
      "    .. _repo: https://github.com/ZhuiyiTechnology/roformer\n",
      "    .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        dim: int,\n",
      "        base: float = 10000.0,\n",
      "        interleaved: bool = False,\n",
      "        scale_factor: float = None,\n",
      "        scale_type: Optional[Literal[\"linear\", \"dynamic\"]] = None,\n",
      "        pos_idx_in_fp32: bool = True,\n",
      "        device: Optional[torch.device] = None,\n",
      "        special_impl: Optional[str] = None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n",
      "            of 1st half and 2nd half (GPT-NeoX style).\n",
      "        pos_idx_in_fp32: if True, the position indices [0.0, ..., seqlen - 1] are in fp32,\n",
      "            otherwise they might be in lower precision.\n",
      "            This option was added because previously (before 2023-07-02), when we construct\n",
      "            the position indices, we use the dtype of self.inv_freq. In most cases this would\n",
      "            be fp32, but if the model is trained in pure bf16 (not mixed precision), then\n",
      "            self.inv_freq would be bf16, and the position indices are also in bf16.\n",
      "            Because of the limited precision of bf16 (e.g. 1995.0 is rounded to 2000.0), the\n",
      "            embeddings for some positions will coincide.\n",
      "            To maintain compatibility with models previously trained in pure bf16,\n",
      "            we add this option.\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self.dim = dim\n",
      "        self.base = float(base)\n",
      "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
      "        # Generate and save the inverse frequency buffer (non trainable)\n",
      "        inv_freq = self._compute_inv_freq(device)\n",
      "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
      "        self.interleaved = interleaved\n",
      "        self.scale_factor = scale_factor\n",
      "        self.scale_type = scale_type\n",
      "\n",
      "        self._seq_len_cached = 0\n",
      "        self._cos_cached = None\n",
      "        self._sin_cached = None\n",
      "        self._cos_k_cached = None\n",
      "        self._sin_k_cached = None\n",
      "\n",
      "        self.special_impl = special_impl\n",
      "\n",
      "    def _compute_inv_freq(self, device=None):\n",
      "        return 1.0 / (\n",
      "            self.base\n",
      "            ** (\n",
      "                torch.arange(0, self.dim, 2, device=device, dtype=torch.float32)\n",
      "                / self.dim\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def _update_cos_sin_cache(self, seqlen, device=None, dtype=None):\n",
      "        # Reset the tables if the sequence length has changed,\n",
      "        # if we're on a new device (possibly due to tracing for instance),\n",
      "        # or if we're switching from inference mode to training\n",
      "        if (\n",
      "            seqlen > self._seq_len_cached\n",
      "            or self._cos_cached is None\n",
      "            or self._cos_cached.device != device\n",
      "            or self._cos_cached.dtype != dtype\n",
      "            or (self.training and self._cos_cached.is_inference())\n",
      "        ):\n",
      "            self._seq_len_cached = seqlen\n",
      "            # We want fp32 here, not self.inv_freq.dtype, since the model could be loaded in bf16\n",
      "            # And the output of arange can be quite large, so bf16 would lose a lot of precision.\n",
      "            # However, for compatibility reason, we add an option to use the dtype of self.inv_freq.\n",
      "            if self.pos_idx_in_fp32:\n",
      "                t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
      "                # We want fp32 here as well since inv_freq will be multiplied with t, and the output\n",
      "                # will be large. Having it in bf16 will lose a lot of precision and cause the\n",
      "                # cos & sin output to change significantly.\n",
      "                # We want to recompute self.inv_freq if it was not loaded in fp32\n",
      "                if self.inv_freq.dtype != torch.float32:\n",
      "                    inv_freq = self._compute_inv_freq(device=device)\n",
      "                else:\n",
      "                    inv_freq = self.inv_freq\n",
      "            else:\n",
      "                t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
      "                inv_freq = self.inv_freq\n",
      "            if self.scale_factor is not None:\n",
      "                if self.scale_type == \"linear\":\n",
      "                    t = t / self.scale_factor\n",
      "                elif self.scale_type == \"dynamic\":\n",
      "                    base = self.base * (\n",
      "                        (self.scale_factor * seqlen / self._seq_len_cached)\n",
      "                        - (self.scale_factor - 1)\n",
      "                    ) ** (self.dim / (self.dim - 2))\n",
      "                    inv_freq = 1.0 / (\n",
      "                        base\n",
      "                        ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)\n",
      "                    )\n",
      "                    self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
      "                else:\n",
      "                    raise NotImplementedError(\"Unsupported scale type\", self.scale_type)\n",
      "            # Don't do einsum, it converts fp32 to fp16 under AMP\n",
      "            # freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
      "            freqs = torch.outer(t, inv_freq)\n",
      "            if self.special_impl == \"bailing\":\n",
      "                freqs = torch.cat((freqs, freqs), dim=-1)\n",
      "                self._cos_cached = torch.cos(freqs)[:seqlen].to(dtype)\n",
      "                self._sin_cached = torch.sin(freqs)[:seqlen].to(dtype)\n",
      "            elif self.special_impl is None:\n",
      "                self._cos_cached = torch.cos(freqs).to(dtype)\n",
      "                self._sin_cached = torch.sin(freqs).to(dtype)\n",
      "            else:\n",
      "                raise NotImplementedError()\n",
      "            # power = (\n",
      "            #     torch.arange(seqlen, dtype=self.scale.dtype, device=self.scale.device)\n",
      "            #     - seqlen // 2\n",
      "            # ) / self.scale_base\n",
      "            # scale = self.scale.to(device=power.device) ** rearrange(power, \"s -> s 1\")\n",
      "            # # We want the multiplication by scale to happen in fp32\n",
      "            # self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
      "            # self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
      "            # self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
      "            # self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        qk: torch.Tensor,\n",
      "        cu_seqlens: Optional[torch.Tensor] = None,\n",
      "        max_seqlen: Optional[int] = None,\n",
      "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
      "        \"\"\"\n",
      "        qkv: (batch, seqlen, nq + nkv, headdim),\n",
      "        seqlen_offset: (batch_size,) or int. Each sequence in x is shifted by this amount.\n",
      "            Most commonly used in inference when we have KV cache.\n",
      "            If it's a tensor of shape (batch_size,), then to update the cos / sin cache, one\n",
      "            should pass in max_seqlen, which will update the cos / sin cache up to that length.\n",
      "        Apply rotary embedding *inplace* to qkv and / or kv.\n",
      "        \"\"\"\n",
      "        assert max_seqlen is None or isinstance(max_seqlen, int)\n",
      "        if cu_seqlens is not None:\n",
      "            assert max_seqlen is not None\n",
      "        seqlen = qk.shape[1] if max_seqlen is None else max_seqlen\n",
      "        self._update_cos_sin_cache(seqlen, device=qk.device, dtype=qk.dtype)\n",
      "        return apply_rotary_emb_func(\n",
      "            qk,\n",
      "            self._cos_cached,\n",
      "            self._sin_cached,\n",
      "            interleaved=self.interleaved,\n",
      "            inplace=True,\n",
      "            cu_seqlens=cu_seqlens,\n",
      "            max_seqlen=max_seqlen,\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/activations.py ====\n",
      "\n",
      "# Modified from flash-attention under BSD-3 license.\n",
      "# Copyright (c) 2023, Tri Dao.\n",
      "import math\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "# 1/sqrt(2*pi)-> 0.3989423\n",
      "# 1/sqrt(2)   -> 0.70710678\n",
      "# sqrt(2/pi)  -> 0.79788456\n",
      "\n",
      "\n",
      "# this function is tanh approximation of gelu\n",
      "# actual gelu is:\n",
      "# x * 0.5 * (1.0 + torch.erf(x * 0.70710678))\n",
      "@torch.jit.script\n",
      "def bias_gelu(y, bias):\n",
      "    x = bias + y\n",
      "    return (x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))).to(\n",
      "        dtype=y.dtype\n",
      "    )\n",
      "\n",
      "\n",
      "# gradient of tanh approximation of gelu\n",
      "# gradient of actual gelu is:\n",
      "# 0.5 * (1. + torch.erf(x * 0.70710678)) + 0.3989423 * x * torch.exp(-0.5 * x * x)\n",
      "@torch.jit.script\n",
      "def bias_gelu_back(g, y, bias):\n",
      "    \"\"\"Assume that y has shape (B, D) and bias has shape (D)\"\"\"\n",
      "    x = bias + y\n",
      "    tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n",
      "    # sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243\n",
      "    ff = 0.5 * x * (\n",
      "        (1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)\n",
      "    ) + 0.5 * (1 + tanh_out)\n",
      "    grad_y = ff * g\n",
      "    return grad_y.to(dtype=y.dtype), grad_y.sum(dim=(0), dtype=bias.dtype)\n",
      "\n",
      "\n",
      "class GeLUFunction(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    # bias is an optional argument\n",
      "    def forward(ctx, input, bias):\n",
      "        ctx.save_for_backward(input, bias)\n",
      "        return bias_gelu(input, bias)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        input, bias = ctx.saved_tensors\n",
      "        tmp = bias_gelu_back(grad_output, input, bias)\n",
      "        return tmp, tmp\n",
      "\n",
      "\n",
      "bias_gelu_impl = GeLUFunction.apply\n",
      "\n",
      "\n",
      "# this function is tanh approximation of gelu\n",
      "# actual gelu is:\n",
      "# x * 0.5 * (1.0 + torch.erf(x * 0.70710678))\n",
      "@torch.jit.script\n",
      "def gelu_fwd(x):\n",
      "    return (x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))).to(\n",
      "        dtype=x.dtype\n",
      "    )\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def new_gelu_activation(input: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"Implementation of the GELU activation function currently in Google BERT\n",
      "    repo (identical to OpenAI GPT).\n",
      "\n",
      "    Also see\n",
      "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
      "    \"\"\"\n",
      "    return (\n",
      "        0.5\n",
      "        * input\n",
      "        * (\n",
      "            1.0\n",
      "            + torch.tanh(\n",
      "                math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "# gradient of tanh approximation of gelu\n",
      "# gradient of actual gelu is:\n",
      "# 0.5 * (1. + torch.erf(x * 0.70710678)) + 0.3989423 * x * torch.exp(-0.5 * x * x)\n",
      "@torch.jit.script\n",
      "def gelu_bwd(g, x):\n",
      "    tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n",
      "    # sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243\n",
      "    ff = 0.5 * x * (\n",
      "        (1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)\n",
      "    ) + 0.5 * (1 + tanh_out)\n",
      "    return (ff * g).to(dtype=x.dtype)\n",
      "\n",
      "\n",
      "class FastGeLUFunction(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    # bias is an optional argument\n",
      "    def forward(ctx, input):\n",
      "        ctx.save_for_backward(input)\n",
      "        return gelu_fwd(input)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        (input,) = ctx.saved_tensors\n",
      "        tmp = gelu_bwd(grad_output, input)\n",
      "        return tmp\n",
      "\n",
      "\n",
      "fast_gelu_impl = FastGeLUFunction.apply\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def relu_bwd(g, x):\n",
      "    return torch.where(x >= 0, g, 0.0).to(dtype=x.dtype)\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def sqrelu_fwd(x):\n",
      "    r = F.relu(x)\n",
      "    return (r * r).to(dtype=x.dtype)\n",
      "\n",
      "\n",
      "@torch.jit.script\n",
      "def sqrelu_bwd(g, x):\n",
      "    return (2.0 * g * F.relu(x)).to(dtype=x.dtype)\n",
      "\n",
      "\n",
      "swiglu_fwd_codestring = \"\"\"\n",
      "template <typename T> T swiglu_fwd(T x, T y) {\n",
      "    return float(x) * float(y) / (1.0f + ::exp(-float(x)));\n",
      "}\n",
      "\"\"\"\n",
      "swiglu_bwd_codestring = \"\"\"\n",
      "template <typename T> T swiglu_bwd(T x, T y, T g, T& dx, T& dy) {\n",
      "    float x_sigmoid = 1.0f / (1.0f + ::exp(-float(x)));\n",
      "    dx = x_sigmoid * (1 + float(x) * (1.0f - x_sigmoid)) * float(g) * float(y);\n",
      "    dy = float(x) * x_sigmoid * float(g);\n",
      "}\n",
      "\"\"\"\n",
      "swiglu_fwd = torch.cuda.jiterator._create_jit_fn(swiglu_fwd_codestring)\n",
      "swiglu_bwd = torch.cuda.jiterator._create_multi_output_jit_fn(\n",
      "    swiglu_bwd_codestring, num_outputs=2\n",
      ")\n",
      "\n",
      "\n",
      "class SwiGLUFunction(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, x, y):\n",
      "        ctx.save_for_backward(x, y)\n",
      "        return swiglu_fwd(x, y)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, dout):\n",
      "        x, y = ctx.saved_tensors\n",
      "        return swiglu_bwd(x, y, dout)\n",
      "\n",
      "\n",
      "swiglu = SwiGLUFunction.apply\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/rms.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import functools\n",
      "import math\n",
      "from typing import Callable, Optional, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.nn as nn\n",
      "\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(\"Modules\")\n",
      "\n",
      "\n",
      "class ExponentialRunningMeanStd(nn.Module):\n",
      "\n",
      "    def __init__(self, beta=0.999, epsilon=1e-5, high_precision=True):\n",
      "        super().__init__()\n",
      "        self.__beta = beta\n",
      "        self.__eps = epsilon\n",
      "\n",
      "        self.__dtype = torch.float64 if high_precision else torch.float32\n",
      "\n",
      "        self.__mean = nn.Parameter(\n",
      "            torch.zeros((1,), dtype=self.__dtype, device=constants.current_device()),\n",
      "            requires_grad=False,\n",
      "        )\n",
      "        self.__mean_sq = nn.Parameter(\n",
      "            torch.zeros((1,), dtype=self.__dtype, device=constants.current_device()),\n",
      "            requires_grad=False,\n",
      "        )\n",
      "        self.__debiasing_term = nn.Parameter(\n",
      "            torch.zeros((1,), dtype=self.__dtype, device=constants.current_device()),\n",
      "            requires_grad=False,\n",
      "        )\n",
      "\n",
      "        self.reset_parameters()\n",
      "\n",
      "    def reset_parameters(self):\n",
      "        self.__mean.zero_()\n",
      "        self.__mean_sq.zero_()\n",
      "        self.__debiasing_term.zero_()\n",
      "\n",
      "    def forward(self, *args, **kwargs):\n",
      "        # we don't implement the forward function because its meaning\n",
      "        # is somewhat ambiguous\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def update(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
      "        x = x.to(self.__dtype)\n",
      "        if mask is not None:\n",
      "            mask = mask.to(self.__dtype)\n",
      "        if mask is None:\n",
      "            factor = torch.tensor(np.prod(x.shape), dtype=self.__dtype, device=x.device)\n",
      "        else:\n",
      "            x = x * mask\n",
      "            factor = mask.sum()\n",
      "\n",
      "        x_sum = x.sum()\n",
      "        x_sum_sq = x.square().sum()\n",
      "        if dist.is_initialized():\n",
      "            dist.all_reduce(\n",
      "                factor, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                x_sum, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                x_sum_sq, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "        batch_mean = x_sum / factor\n",
      "        batch_sq_mean = x_sum_sq / factor\n",
      "\n",
      "        self.__mean.data[:] = self.__beta * self.__mean.data[:] + batch_mean * (\n",
      "            1.0 - self.__beta\n",
      "        )\n",
      "        self.__mean_sq.data[:] = self.__beta * self.__mean_sq.data[\n",
      "            :\n",
      "        ] + batch_sq_mean * (1.0 - self.__beta)\n",
      "        self.__debiasing_term.data[:] = (\n",
      "            self.__beta * self.__debiasing_term.data[:] + 1.0 - self.__beta\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def mean_std(self):\n",
      "        debiased_mean = self.__mean / self.__debiasing_term.clamp(min=self.__eps)\n",
      "        debiased_mean_sq = self.__mean_sq / self.__debiasing_term.clamp(min=self.__eps)\n",
      "        debiased_var = (debiased_mean_sq - debiased_mean**2).clamp(min=1e-2)\n",
      "        return debiased_mean, debiased_var.sqrt()\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def normalize(self, x):\n",
      "        x = x.to(self.__dtype)\n",
      "        mean, std = self.mean_std()\n",
      "        return (\n",
      "            ((x - mean) / std).clip(-5, 5).float()\n",
      "        )  # clipping is a trick from hide and seek\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def denormalize(self, x):\n",
      "        x = x.to(self.__dtype)\n",
      "        mean, std = self.mean_std()\n",
      "        return (x * std + mean).float()\n",
      "\n",
      "\n",
      "class MovingAverageRunningMeanStd(nn.Module):\n",
      "\n",
      "    def __init__(self, high_precision=True):\n",
      "        super().__init__()\n",
      "\n",
      "        self.__dtype = torch.float64 if high_precision else torch.float32\n",
      "\n",
      "        self.__mean = nn.Parameter(\n",
      "            torch.zeros((1,), dtype=self.__dtype, device=constants.current_device()),\n",
      "            requires_grad=False,\n",
      "        )\n",
      "        self.__mean_sq = nn.Parameter(\n",
      "            torch.zeros((1,), dtype=self.__dtype, device=constants.current_device()),\n",
      "            requires_grad=False,\n",
      "        )\n",
      "        self.__accum_denominator = 0\n",
      "\n",
      "        self.reset_parameters()\n",
      "\n",
      "    def reset_parameters(self):\n",
      "        self.__mean.zero_()\n",
      "        self.__mean_sq.zero_()\n",
      "        self.__accum_denominator = 0\n",
      "\n",
      "    def forward(self, *args, **kwargs):\n",
      "        # we don't implement the forward function because its meaning\n",
      "        # is somewhat ambiguous\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def update(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
      "        x = x.to(self.__dtype)\n",
      "        if mask is not None:\n",
      "            mask = mask.to(self.__dtype)\n",
      "        if mask is None:\n",
      "            factor = torch.tensor(np.prod(x.shape), dtype=self.__dtype, device=x.device)\n",
      "        else:\n",
      "            x = x * mask\n",
      "            factor = mask.sum()\n",
      "\n",
      "        x_sum = x.sum()\n",
      "        x_sum_sq = x.square().sum()\n",
      "        if dist.is_initialized():\n",
      "            dist.all_reduce(\n",
      "                factor, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                x_sum, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "            dist.all_reduce(\n",
      "                x_sum_sq, op=dist.ReduceOp.SUM, group=constants.data_parallel_group()\n",
      "            )\n",
      "\n",
      "        self.__mean.data[:] = (\n",
      "            self.__accum_denominator * self.__mean.data[:] + x_sum\n",
      "        ) / (self.__accum_denominator + factor)\n",
      "        self.__mean_sq.data[:] = (\n",
      "            self.__accum_denominator * self.__mean_sq.data[:] + x_sum_sq\n",
      "        ) / (self.__accum_denominator + factor)\n",
      "        self.__accum_denominator += factor\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def mean_std(self):\n",
      "        return (\n",
      "            self.__mean.clone(),\n",
      "            (self.__mean_sq - self.__mean**2).clamp(min=1e-2).sqrt(),\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def normalize(self, x):\n",
      "        x = x.to(self.__dtype)\n",
      "        mean, std = self.mean_std()\n",
      "        return (\n",
      "            ((x - mean) / std).clip(-5, 5).float()\n",
      "        )  # clipping is a trick from hide and seek\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def denormalize(self, x):\n",
      "        x = x.to(self.__dtype)\n",
      "        mean, std = self.mean_std()\n",
      "        return (x * std + mean).float()\n",
      "\n",
      "\n",
      "class PopArtValueHead(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_dim,\n",
      "        critic_dim,\n",
      "        beta=0.99999,\n",
      "        epsilon=1e-5,\n",
      "        burn_in_updates=torch.inf,\n",
      "        rms_type: str = \"exp\",\n",
      "        high_precision=True,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        if rms_type == \"exp\":\n",
      "            rms_cls = functools.partial(ExponentialRunningMeanStd, beta=beta)\n",
      "        elif rms_type == \"ma\":\n",
      "            rms_cls = MovingAverageRunningMeanStd\n",
      "        else:\n",
      "            raise NotImplementedError(f\"Unknown rms type {rms_type}\")\n",
      "        self.__rms = rms_cls(\n",
      "            input_shape=(critic_dim,),\n",
      "            epsilon=epsilon,\n",
      "            high_precision=high_precision,\n",
      "        )\n",
      "\n",
      "        self.__weight = nn.Parameter(torch.zeros(critic_dim, input_dim))\n",
      "        self.__bias = nn.Parameter(torch.zeros(critic_dim))\n",
      "        # The same initialization as `nn.Linear`.\n",
      "        torch.nn.init.kaiming_uniform_(self.__weight, a=math.sqrt(5))\n",
      "        torch.nn.init.uniform_(\n",
      "            self.__bias, -1 / math.sqrt(input_dim), 1 / math.sqrt(input_dim)\n",
      "        )\n",
      "\n",
      "        self.__burn_in_updates = burn_in_updates\n",
      "        self.__update_cnt = 0\n",
      "\n",
      "    @property\n",
      "    def weight(self):\n",
      "        return self.__weight\n",
      "\n",
      "    @property\n",
      "    def bias(self):\n",
      "        return self.__bias\n",
      "\n",
      "    def forward(self, feature):\n",
      "        return torch.nn.functional.linear(feature, self.__weight, self.__bias)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def update(self, x, mask):\n",
      "        old_mean, old_std = self.__rms.mean_std()\n",
      "        self.__rms.update(x, mask)\n",
      "        new_mean, new_std = self.__rms.mean_std()\n",
      "        self.__update_cnt += 1\n",
      "\n",
      "        if self.__update_cnt > self.__burn_in_updates:\n",
      "            self.__weight.data[:] = self.__weight * (old_std / new_std).unsqueeze(-1)\n",
      "            self.__bias.data[:] = (\n",
      "                old_std * self.__bias + old_mean - new_mean\n",
      "            ) / new_std\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def normalize(self, x):\n",
      "        return self.__rms.normalize(x)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def denormalize(self, x):\n",
      "        return self.__rms.denormalize(x)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/mlp.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import functools\n",
      "import math\n",
      "import os\n",
      "from typing import Callable, Optional, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.nn as nn\n",
      "from transformers.activations import ACT2FN\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "from realhf.impl.model.parallelism.tensor_parallel.modules import (\n",
      "    ColumnParallelLinear,\n",
      "    RowParallelLinear,\n",
      "    merged_linear_with_grad_accumulation_and_async_allreduce,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"Modules\")\n",
      "\n",
      "\n",
      "def get_activation_fn(activation_function: str) -> Callable:\n",
      "    return ACT2FN[activation_function]\n",
      "\n",
      "\n",
      "SEQUENCE_PARALLEL_WARNED = False\n",
      "\n",
      "\n",
      "class LayerNormQKVLinear(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_dim: int,\n",
      "        head_dim: int,\n",
      "        n_q_heads: int,\n",
      "        n_kv_heads: int,\n",
      "        layer_norm_epsilon: float,\n",
      "        use_attention_bias: bool,\n",
      "        layer_norm_type: Optional[str] = None,\n",
      "        do_layernorm_before: bool = True,\n",
      "        # dtype and device\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[torch.device] = None,\n",
      "        layer_index=None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        tensor_parallel = constants.tensor_parallel_world_size() > 1\n",
      "        sequence_parallel = constants.sequence_parallel()\n",
      "        gradient_accumulation_fusion = constants.gradient_accumulation_fusion()\n",
      "        if not tensor_parallel and (sequence_parallel or gradient_accumulation_fusion):\n",
      "            global SEQUENCE_PARALLEL_WARNED\n",
      "            if not SEQUENCE_PARALLEL_WARNED:\n",
      "                logger.warning(\n",
      "                    \"sequence_parallel and gradient_accumulation_fusion are only available in model parallel mode\"\n",
      "                )\n",
      "                SEQUENCE_PARALLEL_WARNED = True\n",
      "            sequence_parallel = False\n",
      "            gradient_accumulation_fusion = False\n",
      "        if dtype is None:\n",
      "            dtype = torch.float16\n",
      "        if layer_norm_type is None:\n",
      "            layer_norm_fn = nn.LayerNorm\n",
      "        elif layer_norm_type == \"rms\":\n",
      "            layer_norm_fn = LlamaRMSNorm\n",
      "        elif layer_norm_type == \"gemma\":\n",
      "            layer_norm_fn = GemmaRMSNorm\n",
      "        self.ln = layer_norm_fn(\n",
      "            input_dim, eps=layer_norm_epsilon, dtype=dtype, device=device\n",
      "        )\n",
      "\n",
      "        self.tensor_parallel = tensor_parallel\n",
      "        self.layer_index = layer_index\n",
      "        self.tp_worldsize = constants.tensor_parallel_world_size()\n",
      "        assert n_q_heads % self.tp_worldsize == 0, (\n",
      "            f\"n_q_heads {n_q_heads} must be divisible by \"\n",
      "            f\"tp_worldsize {self.tp_worldsize}\"\n",
      "        )\n",
      "        assert n_kv_heads % self.tp_worldsize == 0, (\n",
      "            f\"n_kv_heads {n_kv_heads} must be divisible by \"\n",
      "            f\"tp_worldsize {self.tp_worldsize}\"\n",
      "        )\n",
      "        hidden_dim = input_dim\n",
      "        # TODO: we can fuse the forward of qkv attention\n",
      "        self.q_attn = ColumnParallelLinear(\n",
      "            hidden_dim,\n",
      "            head_dim * n_q_heads,\n",
      "            bias=use_attention_bias,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.k_attn = ColumnParallelLinear(\n",
      "            hidden_dim,\n",
      "            head_dim * n_kv_heads,\n",
      "            bias=use_attention_bias,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.v_attn = ColumnParallelLinear(\n",
      "            hidden_dim,\n",
      "            head_dim * n_kv_heads,\n",
      "            bias=use_attention_bias,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "\n",
      "        self.d = head_dim\n",
      "        self.nq = n_q_heads\n",
      "        self.nkv = n_kv_heads\n",
      "\n",
      "        self.do_layernorm_before = do_layernorm_before\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        if self.do_layernorm_before:\n",
      "            hidden_states = self.ln(x)\n",
      "        _gradient_accumulation_fusion = self.q_attn.gradient_accumulation_fusion\n",
      "        _sequence_parallel = constants.sequence_parallel()\n",
      "        _async_grad_allreduce = not _sequence_parallel\n",
      "        _is_w_parallel = [\n",
      "            True,\n",
      "            isinstance(self.k_attn, ColumnParallelLinear),\n",
      "            isinstance(self.v_attn, ColumnParallelLinear),\n",
      "        ]\n",
      "        q, k, v = merged_linear_with_grad_accumulation_and_async_allreduce(\n",
      "            hidden_states,\n",
      "            _gradient_accumulation_fusion,\n",
      "            _async_grad_allreduce,\n",
      "            _sequence_parallel,\n",
      "            _is_w_parallel,\n",
      "            self.q_attn.weight,\n",
      "            self.q_attn.bias,\n",
      "            self.k_attn.weight,\n",
      "            self.k_attn.bias,\n",
      "            self.v_attn.weight,\n",
      "            self.v_attn.bias,\n",
      "        )\n",
      "        q = q.view(*q.shape[:-1], self.nq // self.tp_worldsize, self.d)\n",
      "        k = k.view(*k.shape[:-1], self.nkv // self.tp_worldsize, self.d)\n",
      "        v = v.view(*v.shape[:-1], self.nkv // self.tp_worldsize, self.d)\n",
      "        return q, k, v\n",
      "\n",
      "\n",
      "class LayerNormMLP(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_dim: int,\n",
      "        intermediate_dim: int,\n",
      "        use_bias: bool,\n",
      "        resid_pdrop: float,\n",
      "        activation_function: str,\n",
      "        layer_norm_epsilon: float,\n",
      "        do_layernorm_before: bool = True,\n",
      "        # dtype and device\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        tensor_parallel = constants.tensor_parallel_world_size() > 1\n",
      "        sequence_parallel = constants.sequence_parallel()\n",
      "        gradient_accumulation_fusion = constants.gradient_accumulation_fusion()\n",
      "        if not tensor_parallel and (sequence_parallel or gradient_accumulation_fusion):\n",
      "            global SEQUENCE_PARALLEL_WARNED\n",
      "            if not SEQUENCE_PARALLEL_WARNED:\n",
      "                logger.warning(\n",
      "                    \"sequence_parallel and gradient_accumulation_fusion are only available in model parallel mode\"\n",
      "                )\n",
      "                SEQUENCE_PARALLEL_WARNED = True\n",
      "            sequence_parallel = False\n",
      "            gradient_accumulation_fusion = False\n",
      "        if dtype is None:\n",
      "            dtype = torch.float16\n",
      "\n",
      "        self.ln = nn.LayerNorm(\n",
      "            hidden_dim, eps=layer_norm_epsilon, dtype=dtype, device=device\n",
      "        )\n",
      "        self.c_fc = ColumnParallelLinear(\n",
      "            hidden_dim,\n",
      "            intermediate_dim,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            bias=use_bias,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.c_proj = RowParallelLinear(\n",
      "            intermediate_dim,\n",
      "            hidden_dim,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            bias=use_bias,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.act = get_activation_fn(activation_function)\n",
      "        self.dropout = nn.Dropout(resid_pdrop)\n",
      "        self.do_layernorm_before = do_layernorm_before\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
      "        if self.do_layernorm_before:\n",
      "            hidden_states = self.ln(hidden_states)\n",
      "        hidden_states = self.c_fc(hidden_states)\n",
      "        hidden_states = self.act(hidden_states)\n",
      "        hidden_states = self.c_proj(hidden_states)\n",
      "        return self.dropout(hidden_states)\n",
      "\n",
      "\n",
      "class LlamaLayerNormMLP(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_dim: int,\n",
      "        intermediate_dim: int,\n",
      "        activation_function: str,\n",
      "        use_bias: bool,\n",
      "        # layer norm\n",
      "        layer_norm_epsilon: float = 1e-5,\n",
      "        layer_norm_type: str = \"rms\",\n",
      "        # whether this MLP is used as expert\n",
      "        is_expert: bool = False,\n",
      "        # dtype and device\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.tensor_parallel = constants.tensor_parallel_world_size() > 1\n",
      "        gradient_accumulation_fusion = constants.gradient_accumulation_fusion()\n",
      "        self.is_expert = is_expert\n",
      "        # when used as experts the MLP always compute without sequence parallel\n",
      "        sequence_parallel = constants.sequence_parallel() and not is_expert\n",
      "        if not self.tensor_parallel and (\n",
      "            sequence_parallel or gradient_accumulation_fusion\n",
      "        ):\n",
      "            global SEQUENCE_PARALLEL_WARNED\n",
      "            if not SEQUENCE_PARALLEL_WARNED:\n",
      "                logger.warning(\n",
      "                    \"sequence_parallel and gradient_accumulation_fusion are only available in model parallel mode\"\n",
      "                )\n",
      "                SEQUENCE_PARALLEL_WARNED = True\n",
      "            gradient_accumulation_fusion = False\n",
      "\n",
      "        if dtype is None:\n",
      "            dtype = torch.float16\n",
      "        self.hidden_size = hidden_dim\n",
      "        self.intermediate_size = intermediate_dim\n",
      "        self.use_layer_norm = (\n",
      "            not is_expert\n",
      "        )  # when used as experts layer norm is computed outside\n",
      "\n",
      "        if self.use_layer_norm:\n",
      "            if layer_norm_type == \"rms\":\n",
      "                self.ln = LlamaRMSNorm(\n",
      "                    hidden_dim, eps=layer_norm_epsilon, dtype=dtype, device=device\n",
      "                )\n",
      "            elif layer_norm_type == \"gemma\":\n",
      "                self.ln = GemmaRMSNorm(\n",
      "                    hidden_dim, eps=layer_norm_epsilon, dtype=dtype, device=device\n",
      "                )\n",
      "            else:\n",
      "                raise NotImplementedError()\n",
      "\n",
      "        # TODO: we can fuse gate and up proj, as well as the silu and mul operations\n",
      "        self.gate_proj = ColumnParallelLinear(\n",
      "            self.hidden_size,\n",
      "            self.intermediate_size,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            is_expert=is_expert,\n",
      "            bias=use_bias,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.up_proj = ColumnParallelLinear(\n",
      "            self.hidden_size,\n",
      "            self.intermediate_size,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            is_expert=is_expert,\n",
      "            bias=use_bias,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.down_proj = RowParallelLinear(\n",
      "            self.intermediate_size,\n",
      "            self.hidden_size,\n",
      "            gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "            is_expert=is_expert,\n",
      "            bias=use_bias,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "        self.act_fn = get_activation_fn(activation_function)\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        if self.use_layer_norm:\n",
      "            x = self.ln(x)\n",
      "        _gradient_accumulation_fusion = self.gate_proj.gradient_accumulation_fusion\n",
      "        _sequence_parallel = constants.sequence_parallel() and not self.is_expert\n",
      "        _async_grad_allreduce = not _sequence_parallel\n",
      "        _is_w_parallel = [True, True]\n",
      "\n",
      "        gate, upproj = merged_linear_with_grad_accumulation_and_async_allreduce(\n",
      "            x,\n",
      "            _gradient_accumulation_fusion,\n",
      "            _async_grad_allreduce,\n",
      "            _sequence_parallel,\n",
      "            _is_w_parallel,\n",
      "            self.gate_proj.weight,\n",
      "            self.gate_proj.bias,\n",
      "            self.up_proj.weight,\n",
      "            self.up_proj.bias,\n",
      "        )\n",
      "        return self.down_proj(self.act_fn(gate) * upproj)\n",
      "\n",
      "\n",
      "class _LlamaRMSNorm(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_size: int,\n",
      "        eps: float = 1e-6,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        \"\"\"LlamaRMSNorm is equivalent to T5LayerNorm.\"\"\"\n",
      "        super().__init__()\n",
      "        self.weight = nn.Parameter(torch.ones(hidden_size, dtype=dtype, device=device))\n",
      "        self.variance_epsilon = eps\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n",
      "\n",
      "class GemmaRMSNorm(nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_size: int,\n",
      "        eps: float = 1e-6,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.eps = eps\n",
      "        self.weight = nn.Parameter(torch.ones(hidden_size, dtype=dtype, device=device))\n",
      "\n",
      "    def _norm(self, x):\n",
      "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
      "\n",
      "    def forward(self, x: torch.Tensor):\n",
      "        output = self._norm(x.float())\n",
      "        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\n",
      "        # See https://github.com/huggingface/transformers/pull/29402\n",
      "        output = output * (1.0 + self.weight.float())\n",
      "        return output.type_as(x)\n",
      "\n",
      "\n",
      "if constants.use_te_impl():\n",
      "    try:\n",
      "        # HACK: we use transformer engine's rms norm as long as we can find the transformer engine package\n",
      "        import transformer_engine.pytorch as te\n",
      "\n",
      "        def _TELlamaRMSNorm(\n",
      "            hidden_size: int,\n",
      "            eps: float = 1e-6,\n",
      "            dtype: Optional[torch.dtype] = None,\n",
      "            device: Optional[Union[str, torch.device]] = None,\n",
      "        ):\n",
      "            return te.module.rmsnorm.RMSNorm(\n",
      "                hidden_size=hidden_size,\n",
      "                eps=eps,\n",
      "                sequence_parallel=constants.sequence_parallel(),\n",
      "                params_dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "\n",
      "        LlamaRMSNorm = _TELlamaRMSNorm\n",
      "    except ModuleNotFoundError:\n",
      "        LlamaRMSNorm = _LlamaRMSNorm\n",
      "    except ImportError:\n",
      "        LlamaRMSNorm = _LlamaRMSNorm\n",
      "else:\n",
      "    LlamaRMSNorm = _LlamaRMSNorm\n",
      "\n",
      "if constants.use_te_impl():\n",
      "    from transformer_engine.pytorch.module.layernorm_mlp import (\n",
      "        LayerNormMLP as _TELayerNormMLP,\n",
      "    )\n",
      "\n",
      "    # The same signature as LlamaLayerNormMLP\n",
      "    def LlamaLayerNormMLP(\n",
      "        hidden_dim: int,\n",
      "        intermediate_dim: int,\n",
      "        activation_function: str,\n",
      "        use_bias: bool,\n",
      "        # layer norm\n",
      "        layer_norm_epsilon: float = 1e-5,\n",
      "        layer_norm_type: str = \"rms\",\n",
      "        # moe\n",
      "        is_expert: bool = False,\n",
      "        # dtype and device\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        assert not use_bias\n",
      "        assert layer_norm_type == \"rms\"\n",
      "        assert not is_expert\n",
      "        assert activation_function == \"silu\"\n",
      "        return _TELayerNormMLP(\n",
      "            hidden_size=hidden_dim,\n",
      "            ffn_hidden_size=intermediate_dim,\n",
      "            eps=layer_norm_epsilon,\n",
      "            sequence_parallel=constants.sequence_parallel(),\n",
      "            return_bias=False,\n",
      "            tp_group=constants.tensor_parallel_group(),\n",
      "            tp_size=constants.tensor_parallel_world_size(),\n",
      "            bias=False,\n",
      "            normalization=\"RMSNorm\",\n",
      "            activation=\"swiglu\",\n",
      "            fuse_wgrad_accumulation=constants.gradient_accumulation_fusion(),\n",
      "            params_dtype=dtype,\n",
      "            set_parallel_mode=constants.tensor_parallel_world_size() > 1,\n",
      "            device=device,\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/attn.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.utils.checkpoint\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "from realhf.impl.model.parallelism.tensor_parallel.modules import RowParallelLinear\n",
      "from realhf.impl.model.utils.functional import (\n",
      "    apply_rotary_varlen,\n",
      "    compute_varlen_position_indices,\n",
      "    torch_attn_func,\n",
      ")\n",
      "\n",
      "from .mlp import GemmaRMSNorm, LayerNormQKVLinear, LlamaRMSNorm\n",
      "from .rotary import RotaryEmbedding\n",
      "\n",
      "try:\n",
      "    from flash_attn import (\n",
      "        flash_attn_func,\n",
      "        flash_attn_varlen_func,\n",
      "        flash_attn_with_kvcache,\n",
      "    )\n",
      "except ModuleNotFoundError:\n",
      "    pass\n",
      "\n",
      "logger = logging.getLogger(\"Attention\")\n",
      "\n",
      "\n",
      "class CausalSelfAttentionLayer(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_dim: int,\n",
      "        n_kv_heads: int,\n",
      "        n_q_heads: int,\n",
      "        head_dim: int,\n",
      "        resid_pdrop: float,\n",
      "        attn_pdrop: float,\n",
      "        layer_index: int,\n",
      "        layer_norm_epsilon: float,\n",
      "        scale_attn_by_inverse_layer_idx: bool,\n",
      "        scale_attn_weights: bool,\n",
      "        # llama does not require attention bias\n",
      "        use_attention_bias: bool,\n",
      "        use_attn_proj_bias: bool,\n",
      "        # layer norm type is special for llama\n",
      "        layer_norm_type: Optional[str] = None,\n",
      "        # opt applies layer norm after attn\n",
      "        do_layernorm_before: bool = True,\n",
      "        # qk layer norm (Qwen3)\n",
      "        qk_layernorm: bool = False,\n",
      "        # rotary embedding\n",
      "        apply_rotary: bool = False,\n",
      "        rotary_base: float = 10000.0,\n",
      "        rotary_interleaved: bool = False,  # False for LLaMA, GPT-neoX; True for GPT-J\n",
      "        rotary_scaling: Optional[float] = None,\n",
      "        rotary_scaling_type: Optional[str] = None,\n",
      "        rotary_special_impl: Optional[str] = None,\n",
      "        # device and dtype\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        if dtype is None:\n",
      "            dtype = torch.float16\n",
      "        assert hidden_dim % head_dim == 0, (hidden_dim, head_dim)\n",
      "        self.c_attn = LayerNormQKVLinear(\n",
      "            input_dim=hidden_dim,\n",
      "            head_dim=head_dim,\n",
      "            n_q_heads=n_q_heads,\n",
      "            n_kv_heads=n_kv_heads,\n",
      "            layer_norm_epsilon=layer_norm_epsilon,\n",
      "            layer_norm_type=layer_norm_type,\n",
      "            use_attention_bias=use_attention_bias,\n",
      "            do_layernorm_before=do_layernorm_before,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "            layer_index=layer_index,\n",
      "        )\n",
      "\n",
      "        if constants.tensor_parallel_world_size() > 1:\n",
      "            self.c_proj = RowParallelLinear(\n",
      "                n_q_heads * head_dim,\n",
      "                hidden_dim,\n",
      "                bias=use_attn_proj_bias,\n",
      "                gradient_accumulation_fusion=constants.gradient_accumulation_fusion(),\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "        else:\n",
      "            self.c_proj = nn.Linear(\n",
      "                n_q_heads * head_dim,\n",
      "                hidden_dim,\n",
      "                bias=use_attn_proj_bias,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "\n",
      "        self.qk_layernorm = qk_layernorm\n",
      "        if qk_layernorm:\n",
      "            if layer_norm_type is None:\n",
      "                layer_norm_fn = nn.LayerNorm\n",
      "            elif layer_norm_type == \"rms\":\n",
      "                layer_norm_fn = LlamaRMSNorm\n",
      "            elif layer_norm_type == \"gemma\":\n",
      "                layer_norm_fn = GemmaRMSNorm\n",
      "            self.q_ln = layer_norm_fn(\n",
      "                head_dim, eps=layer_norm_epsilon, dtype=dtype, device=device\n",
      "            )\n",
      "            self.k_ln = layer_norm_fn(\n",
      "                head_dim, eps=layer_norm_epsilon, dtype=dtype, device=device\n",
      "            )\n",
      "\n",
      "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
      "\n",
      "        self.attn_pdrop = attn_pdrop\n",
      "\n",
      "        self.applied_attn_pdrop = attn_pdrop\n",
      "\n",
      "        self.apply_rotary = apply_rotary\n",
      "        self.rotary_interleaved = rotary_interleaved\n",
      "        if self.apply_rotary:\n",
      "            # Will layzily update the cache sequence length of cache.,\n",
      "            # so we don't need to pass in max_positions.\n",
      "            self.rotary_emb = RotaryEmbedding(\n",
      "                head_dim,\n",
      "                base=rotary_base,\n",
      "                scale_factor=rotary_scaling,\n",
      "                scale_type=rotary_scaling_type,\n",
      "                interleaved=rotary_interleaved,\n",
      "                device=device,\n",
      "                special_impl=rotary_special_impl,\n",
      "            )\n",
      "            self.rotary_special_impl = rotary_special_impl\n",
      "\n",
      "        # constant\n",
      "        self.nq = n_q_heads\n",
      "        self.nkv = n_kv_heads\n",
      "        if self.nq % self.nkv != 0:\n",
      "            raise ValueError(\n",
      "                f\"n_kv_heads ({self.nkv}) must divide n_q_heads ({self.nq}).\"\n",
      "            )\n",
      "        self.d = head_dim\n",
      "\n",
      "        self.layer_index = layer_index\n",
      "\n",
      "        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n",
      "        self.scale_attn_weights = scale_attn_weights\n",
      "\n",
      "    def train(self, mode: bool):\n",
      "        if not mode:\n",
      "            self.applied_attn_pdrop = 0.0\n",
      "        else:\n",
      "            self.applied_attn_pdrop = self.attn_pdrop\n",
      "        super().train(mode)\n",
      "        return self\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        cu_seqlens: Optional[torch.Tensor] = None,\n",
      "        k_cache: Optional[torch.Tensor] = None,\n",
      "        v_cache: Optional[torch.Tensor] = None,\n",
      "        cache_seqlens: Optional[Union[int, torch.Tensor]] = None,\n",
      "        max_seqlen: Optional[int] = None,\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "        # input shape: [bs, seq, hidden_dim]\n",
      "\n",
      "        # NOTE: we must ensure the passed-in argument is an interger\n",
      "        # if we convert the argument to implicitly when calling rotary embedding or flash-attn,\n",
      "        # aten::item will be called, which will cause a device-host sync and slow down performance.\n",
      "        assert max_seqlen is None or isinstance(max_seqlen, int), type(max_seqlen)\n",
      "        assert cu_seqlens is None or cu_seqlens.dtype == torch.int32\n",
      "\n",
      "        # default upcast, scale\n",
      "        if self.scale_attn_by_inverse_layer_idx:\n",
      "            unscale = self.layer_index + 1\n",
      "            scale_factor = unscale**-1\n",
      "        else:\n",
      "            unscale = 1.0\n",
      "            scale_factor = 1\n",
      "        if self.scale_attn_weights:\n",
      "            scale_factor /= self.d**0.5\n",
      "\n",
      "        q, k, v = self.c_attn(hidden_states)\n",
      "\n",
      "        if self.qk_layernorm:\n",
      "            q = self.q_ln(q)\n",
      "            k = self.k_ln(k)\n",
      "\n",
      "        if self.apply_rotary and (k_cache is None or str(q.device) == \"cpu\"):\n",
      "            # otherwise, we input rotary cos/sin directly into flash_attn_with_kvcache\n",
      "            rotary_cache_len = max_seqlen\n",
      "            if k_cache is not None and str(q.device) == \"cpu\":\n",
      "                rotary_cache_len = k_cache.shape[1]\n",
      "            self.rotary_emb._update_cos_sin_cache(rotary_cache_len, q.device, q.dtype)\n",
      "            rotary_indices = compute_varlen_position_indices(q.shape[0], cu_seqlens)\n",
      "            qk = apply_rotary_varlen(\n",
      "                torch.cat([q, k], dim=-2),\n",
      "                cos=self.rotary_emb._cos_cached,\n",
      "                sin=self.rotary_emb._sin_cached,\n",
      "                cu_seqlens=cu_seqlens,\n",
      "                interleaved=self.rotary_emb.interleaved,\n",
      "                rotary_indices=rotary_indices,\n",
      "                seqlen_offsets=cache_seqlens,\n",
      "                special_impl=self.rotary_special_impl,\n",
      "            )\n",
      "            q, k = qk.split((q.shape[-2], k.shape[-2]), dim=-2)\n",
      "        elif self.apply_rotary:\n",
      "            self.rotary_emb._update_cos_sin_cache(\n",
      "                k_cache.shape[1], device=q.device, dtype=q.dtype\n",
      "            )\n",
      "            # Rotary cos/sin will be automatically offset by cache_seqlens in flash_attn.\n",
      "            rotary_cos, rotary_sin = (\n",
      "                self.rotary_emb._cos_cached,\n",
      "                self.rotary_emb._sin_cached,\n",
      "            )\n",
      "        else:\n",
      "            rotary_cos = rotary_sin = None\n",
      "\n",
      "        if str(q.device) == \"cpu\":\n",
      "            cu_seqlens_k = cu_seqlens\n",
      "            max_seqlen_k = max_seqlen\n",
      "            if k_cache is not None:\n",
      "                new_k, new_v = [], []\n",
      "                for i, cache_len in enumerate(cache_seqlens):\n",
      "                    assert k.shape[0] == cu_seqlens.shape[0] - 1, (k.shape, cu_seqlens)\n",
      "                    k_cache[i, cache_len] = k[i]\n",
      "                    new_k.append(k_cache[i, : cache_len + 1])\n",
      "                    v_cache[i, cache_len] = v[i]\n",
      "                    new_v.append(v_cache[i, : cache_len + 1])\n",
      "                k = torch.cat(new_k, dim=0)\n",
      "                v = torch.cat(new_v, dim=0)\n",
      "                cu_seqlens_k = torch.nn.functional.pad(\n",
      "                    (cache_seqlens + 1).cumsum(0), (1, 0)\n",
      "                )\n",
      "                max_seqlen_k = max(cache_seqlens) + 1\n",
      "                cu_seqlens = torch.arange(\n",
      "                    cu_seqlens_k.shape[0], device=k.device, dtype=k.dtype\n",
      "                )\n",
      "                max_seqlen = 1\n",
      "            # Use vanilla pytorch attention, for debugging.\n",
      "            hidden_states = torch_attn_func(\n",
      "                q,\n",
      "                k,\n",
      "                v,\n",
      "                causal=True,\n",
      "                cu_seqlens_q=cu_seqlens,\n",
      "                max_seqlen_q=max_seqlen,\n",
      "                cu_seqlens_k=cu_seqlens_k,\n",
      "                max_seqlen_k=max_seqlen_k,\n",
      "                dropout_p=self.applied_attn_pdrop,\n",
      "                softmax_scale=scale_factor,\n",
      "                upcast_unscale=unscale,\n",
      "            )\n",
      "        elif k_cache is not None:\n",
      "            # k_cache/v_cache shape: [bs, max_seq, n_kv_heads, head_dim]\n",
      "            if cache_seqlens is None:\n",
      "                raise RuntimeError(\n",
      "                    \"cache_seqlens must be provided if kv_cache is not None.\"\n",
      "                )\n",
      "            q = q.unsqueeze(1)\n",
      "            k = k.unsqueeze(1)\n",
      "            v = v.unsqueeze(1)\n",
      "            # k_cache and v_cache will be modified in-place.\n",
      "            hidden_states = flash_attn_with_kvcache(\n",
      "                q,\n",
      "                k_cache,\n",
      "                v_cache,\n",
      "                k=k,\n",
      "                v=v,\n",
      "                cache_seqlens=cache_seqlens,\n",
      "                softmax_scale=scale_factor,\n",
      "                causal=False,  # True or False doesn't matter because seqlen=1\n",
      "                rotary_cos=rotary_cos,\n",
      "                rotary_sin=rotary_sin,\n",
      "                rotary_interleaved=self.rotary_interleaved,\n",
      "            )\n",
      "            hidden_states = hidden_states.squeeze(1)\n",
      "        elif cu_seqlens is not None:\n",
      "            assert max_seqlen is not None\n",
      "            assert len(q.shape) == 3\n",
      "            hidden_states = flash_attn_varlen_func(\n",
      "                q,\n",
      "                k,\n",
      "                v,\n",
      "                cu_seqlens,\n",
      "                cu_seqlens,\n",
      "                max_seqlen,\n",
      "                max_seqlen,\n",
      "                dropout_p=self.applied_attn_pdrop,\n",
      "                softmax_scale=scale_factor,\n",
      "                causal=True,\n",
      "            )\n",
      "        else:\n",
      "            raise NotImplementedError(\n",
      "                \"Don't know which attention implementation to use.\"\n",
      "            )\n",
      "        hidden_states = self.c_proj(hidden_states.flatten(start_dim=-2))\n",
      "        hidden_states = self.resid_dropout(hidden_states)\n",
      "        return hidden_states, k, v\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from .activations import *\n",
      "from .attn import *\n",
      "from .embedding import *\n",
      "from .mlp import *\n",
      "from .moe import *\n",
      "from .rms import *\n",
      "from .rotary import *\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/embedding.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.nn import init\n",
      "\n",
      "from realhf.impl.model.parallelism.tensor_parallel.modules import ParallelEmbedding\n",
      "\n",
      "\n",
      "class OffsetPositionalEmbedding(nn.Embedding):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        num_embeddings: int,\n",
      "        embedding_dim: int,\n",
      "        offset: int,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
      "        # and adjust num_embeddings appropriately. Other models don't have this hack\n",
      "        self.__offset = offset\n",
      "        super().__init__(\n",
      "            num_embeddings + self.__offset,\n",
      "            embedding_dim,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "\n",
      "    def forward(self, position_ids: torch.LongTensor):\n",
      "        return super().forward(position_ids + self.__offset)\n",
      "\n",
      "\n",
      "class OffsetParallelPositionalEmbedding(ParallelEmbedding):\n",
      "    def __init__(\n",
      "        self,\n",
      "        num_embeddings: int,\n",
      "        embedding_dim: int,\n",
      "        offset: int,\n",
      "        init_method=init.xavier_normal_,\n",
      "        # params_dtype: torch.dtype=torch.float32,\n",
      "        perform_initialization: bool = True,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        self.__offset = offset\n",
      "        super(OffsetParallelPositionalEmbedding, self).__init__(\n",
      "            num_embeddings=num_embeddings + offset,\n",
      "            embedding_dim=embedding_dim,\n",
      "            init_method=init_method,\n",
      "            perform_initialization=perform_initialization,\n",
      "            dtype=dtype,\n",
      "            device=device,\n",
      "        )\n",
      "\n",
      "    def forward(self, input_: torch.LongTensor) -> torch.Tensor:\n",
      "        return super().forward(input_ + self.__offset)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/moe/layer.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "from typing import Optional, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.impl.model.modules.mlp import GemmaRMSNorm, LlamaRMSNorm\n",
      "from realhf.impl.model.modules.moe.experts import GroupedMLP, SequentialMLP\n",
      "from realhf.impl.model.modules.moe.router import TopKRouter\n",
      "from realhf.impl.model.modules.moe.token_dispatcher import MoETokenDispatcher\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class LayerNormMoELayer(torch.nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ReaLModelConfig,\n",
      "        layer_idx: int,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super(LayerNormMoELayer, self).__init__()\n",
      "\n",
      "        self.config = config\n",
      "        self.dtype = dtype\n",
      "        self.device = device\n",
      "        self.num_experts = self.config.moe.num_experts\n",
      "\n",
      "        if config.layer_norm_type is None:\n",
      "            layer_norm_fn = nn.LayerNorm\n",
      "        elif config.layer_norm_type == \"rms\":\n",
      "            layer_norm_fn = LlamaRMSNorm\n",
      "        elif config.layer_norm_type == \"gemma\":\n",
      "            layer_norm_fn = GemmaRMSNorm\n",
      "        self.ln = layer_norm_fn(\n",
      "            config.hidden_dim, eps=config.layer_norm_epsilon, dtype=dtype, device=device\n",
      "        )\n",
      "\n",
      "        self.router = TopKRouter(config=self.config, layer_idx=layer_idx)\n",
      "        self.token_dispatcher = MoETokenDispatcher(config=self.config)\n",
      "        if config.moe.use_grouped_gemm and dtype == torch.bfloat16:\n",
      "            self.experts = GroupedMLP(self.config, dtype=dtype, device=device)\n",
      "        else:\n",
      "            if config.moe.use_grouped_gemm:\n",
      "                logger.warning(\n",
      "                    \"GroupedGemm only supports bfloat16. Fallback to SequentialMLP.\"\n",
      "                )\n",
      "            self.experts = SequentialMLP(self.config, dtype=dtype, device=device)\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor):\n",
      "        hidden_states = self.ln(hidden_states)\n",
      "        probs, indices = self.router(hidden_states)\n",
      "        (dispatched_input, tokens_per_expert) = self.token_dispatcher.token_permutation(\n",
      "            hidden_states, probs, indices\n",
      "        )\n",
      "        expert_output = self.experts(dispatched_input, tokens_per_expert)\n",
      "        output = self.token_dispatcher.token_unpermutation(\n",
      "            expert_output,\n",
      "        )\n",
      "        return output\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/moe/token_dispatcher.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "from typing import List, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.impl.model.parallelism.tensor_parallel.mappings import (\n",
      "    gather_from_sequence_parallel_region,\n",
      "    scatter_to_sequence_parallel_region,\n",
      ")\n",
      "from realhf.impl.model.utils.moe import custom_histc, permute, unpermute\n",
      "\n",
      "\n",
      "class MoETokenDispatcher:\n",
      "    \"\"\"AlltoAll Based Token dispatcher.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ReaLModelConfig,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ) -> None:\n",
      "        \"\"\"Initialize the AlltoAll token dispatcher. Currently does not support\n",
      "        expert parallel.\n",
      "\n",
      "        Args:\n",
      "            num_local_experts (int): Number of local experts on the current device.\n",
      "            config (TransformerConfig): Configuration for the transformer model.\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.dtype = dtype\n",
      "        self.device = device\n",
      "\n",
      "        self.hidden_shape = None\n",
      "        self.num_experts = self.config.moe.num_experts\n",
      "\n",
      "        assert self.num_experts > 0, \"Expected at least one expert\"\n",
      "\n",
      "        self.router_topk = self.config.moe.top_k\n",
      "        self.probs = None\n",
      "\n",
      "        # Token drop and padding.\n",
      "        # We need to keep track of the token num if we drop tokens without padding them.\n",
      "        self.num_out_tokens = None\n",
      "        # Drop and pad the input to capacity.\n",
      "        self.capacity_factor = self.config.moe.capacity_factor\n",
      "        self.drop_and_pad = self.config.moe.pad_to_capacity\n",
      "        if self.drop_and_pad:\n",
      "            assert self.capacity_factor is not None\n",
      "        self.capacity = torch.tensor(0, dtype=torch.long, device=self.device)\n",
      "\n",
      "        self.num_tokens_per_expert = torch.zeros(\n",
      "            (self.num_experts,), dtype=torch.long, device=self.device\n",
      "        )\n",
      "        self.num_out_tokens = torch.tensor(0, dtype=torch.long, device=self.device)\n",
      "\n",
      "    def preprocess(self, indices: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"Preprocess token indices for AlltoAll communication and token\n",
      "        permutation. This method computes the number of tokens assigned to each\n",
      "        expert based on the input indices. It also initializes the necessary\n",
      "        data structures for AlltoAll communication, such as input and output\n",
      "        splits, and the mapping between global tokens and local experts.\n",
      "\n",
      "        Args:\n",
      "            indices (torch.Tensor): Tensor of indices mapping tokens to experts.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Tensor containing the number of tokens assigned to local expert.\n",
      "        \"\"\"\n",
      "        self.num_tokens_per_expert = custom_histc(\n",
      "            indices,\n",
      "            bins=self.num_experts,\n",
      "            min=0,\n",
      "            max=self.num_experts,\n",
      "        )\n",
      "        # num_local_tokens_per_expert: [num_experts]\n",
      "\n",
      "        if self.drop_and_pad:\n",
      "            # probs: [num_experts, capacity]\n",
      "            self.capacity = self.probs.size(1)\n",
      "            self.num_tokens_per_expert.fill_(self.capacity)\n",
      "            return self.num_tokens_per_expert\n",
      "\n",
      "        self.num_out_tokens = self.num_tokens_per_expert.sum()\n",
      "        return self.num_tokens_per_expert\n",
      "\n",
      "    def token_permutation(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        probs: torch.Tensor,\n",
      "        indices: torch.Tensor,\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
      "        \"\"\"Dispatch tokens to local experts using AlltoAll communication.\n",
      "\n",
      "        Args:\n",
      "            hidden_states (torch.Tensor): Input token embeddings.\n",
      "            probs (torch.Tensor): Probs of tokens assigned to experts.\n",
      "            indices (torch.Tensor): Indices of tokens assigned to experts.\n",
      "\n",
      "        Returns:\n",
      "            Tuple[torch.Tensor, torch.Tensor]:\n",
      "                - Permuted token embeddings for local experts.\n",
      "                - Number of tokens per expert.\n",
      "        \"\"\"\n",
      "\n",
      "        # Preprocess: Get the metadata for communication, permutation and computation operations.\n",
      "        self.hidden_shape = hidden_states.shape\n",
      "        self.probs = probs\n",
      "        assert probs.dim() == 2, \"Expected 2D tensor for probs\"\n",
      "        assert indices.dim() == 2, \"Expected 2D tensor for indices\"\n",
      "        hidden_states = hidden_states.view(-1, self.hidden_shape[-1])\n",
      "        tokens_per_expert = self.preprocess(indices)\n",
      "\n",
      "        if constants.sequence_parallel():\n",
      "            hidden_states = gather_from_sequence_parallel_region(hidden_states)\n",
      "\n",
      "        # Permutation\n",
      "        self.hiddden_shape_before_permute = hidden_states.shape\n",
      "\n",
      "        permutated_input_tokens, self.reversed_input_permutation_mapping = permute(\n",
      "            hidden_states,\n",
      "            indices,\n",
      "            num_out_tokens=self.num_out_tokens,\n",
      "            padded_mode=self.drop_and_pad,\n",
      "        )\n",
      "\n",
      "        return permutated_input_tokens, tokens_per_expert\n",
      "\n",
      "    def token_unpermutation(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "    ) -> torch.Tensor:\n",
      "        \"\"\"Reverse the token permutation to restore the original order.\n",
      "\n",
      "        Args:\n",
      "            hidden_states (torch.Tensor): Output from local experts.\n",
      "            bias (torch.Tensor, optional): Bias tensor (not supported).\n",
      "\n",
      "        Returns:\n",
      "            Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
      "                - Unpermuted token embeddings in the original order.\n",
      "                - None (bias is not supported).\n",
      "        \"\"\"\n",
      "\n",
      "        # Unpermutation\n",
      "        output = unpermute(\n",
      "            hidden_states,\n",
      "            self.reversed_input_permutation_mapping,\n",
      "            probs=self.probs,\n",
      "            padded_mode=self.drop_and_pad,\n",
      "            restore_shape=self.hiddden_shape_before_permute,\n",
      "        )\n",
      "\n",
      "        if constants.sequence_parallel():\n",
      "            output = scatter_to_sequence_parallel_region(output)\n",
      "\n",
      "        # Reshape the output tensor\n",
      "        output = output.view(self.hidden_shape)\n",
      "        return output\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/moe/router.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "from typing import Callable\n",
      "\n",
      "import torch\n",
      "import torch.nn.init as init\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.impl.model.parallelism.tensor_parallel.mappings import (\n",
      "    gather_from_sequence_parallel_region,\n",
      ")\n",
      "from realhf.impl.model.utils.moe import (\n",
      "    MoEAuxLossAutoScaler,\n",
      "    sinkhorn,\n",
      "    switch_load_balancing_loss_func,\n",
      "    topk_softmax_with_capacity,\n",
      "    update_aux_losses_tracker,\n",
      "    z_loss_func,\n",
      ")\n",
      "\n",
      "\n",
      "class TopKRouter(torch.nn.Module):\n",
      "    \"\"\"Route each token to the top-k experts.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ReaLModelConfig,\n",
      "        layer_idx: int,\n",
      "        init_method: Callable = init.xavier_normal_,\n",
      "    ) -> None:\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "        self.num_experts = self.config.moe.num_experts\n",
      "        self.hidden_dim = config.hidden_dim\n",
      "\n",
      "        # Initialize the gate weights.\n",
      "        self.weight = torch.nn.Parameter(\n",
      "            torch.empty((self.num_experts, self.hidden_dim))\n",
      "        )\n",
      "        init_method(self.weight)\n",
      "\n",
      "        self.input_jitter = None\n",
      "        self.layer_idx = layer_idx\n",
      "        self.num_layers = config.n_layers\n",
      "\n",
      "    def gating(self, input: torch.Tensor):\n",
      "        \"\"\"Forward pass of the router gate.\"\"\"\n",
      "        logits = torch.nn.functional.linear(input, self.weight)\n",
      "        return logits\n",
      "\n",
      "    def sinkhorn_load_balancing(self, logits: torch.Tensor):\n",
      "        \"\"\"Apply sinkhorn routing to the logits tensor.\"\"\"\n",
      "\n",
      "        def _sinkhorn_activation(logits):\n",
      "            if self.config.moe.top_k == 1:\n",
      "                logits = torch.sigmoid(logits)\n",
      "            else:  # k > 1\n",
      "                logits = torch.softmax(logits, dim=-1, dtype=torch.float32).type_as(\n",
      "                    logits\n",
      "                )\n",
      "            return logits\n",
      "\n",
      "        if self.training:\n",
      "            with torch.no_grad():\n",
      "                norm_logits = sinkhorn(\n",
      "                    logits.to(dtype=torch.float32)\n",
      "                )  # explicit fp32 conversion for stability\n",
      "                _, indices = torch.topk(norm_logits, k=self.config.moe.top_k, dim=1)\n",
      "            logits = _sinkhorn_activation(logits)\n",
      "            scores = torch.gather(logits, 1, indices)\n",
      "        else:\n",
      "            logits = _sinkhorn_activation(logits)\n",
      "            scores, indices = torch.topk(logits, k=self.config.moe.top_k, dim=1)\n",
      "        return scores, indices\n",
      "\n",
      "    def aux_loss_load_balancing(self, logits: torch.Tensor):\n",
      "        \"\"\"Apply loss-based load balancing to the logits tensor.\n",
      "\n",
      "        Args:\n",
      "            logits (torch.Tensor): the logits tensor after gating, shape: [num_tokens, num_experts].\n",
      "\n",
      "        Returns:\n",
      "            probs (torch.Tensor): the probabilities tensor after load balancing.\n",
      "            indices (torch.Tensor): the indices tensor after top-k selection.\n",
      "        \"\"\"\n",
      "        probs, indices, tokens_per_expert = topk_softmax_with_capacity(\n",
      "            logits,\n",
      "            self.config.moe.top_k,\n",
      "            capacity_factor=self.config.moe.capacity_factor,\n",
      "            pad_to_capacity=self.config.moe.pad_to_capacity,\n",
      "            drop_policy=self.config.moe.token_drop_policy,\n",
      "        )\n",
      "\n",
      "        # Apply load balancing loss\n",
      "        scores = torch.softmax(logits, dim=-1, dtype=torch.float32)\n",
      "        probs = self.apply_load_balancing_loss(\n",
      "            scores, tokens_per_expert, activation=probs\n",
      "        )\n",
      "        return probs, indices\n",
      "\n",
      "    def apply_load_balancing_loss(\n",
      "        self,\n",
      "        probs: torch.Tensor,\n",
      "        num_local_tokens_per_expert: torch.Tensor,\n",
      "        activation: torch.Tensor,\n",
      "    ):\n",
      "        \"\"\"Applies auxiliary loss to the MoE layer.\n",
      "\n",
      "        Args:\n",
      "            probs (torch.Tensor): The probs output by the router for each token. [num_tokens, num_experts]\n",
      "            num_local_tokens_per_expert (torch.Tensor): The number of tokens per expert. [num_experts]\n",
      "            activation (torch.Tensor): The activation tensor to attach the gradient function to.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: The activation tensor with the attached gradient function.\n",
      "        \"\"\"\n",
      "        moe_aux_loss_coeff = self.config.moe.aux_loss_coeff\n",
      "        moe_aux_loss_coeff /= constants.tensor_parallel_world_size()\n",
      "        scale_for_logging = 1.0\n",
      "        if constants.sequence_parallel():\n",
      "            scale_for_logging *= constants.tensor_parallel_world_size()\n",
      "\n",
      "        aux_loss = switch_load_balancing_loss_func(\n",
      "            probs,\n",
      "            num_local_tokens_per_expert,\n",
      "            self.config.moe.top_k,\n",
      "            moe_aux_loss_coeff,\n",
      "            sequence_partition_group=(\n",
      "                constants.tensor_parallel_group()\n",
      "                if constants.sequence_parallel()\n",
      "                else None\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        update_aux_losses_tracker(\n",
      "            \"load_balancing_loss\",\n",
      "            aux_loss / moe_aux_loss_coeff,\n",
      "            self.layer_idx,\n",
      "            self.num_layers,\n",
      "        )\n",
      "        activation = MoEAuxLossAutoScaler.apply(activation, aux_loss)\n",
      "        return activation\n",
      "\n",
      "    def apply_z_loss(self, logits):\n",
      "        \"\"\"Encourages the router's logits to remain small to enhance stability.\n",
      "        Please refer to the ST-MoE paper (https://arxiv.org/pdf/2202.08906.pdf) for details.\n",
      "\n",
      "        Args:\n",
      "            logits (torch.Tensor): The logits of the router.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: The logits after applying the z-loss.\n",
      "        \"\"\"\n",
      "        if self.config.moe.z_loss_coeff > 0:\n",
      "            moe_z_loss_coeff = (\n",
      "                self.config.moe.z_loss_coeff / constants.tensor_parallel_world_size()\n",
      "            )\n",
      "            z_loss = z_loss_func(logits, moe_z_loss_coeff)\n",
      "            logits = MoEAuxLossAutoScaler.apply(logits, z_loss)\n",
      "            update_aux_losses_tracker(\n",
      "                \"z_loss\",\n",
      "                z_loss / moe_z_loss_coeff,\n",
      "                self.layer_idx,\n",
      "                self.num_layers,\n",
      "            )\n",
      "        return logits\n",
      "\n",
      "    def apply_input_jitter(self, input: torch.Tensor):\n",
      "        \"\"\"Add noise to the input tensor.\n",
      "        Refer to https://arxiv.org/abs/2101.03961.\n",
      "\n",
      "        Args:\n",
      "            input (Tensor): Input tensor.\n",
      "\n",
      "        Returns:\n",
      "            Tensor: Jittered input.\n",
      "        \"\"\"\n",
      "        if self.config.moe.input_jitter_eps is not None:\n",
      "            eps = self.config.moe.input_jitter_eps\n",
      "            if self.input_jitter is None:\n",
      "                self.input_jitter = torch.distributions.uniform.Uniform(\n",
      "                    torch.tensor(1.0 - eps, device=input.device),\n",
      "                    torch.tensor(1.0 + eps, device=input.device),\n",
      "                ).rsample\n",
      "\n",
      "            input = (input * self.input_jitter(input.shape)).to(input.dtype)\n",
      "        return input\n",
      "\n",
      "    def routing(self, logits: torch.Tensor):\n",
      "        \"\"\"Top-k routing function.\n",
      "\n",
      "        Args:\n",
      "            logits (torch.Tensor): Logits tensor after gating.\n",
      "\n",
      "        Returns:\n",
      "            probs (torch.Tensor): the probabilities tensor after load balancing.\n",
      "            indices (torch.Tensor): the indices tensor after top-k selection.\n",
      "        \"\"\"\n",
      "        logits = logits.view(-1, self.num_experts)\n",
      "\n",
      "        # Apply Z-Loss\n",
      "        logits = self.apply_z_loss(logits)\n",
      "\n",
      "        if constants.sequence_parallel():\n",
      "            # Gather the logits from the TP region\n",
      "            logits = gather_from_sequence_parallel_region(logits)\n",
      "\n",
      "        if self.config.moe.routing_type == \"sinkhorn\":\n",
      "            scores, indices = self.sinkhorn_load_balancing(logits)\n",
      "        elif self.config.moe.routing_type == \"aux_loss\":\n",
      "            scores, indices = self.aux_loss_load_balancing(logits)\n",
      "        elif self.config.moe.routing_type == \"none\":\n",
      "            # A naive top-k routing without load balancing\n",
      "            scores, indices, _ = topk_softmax_with_capacity(\n",
      "                logits,\n",
      "                self.config.moe.top_k,\n",
      "                capacity_factor=self.config.moe.capacity_factor,\n",
      "                pad_to_capacity=self.config.moe.pad_to_capacity,\n",
      "                drop_policy=self.config.moe.token_drop_policy,\n",
      "            )\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                f\"Unsupported MoE routing type: {self.config.moe.routing_type}\"\n",
      "            )\n",
      "\n",
      "        return scores, indices\n",
      "\n",
      "    def forward(self, input: torch.Tensor):\n",
      "        \"\"\"Forward pass of the router.\n",
      "\n",
      "        Args:\n",
      "            input (torch.Tensor): Input tensor.\n",
      "        \"\"\"\n",
      "        # Apply input jitter\n",
      "        input = self.apply_input_jitter(input)\n",
      "        logits = self.gating(input)\n",
      "        logits = logits.view(-1, self.num_experts)\n",
      "        # logits (S/TP, E)\n",
      "        scores, indices = self.routing(logits)\n",
      "        return scores, indices\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/moe/experts.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "from typing import Callable, List, Optional, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn.init as init\n",
      "from torch.nn.parameter import Parameter\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "from realhf.api.core.model_api import ReaLModelConfig\n",
      "from realhf.impl.model.modules.mlp import LlamaLayerNormMLP, get_activation_fn\n",
      "from realhf.impl.model.parallelism.tensor_parallel.mappings import (\n",
      "    copy_to_tensor_model_parallel_region,\n",
      "    reduce_from_tensor_model_parallel_region,\n",
      ")\n",
      "from realhf.impl.model.parallelism.tensor_parallel.utils import divide\n",
      "from realhf.impl.model.utils.random import _initialize_affine_weight_gpu\n",
      "\n",
      "try:\n",
      "    import grouped_gemm\n",
      "except ImportError:\n",
      "    grouped_gemm = None\n",
      "\n",
      "\n",
      "class SequentialMLP(torch.nn.Module):\n",
      "    \"\"\"An implementation of the Experts layer using a sequence of MLP layers.\n",
      "\n",
      "    This class executes each expert sequentially.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ReaLModelConfig,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "\n",
      "        self.num_experts = self.config.moe.num_experts\n",
      "        self.local_experts = torch.nn.ModuleList()\n",
      "\n",
      "        for _ in range(self.num_experts):\n",
      "            expert = LlamaLayerNormMLP(\n",
      "                hidden_dim=config.hidden_dim,\n",
      "                intermediate_dim=config.intermediate_dim,\n",
      "                activation_function=config.activation_function,\n",
      "                use_bias=config.use_mlp_bias,\n",
      "                is_expert=True,\n",
      "                dtype=dtype,\n",
      "                device=device,\n",
      "            )\n",
      "            self.local_experts.append(expert)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        permuted_local_hidden_states: torch.Tensor,\n",
      "        tokens_per_expert: torch.Tensor,\n",
      "    ):\n",
      "        output_local = torch.zeros_like(permuted_local_hidden_states)\n",
      "        cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=0)\n",
      "        # Insert zero at the begining for offset index's convenience\n",
      "        zero_tensor = torch.zeros(1, dtype=torch.long, device=cumsum_num_tokens.device)\n",
      "        cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n",
      "\n",
      "        for expert_num, expert in enumerate(self.local_experts):\n",
      "            start = cumsum_num_tokens[expert_num]\n",
      "            end = cumsum_num_tokens[expert_num + 1]\n",
      "            hidden = permuted_local_hidden_states[start:end]\n",
      "            output = expert(hidden)\n",
      "            output_local[start:end] = output\n",
      "\n",
      "        return output_local\n",
      "\n",
      "\n",
      "class ExpertParam(torch.nn.Module):\n",
      "    \"\"\"A dummy class that maps weight tensors in GroupedMLP to pytorch\n",
      "    parameters for compatibility of weight saving/loading.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        gate_proj: torch.Tensor,\n",
      "        up_proj: torch.Tensor,\n",
      "        down_proj: torch.Tensor,\n",
      "    ):\n",
      "        class LinearParam(torch.nn.Module):\n",
      "            def __init__(self, param: torch.Tensor):\n",
      "                super(LinearParam, self).__init__()\n",
      "                self.weight = Parameter(param)\n",
      "\n",
      "        super(ExpertParam, self).__init__()\n",
      "\n",
      "        self.gate_proj = LinearParam(gate_proj)\n",
      "        self.up_proj = LinearParam(up_proj)\n",
      "        self.down_proj = LinearParam(down_proj)\n",
      "\n",
      "\n",
      "class GroupedMLP(torch.nn.Module):\n",
      "    \"\"\"An efficient implementation of the Experts layer using CUTLASS GroupedGEMM.\n",
      "    See https://github.com/tgale96/grouped_gemm for details.\n",
      "\n",
      "    This class is designed to execute multiple experts in parallel, thereby maximizing computational efficiency.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ReaLModelConfig,\n",
      "        init_method: Callable = init.xavier_normal_,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        assert (\n",
      "            not constants.sequence_parallel()\n",
      "        ), \"Grouped GEMM does not support sequence parallel\"\n",
      "\n",
      "        self.config = config\n",
      "        self.dtype = dtype\n",
      "        self.device = device\n",
      "        self.num_experts = config.moe.num_experts\n",
      "\n",
      "        assert grouped_gemm is not None, \"Grouped GEMM is not available.\"\n",
      "\n",
      "        self.activation_func = get_activation_fn(self.config.activation_function)\n",
      "\n",
      "        # How many feature each rank holds for fc1 and fc2, respectively.\n",
      "        tp_size = constants.tensor_parallel_world_size()\n",
      "        intermediate_dim_per_partition = divide(self.config.intermediate_dim, tp_size)\n",
      "\n",
      "        # Note: The current kernel implementations of grouped_gemm\n",
      "        # does not support transposition with CUTLASS grouped GEMM\n",
      "        # and as a result we avoid allocate the transpose of weights.\n",
      "        self.grouped_gate_proj = torch.empty(\n",
      "            self.num_experts,\n",
      "            self.config.hidden_dim,\n",
      "            intermediate_dim_per_partition,\n",
      "            device=self.device,\n",
      "            dtype=self.dtype,\n",
      "        )\n",
      "        self.grouped_up_proj = torch.empty(\n",
      "            self.num_experts,\n",
      "            self.config.hidden_dim,\n",
      "            intermediate_dim_per_partition,\n",
      "            device=self.device,\n",
      "            dtype=self.dtype,\n",
      "        )\n",
      "        self.grouped_down_proj = torch.empty(\n",
      "            self.num_experts,\n",
      "            intermediate_dim_per_partition,\n",
      "            self.config.hidden_dim,\n",
      "            device=self.device,\n",
      "            dtype=self.dtype,\n",
      "        )\n",
      "        # Initialize weight.\n",
      "        _initialize_affine_weight_gpu(\n",
      "            self.grouped_gate_proj,\n",
      "            init_method,\n",
      "            partition_dim=1,\n",
      "        )\n",
      "        _initialize_affine_weight_gpu(\n",
      "            self.grouped_up_proj,\n",
      "            init_method,\n",
      "            partition_dim=0,\n",
      "        )\n",
      "        _initialize_affine_weight_gpu(\n",
      "            self.grouped_down_proj,\n",
      "            init_method,\n",
      "            partition_dim=0,\n",
      "        )\n",
      "\n",
      "        # Parameters for weight loading\n",
      "        self.local_experts = torch.nn.ModuleList()\n",
      "        for i in range(self.num_experts):\n",
      "            expert = ExpertParam(\n",
      "                self.grouped_gate_proj[i, :].transpose_(0, 1),\n",
      "                self.grouped_up_proj[i, :].transpose_(0, 1),\n",
      "                self.grouped_down_proj[i, :].transpose_(0, 1),\n",
      "            )\n",
      "            self.local_experts.append(expert)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        permuted_local_hidden_states: torch.Tensor,\n",
      "        tokens_per_expert: torch.Tensor,\n",
      "    ):\n",
      "        tokens_per_expert = tokens_per_expert.cpu()\n",
      "        if permuted_local_hidden_states.nelement() != 0:\n",
      "            if constants.tensor_parallel_world_size() > 1:\n",
      "                permuted_local_hidden_states = copy_to_tensor_model_parallel_region(\n",
      "                    permuted_local_hidden_states\n",
      "                )\n",
      "\n",
      "            # Reshape the weights for the grouped GEMMs.\n",
      "            o1 = grouped_gemm.ops.gmm(\n",
      "                permuted_local_hidden_states,\n",
      "                self.grouped_gate_proj,\n",
      "                tokens_per_expert,\n",
      "                trans_b=False,\n",
      "            )\n",
      "            o2 = grouped_gemm.ops.gmm(\n",
      "                permuted_local_hidden_states,\n",
      "                self.grouped_up_proj,\n",
      "                tokens_per_expert,\n",
      "                trans_b=False,\n",
      "            )\n",
      "            inter = self.activation_func(o1) * o2\n",
      "            output = grouped_gemm.ops.gmm(\n",
      "                inter, self.grouped_down_proj, tokens_per_expert, trans_b=False\n",
      "            )\n",
      "            if constants.tensor_parallel_world_size() > 1:\n",
      "                output = reduce_from_tensor_model_parallel_region(output)\n",
      "        else:\n",
      "            # No token is allocated for local experts.\n",
      "            assert torch.count_nonzero(tokens_per_expert) == 0\n",
      "\n",
      "            # Make sure parameters still have gradients when no tokens are routed to this set of experts.\n",
      "            gate_proj = self.grouped_gate_proj.view(self.config.hidden_dim, -1)\n",
      "            up_proj = self.grouped_up_proj.view(self.config.hidden_dim, -1)\n",
      "            down_proj = self.grouped_down_proj.view(-1, self.config.hidden_dim)\n",
      "\n",
      "            o1 = torch.matmul(permuted_local_hidden_states, gate_proj)\n",
      "            o2 = torch.matmul(permuted_local_hidden_states, up_proj)\n",
      "            inter = self.activation_func(o1 * o2)\n",
      "            output = torch.matmul(inter, down_proj)\n",
      "        return output\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/modules/moe/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from .experts import *\n",
      "from .layer import *\n",
      "from .router import *\n",
      "from .token_dispatcher import *\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/pipe_runner.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import copy\n",
      "import dataclasses\n",
      "import os\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import transformers\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "import realhf.impl.model.parallelism.pipeline_parallel.p2p as p2p\n",
      "import realhf.impl.model.parallelism.pipeline_parallel.static_schedule as schedule\n",
      "import realhf.impl.model.utils.cuda_graph as cuda_graph\n",
      "from realhf.api.core.data_api import MicroBatchSpec, SequenceSample\n",
      "from realhf.api.core.model_api import (\n",
      "    GenerationHyperparameters,\n",
      "    ZeroTotalLossWeightException,\n",
      ")\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.nn.real_llm_base import PipeCacheData, PipeTransferData\n",
      "from realhf.impl.model.nn.real_llm_generate import (\n",
      "    _gather_gen_output_from_list,\n",
      "    _gather_minibatch_gen_outputs,\n",
      "    genstep,\n",
      "    maybe_capture_cudagraph,\n",
      "    prepare_generate_inputs,\n",
      ")\n",
      "from realhf.impl.model.parallelism.pipeline_parallel.instruction import PipeInstruction\n",
      "from realhf.impl.model.parallelism.pipeline_parallel.static_schedule import PipeSchedule\n",
      "from realhf.impl.model.parallelism.pipeline_parallel.tensor_storage import TensorBuffer\n",
      "from realhf.impl.model.utils.padding import pad_sequence_parallel_input\n",
      "\n",
      "logger = logging.getLogger(\"Pipeline Runner\", \"benchmark\")\n",
      "\n",
      "\n",
      "class PipelineError(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "def _split_and_prefill_pipe_input(\n",
      "    module: ReaLModel,\n",
      "    splitted: List[SequenceSample],\n",
      "    tensor_buffer: TensorBuffer,\n",
      "    store_kv_cache: bool,\n",
      "    store_input_cache: bool = False,\n",
      "):\n",
      "    \"\"\"Prepare input for pipelined generate, train, or inference.\n",
      "\n",
      "    Basically, splitting all input tensors into micro batches for\n",
      "    pipeline parallel.\n",
      "    \"\"\"\n",
      "    batch_seqlens = [\n",
      "        torch.tensor(flat2d(s.seqlens[\"packed_input_ids\"])) for s in splitted\n",
      "    ]\n",
      "    assert all(all(x > 0 for x in sls) for sls in batch_seqlens)\n",
      "\n",
      "    # Sanity check to ensure that the order of splitted sequences\n",
      "    # is the same across pipeline parallel ranks.\n",
      "    _batch_seqlen = torch.tensor(\n",
      "        [sum(x) for x in batch_seqlens],\n",
      "        device=module.device,\n",
      "        dtype=torch.long,\n",
      "    )\n",
      "    _batch_seqlen_all_gathered = [\n",
      "        torch.zeros_like(_batch_seqlen)\n",
      "        for _ in range(constants.pipe_parallel_world_size())\n",
      "    ]\n",
      "    _batch_seqlen_all_gathered[constants.pipe_parallel_rank()] = _batch_seqlen\n",
      "    dist.all_gather(\n",
      "        _batch_seqlen_all_gathered,\n",
      "        _batch_seqlen,\n",
      "        group=constants.pipe_parallel_group(),\n",
      "    )\n",
      "    for i in range(constants.pipe_parallel_world_size()):\n",
      "        if not torch.allclose(_batch_seqlen_all_gathered[i], _batch_seqlen):\n",
      "            raise PipelineError(\n",
      "                \"Partitioned seqlens are not equal across pipeline parallel ranks. \"\n",
      "                f\"Current rank (dp={constants.data_parallel_rank()},\"\n",
      "                f\"tp={constants.tensor_parallel_rank()},pp={constants.pipe_parallel_rank()}), \"\n",
      "                f\"gathered batch seqlens={_batch_seqlen_all_gathered}, \"\n",
      "                f\"Have you ensured that the order of dataset across ranks is the same?\",\n",
      "            )\n",
      "\n",
      "    mb_seq_lens = []\n",
      "\n",
      "    # Store partitioned inputs into tensor buffer for later use.\n",
      "    def input_to_pipe_model_input(input: SequenceSample, mbid: int):\n",
      "        max_seqlen = int(max(batch_seqlens[mbid]))\n",
      "\n",
      "        cu_seqlens = torch.nn.functional.pad(\n",
      "            batch_seqlens[mbid].to(module.device).cumsum(0), (1, 0)\n",
      "        ).int()\n",
      "        packed_input_ids = input.data[\"packed_input_ids\"]\n",
      "\n",
      "        # sequence parallel input padding\n",
      "        if constants.sequence_parallel():\n",
      "            packed_input_ids, cu_seqlens, max_seqlen, pad_size = (\n",
      "                pad_sequence_parallel_input(packed_input_ids, cu_seqlens, max_seqlen)\n",
      "            )\n",
      "            tensor_buffer.put(\"pad_size\", mbid, pad_size)\n",
      "        x = PipeTransferData(\n",
      "            cu_seqlens=cu_seqlens.int(),\n",
      "            max_seqlen=int(max_seqlen),\n",
      "            store_kv_cache=store_kv_cache,\n",
      "        )\n",
      "        if constants.is_first_pipe_stage():\n",
      "            ys = [PipeCacheData(packed_input_ids=packed_input_ids)] + [\n",
      "                PipeCacheData() for _ in range(module.num_layers - 1)\n",
      "            ]\n",
      "        else:\n",
      "            ys = [PipeCacheData() for _ in range(module.num_layers)]\n",
      "        total_len = (\n",
      "            packed_input_ids.shape[0]\n",
      "            if not constants.sequence_parallel()\n",
      "            else packed_input_ids.shape[0] // constants.tensor_parallel_world_size()\n",
      "        )\n",
      "        mb_seq_lens.append(total_len)\n",
      "        return (x, ys)\n",
      "\n",
      "    batches = [input_to_pipe_model_input(x, i) for i, x in enumerate(splitted)]\n",
      "    for mbid, batch in enumerate(batches):\n",
      "        x, ys = batch\n",
      "        tensor_buffer.put(\"batch_input_x\", mbid, x)\n",
      "        tensor_buffer.put(\"batch_input_ys\", mbid, ys)\n",
      "        tensor_buffer.put(\"batch_lengths\", mbid, x.cu_seqlens.shape[0] - 1)\n",
      "        tensor_buffer.put(\"mb_seq_lens\", mbid, mb_seq_lens[mbid])\n",
      "\n",
      "    # pre allocate receive buffers and pre store other information\n",
      "    for mbid, batch in enumerate(batches):\n",
      "        others_cache = dict(\n",
      "            cu_seqlens=batch[0].cu_seqlens.int(),\n",
      "            max_seqlen=int(batch[0].max_seqlen),\n",
      "            store_kv_cache=batch[0].store_kv_cache,\n",
      "        )\n",
      "        tensor_buffer.put(\"pipe_transfer_infos\", mbid, others_cache)\n",
      "\n",
      "    if store_input_cache:\n",
      "        for mbid, x1 in enumerate(splitted):\n",
      "            tensor_buffer.put(\"input_cache\", mbid, x1)\n",
      "\n",
      "\n",
      "def _exec_pipe_schedule(\n",
      "    module: ReaLModel,\n",
      "    tensor_buffer: TensorBuffer,\n",
      "    instr_map: Dict[PipeInstruction, Callable],\n",
      "    pipe_schedule: PipeSchedule,\n",
      "    terminate_condition: Optional[Callable] = None,\n",
      "):\n",
      "    \"\"\"Execute schedules\n",
      "    Args:\n",
      "        module: The model to execute the schedule on.\n",
      "        tensor_buffer: A temporary buffer that stores necessary information during running.\n",
      "        instr_map: A map of PipeInstruction types to methods. Each method will be executed with the\n",
      "            kwargs provided to the PipeInstruction from the scheduler.\n",
      "        pipe_schedule: an instance of schedule\n",
      "        terminate_condition: a callable that returns boolean value indicating if\n",
      "                                the pipeline execution should terminate\n",
      "    \"\"\"\n",
      "    step_count = 0\n",
      "    is_last_stage = constants.is_last_pipe_stage()\n",
      "    num_stages = constants.pipe_parallel_world_size()\n",
      "    stage_id = constants.pipe_parallel_rank()\n",
      "    global_rank = dist.get_rank()\n",
      "    parllelism_rank = constants.parallelism_rank()\n",
      "    will_break = False\n",
      "\n",
      "    tensor_buffer.put(\n",
      "        \"terminate\",\n",
      "        0,\n",
      "        torch.tensor(False, dtype=torch.bool, device=constants.current_device()),\n",
      "    )  # a global terminate signal for all micro batches that is transferred across stages\n",
      "\n",
      "    # A termination mechanism to avoid all-reduce at each step.\n",
      "    # If the schedule is about to terminate (i.e., will_break is True),\n",
      "    # the last stage will send this message to the previous stages with\n",
      "    # one more pipeline round (last -> 0 -> 1 -> .. -> last-1 -> last).\n",
      "    # After a stage receive terminate signal (or meet the terminate\n",
      "    # condition in the last stage), the stage will enter burnout in\n",
      "    # the next step. In burnout, the stage will only execute necessary\n",
      "    # communication instructions that send terminate to next stage or\n",
      "    # avoid communication stuck. Specifically:\n",
      "    # 1. The last stage: Send terminal signal to the first stage +\n",
      "    #    recv activations for num_stages // 2 + 1 steps;\n",
      "    # 2. stage_id % 2 == 0: Burnout one step, send terminal signal to\n",
      "    #    the next stage;\n",
      "    # 3. stage_id % 2 == 1: No burnout step, since receiving and sending\n",
      "    #    terminate signal happen in the same stage.\n",
      "    if is_last_stage:\n",
      "        burn_out_steps = num_stages // 2 + 1\n",
      "    else:\n",
      "        if stage_id % 2 == 1:\n",
      "            burn_out_steps = 0\n",
      "        else:\n",
      "            burn_out_steps = 1\n",
      "\n",
      "    # For each step in the schedule\n",
      "    for step_cmds in pipe_schedule:\n",
      "        # For each instruction in the step\n",
      "        step_id, micro_batch_id, step_cmds = step_cmds\n",
      "        for cmd in step_cmds:\n",
      "            if type(cmd) not in instr_map:\n",
      "                raise RuntimeError(\n",
      "                    f\"Pipeline instruction executor does not understand instruction {repr(cmd)}\"\n",
      "                )\n",
      "\n",
      "            if will_break:\n",
      "                if is_last_stage:\n",
      "                    if burn_out_steps == num_stages // 2 + 1 and type(cmd) not in [\n",
      "                        schedule.SendNextTokens,\n",
      "                        schedule.RecvActivation,\n",
      "                    ]:\n",
      "                        continue\n",
      "                    if (\n",
      "                        burn_out_steps < num_stages // 2 + 1\n",
      "                        and type(cmd) != schedule.RecvActivation\n",
      "                    ):\n",
      "                        continue\n",
      "                elif (\n",
      "                    not is_last_stage\n",
      "                    and burn_out_steps == 1\n",
      "                    and type(cmd) != schedule.SendActivation\n",
      "                ):\n",
      "                    continue\n",
      "\n",
      "            try:\n",
      "                instr_map[type(cmd)](module, tensor_buffer, *cmd.args)\n",
      "\n",
      "            except Exception as e:\n",
      "                logger.error(\n",
      "                    f\"Model name {constants.model_name()} rank {parllelism_rank}\"\n",
      "                    f\" (global rank {global_rank}) step {step_count}, \"\n",
      "                    f\"Exception in cmd: {cmd}\"\n",
      "                )\n",
      "                raise e\n",
      "\n",
      "        step_count += 1\n",
      "\n",
      "        if will_break:\n",
      "            burn_out_steps -= 1\n",
      "        if terminate_condition is not None and terminate_condition():\n",
      "            tensor_buffer.put(\n",
      "                \"terminate\",\n",
      "                0,\n",
      "                torch.tensor(True, dtype=torch.bool, device=constants.current_device()),\n",
      "            )\n",
      "        if tensor_buffer.get(\"terminate\", 0):\n",
      "            will_break = True\n",
      "        if will_break and burn_out_steps <= 0:\n",
      "            break\n",
      "\n",
      "\n",
      "def _zero_grads(inputs):\n",
      "    if isinstance(inputs, torch.Tensor):\n",
      "        if inputs.grad is not None:\n",
      "            inputs.grad.data.zero_()\n",
      "    elif isinstance(inputs, tuple):\n",
      "        for t in inputs:\n",
      "            if t.grad is not None:\n",
      "                t.grad.data.zero_()\n",
      "    elif dataclasses.is_dataclass(inputs):\n",
      "        for f in dataclasses.fields(inputs):\n",
      "            _zero_grads(getattr(inputs, f.name))\n",
      "    else:\n",
      "        # do nothing for non tensor\n",
      "        pass\n",
      "\n",
      "\n",
      "class PipeInferenceInstrSet:\n",
      "\n",
      "    def _fwd_impl(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        buf = tensor_buffer.get(\n",
      "            \"recv_act_buf\", micro_batch_id, remove=True, raise_error=False\n",
      "        )\n",
      "        ys = tensor_buffer.get(\"batch_input_ys\", micro_batch_id, remove=False)\n",
      "\n",
      "        if buf is not None:\n",
      "            others = tensor_buffer.get(\n",
      "                \"pipe_transfer_infos\", micro_batch_id, remove=False\n",
      "            )\n",
      "            x = PipeTransferData(pp_input=buf, **others)\n",
      "            # tensor_buffer.put(\"batch_input_x\", micro_batch_id, x)\n",
      "        else:\n",
      "            x = tensor_buffer.get(\"batch_input_x\", micro_batch_id, remove=True)\n",
      "\n",
      "        _zero_grads(x)\n",
      "        _zero_grads(ys)\n",
      "        x, ys = module.forward(x, ys)\n",
      "\n",
      "        tensor_buffer.put(\n",
      "            \"batch_output_x\", micro_batch_id, x\n",
      "        )  # Used by send_activation\n",
      "\n",
      "    def _exec_forward_pass(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        PipeInferenceInstrSet._fwd_impl(\n",
      "            module, tensor_buffer, stage_id, micro_batch_id, step_id\n",
      "        )\n",
      "\n",
      "        x = tensor_buffer.get(\"batch_output_x\", micro_batch_id, remove=False)\n",
      "        if constants.is_last_pipe_stage():\n",
      "            logits = x.pp_output\n",
      "            post_hook = tensor_buffer.get(\n",
      "                \"post_hook\", micro_batch_id, raise_error=False\n",
      "            )\n",
      "            if constants.sequence_parallel():\n",
      "                pad_size = tensor_buffer.get(\"pad_size\", micro_batch_id, remove=True)\n",
      "                logits = logits[:-pad_size] if pad_size > 0 else logits\n",
      "            tensor_buffer.remove(\"batch_output_x\", micro_batch_id)\n",
      "            if not post_hook:\n",
      "                tensor_buffer.put(\"output\", micro_batch_id, logits)\n",
      "            else:\n",
      "                input_ = tensor_buffer.get(\"input_cache\", micro_batch_id)\n",
      "                output = post_hook(logits, input_)\n",
      "                tensor_buffer.put(\"output\", micro_batch_id, output)\n",
      "\n",
      "    def _exec_send_activations(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert stage_id != constants.pipe_parallel_world_size() - 1\n",
      "        x: PipeTransferData = tensor_buffer.get(\n",
      "            \"batch_output_x\",\n",
      "            micro_batch_id,\n",
      "            remove=True,\n",
      "        )\n",
      "        p2p.send(x.pp_output, constants.next_pipe_stage(), async_op=False)\n",
      "\n",
      "    def _exec_recv_activations(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert not constants.is_first_pipe_stage()\n",
      "\n",
      "        device = module.device\n",
      "        dtype = module.dtype\n",
      "        hidden_dim = module.config.hidden_dim\n",
      "\n",
      "        mb_seq_len = tensor_buffer.get(\"mb_seq_lens\", micro_batch_id, remove=False)\n",
      "        act_shape = (mb_seq_len, hidden_dim)\n",
      "        buf = torch.empty(act_shape, dtype=dtype, device=device, requires_grad=False)\n",
      "\n",
      "        p2p.recv(buf, constants.prev_pipe_stage(), async_op=False)\n",
      "        tensor_buffer.put(\"recv_act_buf\", micro_batch_id, buf)\n",
      "\n",
      "    INSTRUCTION_MAP = {\n",
      "        schedule.ForwardPass: _exec_forward_pass,\n",
      "        schedule.SendActivation: _exec_send_activations,\n",
      "        schedule.RecvActivation: _exec_recv_activations,\n",
      "    }\n",
      "\n",
      "\n",
      "class PipeGenInstrSet:\n",
      "\n",
      "    def _exec_forward_pass(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        tokenizer = tensor_buffer.get(\"tokenizer\", micro_batch_id)\n",
      "        gconfig = tensor_buffer.get(\"gconfig\", micro_batch_id)\n",
      "\n",
      "        is_first_stage = constants.is_first_pipe_stage()\n",
      "        if is_first_stage:\n",
      "            buf = tensor_buffer.get(\n",
      "                \"recv_next_tokens_buf\",\n",
      "                micro_batch_id,\n",
      "                remove=True,\n",
      "                raise_error=False,\n",
      "            )\n",
      "        else:\n",
      "            buf = tensor_buffer.get(\n",
      "                \"recv_act_buf\",\n",
      "                micro_batch_id,\n",
      "                remove=True,\n",
      "                raise_error=False,\n",
      "            )\n",
      "\n",
      "        ys = tensor_buffer.get(\"batch_input_ys\", micro_batch_id, remove=False)\n",
      "\n",
      "        others = None\n",
      "        if buf is not None:\n",
      "            if is_first_stage:\n",
      "                x = tensor_buffer.get(\"batch_input_x\", micro_batch_id, remove=True)\n",
      "                ys = tensor_buffer.get(\"batch_input_ys\", micro_batch_id, remove=False)\n",
      "                ys[0].packed_input_ids = buf\n",
      "                ys[0].packed_position_ids = None\n",
      "            else:\n",
      "                others = tensor_buffer.get(\n",
      "                    \"pipe_transfer_infos\", micro_batch_id, remove=False\n",
      "                )\n",
      "                x = PipeTransferData(pp_input=buf, **others)\n",
      "                tensor_buffer.put(\"batch_input_x\", micro_batch_id, x)\n",
      "        else:\n",
      "            x = tensor_buffer.get(\"batch_input_x\", micro_batch_id, remove=True)\n",
      "\n",
      "        # Capture CUDAGraph in the first decoding step.\n",
      "        cuda_graph_name = f\"decoding_{micro_batch_id}\"\n",
      "        # Get the graph from the buffer instead of the global handle.\n",
      "        # This is because the graph may not be destroyed in the previous generation call,\n",
      "        # but we need to call into the `capture_decoding_graph` function to reinitialize\n",
      "        # the graph anyway. Getting from the buffer ensures that the `graph` variable at\n",
      "        # the first decoding step is None and we can get into the if branch.\n",
      "        graph = tensor_buffer.get(cuda_graph_name, micro_batch_id, raise_error=False)\n",
      "        if (\n",
      "            tensor_buffer.get(\"kv_cache_reserved\", micro_batch_id)\n",
      "            and gconfig.use_cuda_graph\n",
      "            and graph is None\n",
      "        ):\n",
      "            # NOTE: we need to capture separate graphs for different micro-batches\n",
      "            # because the addresses of KV-caches are different.\n",
      "            # One CUDAGraph operates on exactly one KV-cache address.\n",
      "            graph, _, _ = maybe_capture_cudagraph(\n",
      "                module,\n",
      "                x,\n",
      "                ys,\n",
      "                cuda_graph_name,\n",
      "                force_recapture=gconfig.force_cudagraph_recapture,\n",
      "            )\n",
      "            tensor_buffer.put(cuda_graph_name, micro_batch_id, graph)\n",
      "\n",
      "        # Run model forward.\n",
      "        # NOTE: `step_id` is not the position of the instruction,\n",
      "        # but the position of the generated token.\n",
      "        if graph is None or step_id == 0:\n",
      "            x, ys = module.forward(x, ys)\n",
      "        else:\n",
      "            # only replay decoding phase\n",
      "            bs = ys[0].cache_seqlens.shape[0]\n",
      "            if is_first_stage:\n",
      "                cuda_graph.input_buffer_handle(cuda_graph_name, \"input_ids\")[:bs].copy_(\n",
      "                    ys[0].packed_input_ids, non_blocking=True\n",
      "                )\n",
      "            if not is_first_stage:\n",
      "                cuda_graph.input_buffer_handle(cuda_graph_name, \"hidden_states\").copy_(\n",
      "                    x.pp_input, non_blocking=True\n",
      "                )\n",
      "                cuda_graph.input_buffer_handle(cuda_graph_name, \"cu_seqlens\").copy_(\n",
      "                    x.cu_seqlens, non_blocking=True\n",
      "                )\n",
      "            cuda_graph.input_buffer_handle(cuda_graph_name, \"position_ids\")[:bs].copy_(\n",
      "                ys[0].cache_seqlens, non_blocking=True\n",
      "            )\n",
      "            cuda_graph.input_buffer_handle(cuda_graph_name, \"cache_seqlens\")[:bs].copy_(\n",
      "                ys[0].cache_seqlens, non_blocking=True\n",
      "            )\n",
      "\n",
      "            graph.replay()\n",
      "            x.pp_output = cuda_graph.output_buffer_handle(cuda_graph_name, \"output\")\n",
      "\n",
      "        tensor_buffer.put(\"batch_output_x\", micro_batch_id, x)\n",
      "\n",
      "        # Init KV cache.\n",
      "        is_prefill_phase = False\n",
      "        if not tensor_buffer.get(\"kv_cache_reserved\", micro_batch_id):\n",
      "            # KV cache is attached to x and ys.\n",
      "            assert constants.pipe_parallel_world_size() >= 2\n",
      "            x, ys = prepare_generate_inputs(module, gconfig, x, ys, cuda_graph_name)\n",
      "            is_prefill_phase = True\n",
      "            tensor_buffer.put(\"kv_cache_reserved\", micro_batch_id, True)\n",
      "\n",
      "        # Increase cache_seqlens in the decoding phase.\n",
      "        if not is_prefill_phase:\n",
      "            ys[0].cache_seqlens += 1  # global handle\n",
      "\n",
      "        # Perform a decoding step.\n",
      "        if constants.is_last_pipe_stage():\n",
      "            # Gather logits of the final token\n",
      "            logits = x.pp_output\n",
      "            if is_prefill_phase:\n",
      "                logits = logits[x.cu_seqlens[1:] - 1]\n",
      "\n",
      "            unfinished_sequences = tensor_buffer.get(\n",
      "                \"unfinished_sequences\", micro_batch_id\n",
      "            )\n",
      "            generated_idx = tensor_buffer.get(\"generated_idx\", micro_batch_id)\n",
      "\n",
      "            (\n",
      "                next_tokens,\n",
      "                logprob,\n",
      "                logits_mask,\n",
      "                terminate,\n",
      "                unfinished_sequences,\n",
      "            ) = genstep(\n",
      "                logits,\n",
      "                tokenizer,\n",
      "                unfinished_sequences,\n",
      "                generated_idx,\n",
      "                gconfig,\n",
      "            )\n",
      "\n",
      "            if isinstance(terminate, bool):\n",
      "                terminate = torch.tensor(\n",
      "                    terminate, device=logits.device, dtype=torch.bool\n",
      "                )\n",
      "\n",
      "            tensor_buffer.put(\"_terminate\", micro_batch_id, terminate)\n",
      "            tensor_buffer.put(\n",
      "                \"unfinished_sequences\", micro_batch_id, unfinished_sequences\n",
      "            )\n",
      "            tensor_buffer.put(\"generated_idx\", micro_batch_id, generated_idx + 1)\n",
      "            assert next_tokens is not None and logprob is not None\n",
      "            tensor_buffer.get(\"gen_token_ph\", micro_batch_id).append(next_tokens)\n",
      "            tensor_buffer.get(\"gen_logprob_ph\", micro_batch_id).append(logprob)\n",
      "            tensor_buffer.get(\"gen_logits_mask_ph\", micro_batch_id).append(logits_mask)\n",
      "            tensor_buffer.put(\"next_tokens_to_send\", micro_batch_id, next_tokens)\n",
      "\n",
      "    def _exec_send_activations(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        PipeInferenceInstrSet._exec_send_activations(\n",
      "            module, tensor_buffer, stage_id, micro_batch_id, step_id\n",
      "        )\n",
      "        tensor_buffer.put(\"first_token\", micro_batch_id, False)\n",
      "        terminate = tensor_buffer.get(\"terminate\", 0)\n",
      "        p2p.send(terminate, constants.next_pipe_stage())\n",
      "\n",
      "    def _exec_recv_activations(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert not constants.is_first_pipe_stage()\n",
      "\n",
      "        device = module.device\n",
      "        dtype = module.dtype\n",
      "        hidden_dim = module.config.hidden_dim\n",
      "\n",
      "        mb_seq_len = tensor_buffer.get(\"mb_seq_lens\", micro_batch_id, remove=False)\n",
      "        act_shape = (mb_seq_len, hidden_dim)\n",
      "\n",
      "        ft = tensor_buffer.get(\"first_token\", micro_batch_id, remove=False)\n",
      "        if ft:\n",
      "            buf = torch.empty(\n",
      "                act_shape, dtype=dtype, device=device, requires_grad=False\n",
      "            )\n",
      "        else:\n",
      "            batch_length = tensor_buffer.get(\n",
      "                \"batch_lengths\", micro_batch_id, remove=False\n",
      "            )\n",
      "            batch_length = (\n",
      "                batch_length // constants.tensor_parallel_world_size()\n",
      "                if constants.sequence_parallel()\n",
      "                else batch_length\n",
      "            )\n",
      "            act_shape = (batch_length, hidden_dim)\n",
      "            buf = torch.empty(\n",
      "                act_shape, dtype=dtype, device=device, requires_grad=False\n",
      "            )\n",
      "\n",
      "        prev_stage = constants.prev_pipe_stage()\n",
      "        p2p.recv(buf, prev_stage, async_op=False)\n",
      "        tensor_buffer.put(\"recv_act_buf\", micro_batch_id, buf)\n",
      "\n",
      "        terminate = torch.empty((), dtype=torch.bool, device=device)\n",
      "        p2p.recv(terminate, prev_stage)\n",
      "        if terminate:\n",
      "            tensor_buffer.put(\"terminate\", 0, terminate)\n",
      "\n",
      "    def _exec_send_next_tokens(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        \"\"\"When generating, send next tokens from the last stage to the first\n",
      "        stage.\"\"\"\n",
      "        assert constants.is_last_pipe_stage()\n",
      "        next_stage = constants.next_pipe_stage()\n",
      "        next_tokens_to_send = tensor_buffer.get(\n",
      "            \"next_tokens_to_send\", micro_batch_id, remove=True\n",
      "        )\n",
      "        p2p.send(next_tokens_to_send, next_stage, async_op=False)\n",
      "        p2p.send(tensor_buffer.get(\"terminate\", 0), next_stage)\n",
      "        tensor_buffer.put(\"first_token\", micro_batch_id, False)\n",
      "\n",
      "    def _exec_recv_next_tokens(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        \"\"\"When generating, recv next tokens from the last stage on the first\n",
      "        stage Construct next forward input.\"\"\"\n",
      "        assert constants.is_first_pipe_stage()\n",
      "        batch_length = tensor_buffer.get(\"batch_lengths\", micro_batch_id, remove=False)\n",
      "\n",
      "        device = module.device\n",
      "        prev_stage = constants.prev_pipe_stage()\n",
      "\n",
      "        recv_buf = torch.empty((batch_length,), dtype=torch.long, device=device)\n",
      "        p2p.recv(recv_buf, prev_stage, async_op=False)\n",
      "        tensor_buffer.put(\"recv_next_tokens_buf\", micro_batch_id, recv_buf)\n",
      "\n",
      "        x = PipeTransferData(\n",
      "            store_kv_cache=True,\n",
      "            cu_seqlens=torch.arange(batch_length + 1, dtype=torch.int32, device=device),\n",
      "            max_seqlen=1,\n",
      "        )\n",
      "        tensor_buffer.put(\"batch_input_x\", micro_batch_id, x)\n",
      "\n",
      "        terminate = torch.empty((), dtype=torch.bool, device=device)\n",
      "        p2p.recv(terminate, prev_stage)\n",
      "\n",
      "        if terminate:\n",
      "            tensor_buffer.put(\"terminate\", 0, terminate)\n",
      "\n",
      "    INSTRUCTION_MAP = {\n",
      "        schedule.ForwardPass: _exec_forward_pass,\n",
      "        schedule.SendActivation: _exec_send_activations,\n",
      "        schedule.RecvActivation: _exec_recv_activations,\n",
      "        schedule.SendNextTokens: _exec_send_next_tokens,\n",
      "        schedule.RecvNextTokens: _exec_recv_next_tokens,\n",
      "    }\n",
      "\n",
      "\n",
      "class PipeTrainForwardCommInstrSet:\n",
      "\n",
      "    def _exec_forward_pass(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        PipeInferenceInstrSet._fwd_impl(\n",
      "            module, tensor_buffer, stage_id, micro_batch_id, step_id\n",
      "        )\n",
      "\n",
      "        loss_fn = tensor_buffer.get(\"loss_fn\", micro_batch_id)\n",
      "        if loss_fn is not None and constants.is_last_pipe_stage():\n",
      "            model_output = tensor_buffer.get(\"batch_output_x\", micro_batch_id).pp_output\n",
      "            if constants.sequence_parallel():\n",
      "                pad_size = tensor_buffer.get(\"pad_size\", micro_batch_id, remove=True)\n",
      "                model_output = (\n",
      "                    model_output[:-pad_size] if pad_size > 0 else model_output\n",
      "                )\n",
      "            input_cache: SequenceSample = tensor_buffer.get(\n",
      "                \"input_cache\", micro_batch_id, remove=True\n",
      "            )\n",
      "            loss = loss_fn(model_output, input_cache)\n",
      "            loss = loss * tensor_buffer.get(\"loss_scale\", micro_batch_id)\n",
      "            tensor_buffer.put(\"losses\", micro_batch_id, loss)\n",
      "\n",
      "    def _exec_send_activations(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert stage_id != constants.pipe_parallel_world_size() - 1\n",
      "        # NOTE: This is different from inference, we remain batch_output_x for backward.\n",
      "        x: PipeTransferData = tensor_buffer.get(\"batch_output_x\", micro_batch_id)\n",
      "        p2p.send(x.pp_output, constants.next_pipe_stage(), async_op=False)\n",
      "\n",
      "    def _exec_recv_activations(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert not constants.is_first_pipe_stage()\n",
      "\n",
      "        device = module.device\n",
      "        dtype = module.dtype\n",
      "        hidden_dim = module.config.hidden_dim\n",
      "\n",
      "        mb_seq_len = tensor_buffer.get(\"mb_seq_lens\", micro_batch_id, remove=False)\n",
      "        act_shape = (mb_seq_len, hidden_dim)\n",
      "\n",
      "        buf = tensor_buffer.alloc(\n",
      "            \"activation\",\n",
      "            micro_batch_id,\n",
      "            act_shape,\n",
      "            dtype,\n",
      "            device,\n",
      "            require_grads=True,\n",
      "        )\n",
      "\n",
      "        p2p.recv(buf, constants.prev_pipe_stage(), async_op=False)\n",
      "        tensor_buffer.put(\"recv_act_buf\", micro_batch_id, buf)\n",
      "\n",
      "    def _exec_send_grads(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert not constants.is_first_pipe_stage()\n",
      "        activation = tensor_buffer.get(\"activation\", micro_batch_id, remove=True)\n",
      "        assert activation.grad is not None\n",
      "        p2p.send(activation.grad, constants.prev_pipe_stage(), async_op=False)\n",
      "\n",
      "    def _exec_recv_grads(\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        assert not constants.is_last_pipe_stage()\n",
      "        device = module.device\n",
      "        dtype = module.dtype\n",
      "        hidden_dim = module.config.hidden_dim\n",
      "        mb_seq_len = tensor_buffer.get(\"mb_seq_lens\", micro_batch_id, remove=False)\n",
      "        grad_shape = (mb_seq_len, hidden_dim)\n",
      "        buf = tensor_buffer.alloc(\"grad\", micro_batch_id, grad_shape, dtype, device)\n",
      "        p2p.recv(buf, constants.next_pipe_stage(), async_op=False)\n",
      "\n",
      "    INSTRUCTION_MAP = {\n",
      "        schedule.ForwardPass: _exec_forward_pass,\n",
      "        schedule.SendActivation: _exec_send_activations,\n",
      "        schedule.RecvActivation: _exec_recv_activations,\n",
      "        schedule.SendGrad: _exec_send_grads,\n",
      "        schedule.RecvGrad: _exec_recv_grads,\n",
      "    }\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PipeTrainInstrSet:\n",
      "    engine: Any\n",
      "\n",
      "    def _exec_optimizer_step(self, *args, **kwargs):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def _exec_reduce_grads(self, *args, **kwargs):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def _exec_backward_pass(self, *args, **kwargs):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @property\n",
      "    def INSTRUCTION_MAP(self):\n",
      "        return {\n",
      "            **PipeTrainForwardCommInstrSet.INSTRUCTION_MAP,\n",
      "            schedule.OptimizerStep: self._exec_optimizer_step,\n",
      "            schedule.ReduceGrads: self._exec_reduce_grads,\n",
      "            schedule.BackwardPass: self._exec_backward_pass,\n",
      "        }\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PipelineRunner:\n",
      "    module: ReaLModel\n",
      "\n",
      "    @property\n",
      "    def default_train_mbs(self):\n",
      "        return constants.pipe_parallel_world_size() * 2\n",
      "\n",
      "    @property\n",
      "    def default_inf_mbs(self):\n",
      "        return constants.pipe_parallel_world_size()\n",
      "\n",
      "    def eval(self, *args, **kwargs):\n",
      "        return self.module.eval(*args, **kwargs)\n",
      "\n",
      "    def train(self, *args, **kwargs):\n",
      "        return self.module.train(*args, **kwargs)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def forward(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        output_seqlens: List[List[int]] | None = None,\n",
      "        post_hook: Callable[[torch.Tensor, SequenceSample], Any] | None = None,\n",
      "        aggregate_fn: Callable[[List[Any]], Any] = torch.cat,\n",
      "    ):\n",
      "        \"\"\"Run one forward step over a batch of tokens and return the\n",
      "        logits.\"\"\"\n",
      "\n",
      "        mb_spec = MicroBatchSpec.new(\n",
      "            mb_spec, n_mbs=self.default_inf_mbs * mb_spec.n_mbs\n",
      "        )\n",
      "        mb_inputs, fwd_indices, bwd_indices = input_.split(mb_spec)\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "            )\n",
      "        n_pp_mbs = len(mb_inputs)\n",
      "\n",
      "        tensor_buffer = TensorBuffer()\n",
      "        if post_hook is not None:\n",
      "            for i in range(n_pp_mbs):\n",
      "                tensor_buffer.put(\"post_hook\", i, post_hook)\n",
      "\n",
      "        _split_and_prefill_pipe_input(\n",
      "            module=self.module,\n",
      "            tensor_buffer=tensor_buffer,\n",
      "            splitted=mb_inputs,\n",
      "            store_kv_cache=False,\n",
      "            store_input_cache=post_hook is not None,\n",
      "        )\n",
      "\n",
      "        sched = schedule.InferenceSchedule(\n",
      "            micro_batches=n_pp_mbs,\n",
      "            stages=constants.pipe_parallel_world_size(),\n",
      "            stage_id=constants.pipe_parallel_rank(),\n",
      "        )\n",
      "        _exec_pipe_schedule(\n",
      "            self.module,\n",
      "            tensor_buffer,\n",
      "            instr_map=PipeInferenceInstrSet.INSTRUCTION_MAP,\n",
      "            pipe_schedule=sched,\n",
      "        )\n",
      "\n",
      "        agg_output = None\n",
      "        if constants.is_last_pipe_stage():\n",
      "            output_list = []\n",
      "            for i in range(n_pp_mbs):\n",
      "                output = tensor_buffer.get(\"output\", i, remove=True)\n",
      "                output_list.append(output)\n",
      "            agg_output = aggregate_fn(output_list)\n",
      "\n",
      "            if isinstance(agg_output, torch.Tensor):\n",
      "                agg_output = SequenceSample.reorder_output(\n",
      "                    agg_output,\n",
      "                    forward_indices=fwd_indices,\n",
      "                    backward_indices=bwd_indices,\n",
      "                    expected_seqlens=(\n",
      "                        output_seqlens\n",
      "                        if output_seqlens is not None\n",
      "                        else input_.seqlens[\"packed_input_ids\"]\n",
      "                    ),\n",
      "                )\n",
      "\n",
      "        return agg_output\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=GenerationHyperparameters\n",
      "        ),\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[PipeCacheData]]:\n",
      "        if constants.sequence_parallel():\n",
      "            raise NotImplementedError(\n",
      "                \"Sequence parallel is not supported for generation\"\n",
      "            )\n",
      "\n",
      "        # This function does not support micro-batch.\n",
      "        # We use micro-batch generation to reduce the memory usage of KV-cache.\n",
      "        # When the global batch is fixed, not matter how many micro-batches we\n",
      "        # split, the all-together KV-cache memory usage will not be changed,\n",
      "        # so it's useless to split micro-batches here.\n",
      "        mb_spec = MicroBatchSpec(n_mbs=self.default_inf_mbs)\n",
      "        mb_inputs, *_ = input_.split(mb_spec)\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "            )\n",
      "        n_pp_mbs = len(mb_inputs)\n",
      "\n",
      "        max_seqlen = max(\n",
      "            [max(flat2d(input_.seqlens[\"packed_input_ids\"])) for input_ in mb_inputs]\n",
      "        )\n",
      "        if constants.max_prompt_len() < max_seqlen:\n",
      "            raise RuntimeError(\n",
      "                f\"Input sequence length {max_seqlen} is larger than the maximum sequence length \"\n",
      "                f\"supported by the model {constants.max_prompt_len()}.\"\n",
      "            )\n",
      "\n",
      "        tensor_buffer = TensorBuffer()\n",
      "\n",
      "        _split_and_prefill_pipe_input(\n",
      "            module=self.module,\n",
      "            tensor_buffer=tensor_buffer,\n",
      "            splitted=mb_inputs,\n",
      "            store_kv_cache=True,\n",
      "        )\n",
      "\n",
      "        # for elegant generation termination\n",
      "        for mbid in range(n_pp_mbs):\n",
      "            tensor_buffer.put(\"kv_cache_reserved\", mbid, False)\n",
      "            tensor_buffer.put(\n",
      "                \"_terminate\",\n",
      "                mbid,\n",
      "                torch.tensor(0, dtype=torch.bool, device=self.module.device),\n",
      "            )\n",
      "            tensor_buffer.put(\"generated_idx\", mbid, 0)\n",
      "            batch_length = tensor_buffer.get(\"batch_lengths\", mbid)\n",
      "            tensor_buffer.put(\n",
      "                \"unfinished_sequences\",\n",
      "                mbid,\n",
      "                torch.ones(batch_length, dtype=torch.long, device=self.module.device),\n",
      "            )\n",
      "            tensor_buffer.put(\"gen_token_ph\", mbid, [])\n",
      "            tensor_buffer.put(\"gen_logprob_ph\", mbid, [])\n",
      "            tensor_buffer.put(\"gen_logits_mask_ph\", mbid, [])\n",
      "            tensor_buffer.put(\"first_token\", mbid, True)\n",
      "            tensor_buffer.put(\"tokenizer\", mbid, tokenizer)\n",
      "            tensor_buffer.put(\"gconfig\", mbid, gconfig)\n",
      "\n",
      "        num_stages = constants.pipe_parallel_world_size()\n",
      "        sched = schedule.GenerateSchedule(\n",
      "            micro_batches=n_pp_mbs,\n",
      "            stages=constants.pipe_parallel_world_size(),\n",
      "            stage_id=constants.pipe_parallel_rank(),\n",
      "            max_new_tokens=gconfig.max_new_tokens + num_stages // 2 + 10,\n",
      "            # extend generate schedule for graceful terminate\n",
      "        )\n",
      "\n",
      "        def terminate_condition():\n",
      "            term = all(\n",
      "                [tensor_buffer.get(\"_terminate\", mbid) for mbid in range(n_pp_mbs)]\n",
      "            )\n",
      "            return term\n",
      "\n",
      "        _exec_pipe_schedule(\n",
      "            self.module,\n",
      "            tensor_buffer,\n",
      "            instr_map=PipeGenInstrSet.INSTRUCTION_MAP,\n",
      "            pipe_schedule=sched,\n",
      "            terminate_condition=terminate_condition,\n",
      "        )\n",
      "\n",
      "        if gconfig.use_cuda_graph and gconfig.force_cudagraph_recapture:\n",
      "            for micro_batch_id in range(n_pp_mbs):\n",
      "                cuda_graph.destroy(f\"decoding_{micro_batch_id}\")\n",
      "\n",
      "        if not constants.is_last_pipe_stage():\n",
      "            return None\n",
      "\n",
      "        # Gather generation outputs, including generated tokens, logprobs, and logits_mask.\n",
      "        generate_output = []\n",
      "        for mbid in range(n_pp_mbs):\n",
      "            generate_output += [\n",
      "                _gather_gen_output_from_list(\n",
      "                    gen_token_ph=tensor_buffer.get(\"gen_token_ph\", mbid, remove=True),\n",
      "                    gen_logprob_ph=tensor_buffer.get(\n",
      "                        \"gen_logprob_ph\", mbid, remove=True\n",
      "                    ),\n",
      "                    gen_logits_mask_ph=tensor_buffer.get(\n",
      "                        \"gen_logits_mask_ph\", mbid, remove=True\n",
      "                    ),\n",
      "                )\n",
      "            ]\n",
      "\n",
      "        gen_tokens, log_probs, logits_mask = _gather_minibatch_gen_outputs(\n",
      "            *list(zip(*generate_output)),\n",
      "            pad_token_id=tokenizer.pad_token_id,\n",
      "        )\n",
      "\n",
      "        return gen_tokens, log_probs, logits_mask, None, None\n",
      "\n",
      "    def train_batch(\n",
      "        self,\n",
      "        instr_set: PipeTrainInstrSet,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        loss_fn: Callable,\n",
      "        loss_weight_fn: Callable,\n",
      "        token_normalize_scope: str,\n",
      "        version_steps: int,\n",
      "    ):\n",
      "        # TODO: return whether update success\n",
      "        if not torch._C.is_grad_enabled():\n",
      "            raise RuntimeError(\n",
      "                f\"train_batch() requires gradients enabled. Use eval_batch() instead.\"\n",
      "            )\n",
      "\n",
      "        mb_spec = MicroBatchSpec.new(\n",
      "            mb_spec, n_mbs=mb_spec.n_mbs * self.default_train_mbs\n",
      "        )\n",
      "        mb_inputs = input_.synced_data_parallel_split(mb_spec)\n",
      "        total_loss_weight = torch.tensor(\n",
      "            sum([loss_weight_fn(mb) for mb in mb_inputs]), dtype=torch.float32\n",
      "        )\n",
      "        if token_normalize_scope == \"global\":\n",
      "            dist.all_reduce(total_loss_weight, group=constants.data_parallel_group())\n",
      "\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "            )\n",
      "        n_pp_mbs = len(mb_inputs)\n",
      "\n",
      "        tensor_buffer = TensorBuffer()\n",
      "        for i in range(n_pp_mbs):\n",
      "            tensor_buffer.put(\"n_pp_mbs\", i, n_pp_mbs)\n",
      "            loss_scale = loss_weight_fn(mb_inputs[i]) / total_loss_weight\n",
      "            if token_normalize_scope == \"global\":\n",
      "                # Megatron will average gradients across DP ranks.\n",
      "                # If we normalize loss across micro batches of all DP ranks,\n",
      "                # we should revert the effect of gradient averaging in megatron\n",
      "                # to make sure loss from each token is scaled properly.\n",
      "                loss_scale *= constants.data_parallel_world_size()\n",
      "            loss_scale *= instr_set.engine.optim.get_loss_scale().item()\n",
      "            tensor_buffer.put(\"loss_scale\", i, loss_scale)\n",
      "            tensor_buffer.put(\"version_steps\", i, version_steps)\n",
      "            tensor_buffer.put(\"loss_fn\", i, loss_fn)\n",
      "\n",
      "        _split_and_prefill_pipe_input(\n",
      "            module=self.module,\n",
      "            tensor_buffer=tensor_buffer,\n",
      "            splitted=mb_inputs,\n",
      "            store_kv_cache=False,\n",
      "            store_input_cache=True,\n",
      "        )\n",
      "\n",
      "        sched = schedule.TrainSchedule(\n",
      "            micro_batches=n_pp_mbs,\n",
      "            stages=constants.pipe_parallel_world_size(),\n",
      "            stage_id=constants.pipe_parallel_rank(),\n",
      "        )\n",
      "        _exec_pipe_schedule(\n",
      "            module=self.module,\n",
      "            tensor_buffer=tensor_buffer,\n",
      "            instr_map=instr_set.INSTRUCTION_MAP,\n",
      "            pipe_schedule=sched,\n",
      "        )\n",
      "\n",
      "        agg_stats = {}\n",
      "\n",
      "        stat = tensor_buffer.get(\"stats\", 0, raise_error=False)\n",
      "        stats = [None for _ in range(constants.pipe_parallel_world_size())]\n",
      "        dist.all_gather_object(stats, stat, group=constants.pipe_parallel_cpu_group())\n",
      "\n",
      "        if constants.is_last_pipe_stage():\n",
      "            for key in stats[0].keys():\n",
      "                agg_stats[key] = sum([stat[key] for stat in stats]) / len(stats)\n",
      "\n",
      "        return agg_stats\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/sglang.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import asyncio\n",
      "import dataclasses\n",
      "import json\n",
      "import os\n",
      "import socket\n",
      "import sys\n",
      "import time\n",
      "import traceback\n",
      "from typing import Dict, List, Tuple\n",
      "\n",
      "import aiohttp\n",
      "import requests\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.multiprocessing as mp\n",
      "import transformers\n",
      "from tqdm.asyncio import tqdm\n",
      "\n",
      "from realhf.api.cli_args import SGLangConfig\n",
      "from realhf.api.core import data_api\n",
      "from realhf.api.core.model_api import (\n",
      "    APIGenerateInput,\n",
      "    APIGenerateOutput,\n",
      "    FinetuneSpec,\n",
      "    GenerationHyperparameters,\n",
      "    LLMAPIClient,\n",
      "    Model,\n",
      "    ModelBackend,\n",
      "    PipelinableEngine,\n",
      "    register_backend,\n",
      ")\n",
      "from realhf.base import (\n",
      "    cluster,\n",
      "    constants,\n",
      "    datapack,\n",
      "    gpu_utils,\n",
      "    logging,\n",
      "    name_resolve,\n",
      "    names,\n",
      "    network,\n",
      "    pkg_version,\n",
      "    seeding,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"SGLang backend\")\n",
      "\n",
      "SGLANG_INIT_TIMEOUT = 300\n",
      "\n",
      "\n",
      "def remove_prefix(text: str, prefix: str) -> str:\n",
      "    return text[len(prefix) :] if text.startswith(prefix) else text\n",
      "\n",
      "\n",
      "if pkg_version.is_available(\"sglang\"):\n",
      "    if pkg_version.is_version_greater_or_equal(\"sglang\", \"0.4.4\"):\n",
      "        SGLANG_TOKEN_OUTPUT_IDENTIFIER = \"output_ids\"\n",
      "    else:\n",
      "        SGLANG_TOKEN_OUTPUT_IDENTIFIER = \"token_ids\"\n",
      "\n",
      "\n",
      "class SGLangAPIClient(LLMAPIClient):\n",
      "\n",
      "    async def _do_generate(\n",
      "        self, req: APIGenerateInput, stream: bool = False\n",
      "    ) -> APIGenerateOutput:\n",
      "        gconfig = req.gconfig\n",
      "        sample_params = {\n",
      "            \"n\": gconfig.n,\n",
      "            \"top_p\": gconfig.top_p,\n",
      "            \"top_k\": gconfig.top_k,\n",
      "            \"max_new_tokens\": gconfig.max_new_tokens,\n",
      "            \"temperature\": 0.0 if gconfig.greedy else gconfig.temperature,\n",
      "            \"stop_token_ids\": req.stop_token_ids,\n",
      "        }\n",
      "        payload = {\n",
      "            \"input_ids\": req.input_ids,\n",
      "            \"sampling_params\": sample_params,\n",
      "            \"return_logprob\": req.return_logprob,\n",
      "            \"stream\": stream,\n",
      "        }\n",
      "\n",
      "        assert not stream, \"streaming mode not yet implemented\"\n",
      "        outputs = [APIGenerateOutput.from_input(req) for _ in range(gconfig.n)]\n",
      "        most_recent_timestamps = [time.perf_counter() for _ in range(gconfig.n)]\n",
      "        output_idx = 0\n",
      "\n",
      "        # The following code is partially adopted from sglang/bench_serving.py\n",
      "        st = time.perf_counter()\n",
      "        async with self.session.post(url=self.generate_url, json=payload) as response:\n",
      "            response.raise_for_status()\n",
      "            async for chunk_bytes in response.content:\n",
      "                chunk_bytes = chunk_bytes.strip()\n",
      "                if not chunk_bytes:\n",
      "                    continue\n",
      "\n",
      "                chunk = remove_prefix(chunk_bytes.decode(\"utf-8\"), \"data: \")\n",
      "                latency = time.perf_counter() - st\n",
      "                if chunk == \"[DONE]\":\n",
      "                    pass\n",
      "                else:\n",
      "                    datas = json.loads(chunk)\n",
      "                    if not isinstance(datas, list):\n",
      "                        datas = [datas]\n",
      "                    for data in datas:\n",
      "\n",
      "                        output = outputs[output_idx]\n",
      "                        timestamp = time.perf_counter()\n",
      "                        # First token\n",
      "                        if output.ttft == float(\"inf\"):\n",
      "                            ttft = time.perf_counter() - st\n",
      "                            output.ttft = ttft\n",
      "                        # Decoding phase\n",
      "                        else:\n",
      "                            output.itl.append(\n",
      "                                timestamp - most_recent_timestamps[output_idx]\n",
      "                            )\n",
      "\n",
      "                        most_recent_timestamps[output_idx] = timestamp\n",
      "                        output.output_ids = [data[SGLANG_TOKEN_OUTPUT_IDENTIFIER]]\n",
      "                        finish_reason = data[\"meta_info\"][\"finish_reason\"]\n",
      "                        if req.return_logprob:\n",
      "                            output.output_logprobs = [\n",
      "                                [\n",
      "                                    x[0]\n",
      "                                    for x in data[\"meta_info\"][\"output_token_logprobs\"]\n",
      "                                ]\n",
      "                            ]\n",
      "                        assert finish_reason[\"type\"] in [\n",
      "                            \"length\",\n",
      "                            \"stop\",\n",
      "                        ], finish_reason\n",
      "                        output.no_eos = [finish_reason[\"type\"] == \"length\"]\n",
      "                        output.latency = latency\n",
      "\n",
      "                        output_idx += 1\n",
      "\n",
      "        return APIGenerateOutput.concat(outputs)\n",
      "\n",
      "    async def async_update_weights_from_disk(self, path, retries=5):\n",
      "        for _ in range(retries):\n",
      "            async with self.session.post(\n",
      "                url=self.update_weights_url,\n",
      "                json=dict(model_path=path),\n",
      "            ) as resp:\n",
      "                if resp.status == 200:\n",
      "                    res = await resp.json()\n",
      "                    success = res[\"success\"]\n",
      "                    if success:\n",
      "                        return\n",
      "                    logger.warning(\n",
      "                        f\"Update weights failed: {res['message']}. Retrying.\"\n",
      "                    )\n",
      "                logger.warning(f\"Update weights failed: {resp.reason}. Retrying.\")\n",
      "            time.sleep(0.1)\n",
      "        raise RuntimeError(\"Update weights failed.\")\n",
      "\n",
      "\n",
      "def sglang_server_process(server_args_dict):\n",
      "\n",
      "    from sglang.srt.server_args import ServerArgs\n",
      "    from sglang.srt.utils import kill_process_tree\n",
      "\n",
      "    if pkg_version.is_version_less(\"sglang\", \"0.4.4\"):\n",
      "        server_args_dict.pop(\"log_requests_level\")\n",
      "\n",
      "    if pkg_version.is_version_less(\"sglang\", \"0.4.3\"):\n",
      "        from sglang.srt.server import launch_server\n",
      "\n",
      "        server_args_dict.pop(\"enable_nccl_nvls\")\n",
      "        server_args_dict.pop(\"triton_attention_num_kv_splits\")\n",
      "        server_args_dict.pop(\"cuda_graph_bs\")\n",
      "        server_args_dict.pop(\"enable_memory_saver\")\n",
      "        server_args_dict.pop(\"allow_auto_truncate\")\n",
      "        server_args_dict.pop(\"file_storage_path\")\n",
      "    else:\n",
      "        from sglang.srt.entrypoints.http_server import launch_server\n",
      "\n",
      "    server_args = ServerArgs(**server_args_dict)\n",
      "\n",
      "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(\n",
      "        map(str, list(range(gpu_utils.gpu_count())))\n",
      "    )\n",
      "\n",
      "    try:\n",
      "        logger.info(f\"SGLang Server Args: {server_args}\")\n",
      "        launch_server(server_args)\n",
      "    finally:\n",
      "        kill_process_tree(os.getpid(), include_parent=False)\n",
      "\n",
      "\n",
      "class SGLangGenerationEngine(PipelinableEngine):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        server_args_dict: Dict,\n",
      "        hybrid_train: bool,\n",
      "        request_timeout: int = 1800,\n",
      "    ):\n",
      "        if constants.tensor_parallel_rank() != 0:\n",
      "            dist.barrier(group=constants.tensor_parallel_cpu_group())\n",
      "            return\n",
      "        # Start the serving process\n",
      "        self.server_proc = mp.Process(\n",
      "            target=sglang_server_process,\n",
      "            args=(server_args_dict,),\n",
      "        )\n",
      "        self.server_proc.start()\n",
      "\n",
      "        self.base_url = f\"http://{server_args_dict['host']}:{server_args_dict['port']}\"\n",
      "\n",
      "        self.api_urls = {\n",
      "            \"generate\": f\"{self.base_url}/generate\",\n",
      "            \"offload_weights\": f\"{self.base_url}/offload_weights\",\n",
      "            \"init_kv_cache\": f\"{self.base_url}/init_kv_cache\",\n",
      "            \"clear_kv_cache\": f\"{self.base_url}/clear_kv_cache\",\n",
      "            \"init_model_weights\": f\"{self.base_url}/init_model_weights\",\n",
      "            \"update_weights_from_disk\": f\"{self.base_url}/update_weights_from_disk\",\n",
      "        }\n",
      "\n",
      "        self.wait_server()\n",
      "\n",
      "        if server_args_dict[\"enable_metrics\"]:\n",
      "            dp_rank = constants.data_parallel_rank()\n",
      "            pp_rank = constants.pipe_parallel_rank()\n",
      "            tp_rank = constants.tensor_parallel_rank()\n",
      "            metric_server_name = f\"d{dp_rank}p{pp_rank}t{tp_rank}\"\n",
      "            key = names.metric_server(\n",
      "                constants.experiment_name(),\n",
      "                constants.trial_name(),\n",
      "                \"sglang\",\n",
      "                metric_server_name,\n",
      "            )\n",
      "            host_ip = server_args_dict[\"host\"]\n",
      "            host_port = server_args_dict[\"port\"]\n",
      "            address = f\"{host_ip}:{host_port}\"\n",
      "            name_resolve.add(key, address, keepalive_ttl=None, delete_on_exit=True)\n",
      "            logger.info(f\"SGLang {metric_server_name} metrics URL: {address}\")\n",
      "\n",
      "        self.request_timeout = request_timeout\n",
      "\n",
      "        # offload weights/cache\n",
      "        self.hybrid_train = hybrid_train\n",
      "\n",
      "        dist.barrier(group=constants.tensor_parallel_cpu_group())\n",
      "\n",
      "    def __del__(self):\n",
      "        if hasattr(self, \"server_proc\"):\n",
      "            from sglang.srt.utils import kill_process_tree\n",
      "\n",
      "            self.server_proc.terminate()\n",
      "\n",
      "            kill_process_tree(os.getpid())\n",
      "\n",
      "    # NOTE: A placeholder function.\n",
      "    def train(self, mode: bool = True):\n",
      "        return self\n",
      "\n",
      "    # NOTE: A placeholder function.\n",
      "    def eval(self):\n",
      "        return self\n",
      "\n",
      "    def wait_server(self):\n",
      "        # Wait until the server is launched\n",
      "        from sglang.srt.utils import kill_process_tree\n",
      "        from sglang.utils import get_exception_traceback\n",
      "\n",
      "        success = False\n",
      "        for _ in range(SGLANG_INIT_TIMEOUT):\n",
      "            time.sleep(1)\n",
      "            try:\n",
      "                res = requests.get(\n",
      "                    self.base_url + \"/get_model_info\", timeout=5, headers={}\n",
      "                )\n",
      "                assert res.status_code == 200, f\"{res=}, {res.text=}\"\n",
      "                success = True\n",
      "                break\n",
      "            except (AssertionError, requests.exceptions.RequestException):\n",
      "                last_traceback = get_exception_traceback()\n",
      "                pass\n",
      "        if not success:\n",
      "            logger.error(f\"Initialization failed. warmup error: {last_traceback}\")\n",
      "            kill_process_tree(os.getpid())\n",
      "            return\n",
      "\n",
      "    async def async_generate(\n",
      "        self,\n",
      "        input_: data_api.SequenceSample,\n",
      "        mb_spec: data_api.MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=GenerationHyperparameters\n",
      "        ),\n",
      "        stream: bool = False,\n",
      "        disable_tqdm: bool = False,\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor | None] | None:\n",
      "\n",
      "        pbar = None if disable_tqdm else tqdm(total=input_.bs * gconfig.n)\n",
      "\n",
      "        async with SGLangAPIClient(\n",
      "            generate_url=self.api_urls[\"generate\"],\n",
      "            update_weights_url=self.api_urls[\"update_weights_from_disk\"],\n",
      "        ) as client:\n",
      "            tasks = []\n",
      "            for d in input_.unpack():\n",
      "                if len(d.seqlens[\"packed_input_ids\"]) > 1:\n",
      "                    raise RuntimeError(\n",
      "                        f\"sglang backend does not support grouped generation \"\n",
      "                        f\"for now. Group size {len(d.seqlens['packed_input_ids'])}.\"\n",
      "                    )\n",
      "\n",
      "                prompt_token_ids = d.data[\"packed_input_ids\"].cpu().numpy().tolist()\n",
      "                qid = d.ids[0]\n",
      "                req = APIGenerateInput(\n",
      "                    qid=qid,\n",
      "                    prompt_ids=prompt_token_ids,\n",
      "                    input_ids=prompt_token_ids,\n",
      "                    gconfig=gconfig,\n",
      "                    stop_token_ids=[tokenizer.pad_token_id, tokenizer.eos_token_id],\n",
      "                    return_logprob=True,\n",
      "                )\n",
      "                tasks.append(\n",
      "                    client.async_add_generate_request(\n",
      "                        req,\n",
      "                        stream=stream,\n",
      "                    )\n",
      "                )\n",
      "\n",
      "            outputs = {}\n",
      "            for r in asyncio.as_completed(tasks):\n",
      "                out = await r\n",
      "                outputs[out.qid] = out\n",
      "                if pbar:\n",
      "                    pbar.update(1)\n",
      "\n",
      "            if pbar is not None:\n",
      "                pbar.close()\n",
      "\n",
      "            results: List[APIGenerateOutput] = [outputs[key] for key in input_.ids]\n",
      "\n",
      "        # Build the output: generated token ids, generated token scores,\n",
      "        # and logits mask (which will always be None in sglang).\n",
      "        batch_token_ids = []\n",
      "        batch_logprobs = []\n",
      "        max_seqlen = -1\n",
      "        for x in results:\n",
      "            max_seqlen = max(max_seqlen, max(x.output_lens))\n",
      "            batch_token_ids += x.output_ids\n",
      "            batch_logprobs += x.output_logprobs\n",
      "\n",
      "        # To be consistent with our internal implementation,\n",
      "        # we should pad generated tokens and logprobs\n",
      "        batch_token_ids = [\n",
      "            t + [tokenizer.pad_token_id] * (max_seqlen - len(t))\n",
      "            for t in batch_token_ids\n",
      "        ]\n",
      "        batch_logprobs = [p + [0.0] * (max_seqlen - len(p)) for p in batch_logprobs]\n",
      "\n",
      "        return (\n",
      "            torch.tensor(\n",
      "                batch_token_ids, dtype=torch.long, device=constants.current_device()\n",
      "            ),\n",
      "            torch.tensor(\n",
      "                batch_logprobs, dtype=torch.float32, device=constants.current_device()\n",
      "            ),\n",
      "            None,\n",
      "        )\n",
      "\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: data_api.SequenceSample,\n",
      "        mb_spec: data_api.MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=GenerationHyperparameters\n",
      "        ),\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor | None] | None:\n",
      "        if gconfig.min_new_tokens != 0:\n",
      "            raise RuntimeError(\n",
      "                \"NOTE: passing in an arbitrary `min_new_tokens` will lead to a bug for SGLang v0.4.3 \"\n",
      "                \"because we force to skip_tokenizer_init.\"\n",
      "            )\n",
      "        if constants.tensor_parallel_rank() != 0:\n",
      "            dist.barrier(group=constants.tensor_parallel_cpu_group())\n",
      "            return None, None, None\n",
      "\n",
      "        def run_in_thread():\n",
      "            # Create a new event loop for this thread\n",
      "            new_loop = asyncio.new_event_loop()\n",
      "            asyncio.set_event_loop(new_loop)\n",
      "            try:\n",
      "                return new_loop.run_until_complete(\n",
      "                    self.async_generate(\n",
      "                        input_=input_,\n",
      "                        mb_spec=mb_spec,\n",
      "                        tokenizer=tokenizer,\n",
      "                        gconfig=gconfig,\n",
      "                    )\n",
      "                )\n",
      "            finally:\n",
      "                new_loop.close()\n",
      "\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "\n",
      "        with ThreadPoolExecutor() as executor:\n",
      "            future = executor.submit(run_in_thread)\n",
      "            results = future.result()\n",
      "        dist.barrier(group=constants.tensor_parallel_cpu_group())\n",
      "        return results\n",
      "\n",
      "    def update_weights_from_disk(self, path):\n",
      "        if constants.tensor_parallel_rank() != 0:\n",
      "            dist.barrier(group=constants.tensor_parallel_cpu_group())\n",
      "            return\n",
      "\n",
      "        resp = requests.post(\n",
      "            url=self.api_urls[\"update_weights_from_disk\"],\n",
      "            json=dict(model_path=path),\n",
      "        )\n",
      "        resp.raise_for_status()\n",
      "        res = resp.json()\n",
      "        assert res[\"success\"]\n",
      "        dist.barrier(group=constants.tensor_parallel_cpu_group())\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class SGLangGenerationBackend(ModelBackend, SGLangConfig):\n",
      "    model_path: str = \"\"\n",
      "\n",
      "    def _initialize(self, model: Model, spec: FinetuneSpec) -> Model:\n",
      "        if constants.pipe_parallel_world_size() != 1:\n",
      "            raise RuntimeError(\"SGLang does not support pipe parallel size > 1.\")\n",
      "        if constants.tensor_parallel_world_size() > cluster.spec.n_gpus_per_node:\n",
      "            raise RuntimeError(\n",
      "                \"AReaL's SGLang integration does not support model parallel size > n_gpus_per_node.\"\n",
      "            )\n",
      "\n",
      "        additional_args = dataclasses.asdict(self)\n",
      "        additional_args.pop(\"hybrid_train\")\n",
      "        additional_args[\"random_seed\"] = seeding.get_seed()\n",
      "\n",
      "        # For simplicity, we let all DP ranks have different ports.\n",
      "        ports = [None for _ in range(constants.data_parallel_world_size())]\n",
      "        while any(port is None for port in ports) or len(\n",
      "            set(datapack.flat2d(ports))\n",
      "        ) != len(datapack.flat2d(ports)):\n",
      "            dist.all_gather_object(\n",
      "                ports,\n",
      "                network.find_multiple_free_ports(\n",
      "                    2,\n",
      "                    low=10000,\n",
      "                    high=60000,\n",
      "                    experiment_name=constants.experiment_name(),\n",
      "                    trial_name=constants.trial_name(),\n",
      "                ),\n",
      "                group=constants.data_parallel_group(),\n",
      "            )\n",
      "        api_server_port, dist_port = ports[constants.data_parallel_rank()]\n",
      "        additional_args[\"port\"] = api_server_port\n",
      "\n",
      "        host_ip = socket.gethostbyname(socket.gethostname())\n",
      "        server_args_dict = dict(\n",
      "            host=\"localhost\" if not self.enable_metrics else host_ip,\n",
      "            # Model and tokenizer\n",
      "            tokenizer_path=self.model_path,\n",
      "            tokenizer_mode=\"auto\",\n",
      "            load_format=\"auto\",\n",
      "            trust_remote_code=True,\n",
      "            device=\"cuda\",\n",
      "            served_model_name=f\"{constants.experiment_name()}/{constants.trial_name()}/{constants.model_name().role}\",\n",
      "            is_embedding=False,\n",
      "            skip_tokenizer_init=True,\n",
      "            # Other runtime options\n",
      "            tp_size=constants.tensor_parallel_world_size(),\n",
      "            # Because we have set CUDA_VISIBLE_DEVICES to a single GPU in each process\n",
      "            base_gpu_id=int(os.environ[\"CUDA_VISIBLE_DEVICES\"]),\n",
      "            file_storage_path=os.path.join(\n",
      "                constants.SGLANG_CACHE_PATH,\n",
      "                f\"sglang_storage{constants.data_parallel_rank()}\",\n",
      "            ),\n",
      "            # Data parallelism\n",
      "            dp_size=1,  # TODO: check whether we require SGLang dp\n",
      "            load_balance_method=\"round_robin\",\n",
      "            # Expert parallelism\n",
      "            ep_size=1,  # TODO: check\n",
      "            # Multi-node distributed serving\n",
      "            dist_init_addr=f\"{network.gethostip()}:{dist_port}\",\n",
      "            nnodes=1,\n",
      "            node_rank=0,\n",
      "            **additional_args,\n",
      "        )\n",
      "\n",
      "        model.module = SGLangGenerationEngine(\n",
      "            server_args_dict,\n",
      "            hybrid_train=self.hybrid_train,\n",
      "        )\n",
      "        model.backend_name = \"sglang\"\n",
      "        return model\n",
      "\n",
      "    def load(self, model: Model, load_dir: str):\n",
      "        model.module.update_weights_from_disk(load_dir)\n",
      "\n",
      "\n",
      "register_backend(\"sglang\", SGLangGenerationBackend)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/mock_train.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import dataclasses\n",
      "import random\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core import model_api\n",
      "from realhf.api.core.data_api import MicroBatchSpec, SequenceSample\n",
      "from realhf.base import constants, logging\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.model.backend.inference import PipelinableInferenceEngine\n",
      "from realhf.impl.model.backend.pipe_runner import PipelineRunner, PipeTrainInstrSet\n",
      "from realhf.impl.model.modules.mlp import get_activation_fn\n",
      "from realhf.impl.model.nn.flatten_param import ContiguousParamSpec\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.nn.real_llm_base import ReaLModelBlock\n",
      "from realhf.impl.model.parallelism.pipeline_parallel.tensor_storage import TensorBuffer\n",
      "\n",
      "logger = logging.getLogger(\"Mock Train Backend\", \"benchmark\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MockPipeTrainInstrSet(PipeTrainInstrSet):\n",
      "    \"\"\"A trivial pipelined intrsuction set for training.\n",
      "\n",
      "    Used for testing only.\n",
      "    \"\"\"\n",
      "\n",
      "    optim: torch.optim.Optimizer\n",
      "\n",
      "    def _exec_backward_pass(\n",
      "        self,\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        output_x = tensor_buffer.get(\"batch_output_x\", micro_batch_id, remove=True)\n",
      "\n",
      "        is_last_stage = constants.is_last_pipe_stage()\n",
      "        if is_last_stage:\n",
      "            loss: torch.Tensor = tensor_buffer.get(\n",
      "                \"losses\", micro_batch_id, remove=True\n",
      "            )\n",
      "            loss.backward()\n",
      "            tensor_buffer.put(\"losses\", micro_batch_id, loss.detach().clone())\n",
      "            return\n",
      "\n",
      "        grad = tensor_buffer.get(\"grad\", micro_batch_id, remove=True)\n",
      "        output_tensor = output_x.pp_output\n",
      "        torch.autograd.backward(tensors=output_tensor, grad_tensors=grad)\n",
      "\n",
      "    def _exec_reduce_grads(\n",
      "        self,\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        for p in module.parameters():\n",
      "            if not p.requires_grad:\n",
      "                continue\n",
      "            dist.all_reduce(p.grad, group=constants.data_parallel_group())\n",
      "\n",
      "    def _exec_optimizer_step(\n",
      "        self,\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        self.optim.step()\n",
      "        # NOTE: we only have one optimizer step for each stage, so micro_batch_id can be 0\n",
      "        tensor_buffer.put(\"stats\", 0, dict(random_stat=random.random()))\n",
      "\n",
      "\n",
      "class AdamWithLossScale(torch.optim.Adam):\n",
      "    def get_loss_scale(self) -> torch.Tensor:\n",
      "        return torch.tensor([1.0], device=constants.current_device())\n",
      "\n",
      "\n",
      "class MockTrainEngine(model_api.PipelinableEngine):\n",
      "\n",
      "    def __init__(self, module: ReaLModel, optimizer: AdamWithLossScale):\n",
      "        self.module = module\n",
      "        self.optim = optimizer\n",
      "\n",
      "        self.inf_engine = PipelinableInferenceEngine(module)\n",
      "        if constants.pipe_parallel_world_size() > 1:\n",
      "            self.pipe_runner = self.inf_engine.pipe_runner\n",
      "\n",
      "        self.device = module.device\n",
      "        self.dtype = module.dtype\n",
      "\n",
      "    def train(self, mode: bool = True):\n",
      "        self.module.train(mode)\n",
      "        return self\n",
      "\n",
      "    def eval(self):\n",
      "        self.module.eval()\n",
      "        return self\n",
      "\n",
      "    def train_batch(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        loss_fn: Callable,\n",
      "        loss_weight_fn: Callable,\n",
      "        token_normalize_scope: str,\n",
      "        version_steps: int,\n",
      "    ):\n",
      "        self.optim.zero_grad()\n",
      "        if constants.pipe_parallel_world_size() > 1:\n",
      "            # Fusing the minibatched forward-backward in a pipeline training schedule.\n",
      "            instr_set = MockPipeTrainInstrSet(self, self.optim)\n",
      "            # NOTE: When training with pipeline parallel, num micro batches should be\n",
      "            # larger than 2 x num_pipeline_stages to avoid idle time.\n",
      "            return self.pipe_runner.train_batch(\n",
      "                instr_set=instr_set,\n",
      "                input_=input_,\n",
      "                mb_spec=mb_spec,\n",
      "                loss_fn=loss_fn,\n",
      "                loss_weight_fn=loss_weight_fn,\n",
      "                token_normalize_scope=token_normalize_scope,\n",
      "                version_steps=version_steps,\n",
      "            )\n",
      "\n",
      "        mb_inputs = input_.synced_data_parallel_split(mb_spec)\n",
      "        total_loss_weight = torch.tensor(\n",
      "            sum([loss_weight_fn(mb) for mb in mb_inputs]), dtype=torch.float32\n",
      "        )\n",
      "        if token_normalize_scope == \"global\":\n",
      "            dist.all_reduce(total_loss_weight, group=constants.data_parallel_group())\n",
      "        if total_loss_weight == 0:\n",
      "            raise model_api.ZeroTotalLossWeightException(\n",
      "                \"The sum of loss weights of all micro batches is zero.\"\n",
      "            )\n",
      "\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.info(\n",
      "                f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "            )\n",
      "        for i, mb_input in enumerate(mb_inputs):\n",
      "            input_lens = torch.tensor(\n",
      "                flat2d(mb_input.seqlens[\"packed_input_ids\"]),\n",
      "                dtype=torch.int32,\n",
      "                device=self.device,\n",
      "            )\n",
      "            max_seqlen = int(max(input_lens))\n",
      "            cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "            model_output = self.module(\n",
      "                packed_input_ids=mb_input.data[\"packed_input_ids\"],\n",
      "                cu_seqlens=cu_seqlens,\n",
      "                max_seqlen=max_seqlen,\n",
      "            ).logits\n",
      "            loss = loss_fn(model_output, mb_input)\n",
      "            loss_scale = loss_weight_fn(mb_inputs[i]) / total_loss_weight\n",
      "            if token_normalize_scope == \"global\":\n",
      "                loss_scale *= constants.data_parallel_world_size()\n",
      "            loss *= loss_scale\n",
      "\n",
      "        return dict(random_stat=random.random())\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def forward(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        output_seqlens: List[List[int]] | None = None,\n",
      "        post_hook: Callable[[torch.Tensor, SequenceSample], Any] | None = None,\n",
      "        aggregate_fn: Callable[[List[Any]], Any] = torch.cat,\n",
      "    ):\n",
      "        return self.inf_engine.forward(\n",
      "            input_=input_,\n",
      "            mb_spec=mb_spec,\n",
      "            output_seqlens=output_seqlens,\n",
      "            post_hook=post_hook,\n",
      "            aggregate_fn=aggregate_fn,\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: model_api.GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=model_api.GenerationHyperparameters\n",
      "        ),\n",
      "    ):\n",
      "        return self.inf_engine.generate(\n",
      "            input_=input_,\n",
      "            mb_spec=mb_spec,\n",
      "            tokenizer=tokenizer,\n",
      "            gconfig=gconfig,\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MockTrainBackend(model_api.ModelBackend):\n",
      "    optimizer_name: str = dataclasses.field(\n",
      "        metadata={\"choices\": [\"adam\"]},\n",
      "        default=\"adam\",\n",
      "    )\n",
      "    optimizer_config: dict = dataclasses.field(\n",
      "        default_factory=lambda: dict(\n",
      "            lr=1e-5, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-5\n",
      "        )\n",
      "    )\n",
      "\n",
      "    def _initialize(\n",
      "        self, model: model_api.Model, spec: model_api.FinetuneSpec\n",
      "    ) -> model_api.Model:\n",
      "        module = model.module\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            raise ValueError(\"MegatronTrainBackend only supports ReaLModel.\")\n",
      "\n",
      "        if self.optimizer_name == \"adam\":\n",
      "            optimizer = AdamWithLossScale(module.parameters(), **self.optimizer_config)\n",
      "        else:\n",
      "            raise NotImplementedError(\n",
      "                f\"Optimizer {self.optimizer_name} not implemented for testing.\"\n",
      "            )\n",
      "\n",
      "        model.module = MockTrainEngine(module, optimizer)\n",
      "        model.backend_name = \"mock_train\"\n",
      "        return model\n",
      "\n",
      "\n",
      "model_api.register_backend(\"mock_train\", MockTrainBackend)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/vllm.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import dataclasses\n",
      "import functools\n",
      "import time\n",
      "from typing import Dict, List, Optional, Tuple\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "\n",
      "try:\n",
      "    from vllm import LLM\n",
      "    from vllm.engine.arg_utils import EngineArgs\n",
      "    from vllm.inputs.data import TokensPrompt\n",
      "    from vllm.sampling_params import SamplingParams\n",
      "    from vllm.utils import Counter\n",
      "\n",
      "    from realhf.impl.model.backend.thirdparty.vllm import (\n",
      "        GPUExecutor_,\n",
      "        LLMEngine_,\n",
      "        init_vllm,\n",
      "    )\n",
      "except ModuleNotFoundError:\n",
      "\n",
      "    class LLM:\n",
      "        pass\n",
      "\n",
      "    class LLMEngine_:\n",
      "        pass\n",
      "\n",
      "\n",
      "from realhf.api.cli_args import vLLMConfig\n",
      "from realhf.api.core import data_api, model_api\n",
      "from realhf.base import constants, logging, seeding\n",
      "\n",
      "logger = logging.getLogger(\"vLLM backend\")\n",
      "\n",
      "\n",
      "class vLLMGenerationEngine(model_api.PipelinableEngine, LLM):\n",
      "    def __init__(self, llm_engine: LLMEngine_, hybrid_train: bool):\n",
      "        # NOTE: vLLM's `LLM` class exactly assigns the following\n",
      "        # two attributes.\n",
      "        self.llm_engine = llm_engine\n",
      "        self.request_counter = Counter()\n",
      "\n",
      "        self.dtype = llm_engine.model_executor.model_config.dtype\n",
      "        self.device = llm_engine.model_executor.device_config.device\n",
      "\n",
      "        self.hybrid_train = hybrid_train\n",
      "        if self.hybrid_train:\n",
      "            self.llm_engine.model_executor.clear_kv_cache()\n",
      "\n",
      "    # NOTE: A placeholder function.\n",
      "    def train(self, mode: bool = True):\n",
      "        return self\n",
      "\n",
      "    # NOTE: A placeholder function.\n",
      "    def eval(self):\n",
      "        return self\n",
      "\n",
      "    def update_weights_from_disk(self, path):\n",
      "        self.llm_engine.model_executor.update_weights(path)\n",
      "\n",
      "    # A wraper over vLLM's LLM.generate() function.\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: data_api.SequenceSample,\n",
      "        mb_spec: data_api.MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: model_api.GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=model_api.GenerationHyperparameters\n",
      "        ),\n",
      "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor | None] | None:\n",
      "        # init kv cache\n",
      "        tik = time.perf_counter()\n",
      "        if self.hybrid_train:\n",
      "            self.llm_engine._initialize_kv_caches()\n",
      "\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            if not gconfig.force_no_logits_mask:\n",
      "                logger.warning(\"vLLM does not returns the logits mask.\")\n",
      "\n",
      "        # Unpack the input and convert prompts into lists of integers.\n",
      "        prompts = []\n",
      "        sample_params = []\n",
      "        for d in input_.unpack():\n",
      "            if len(d.seqlens[\"packed_input_ids\"]) > 1:\n",
      "                raise RuntimeError(\n",
      "                    f\"vLLM backend does not support grouped generation \"\n",
      "                    f\"for now. Group size {len(d.seqlens['packed_input_ids'])}.\"\n",
      "                )\n",
      "            max_num_seqs = self.llm_engine.scheduler_config.max_num_seqs\n",
      "            if max_num_seqs < gconfig.n:\n",
      "                n_replicas = (gconfig.n + max_num_seqs - 1) // max_num_seqs\n",
      "                sp_ns = [max_num_seqs for _ in range(n_replicas - 1)] + [\n",
      "                    gconfig.n - max_num_seqs * (n_replicas - 1)\n",
      "                ]\n",
      "            else:\n",
      "                n_replicas = 1\n",
      "                sp_ns = [gconfig.n]\n",
      "            prompts += [\n",
      "                TokensPrompt(\n",
      "                    prompt_token_ids=d.data[\"packed_input_ids\"].cpu().numpy().tolist()\n",
      "                )\n",
      "            ] * n_replicas\n",
      "            sample_params += [\n",
      "                SamplingParams(\n",
      "                    n=n,\n",
      "                    top_p=gconfig.top_p,\n",
      "                    top_k=gconfig.top_k,\n",
      "                    max_tokens=gconfig.max_new_tokens,\n",
      "                    min_tokens=gconfig.min_new_tokens,\n",
      "                    temperature=0.0 if gconfig.greedy else gconfig.temperature,\n",
      "                    detokenize=False,\n",
      "                    logprobs=0,\n",
      "                )\n",
      "                for n in sp_ns\n",
      "            ]\n",
      "\n",
      "        # TODO: find a way to get the GPU tensors.\n",
      "        req_outputs = LLM.generate(\n",
      "            self,\n",
      "            prompts=prompts,\n",
      "            sampling_params=sample_params,\n",
      "            use_tqdm=True,\n",
      "        )\n",
      "\n",
      "        # Build the output: generated token ids, generated token scores,\n",
      "        # and logits mask (which will always be None in vLLM).\n",
      "        batch_token_ids = []\n",
      "        batch_logprobs = []\n",
      "        max_seqlen = -1\n",
      "        for req_output in req_outputs:\n",
      "            for output in req_output.outputs:\n",
      "                max_seqlen = max(max_seqlen, len(output.token_ids))\n",
      "                batch_token_ids.append(list(output.token_ids))\n",
      "                assert len(output.logprobs) == len(output.token_ids)\n",
      "                logprobs = []\n",
      "                for t, logp in zip(output.token_ids, output.logprobs):\n",
      "                    logprobs.append(logp[t].logprob)\n",
      "                batch_logprobs.append(logprobs)\n",
      "\n",
      "        # To be consistent with our internal implementation,\n",
      "        # we should pad generated tokens and logprobs\n",
      "        batch_token_ids = [\n",
      "            t + [tokenizer.pad_token_id] * (max_seqlen - len(t))\n",
      "            for t in batch_token_ids\n",
      "        ]\n",
      "        batch_logprobs = [p + [0.0] * (max_seqlen - len(p)) for p in batch_logprobs]\n",
      "\n",
      "        # clear kv cache and offload model weights\n",
      "        if self.hybrid_train:\n",
      "            tik = time.perf_counter()\n",
      "            self.llm_engine.model_executor.offload_weights()\n",
      "            self.llm_engine.model_executor.clear_kv_cache()\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                logger.info(f\"Clear KV cache time: {time.perf_counter() - tik}s\")\n",
      "\n",
      "        return (\n",
      "            torch.tensor(batch_token_ids, dtype=torch.long, device=self.device),\n",
      "            torch.tensor(batch_logprobs, dtype=torch.float32, device=self.device),\n",
      "            None,\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class vLLMGenerationBackend(vLLMConfig, model_api.ModelBackend):\n",
      "    model_path: str = \"\"\n",
      "\n",
      "    def _initialize(\n",
      "        self, model: model_api.Model, spec: model_api.FinetuneSpec\n",
      "    ) -> model_api.Model:\n",
      "\n",
      "        init_vllm()\n",
      "\n",
      "        if constants.pipe_parallel_world_size() != 1:\n",
      "            raise NotImplementedError(\n",
      "                \"vLLM does not support pipeline parallelism for now.\"\n",
      "            )\n",
      "\n",
      "        engine_kwargs = dict(\n",
      "            # Basic config.\n",
      "            model=self.model_path,\n",
      "            tokenizer=self.model_path,\n",
      "            tokenizer_mode=\"auto\",\n",
      "            skip_tokenizer_init=False,\n",
      "            trust_remote_code=True,\n",
      "            max_model_len=self.max_model_len,\n",
      "            seed=seeding.get_seed(),\n",
      "            dtype=getattr(torch, self.dtype),\n",
      "            kv_cache_dtype=self.kv_cache_type,\n",
      "            device=constants.current_device(),\n",
      "            # Parallelism.\n",
      "            tensor_parallel_size=constants.tensor_parallel_world_size(),\n",
      "            pipeline_parallel_size=constants.pipe_parallel_world_size(),\n",
      "            # KV cahce and scheduling.\n",
      "            num_scheduler_steps=self.num_scheduler_steps,\n",
      "            multi_step_stream_outputs=self.multi_step_stream_outputs,\n",
      "            block_size=self.block_size,\n",
      "            swap_space=self.swap_space,\n",
      "            cpu_offload_gb=self.cpu_offload_gb,\n",
      "            max_num_seqs=self.max_num_seqs,\n",
      "            # max_num_batched_tokens=bs * 1024,\n",
      "            # enable_chunked_prefill=False,\n",
      "            # Our default system-wise configs.\n",
      "            max_seq_len_to_capture=self.max_seq_len_to_capture,\n",
      "            enable_prefix_caching=self.enable_prefix_caching,\n",
      "            gpu_memory_utilization=self.gpu_memory_utilization,\n",
      "            disable_sliding_window=self.disable_sliding_window,\n",
      "            enable_chunked_prefill=self.enable_chunked_prefill,\n",
      "            disable_custom_all_reduce=True,\n",
      "            disable_async_output_proc=False,\n",
      "            disable_log_stats=False,\n",
      "            worker_use_ray=False,\n",
      "            enforce_eager=self.enforce_eager,\n",
      "        )\n",
      "        for k, v in self.additional_engine_args.items():\n",
      "            if k in engine_kwargs:\n",
      "                logger.warning(f\"Overriding {k} from {engine_kwargs[k]} to {v}\")\n",
      "            engine_kwargs[k] = v\n",
      "        engine_args = EngineArgs(**engine_kwargs)\n",
      "        # Create the engine configs.\n",
      "        engine_config = engine_args.create_engine_config()\n",
      "        # just a random name\n",
      "        engine_config.parallel_config.distributed_executor_backed = \"realhf\"\n",
      "\n",
      "        executor_class = GPUExecutor_\n",
      "\n",
      "        # Create the LLM engine.\n",
      "        # By default, KV caches will be initialized during LLMEngine initialization.\n",
      "        # We should release them first and then re-initialize them upon each\n",
      "        # generation call.\n",
      "        llm_engine = LLMEngine_(\n",
      "            **engine_config.to_dict(),\n",
      "            executor_class=executor_class,\n",
      "            log_stats=not engine_args.disable_log_stats,\n",
      "            stat_loggers=None,\n",
      "        )\n",
      "        model.module = vLLMGenerationEngine(\n",
      "            llm_engine,\n",
      "            hybrid_train=self.hybrid_train,\n",
      "        )\n",
      "        model.backend_name = \"vllm\"\n",
      "        return model\n",
      "\n",
      "\n",
      "model_api.register_backend(\"vllm\", vLLMGenerationBackend)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/inference.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import collections\n",
      "import dataclasses\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import transformers\n",
      "\n",
      "import realhf.api.core.model_api as model_api\n",
      "import realhf.base.constants as constants\n",
      "import realhf.base.logging as logging\n",
      "from realhf.api.core.data_api import MicroBatchSpec, SequenceSample\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.impl.model.backend.pipe_runner import PipelineRunner\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.nn.real_llm_generate import _gather_minibatch_gen_outputs\n",
      "\n",
      "logger = logging.getLogger(\"PipelinableInferenceEngine\")\n",
      "\n",
      "\n",
      "class PipelinableInferenceEngine(model_api.PipelinableEngine):\n",
      "\n",
      "    def __init__(self, module: ReaLModel):\n",
      "        self.module = module\n",
      "\n",
      "        # NOTE: In profiler, module could be not a instance of ReaLModel.\n",
      "        self.device = module.device if hasattr(module, \"device\") else None\n",
      "        self.dtype = module.dtype if hasattr(module, \"dtype\") else None\n",
      "\n",
      "        if constants.pipe_parallel_world_size() > 1:\n",
      "            self.pipe_runner = PipelineRunner(module)\n",
      "            self._log_trainable_params()\n",
      "\n",
      "    def _log_trainable_params(self):\n",
      "        model_parameters = filter(lambda p: p.requires_grad, self.module.parameters())\n",
      "        num_params = sum([p.numel() for p in model_parameters])\n",
      "        if num_params == 0:\n",
      "            return\n",
      "        shared_params = 0\n",
      "        if self.module.shared_embedding_or_output_weight() is not None:\n",
      "            shared_params = self.module.shared_embedding_or_output_weight().numel()\n",
      "        unique_params = num_params - shared_params\n",
      "\n",
      "        params_tensor = torch.LongTensor(data=[num_params, unique_params]).to(\n",
      "            self.device\n",
      "        )\n",
      "        dist.all_reduce(\n",
      "            params_tensor, group=constants.grid().get_model_parallel_group()\n",
      "        )\n",
      "        params_tensor = params_tensor.tolist()\n",
      "        total_params = params_tensor[0]\n",
      "        unique_params = params_tensor[1]\n",
      "\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"CONFIG: default_train_mbs={self.pipe_runner.default_train_mbs} \"\n",
      "                f\"default_inf_mbs={self.pipe_runner.default_inf_mbs} \"\n",
      "                f\"num_layers(this stage)={self.module.num_layers} \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()} \"\n",
      "                f\"dp_size={constants.data_parallel_world_size()} \"\n",
      "                f\"tp_size={constants.tensor_parallel_world_size()} \"\n",
      "            )\n",
      "        if constants.data_parallel_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"rank={constants.parallelism_rank()} \"\n",
      "                f\"stage={constants.pipe_parallel_rank()} \"\n",
      "                f\"layers={self.module.num_layers} \"\n",
      "                f\"[{self.module.layer_idx_start}, {self.module.layer_idx_end}) \"\n",
      "                f\"stage_params={num_params} ({num_params/1e6:0.3f}M) \"\n",
      "                f\"total_params={total_params} ({total_params/1e6:0.3f}M) \"\n",
      "                f\"unique_params={unique_params} ({unique_params/1e6:0.3f}M)\"\n",
      "            )\n",
      "\n",
      "    def train(self, mode: bool = True):\n",
      "        self.module.train(mode)\n",
      "        return self\n",
      "\n",
      "    def eval(self):\n",
      "        self.module.eval()\n",
      "        return self\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def forward(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        output_seqlens: List[List[int]] | None = None,\n",
      "        post_hook: Callable[[torch.Tensor, SequenceSample], Any] | None = None,\n",
      "        aggregate_fn: Callable[[List[Any]], Any] = torch.cat,\n",
      "    ):\n",
      "        if constants.pipe_parallel_world_size() > 1:\n",
      "            # post_hook will post-process the output tensor immediately,\n",
      "            # so flushing all micro-bathes into the pipline engine will\n",
      "            # not increase the GPU memory usage.\n",
      "            return self.pipe_runner.forward(\n",
      "                input_=input_,\n",
      "                mb_spec=mb_spec,\n",
      "                output_seqlens=output_seqlens,\n",
      "                post_hook=post_hook,\n",
      "                aggregate_fn=aggregate_fn,\n",
      "            )\n",
      "        mb_inputs, fwd_indices, bwd_indices = input_.split(mb_spec)\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "            )\n",
      "        outputs = []\n",
      "        num_micro_batches = len(mb_inputs)\n",
      "        for i, mb_input in enumerate(mb_inputs):\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                logger.debug(\n",
      "                    f\"{constants.model_name()} in forward {i+1}/{num_micro_batches}\"\n",
      "                )\n",
      "\n",
      "            input_lens = torch.tensor(\n",
      "                flat2d(mb_input.seqlens[\"packed_input_ids\"]),\n",
      "                dtype=torch.int32,\n",
      "                device=constants.current_device(),\n",
      "            )\n",
      "            max_seqlen = int(max(input_lens))\n",
      "            cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "            model_output = self.module(\n",
      "                packed_input_ids=mb_input.data[\"packed_input_ids\"],\n",
      "                cu_seqlens=cu_seqlens,\n",
      "                max_seqlen=max_seqlen,\n",
      "            ).logits\n",
      "            if post_hook:\n",
      "                model_output = post_hook(model_output, mb_input)\n",
      "            outputs.append(model_output)\n",
      "        res = aggregate_fn(outputs)\n",
      "        if isinstance(res, torch.Tensor):\n",
      "            res = SequenceSample.reorder_output(\n",
      "                res,\n",
      "                expected_seqlens=(\n",
      "                    output_seqlens\n",
      "                    if output_seqlens is not None\n",
      "                    else input_.seqlens[\"packed_input_ids\"]\n",
      "                ),\n",
      "                forward_indices=fwd_indices,\n",
      "                backward_indices=bwd_indices,\n",
      "            )\n",
      "        return res\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: model_api.GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=model_api.GenerationHyperparameters\n",
      "        ),\n",
      "    ):\n",
      "        # NOTE: Interleave mini-batches in the pipeline results will not decrease\n",
      "        # the memory usage, because we need to hold all KV-caches for different\n",
      "        # mini-batches, so we split mini-batches in the outer loop.\n",
      "        mb_inputs, *_ = input_.split(mb_spec)\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.debug(\n",
      "                f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "            )\n",
      "        sequences, scores, logits_mask = [], [], []\n",
      "        for i, mb_input in enumerate(mb_inputs):\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                logger.debug(\n",
      "                    f\"{constants.model_name()} in generate {i+1}/{len(mb_inputs)}\"\n",
      "                )\n",
      "            if constants.pipe_parallel_world_size() > 1:\n",
      "                res = self.pipe_runner.generate(\n",
      "                    input_=mb_input,\n",
      "                    tokenizer=tokenizer,\n",
      "                    gconfig=gconfig,\n",
      "                )\n",
      "                if res is not None:\n",
      "                    seq, s, lmask, *_ = res\n",
      "                else:\n",
      "                    seq, s, lmask = None, None, None\n",
      "            else:\n",
      "                input_lens = torch.tensor(\n",
      "                    flat2d(mb_input.seqlens[\"packed_input_ids\"]),\n",
      "                    dtype=torch.int32,\n",
      "                    device=constants.current_device(),\n",
      "                )\n",
      "                max_seqlen = int(max(input_lens))\n",
      "                cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "                res = self.module.generate(\n",
      "                    tokenizer=tokenizer,\n",
      "                    packed_input_ids=mb_input.data[\"packed_input_ids\"],\n",
      "                    cu_seqlens=cu_seqlens,\n",
      "                    max_seqlen=max_seqlen,\n",
      "                    gconfig=gconfig,\n",
      "                )\n",
      "                seq, s, lmask = res.sequences, res.scores, res.logits_mask\n",
      "            sequences.append(seq)\n",
      "            scores.append(s)\n",
      "            logits_mask.append(lmask)\n",
      "        if constants.is_last_pipe_stage():\n",
      "            if len(mb_inputs) == 1:\n",
      "                return sequences[0], scores[0], logits_mask[0]\n",
      "            else:\n",
      "                return _gather_minibatch_gen_outputs(\n",
      "                    sequences,\n",
      "                    scores,\n",
      "                    logits_mask,\n",
      "                    pad_token_id=tokenizer.pad_token_id,\n",
      "                )\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PipelineInferenceBackend(model_api.ModelBackend):\n",
      "\n",
      "    def _initialize(self, model: model_api.Model, spec: model_api.FinetuneSpec):\n",
      "        model.module = PipelinableInferenceEngine(model.module)\n",
      "        model.backend_name = \"inference\"\n",
      "        return model\n",
      "\n",
      "\n",
      "model_api.register_backend(\"inference\", PipelineInferenceBackend)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/megatron.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "# Parts of code are modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "import collections\n",
      "import dataclasses\n",
      "import math\n",
      "import pathlib\n",
      "from contextlib import contextmanager\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import transformers\n",
      "from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n",
      "\n",
      "from realhf.api.cli_args import MegatronConfig, MicroBatchSpec, OptimizerConfig\n",
      "from realhf.api.core import model_api\n",
      "from realhf.api.core.data_api import SequenceSample\n",
      "from realhf.base import constants, logging, pkg_version\n",
      "from realhf.base.datapack import flat2d\n",
      "from realhf.base.monitor import CUDATimeMarkType, cuda_tmarked\n",
      "from realhf.impl.model.backend.inference import PipelinableInferenceEngine\n",
      "from realhf.impl.model.backend.pipe_runner import PipelineRunner, PipeTrainInstrSet\n",
      "from realhf.impl.model.modules.mlp import get_activation_fn\n",
      "from realhf.impl.model.nn.flatten_param import ContiguousParamSpec\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.nn.real_llm_base import ReaLModelBlock\n",
      "from realhf.impl.model.parallelism.pipeline_parallel.tensor_storage import TensorBuffer\n",
      "\n",
      "megatron_available = pkg_version.is_available(\"megatron.core\")\n",
      "try:\n",
      "    # Monkey patch\n",
      "    import megatron.core.optimizer as mcore_optim\n",
      "\n",
      "    class DistributedOptimizer(mcore_optim.DistributedOptimizer):\n",
      "        def get_model_parallel_group(self):\n",
      "            return constants.parallelism_group()\n",
      "\n",
      "        def get_grad_stats_parallel_group(self):\n",
      "            return constants.parallelism_group()\n",
      "\n",
      "    mcore_optim.DistributedOptimizer = DistributedOptimizer\n",
      "\n",
      "    from megatron.core import parallel_state\n",
      "    from megatron.core.distributed.distributed_data_parallel import (\n",
      "        DistributedDataParallel,\n",
      "    )\n",
      "    from megatron.core.optimizer import DistributedOptimizer, get_megatron_optimizer\n",
      "    from megatron.core.optimizer.optimizer_config import (\n",
      "        OptimizerConfig as MegatronOptimizerConfig,\n",
      "    )\n",
      "    from megatron.core.transformer.transformer_config import (\n",
      "        TransformerConfig as MegatronTransformerConfig,\n",
      "    )\n",
      "\n",
      "except (ModuleNotFoundError, ImportError):\n",
      "    # importing megatron.core in CPU container will fail due to the requirement of apex\n",
      "    # Here class types must be defined for type hinting\n",
      "    class MegatronTransformerConfig:\n",
      "        pass\n",
      "\n",
      "    class DistributedDataParallel:\n",
      "        pass\n",
      "\n",
      "    class DistributedOptimizer:\n",
      "        pass\n",
      "\n",
      "\n",
      "if megatron_available:\n",
      "    if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.7.0\"):\n",
      "        from megatron.core.optimizer_param_scheduler import OptimizerParamScheduler\n",
      "    else:\n",
      "        from realhf.impl.model.backend.thirdparty.megatron.v0_6_0.lr_schduler import (\n",
      "            OptimizerParamScheduler,\n",
      "        )\n",
      "\n",
      "\n",
      "WITHIN_MEGATRON_CONTEXT = False\n",
      "\n",
      "logger = logging.getLogger(\"Megatron Backend\", \"benchmark\")\n",
      "\n",
      "\n",
      "@contextmanager\n",
      "def megatron_ctx():\n",
      "    global WITHIN_MEGATRON_CONTEXT\n",
      "    if WITHIN_MEGATRON_CONTEXT:\n",
      "        raise RuntimeError(\"Megatron context is already set up. Destroy it first.\")\n",
      "\n",
      "    WITHIN_MEGATRON_CONTEXT = True\n",
      "\n",
      "    grid = constants.grid()\n",
      "    # TODO: implement context parallel.\n",
      "    # TODO: implement expert parallel.\n",
      "\n",
      "    # Build the data-parallel groups.\n",
      "    g = constants.data_parallel_group()\n",
      "    parallel_state._DATA_PARALLEL_GROUP = g\n",
      "    parallel_state._DATA_PARALLEL_GROUP_GLOO = grid.get_data_parallel_group_gloo()\n",
      "    parallel_state._DATA_PARALLEL_GLOBAL_RANKS = dist.get_process_group_ranks(g)\n",
      "    parallel_state._DATA_PARALLEL_GROUP_WITH_CP = g\n",
      "    parallel_state._DATA_PARALLEL_GROUP_WITH_CP_GLOO = (\n",
      "        grid.get_data_parallel_group_gloo()\n",
      "    )\n",
      "    parallel_state._DATA_PARALLEL_GLOBAL_RANKS_WITH_CP = dist.get_process_group_ranks(g)\n",
      "    if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "        parallel_state._INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP = (\n",
      "            constants.data_parallel_group()\n",
      "        )\n",
      "        parallel_state._INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO = (\n",
      "            grid.get_data_parallel_group_gloo()\n",
      "        )\n",
      "\n",
      "    # Build the context-parallel groups.\n",
      "    parallel_state._CONTEXT_PARALLEL_GROUP = constants.self_group()\n",
      "    parallel_state._CONTEXT_PARALLEL_GLOBAL_RANKS = [dist.get_rank()]\n",
      "\n",
      "    # Build the model-parallel groups.\n",
      "    parallel_state._MODEL_PARALLEL_GROUP = grid.get_model_parallel_group()\n",
      "    if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "        g = grid.get_model_parallel_group()\n",
      "        parallel_state._MODEL_PARALLEL_GLOBAL_RANKS = dist.get_process_group_ranks(g)\n",
      "\n",
      "    # Build the tensor model-parallel groups.\n",
      "    parallel_state._TENSOR_MODEL_PARALLEL_GROUP = g\n",
      "    if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "        g = constants.tensor_parallel_group()\n",
      "        parallel_state._TENSOR_MODEL_PARALLEL_GLOBAL_RANKS = (\n",
      "            dist.get_process_group_ranks(g)\n",
      "        )\n",
      "\n",
      "    # Build the pipeline model-parallel groups and embedding groups\n",
      "    # (first and last rank in each pipeline model-parallel group).\n",
      "    g = constants.pipe_parallel_group()\n",
      "    parallel_state._PIPELINE_MODEL_PARALLEL_GROUP = g\n",
      "    parallel_state._PIPELINE_GLOBAL_RANKS = dist.get_process_group_ranks(g)\n",
      "    parallel_state._EMBEDDING_GROUP = grid.embedding_proc_group\n",
      "    parallel_state._EMBEDDING_GLOBAL_RANKS = (\n",
      "        dist.get_process_group_ranks(grid.embedding_proc_group)\n",
      "        if grid.embedding_proc_group is not None\n",
      "        else list(range(dist.get_world_size()))\n",
      "    )\n",
      "    parallel_state._POSITION_EMBEDDING_GROUP = grid.position_embedding_proc_group\n",
      "    parallel_state._POSITION_EMBEDDING_GLOBAL_RANKS = (\n",
      "        dist.get_process_group_ranks(grid.position_embedding_proc_group)\n",
      "        if grid.position_embedding_proc_group is not None\n",
      "        else list(range(dist.get_world_size()))\n",
      "    )\n",
      "\n",
      "    # Build the tensor + data parallel groups.\n",
      "    parallel_state._TENSOR_AND_DATA_PARALLEL_GROUP = grid.tp_dp_proc_group\n",
      "    parallel_state._TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP = grid.tp_dp_proc_group\n",
      "    if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "        # Build the tensor + context parallel groups\n",
      "        parallel_state._TENSOR_AND_CONTEXT_PARALLEL_GROUP = (\n",
      "            constants.tensor_parallel_group()\n",
      "        )\n",
      "\n",
      "    # Build expert parallel groups.\n",
      "    parallel_state._EXPERT_MODEL_PARALLEL_GROUP = constants.self_group()\n",
      "\n",
      "    if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "        parallel_state._EXPERT_TENSOR_PARALLEL_GROUP = constants.self_group()\n",
      "        parallel_state._EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP = constants.self_group()\n",
      "        parallel_state._EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP = (\n",
      "            grid.get_pipe_parallel_group()\n",
      "        )\n",
      "        parallel_state._EXPERT_DATA_PARALLEL_GROUP = constants.data_parallel_group()\n",
      "        parallel_state._EXPERT_DATA_PARALLEL_GROUP_GLOO = (\n",
      "            grid.get_data_parallel_group_gloo()\n",
      "        )\n",
      "    else:\n",
      "        parallel_state._TENSOR_AND_EXPERT_PARALLEL_GROUP = (\n",
      "            constants.tensor_parallel_group()\n",
      "        )\n",
      "        parallel_state._DATA_MODULO_EXPERT_PARALLEL_GROUP = (\n",
      "            constants.data_parallel_group()\n",
      "        )\n",
      "        parallel_state._DATA_MODULO_EXPERT_PARALLEL_GROUP_WITH_CP = (\n",
      "            constants.data_parallel_group()\n",
      "        )\n",
      "        parallel_state._DATA_MODULO_EXPERT_PARALLEL_GROUP_GLOO = (\n",
      "            grid.get_data_parallel_group_gloo()\n",
      "        )\n",
      "\n",
      "    # Remove the global memory buffer for megatron to save GPU memory.\n",
      "    parallel_state._GLOBAL_MEMORY_BUFFER = None\n",
      "    yield\n",
      "    WITHIN_MEGATRON_CONTEXT = False\n",
      "\n",
      "\n",
      "def get_megatron_transformer_config(\n",
      "    mconfig: model_api.ReaLModelConfig,\n",
      ") -> MegatronTransformerConfig:\n",
      "    nq = mconfig.hidden_dim // mconfig.head_dim\n",
      "    n_group = nq // mconfig.n_kv_heads\n",
      "    return MegatronTransformerConfig(\n",
      "        num_layers=mconfig.n_layers,\n",
      "        hidden_size=mconfig.hidden_dim,\n",
      "        num_attention_heads=nq,\n",
      "        num_query_groups=n_group,\n",
      "        ffn_hidden_size=mconfig.intermediate_dim,\n",
      "        kv_channels=mconfig.n_kv_heads,\n",
      "        hidden_dropout=0.0,\n",
      "        attention_dropout=mconfig.attn_pdrop,\n",
      "        layernorm_epsilon=mconfig.layer_norm_epsilon,\n",
      "        add_qkv_bias=mconfig.use_attention_bias,\n",
      "        activation_func=get_activation_fn(mconfig.activation_function),\n",
      "        rotary_interleaved=mconfig.rotary_interleaved,\n",
      "        normalization=(\"RMSNorm\" if mconfig.layer_norm_type == \"rms\" else \"LayerNorm\"),\n",
      "        attention_softmax_in_fp32=True,\n",
      "        apply_query_key_layer_scaling=mconfig.scale_attn_by_inverse_layer_idx,\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MegatronEngine:\n",
      "    ddp: DistributedDataParallel\n",
      "    optim: DistributedOptimizer\n",
      "    lr_scheduler: Any\n",
      "\n",
      "    def zero_grad(self, set_to_none=True):\n",
      "        self.ddp.zero_grad_buffer()\n",
      "        self.optim.zero_grad(set_to_none=set_to_none)\n",
      "\n",
      "    def _all_reduce_layernorm_grads(self):\n",
      "        if not (\n",
      "            constants.sequence_parallel() and constants.tensor_parallel_world_size() > 1\n",
      "        ):\n",
      "            return\n",
      "        real_model: ReaLModel = self.ddp.module\n",
      "        grads = []\n",
      "        for i in range(real_model.layer_idx_start, real_model.layer_idx_end):\n",
      "            if i == 0:\n",
      "                continue\n",
      "            elif i == real_model.config.n_layers + 1:\n",
      "                continue\n",
      "            else:\n",
      "                assert 0 < i < real_model.config.n_layers + 1\n",
      "                layer: ReaLModelBlock = real_model.layers[\n",
      "                    i - real_model.layer_idx_start\n",
      "                ]\n",
      "                grads.append(layer.attn.c_attn.ln.weight.main_grad)\n",
      "                if getattr(layer.attn.c_attn.ln, \"bias\", None) is not None:\n",
      "                    grads.append(layer.attn.c_attn.ln.bias.main_grad)\n",
      "                grads.append(layer.mlp.ln.weight.main_grad)\n",
      "                if getattr(layer.mlp.ln, \"bias\", None) is not None:\n",
      "                    grads.append(layer.mlp.ln.bias.main_grad)\n",
      "                if i == real_model.config.n_layers:\n",
      "                    grads.append(layer.ln_f.weight.main_grad)\n",
      "                    if getattr(layer.ln_f, \"bias\", None) is not None:\n",
      "                        grads.append(layer.ln_f.bias.main_grad)\n",
      "\n",
      "        assert all(x is not None for x in grads)\n",
      "        coalesced = _flatten_dense_tensors(grads)\n",
      "        dist.all_reduce(coalesced, group=constants.tensor_parallel_group())\n",
      "        for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):\n",
      "            buf.copy_(synced)\n",
      "\n",
      "    def _all_reduce_word_embedding_grads(self):\n",
      "        real_model: ReaLModel = self.ddp.module\n",
      "        if not real_model.config.tied_embedding or real_model.config.is_critic:\n",
      "            return\n",
      "        pp_size = constants.pipe_parallel_world_size()\n",
      "        pp_rank = constants.pipe_parallel_rank()\n",
      "        if pp_size == 1:\n",
      "            return\n",
      "        if pp_rank not in [0, pp_size - 1]:\n",
      "            return\n",
      "\n",
      "        if pp_rank == 0:\n",
      "            grad = real_model.layers[0].wte.weight.main_grad\n",
      "        else:\n",
      "            grad = real_model.layers[-1].weight.main_grad\n",
      "\n",
      "        dist.all_reduce(grad, group=constants.grid().embedding_proc_group)\n",
      "\n",
      "    def finalize_grads(self):\n",
      "        self.ddp.finish_grad_sync()\n",
      "        self._all_reduce_layernorm_grads()\n",
      "        self._all_reduce_word_embedding_grads()\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class PipeTrainInstrSetForMegatron(PipeTrainInstrSet):\n",
      "    # NOTE: merge DistributedDataParallel and DistributedOptimizer into one class\n",
      "    # to remain consistent with DeepSpeed's API\n",
      "    engine: MegatronEngine\n",
      "    num_micro_batches: int\n",
      "\n",
      "    def __post_init__(self):\n",
      "        self._no_sync_context = None\n",
      "        self.disable_grad_sync()\n",
      "\n",
      "    def disable_grad_sync(self):\n",
      "        if self._no_sync_context is None:\n",
      "            self._no_sync_context = self.engine.ddp.no_sync()\n",
      "            self._no_sync_context.__enter__()\n",
      "\n",
      "    def enable_grad_sync(self):\n",
      "        if self._no_sync_context is not None:\n",
      "            self._no_sync_context.__exit__(None, None, None)\n",
      "            self._no_sync_context = None\n",
      "\n",
      "    @cuda_tmarked(\"bwd\", CUDATimeMarkType.backward)\n",
      "    def _exec_backward_pass(\n",
      "        self,\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        output_x = tensor_buffer.get(\"batch_output_x\", micro_batch_id, remove=True)\n",
      "\n",
      "        if micro_batch_id == self.num_micro_batches - 1:\n",
      "            self.enable_grad_sync()\n",
      "\n",
      "        is_last_stage = constants.is_last_pipe_stage()\n",
      "        if is_last_stage:\n",
      "            loss: torch.Tensor = tensor_buffer.get(\n",
      "                \"losses\", micro_batch_id, remove=True\n",
      "            )\n",
      "            loss.backward()\n",
      "            tensor_buffer.put(\"losses\", micro_batch_id, loss.detach().clone())\n",
      "            return\n",
      "\n",
      "        grad = tensor_buffer.get(\"grad\", micro_batch_id, remove=True)\n",
      "        output_tensor = output_x.pp_output\n",
      "        torch.autograd.backward(tensors=output_tensor, grad_tensors=grad)\n",
      "\n",
      "    def _exec_reduce_grads(\n",
      "        self,\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        # self.engine.ddp.start_grad_sync()\n",
      "        self.engine.finalize_grads()\n",
      "\n",
      "    @cuda_tmarked(\"opt\", CUDATimeMarkType.optim_step)\n",
      "    def _exec_optimizer_step(\n",
      "        self,\n",
      "        module: ReaLModel,\n",
      "        tensor_buffer: TensorBuffer,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        step_id: int,\n",
      "    ):\n",
      "        update_successful, grad_norm, num_zeros_in_grad = self.engine.optim.step()\n",
      "\n",
      "        version_steps = tensor_buffer.get(\"version_steps\", 0)\n",
      "        if update_successful:\n",
      "            incr = version_steps - self.engine.lr_scheduler.num_steps\n",
      "            self.engine.lr_scheduler.step(incr)\n",
      "            grad_norm = torch.tensor(\n",
      "                grad_norm, device=constants.current_device(), dtype=torch.float32\n",
      "            )\n",
      "            dist.all_reduce(grad_norm, group=constants.tp_and_pp_group())\n",
      "            grad_norm /= constants.tp_and_pp_world_size()\n",
      "        if (\n",
      "            constants.data_parallel_rank() == 0\n",
      "            and constants.tensor_parallel_rank() == 0\n",
      "        ):\n",
      "            logger.info(\n",
      "                f\"Model name {constants.model_name()}, \"\n",
      "                f\"Pipeline rank {constants.pipe_parallel_rank()}. \"\n",
      "                f\"Update success? {update_successful}. \"\n",
      "                f\"Grad Norm: {grad_norm}. \"\n",
      "                f\"Current loss scale: {self.engine.optim.get_loss_scale()}. \"\n",
      "                f\"Learning rate: {[param_group['lr'] for param_group in self.engine.optim.param_groups]}. \"\n",
      "            )\n",
      "        stat = dict(\n",
      "            update_successful=float(update_successful),\n",
      "            grad_norm=float(grad_norm) if grad_norm is not None else float(\"nan\"),\n",
      "            loss_scale=float(self.engine.optim.get_loss_scale()),\n",
      "        )\n",
      "        for i, param_group in enumerate(self.engine.optim.param_groups):\n",
      "            stat[f\"param_group{i}/lr\"] = param_group[\"lr\"]\n",
      "        # NOTE: we only have one optimizer step for each stage, so micro_batch_id can be 0\n",
      "        tensor_buffer.put(\"stats\", 0, stat)\n",
      "\n",
      "\n",
      "class ReaLMegatronEngine(model_api.PipelinableEngine):\n",
      "\n",
      "    def __init__(self, module: ReaLModel, megatron_engine: MegatronEngine):\n",
      "        self.module = module\n",
      "\n",
      "        self.inf_engine = PipelinableInferenceEngine(module)\n",
      "        if constants.pipe_parallel_world_size() > 1:\n",
      "            self.pipe_runner = self.inf_engine.pipe_runner\n",
      "\n",
      "        # NOTE: In profiler, module could be not a instance of ReaLModel.\n",
      "        self.device = module.device if hasattr(module, \"device\") else None\n",
      "        self.dtype = module.dtype if hasattr(module, \"dtype\") else None\n",
      "\n",
      "        self.engine = megatron_engine\n",
      "\n",
      "    def train(self, mode: bool = True):\n",
      "        self.module.train(mode)\n",
      "        self.engine.ddp.train(mode)\n",
      "        return self\n",
      "\n",
      "    def eval(self):\n",
      "        self.module.eval()\n",
      "        self.engine.ddp.eval()\n",
      "        return self\n",
      "\n",
      "    def train_batch(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        loss_fn: Callable,\n",
      "        loss_weight_fn: Callable,\n",
      "        token_normalize_scope: str,\n",
      "        version_steps: int,\n",
      "    ):\n",
      "        with megatron_ctx():\n",
      "            self.engine.zero_grad()\n",
      "            if constants.pipe_parallel_world_size() > 1:\n",
      "                mb_inputs = input_.synced_data_parallel_split(\n",
      "                    MicroBatchSpec.new(\n",
      "                        mb_spec,\n",
      "                        n_mbs=mb_spec.n_mbs * self.pipe_runner.default_train_mbs,\n",
      "                    )\n",
      "                )\n",
      "                # Fusing the minibatched forward-backward in a pipeline training schedule.\n",
      "                instr_set = PipeTrainInstrSetForMegatron(self.engine, len(mb_inputs))\n",
      "                # NOTE: When training with pipeline parallel, num micro batches should be\n",
      "                # larger than 2 x num_pipeline_stages to avoid idle time.\n",
      "                return self.pipe_runner.train_batch(\n",
      "                    instr_set=instr_set,\n",
      "                    input_=input_,\n",
      "                    mb_spec=mb_spec,\n",
      "                    loss_fn=loss_fn,\n",
      "                    loss_weight_fn=loss_weight_fn,\n",
      "                    token_normalize_scope=token_normalize_scope,\n",
      "                    version_steps=version_steps,\n",
      "                )\n",
      "\n",
      "            mb_inputs = input_.synced_data_parallel_split(mb_spec)\n",
      "            total_loss_weight = torch.tensor(\n",
      "                sum([loss_weight_fn(mb) for mb in mb_inputs]), dtype=torch.float32\n",
      "            )\n",
      "            if token_normalize_scope == \"global\":\n",
      "                dist.all_reduce(\n",
      "                    total_loss_weight, group=constants.data_parallel_group()\n",
      "                )\n",
      "            if total_loss_weight == 0:\n",
      "                raise model_api.ZeroTotalLossWeightException(\n",
      "                    \"The sum of loss weights of all micro batches is zero.\"\n",
      "                )\n",
      "\n",
      "            if constants.parallelism_rank() == 0:\n",
      "                logger.info(\n",
      "                    f\"MB spec: {mb_spec}, #mbs={len(mb_inputs)}, \"\n",
      "                    f\"#tokens: {input_.data['packed_input_ids'].shape[0]}, \"\n",
      "                    f\"pp_size={constants.pipe_parallel_world_size()}, \"\n",
      "                    f\"#tokens per mbs: {[mb.data['packed_input_ids'].shape[0] for mb in mb_inputs]}\"\n",
      "                )\n",
      "            no_sync_ctx = self.engine.ddp.no_sync()\n",
      "            no_sync_ctx.__enter__()\n",
      "            for i, mb_input in enumerate(mb_inputs):\n",
      "                if i == len(mb_inputs) - 1:\n",
      "                    no_sync_ctx.__exit__(None, None, None)\n",
      "                input_lens = torch.tensor(\n",
      "                    flat2d(mb_input.seqlens[\"packed_input_ids\"]),\n",
      "                    dtype=torch.int32,\n",
      "                    device=\"cuda\",\n",
      "                )\n",
      "                max_seqlen = int(max(input_lens))\n",
      "                cu_seqlens = torch.nn.functional.pad(input_lens.cumsum(0), (1, 0)).int()\n",
      "                model_output = self.engine.ddp(\n",
      "                    packed_input_ids=mb_input.data[\"packed_input_ids\"],\n",
      "                    cu_seqlens=cu_seqlens,\n",
      "                    max_seqlen=max_seqlen,\n",
      "                ).logits\n",
      "                loss = loss_fn(model_output, mb_input)\n",
      "                loss_scale = loss_weight_fn(mb_inputs[i]) / total_loss_weight\n",
      "                if token_normalize_scope == \"global\":\n",
      "                    # Megatron will average gradients across DP ranks.\n",
      "                    # If we normalize loss across micro batches of all DP ranks,\n",
      "                    # we should revert the effect of gradient averaging in megatron\n",
      "                    # to make sure loss from each token is scaled properly.\n",
      "                    loss_scale *= constants.data_parallel_world_size()\n",
      "                loss_scale *= self.engine.optim.get_loss_scale().item()\n",
      "                loss *= loss_scale\n",
      "                with cuda_tmarked(\"bwd\", CUDATimeMarkType.backward):\n",
      "                    loss.backward()\n",
      "\n",
      "            self.engine.finalize_grads()\n",
      "            return self._step(version_steps)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def forward(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        output_seqlens: List[List[int]] | None = None,\n",
      "        post_hook: Callable[[torch.Tensor, SequenceSample], Any] | None = None,\n",
      "        aggregate_fn: Callable[[List[Any]], Any] = torch.cat,\n",
      "    ):\n",
      "        return self.inf_engine.forward(\n",
      "            input_=input_,\n",
      "            mb_spec=mb_spec,\n",
      "            output_seqlens=output_seqlens,\n",
      "            post_hook=post_hook,\n",
      "            aggregate_fn=aggregate_fn,\n",
      "        )\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        input_: SequenceSample,\n",
      "        mb_spec: MicroBatchSpec,\n",
      "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
      "        gconfig: model_api.GenerationHyperparameters = dataclasses.field(\n",
      "            default_factory=model_api.GenerationHyperparameters\n",
      "        ),\n",
      "    ):\n",
      "        return self.inf_engine.generate(\n",
      "            input_=input_,\n",
      "            tokenizer=tokenizer,\n",
      "            gconfig=gconfig,\n",
      "            mb_spec=mb_spec,\n",
      "        )\n",
      "\n",
      "    # wrapper for profiler\n",
      "    @cuda_tmarked(\"opt\", CUDATimeMarkType.optim_step)\n",
      "    def _step(self, version_steps):\n",
      "        # omit the number of zeros in grads\n",
      "        update_successful, grad_norm, _ = self.engine.optim.step()\n",
      "        if update_successful:\n",
      "            incr = version_steps - self.engine.lr_scheduler.num_steps\n",
      "            self.engine.lr_scheduler.step(incr)\n",
      "            grad_norm = torch.tensor(\n",
      "                grad_norm, device=constants.current_device(), dtype=torch.float32\n",
      "            )\n",
      "            dist.all_reduce(grad_norm, group=constants.tp_and_pp_group())\n",
      "            grad_norm /= constants.tp_and_pp_world_size()\n",
      "        if (\n",
      "            constants.data_parallel_rank() == 0\n",
      "            and constants.tensor_parallel_rank() == 0\n",
      "        ):\n",
      "            logger.info(\n",
      "                f\"Megatron backend update success? {update_successful}. \"\n",
      "                f\"Grad Norm: {grad_norm}. \"\n",
      "                f\"Current loss scale: {self.engine.optim.get_loss_scale()}. \"\n",
      "                f\"Learning rate: {[param_group['lr'] for param_group in self.engine.optim.param_groups]}. \"\n",
      "            )\n",
      "        stat = dict(\n",
      "            update_successful=float(update_successful),\n",
      "            grad_norm=float(grad_norm) if grad_norm is not None else float(\"nan\"),\n",
      "            loss_scale=float(self.engine.optim.get_loss_scale()),\n",
      "        )\n",
      "        for i, param_group in enumerate(self.engine.optim.param_groups):\n",
      "            stat[f\"param_group{i}/lr\"] = param_group[\"lr\"]\n",
      "        return stat\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class MegatronTrainBackend(model_api.ModelBackend, MegatronConfig):\n",
      "    bf16: bool = False\n",
      "    optimizer: OptimizerConfig = dataclasses.field(default_factory=OptimizerConfig)\n",
      "\n",
      "    def _initialize(\n",
      "        self, model: model_api.Model, spec: model_api.FinetuneSpec\n",
      "    ) -> model_api.Model:\n",
      "        module = model.module\n",
      "\n",
      "        if not isinstance(module, ReaLModel):\n",
      "            raise ValueError(\"MegatronTrainBackend only supports ReaLModel.\")\n",
      "        if isinstance(self.ddp, dict):\n",
      "            if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "                from megatron.core.distributed.distributed_data_parallel_config import (\n",
      "                    DistributedDataParallelConfig,\n",
      "                )\n",
      "            else:\n",
      "                from realhf.api.cli_args import DistributedDataParallelConfig\n",
      "            self.ddp = DistributedDataParallelConfig(**self.ddp)\n",
      "        with megatron_ctx():\n",
      "            if pkg_version.is_version_less(\"megatron.core\", \"0.7.0\"):\n",
      "                module = DistributedDataParallel(\n",
      "                    config=get_megatron_transformer_config(module.config),\n",
      "                    module=module,\n",
      "                    data_parallel_group=constants.data_parallel_group(),\n",
      "                    accumulate_allreduce_grads_in_fp32=self.ddp.grad_reduce_in_fp32,\n",
      "                    overlap_grad_reduce=self.ddp.overlap_grad_reduce,\n",
      "                    use_distributed_optimizer=self.ddp.use_distributed_optimizer,\n",
      "                    expert_data_parallel_group=None,\n",
      "                    disable_bucketing=False,\n",
      "                    check_for_nan_in_grad=self.ddp.check_for_nan_in_grad,\n",
      "                    bucket_size=self.ddp.bucket_size,\n",
      "                )\n",
      "            else:\n",
      "                module = DistributedDataParallel(\n",
      "                    config=get_megatron_transformer_config(module.config),\n",
      "                    ddp_config=self.ddp,\n",
      "                    module=module,\n",
      "                    disable_bucketing=False,\n",
      "                )\n",
      "\n",
      "        real_model: ReaLModel = module.module\n",
      "        if self.ddp.use_distributed_optimizer:\n",
      "            # Remap parameters.\n",
      "            assert len(module.buffers) == 1\n",
      "            param_grad_buf = module.buffers[0]\n",
      "            # Map Megatron flattened parameters to ReaLModel!\n",
      "            real_model.contiguous_param = param_grad_buf.param_data\n",
      "            # Sanity checks.\n",
      "            assert real_model._param_size == param_grad_buf.numel, (\n",
      "                real_model._param_size,\n",
      "                param_grad_buf.numel,\n",
      "                module.bucket_size,\n",
      "            )\n",
      "            for n, p in real_model.layers.named_parameters():\n",
      "                n = \".\".join(\n",
      "                    [\n",
      "                        str(real_model.layer_idx_start + int(n.split(\".\")[0])),\n",
      "                        n.split(\".\", 1)[1],\n",
      "                    ]\n",
      "                )\n",
      "                idx_start, idx_end, _ = param_grad_buf.param_index_map[p]\n",
      "                assert real_model._param_spec[n].start_idx == idx_start\n",
      "                assert real_model._param_spec[n].end_idx == idx_end\n",
      "                assert real_model._param_spec[n].shape == p.shape\n",
      "                assert torch.allclose(\n",
      "                    p,\n",
      "                    real_model.contiguous_param[idx_start:idx_end].view(p.shape),\n",
      "                )\n",
      "\n",
      "        betas = (self.optimizer.beta1, self.optimizer.beta2)\n",
      "        wd = self.optimizer.weight_decay\n",
      "        lr = self.optimizer.lr\n",
      "        opt_cfg = MegatronOptimizerConfig(\n",
      "            optimizer=self.optimizer.type,\n",
      "            bf16=self.bf16,\n",
      "            fp16=not self.bf16,\n",
      "            lr=lr,\n",
      "            min_lr=self.optimizer.min_lr_ratio * lr,\n",
      "            weight_decay=wd,\n",
      "            params_dtype=real_model.dtype,\n",
      "            initial_loss_scale=self.optimizer.initial_loss_scale,\n",
      "            min_loss_scale=self.optimizer.min_loss_scale,\n",
      "            loss_scale_window=self.optimizer.loss_scale_window,\n",
      "            hysteresis=self.optimizer.hysteresis,\n",
      "            adam_beta1=betas[0],\n",
      "            adam_beta2=betas[1],\n",
      "            adam_eps=self.optimizer.eps,\n",
      "            use_distributed_optimizer=self.ddp.use_distributed_optimizer,\n",
      "            clip_grad=self.optimizer.gradient_clipping,\n",
      "            log_num_zeros_in_grad=False,\n",
      "        )\n",
      "        if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "            opt_cfg.overlap_param_gather_with_optimizer_step = (\n",
      "                self.overlap_param_gather_with_optimizer_step\n",
      "            )\n",
      "            opt_cfg.use_precision_aware_optimizer = self.use_precision_aware_optimizer\n",
      "            opt_cfg.main_grads_dtype = getattr(torch, self.main_grads_dtype)\n",
      "            opt_cfg.main_params_dtype = getattr(torch, self.main_params_dtype)\n",
      "            opt_cfg.exp_avg_dtype = getattr(torch, self.exp_avg_dtype)\n",
      "            opt_cfg.exp_avg_sq_dtype = getattr(torch, self.exp_avg_sq_dtype)\n",
      "\n",
      "        with megatron_ctx():\n",
      "            # no_weight_decay_cond and scale_lr_cond have the following signature:\n",
      "            # foo(name: str, param: torch.Tensor) -> bool\n",
      "            optimizer = get_megatron_optimizer(\n",
      "                opt_cfg,\n",
      "                [module],\n",
      "                no_weight_decay_cond=lambda n, p: any(\n",
      "                    k in n for k in [\"bias\", \"ln.weight\", \"ln_f.weight\"]\n",
      "                ),\n",
      "                scale_lr_cond=None,\n",
      "                lr_mult=1.0,\n",
      "            )\n",
      "\n",
      "            warmup_steps_proportion = self.optimizer.warmup_steps_proportion\n",
      "            warmup_steps = int(warmup_steps_proportion * spec.total_train_steps)\n",
      "            lr_scheduler = OptimizerParamScheduler(\n",
      "                optimizer,\n",
      "                init_lr=0.0 if warmup_steps_proportion > 0 else lr,\n",
      "                max_lr=lr,\n",
      "                min_lr=self.optimizer.min_lr_ratio * lr,\n",
      "                lr_warmup_steps=warmup_steps,\n",
      "                lr_decay_steps=spec.total_train_steps - warmup_steps,\n",
      "                lr_decay_style=self.optimizer.lr_scheduler_type,\n",
      "                start_wd=wd,\n",
      "                end_wd=wd,\n",
      "                wd_incr_steps=spec.total_train_steps,\n",
      "                wd_incr_style=\"constant\",\n",
      "            )\n",
      "\n",
      "        mg_engine = MegatronEngine(module, optimizer, lr_scheduler)\n",
      "        model.module = ReaLMegatronEngine(real_model, mg_engine)\n",
      "        model.backend_name = \"megatron\"\n",
      "        return model\n",
      "\n",
      "    def destroy(self, model: model_api.Model):\n",
      "        assert isinstance(model.module, ReaLMegatronEngine)\n",
      "        # The Megatron backend will register forward hooks that\n",
      "        # create circular references (grad -> param -> grad).\n",
      "        # Deleting models directly will not release the memory.\n",
      "        # We must disable hooks at first.\n",
      "        if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "            if self.ddp.use_distributed_optimizer and self.ddp.overlap_param_gather:\n",
      "                model.module.engine.ddp.disable_forward_pre_hook()\n",
      "        else:\n",
      "            optimizer = model.module.engine.optim\n",
      "            if self.ddp.use_distributed_optimizer and self.ddp.overlap_param_gather:\n",
      "                optimizer.disable_pre_hook()\n",
      "\n",
      "    def save(self, model: model_api.Model, save_dir: str):\n",
      "        assert isinstance(model.module, ReaLMegatronEngine)\n",
      "        optimizer = model.module.engine.optim\n",
      "        param_state = optimizer.get_parameter_state_fs_bucket_space()\n",
      "        assert isinstance(optimizer, DistributedOptimizer)\n",
      "        if pkg_version.is_version_greater_or_equal(\"megatron.core\", \"0.11.0\"):\n",
      "            # Fix the keyerror: \"padding\"\n",
      "            for gbuf_idx, gbuf_range_maps in enumerate(optimizer.gbuf_ranges):\n",
      "                assert len(gbuf_range_maps) == 1, \"single dtype supported, for now.\"\n",
      "                for dtype, gbuf_range_map_for_all_buckets in gbuf_range_maps.items():\n",
      "                    for bucket_idx, gbuf_range_map in enumerate(\n",
      "                        gbuf_range_map_for_all_buckets\n",
      "                    ):\n",
      "                        bucket_state = param_state[gbuf_idx][dtype][bucket_idx]\n",
      "                        for elem in bucket_state:\n",
      "                            elem[\"padding\"] = False\n",
      "\n",
      "        sd = optimizer.state_dict()\n",
      "        dp = constants.data_parallel_rank()\n",
      "        pp = constants.pipe_parallel_rank()\n",
      "        tp = constants.tensor_parallel_rank()\n",
      "        # HACK: (bowei) I'm not sure whether there's duplicated information.\n",
      "        torch.save(\n",
      "            sd, pathlib.Path(save_dir) / f\"megatron_optim_sd_d{dp}p{pp}t{tp}.mckpt\"\n",
      "        )\n",
      "        torch.save(\n",
      "            param_state,\n",
      "            pathlib.Path(save_dir) / f\"megatron_optim_param_sd_d{dp}p{pp}t{tp}.mckpt\",\n",
      "        )\n",
      "\n",
      "    def load(self, model: model_api.Model, load_dir: str):\n",
      "        assert isinstance(model.module, ReaLMegatronEngine)\n",
      "        optimizer = model.module.engine.optim\n",
      "\n",
      "        dp = constants.data_parallel_rank()\n",
      "        pp = constants.pipe_parallel_rank()\n",
      "        tp = constants.tensor_parallel_rank()\n",
      "\n",
      "        sd = torch.load(\n",
      "            pathlib.Path(load_dir) / f\"megatron_optim_sd_d{dp}p{pp}t{tp}.mckpt\"\n",
      "        )\n",
      "        optimizer.load_state_dict(sd)\n",
      "\n",
      "        param_state = torch.load(\n",
      "            pathlib.Path(load_dir) / f\"megatron_optim_param_sd_d{dp}p{pp}t{tp}.mckpt\",\n",
      "            weights_only=False,\n",
      "        )\n",
      "        optimizer.load_parameter_state_from_fs_bucket_space(param_state)\n",
      "\n",
      "\n",
      "model_api.register_backend(\"megatron\", MegatronTrainBackend)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/megatron/__init__.py ====\n",
      "\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/megatron/v0_6_0/lr_schduler.py ====\n",
      "\n",
      "import math\n",
      "\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "# Adopted from Megatron-LM/megatron/training/optimizer_param_scheduler.py\n",
      "class OptimizerParamScheduler(object):\n",
      "    \"\"\"Anneals learning rate and weight decay.\n",
      "\n",
      "    Adopted from Megatron-LM. This class is not included in\n",
      "    megatron.core, so we have to copy-paste it here.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        optimizer,\n",
      "        init_lr,\n",
      "        max_lr,\n",
      "        min_lr,\n",
      "        lr_warmup_steps,\n",
      "        lr_decay_steps,\n",
      "        lr_decay_style,\n",
      "        start_wd,\n",
      "        end_wd,\n",
      "        wd_incr_steps,\n",
      "        wd_incr_style,\n",
      "        use_checkpoint_opt_param_scheduler=True,\n",
      "        override_opt_param_scheduler=False,\n",
      "    ):\n",
      "\n",
      "        # Class values.\n",
      "        self.optimizer = optimizer\n",
      "\n",
      "        self.init_lr = init_lr\n",
      "        self.max_lr = float(max_lr)\n",
      "        self.min_lr = min_lr\n",
      "        assert self.min_lr >= 0.0\n",
      "        assert self.max_lr >= self.min_lr\n",
      "        assert self.init_lr <= self.max_lr\n",
      "\n",
      "        self.lr_warmup_steps = lr_warmup_steps\n",
      "        self.num_steps = 0\n",
      "        self.lr_decay_steps = lr_decay_steps\n",
      "        assert self.lr_decay_steps > 0\n",
      "        assert self.lr_warmup_steps < self.lr_decay_steps\n",
      "\n",
      "        self.lr_decay_style = lr_decay_style\n",
      "\n",
      "        self.start_wd = start_wd\n",
      "        self.end_wd = end_wd\n",
      "        assert self.start_wd >= 0.0\n",
      "        assert self.end_wd >= self.start_wd\n",
      "        self.wd_incr_steps = wd_incr_steps\n",
      "        self.wd_incr_style = wd_incr_style\n",
      "\n",
      "        self.override_opt_param_scheduler = override_opt_param_scheduler\n",
      "        self.use_checkpoint_opt_param_scheduler = use_checkpoint_opt_param_scheduler\n",
      "        if self.override_opt_param_scheduler:\n",
      "            assert not self.use_checkpoint_opt_param_scheduler, (\n",
      "                \"both override and \" \"use-checkpoint are set.\"\n",
      "            )\n",
      "\n",
      "        # Set the learning rate\n",
      "        self.step(0)\n",
      "        self.log_rank_0(\"> learning rate decay style: {}\".format(self.lr_decay_style))\n",
      "\n",
      "    def log_rank_0(self, msg):\n",
      "        if constants.parallelism_rank() == 0:\n",
      "            logger.info(msg)\n",
      "\n",
      "    def get_wd(self):\n",
      "        \"\"\"Weight decay incr functions.\"\"\"\n",
      "        if self.num_steps > self.wd_incr_steps:\n",
      "            return self.end_wd\n",
      "\n",
      "        if self.wd_incr_style == \"constant\":\n",
      "            assert self.start_wd == self.end_wd\n",
      "            return self.end_wd\n",
      "\n",
      "        incr_ratio = float(self.num_steps) / float(self.wd_incr_steps)\n",
      "        assert incr_ratio >= 0.0\n",
      "        assert incr_ratio <= 1.0\n",
      "        delta_wd = self.end_wd - self.start_wd\n",
      "\n",
      "        if self.wd_incr_style == \"linear\":\n",
      "            coeff = incr_ratio\n",
      "        elif self.wd_incr_style == \"cosine\":\n",
      "            coeff = 0.5 * (math.cos(math.pi * (1 - incr_ratio)) + 1.0)\n",
      "        else:\n",
      "            raise Exception(\n",
      "                \"{} weight decay increment style is not supported.\".format(\n",
      "                    self.wd_incr_style\n",
      "                )\n",
      "            )\n",
      "\n",
      "        return self.start_wd + coeff * delta_wd\n",
      "\n",
      "    def get_lr(self, param_group):\n",
      "        \"\"\"Learning rate decay functions from:\n",
      "        https://openreview.net/pdf?id=BJYwwY9ll pg. 4\"\"\"\n",
      "\n",
      "        max_lr = param_group.get(\"max_lr\", self.max_lr)\n",
      "        min_lr = param_group.get(\"min_lr\", self.min_lr)\n",
      "\n",
      "        # Use linear warmup for the initial part.\n",
      "        if self.lr_warmup_steps > 0 and self.num_steps <= self.lr_warmup_steps:\n",
      "            return self.init_lr + (\n",
      "                (max_lr - self.init_lr)\n",
      "                * float(self.num_steps)\n",
      "                / float(self.lr_warmup_steps)\n",
      "            )\n",
      "\n",
      "        # If the learning rate is constant, just return the initial value.\n",
      "        if self.lr_decay_style == \"constant\":\n",
      "            return max_lr\n",
      "\n",
      "        # For any steps larger than `self.lr_decay_steps`, use `min_lr`.\n",
      "        if self.num_steps > self.lr_decay_steps:\n",
      "            return min_lr\n",
      "\n",
      "        # If we are done with the warmup period, use the decay style.\n",
      "        if self.lr_decay_style == \"inverse-square-root\":\n",
      "            warmup_steps = max(self.lr_warmup_steps, 1)\n",
      "            num_steps = max(self.num_steps, 1)\n",
      "            lr = max_lr * warmup_steps**0.5 / (num_steps**0.5)\n",
      "            return max(min_lr, lr)\n",
      "\n",
      "        num_steps_ = self.num_steps - self.lr_warmup_steps\n",
      "        decay_steps_ = self.lr_decay_steps - self.lr_warmup_steps\n",
      "        decay_ratio = float(num_steps_) / float(decay_steps_)\n",
      "        assert decay_ratio >= 0.0\n",
      "        assert decay_ratio <= 1.0\n",
      "        delta_lr = max_lr - min_lr\n",
      "\n",
      "        if self.lr_decay_style == \"linear\":\n",
      "            coeff = 1.0 - decay_ratio\n",
      "        elif self.lr_decay_style == \"cosine\":\n",
      "            coeff = 0.5 * (math.cos(math.pi * decay_ratio) + 1.0)\n",
      "        else:\n",
      "            raise Exception(\n",
      "                \"{} decay style is not supported.\".format(self.lr_decay_style)\n",
      "            )\n",
      "\n",
      "        return min_lr + coeff * delta_lr\n",
      "\n",
      "    def step(self, increment):\n",
      "        \"\"\"Set lr for all parameters groups.\"\"\"\n",
      "        self.num_steps += increment\n",
      "        new_wd = self.get_wd()\n",
      "        for param_group in self.optimizer.param_groups:\n",
      "            new_lr = self.get_lr(param_group)\n",
      "            param_group[\"lr\"] = new_lr * param_group.get(\"lr_mult\", 1.0)\n",
      "            param_group[\"weight_decay\"] = new_wd * param_group.get(\"wd_mult\", 1.0)\n",
      "\n",
      "    def state_dict(self):\n",
      "        state_dict = {\n",
      "            \"max_lr\": self.max_lr,\n",
      "            \"lr_warmup_steps\": self.lr_warmup_steps,\n",
      "            \"num_steps\": self.num_steps,\n",
      "            \"lr_decay_style\": self.lr_decay_style,\n",
      "            \"lr_decay_steps\": self.lr_decay_steps,\n",
      "            \"min_lr\": self.min_lr,\n",
      "            \"start_wd\": self.start_wd,\n",
      "            \"end_wd\": self.end_wd,\n",
      "            \"wd_incr_style\": self.wd_incr_style,\n",
      "            \"wd_incr_steps\": self.wd_incr_steps,\n",
      "        }\n",
      "        return state_dict\n",
      "\n",
      "    def _check_and_set(self, cls_value, sd_value, name):\n",
      "        \"\"\"Auxiliary function for checking the values in the checkpoint and\n",
      "        setting them.\"\"\"\n",
      "        if self.override_opt_param_scheduler:\n",
      "            self.log_rank_0(\" > overriding {} value to {}\".format(name, cls_value))\n",
      "            return cls_value\n",
      "\n",
      "        if not self.use_checkpoint_opt_param_scheduler:\n",
      "            assert cls_value == sd_value, (\n",
      "                f\"OptimizerParamScheduler: class input value {cls_value} and checkpoint\"\n",
      "                f\"value {sd_value} for {name} do not match\"\n",
      "            )\n",
      "        self.log_rank_0(\" > using checkpoint value {} for {}\".format(sd_value, name))\n",
      "        return sd_value\n",
      "\n",
      "    def load_state_dict(self, sd):\n",
      "\n",
      "        if \"start_lr\" in sd:\n",
      "            max_lr_ = sd[\"start_lr\"]\n",
      "        else:\n",
      "            max_lr_ = sd[\"max_lr\"]\n",
      "        self.max_lr = self._check_and_set(self.max_lr, max_lr_, \"learning rate\")\n",
      "\n",
      "        self.min_lr = self._check_and_set(\n",
      "            self.min_lr, sd[\"min_lr\"], \"minimum learning rate\"\n",
      "        )\n",
      "\n",
      "        if \"warmup_iter\" in sd:\n",
      "            lr_warmup_steps_ = sd[\"warmup_iter\"]\n",
      "        elif \"warmup_steps\" in sd:\n",
      "            lr_warmup_steps_ = sd[\"warmup_steps\"]\n",
      "        else:\n",
      "            lr_warmup_steps_ = sd[\"lr_warmup_steps\"]\n",
      "        self.lr_warmup_steps = self._check_and_set(\n",
      "            self.lr_warmup_steps, lr_warmup_steps_, \"warmup iterations\"\n",
      "        )\n",
      "\n",
      "        if \"end_iter\" in sd:\n",
      "            lr_decay_steps_ = sd[\"end_iter\"]\n",
      "        elif \"decay_steps\" in sd:\n",
      "            lr_decay_steps_ = sd[\"decay_steps\"]\n",
      "        else:\n",
      "            lr_decay_steps_ = sd[\"lr_decay_steps\"]\n",
      "        self.lr_decay_steps = self._check_and_set(\n",
      "            self.lr_decay_steps, lr_decay_steps_, \"total number of iterations\"\n",
      "        )\n",
      "\n",
      "        if \"decay_style\" in sd:\n",
      "            lr_decay_style_ = sd[\"decay_style\"]\n",
      "        else:\n",
      "            lr_decay_style_ = sd[\"lr_decay_style\"]\n",
      "        self.lr_decay_style = self._check_and_set(\n",
      "            self.lr_decay_style, lr_decay_style_, \"learning rate decay style\"\n",
      "        )\n",
      "\n",
      "        if \"num_iters\" in sd:\n",
      "            num_steps = sd[\"num_iters\"]\n",
      "        else:\n",
      "            num_steps = sd[\"num_steps\"]\n",
      "        self.step(increment=num_steps)\n",
      "\n",
      "        if \"start_wd\" in sd:\n",
      "            self.start_wd = self._check_and_set(\n",
      "                self.start_wd, sd[\"start_wd\"], \"start weight decay\"\n",
      "            )\n",
      "            self.end_wd = self._check_and_set(\n",
      "                self.end_wd, sd[\"end_wd\"], \"end weight decay\"\n",
      "            )\n",
      "            self.wd_incr_steps = self._check_and_set(\n",
      "                self.wd_incr_steps,\n",
      "                sd[\"wd_incr_steps\"],\n",
      "                \"total number of weight decay iterations\",\n",
      "            )\n",
      "            self.wd_incr_style = self._check_and_set(\n",
      "                self.wd_incr_style,\n",
      "                sd[\"wd_incr_style\"],\n",
      "                \"weight decay incr style\",\n",
      "            )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/vllm/engine.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "from typing import List, Union\n",
      "\n",
      "from vllm.distributed import parallel_state\n",
      "from vllm.engine.llm_engine import LLMEngine, SchedulerOutputState\n",
      "from vllm.outputs import EmbeddingRequestOutput, RequestOutput\n",
      "from vllm.sequence import ExecuteModelRequest\n",
      "\n",
      "from realhf.base import logging\n",
      "\n",
      "logger = logging.getLogger(\"vllm LLMEngine\")\n",
      "\n",
      "\n",
      "class LLMEngine_(LLMEngine):\n",
      "    \"\"\"Overwriting the `step` method to synchronize the output across the\n",
      "    tensor parallel group. See the annotation comment for detailed changes.\n",
      "\n",
      "    vLLM implements tensor/pipeline parallelism using a single\n",
      "    `LLMEngine` instance with N-1 remote workers, and only the\n",
      "    driver worker is supposed to be responsible for output processing.\n",
      "    The remote workers are only responsible for executing the model.\n",
      "\n",
      "    Since we follow the convention of replicating data across\n",
      "    the TP/PP group, it's necessary to synchronize the output\n",
      "    from the model executor and replicate it for TP/PP.\n",
      "    \"\"\"\n",
      "\n",
      "    def step(self) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n",
      "        if self.parallel_config.pipeline_parallel_size > 1:\n",
      "            raise NotImplementedError(\n",
      "                \"Pipeline parallelism is only supported through AsyncLLMEngine \"\n",
      "                \"as performance will be severely degraded otherwise.\"\n",
      "            )\n",
      "\n",
      "        # For llm_engine, there is no pipeline parallel support, so the engine\n",
      "        # used is always 0.\n",
      "        virtual_engine = 0\n",
      "\n",
      "        # These are cached outputs from previous iterations. None if on first\n",
      "        # iteration\n",
      "        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n",
      "        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n",
      "        scheduler_outputs = cached_outputs.scheduler_outputs\n",
      "        allow_async_output_proc = cached_outputs.allow_async_output_proc\n",
      "\n",
      "        ctx = self.scheduler_contexts[virtual_engine]\n",
      "\n",
      "        # Clear outputs for each new scheduler iteration\n",
      "        ctx.request_outputs.clear()\n",
      "\n",
      "        # Skip the scheduler if there are any remaining steps in the seq groups.\n",
      "        # This ensures that the scheduler is only called again when the current\n",
      "        # batch has completed.\n",
      "        if not self._has_remaining_steps(seq_group_metadata_list):\n",
      "            # Schedule iteration\n",
      "            (seq_group_metadata_list, scheduler_outputs, allow_async_output_proc) = (\n",
      "                self.scheduler[virtual_engine].schedule()\n",
      "            )\n",
      "\n",
      "            ctx.seq_group_metadata_list = seq_group_metadata_list\n",
      "            ctx.scheduler_outputs = scheduler_outputs\n",
      "\n",
      "            # Maybe switch from async mode to sync mode\n",
      "            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n",
      "                self._process_model_outputs(ctx=ctx)\n",
      "\n",
      "            if (\n",
      "                self.scheduler_config.is_multi_step\n",
      "                and scheduler_outputs.num_lookahead_slots > 0\n",
      "            ):\n",
      "                # cache the scheduler outputs for the next iteration if we have\n",
      "                # lookahead slots\n",
      "                self._cache_scheduler_outputs_for_multi_step(\n",
      "                    virtual_engine,\n",
      "                    seq_group_metadata_list,\n",
      "                    scheduler_outputs,\n",
      "                    allow_async_output_proc,\n",
      "                )\n",
      "\n",
      "        assert seq_group_metadata_list is not None\n",
      "        assert scheduler_outputs is not None\n",
      "\n",
      "        if not scheduler_outputs.is_empty():\n",
      "            finished_requests_ids = self.scheduler[\n",
      "                virtual_engine\n",
      "            ].get_and_reset_finished_requests_ids()\n",
      "\n",
      "            # Check if we have a cached last_output from the previous iteration.\n",
      "            # For supporting PP this is probably the best way to pass the\n",
      "            # sampled_token_ids, as a separate broadcast over all the PP stages\n",
      "            # will cause one virtual engine's microbatch to block the pipeline.\n",
      "            last_sampled_token_ids = self._get_last_sampled_token_ids(virtual_engine)\n",
      "\n",
      "            execute_model_req = ExecuteModelRequest(\n",
      "                seq_group_metadata_list=seq_group_metadata_list,\n",
      "                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n",
      "                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n",
      "                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n",
      "                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n",
      "                running_queue_size=scheduler_outputs.running_queue_size,\n",
      "                finished_requests_ids=finished_requests_ids,\n",
      "                # We use ExecuteModelRequest to pass the last sampled_token_ids\n",
      "                # to each of the non-last PP stages for in-place prepare_input.\n",
      "                last_sampled_token_ids=last_sampled_token_ids,\n",
      "            )\n",
      "\n",
      "            if allow_async_output_proc:\n",
      "                execute_model_req.async_callback = self.async_callbacks[virtual_engine]\n",
      "\n",
      "            outputs = self.model_executor.execute_model(\n",
      "                execute_model_req=execute_model_req\n",
      "            )\n",
      "\n",
      "            ######################## Changed here ########################\n",
      "            if (\n",
      "                not self.model_executor.driver_worker.is_driver_worker\n",
      "                and allow_async_output_proc\n",
      "            ):\n",
      "                # The following line processes the output asynchronously with GPU computation.\n",
      "                # Since non-driver workers exit `execute_model` as long as the logits has been\n",
      "                # computed, we should manually invoke the callback here, otherwise the outputs\n",
      "                # will not be processes and the non-driver worker will run forever.\n",
      "                execute_model_req.async_callback()\n",
      "            # Synchronizing outputs.\n",
      "            outputs = parallel_state.get_tp_group().broadcast_object(outputs, src=0)\n",
      "            ######################## Changed here ########################\n",
      "\n",
      "            # We need to do this here so that last step's sampled_token_ids can\n",
      "            # be passed to the next iteration for PP.\n",
      "            if self.scheduler_config.is_multi_step:\n",
      "                self._update_cached_scheduler_output(virtual_engine, outputs)\n",
      "        else:\n",
      "            # Nothing scheduled => If there is pending async postprocessor,\n",
      "            # then finish it here.\n",
      "            if len(ctx.output_queue) > 0:\n",
      "                self._process_model_outputs(ctx=ctx)\n",
      "            # No outputs in this case\n",
      "            outputs = []\n",
      "\n",
      "        # Finish the current step for all the sequence groups.\n",
      "        if self.scheduler_config.is_multi_step:\n",
      "            for seq_group in seq_group_metadata_list:\n",
      "                seq_group.finish_step()\n",
      "\n",
      "        if not self._has_remaining_steps(seq_group_metadata_list):\n",
      "            # clear the cache if we have finished all the steps.\n",
      "            if self.scheduler_config.is_multi_step:\n",
      "                self.cached_scheduler_outputs[0] = SchedulerOutputState()\n",
      "\n",
      "            # is_first_step_output is True only when the num_steps of all\n",
      "            # the sequences are 1. When the num_steps > 1,\n",
      "            # multi_step_model_runner does the first-step output append.\n",
      "            is_first_step_output: bool = (\n",
      "                False\n",
      "                if not seq_group_metadata_list\n",
      "                else seq_group_metadata_list[0].state.num_steps == 1\n",
      "            )\n",
      "\n",
      "            # Add results to the output_queue\n",
      "            ctx.append_output(\n",
      "                outputs=outputs,\n",
      "                seq_group_metadata_list=seq_group_metadata_list,\n",
      "                scheduler_outputs=scheduler_outputs,\n",
      "                is_async=allow_async_output_proc,\n",
      "                is_last_step=True,\n",
      "                is_first_step_output=is_first_step_output,\n",
      "            )\n",
      "\n",
      "            if outputs and allow_async_output_proc:\n",
      "                assert (\n",
      "                    len(outputs) == 1\n",
      "                ), \"Async postprocessor expects only a single output set\"\n",
      "\n",
      "                self._advance_to_next_step(\n",
      "                    outputs[0],\n",
      "                    seq_group_metadata_list,\n",
      "                    scheduler_outputs.scheduled_seq_groups,\n",
      "                )\n",
      "\n",
      "            # Check if need to run the usual non-async path\n",
      "            if not allow_async_output_proc:\n",
      "                self._process_model_outputs(ctx=ctx)\n",
      "\n",
      "                # Log stats.\n",
      "                self.do_log_stats(scheduler_outputs, outputs)\n",
      "\n",
      "                # Tracing\n",
      "                self.do_tracing(scheduler_outputs)\n",
      "        else:\n",
      "            # Multi-step case\n",
      "            return ctx.request_outputs\n",
      "\n",
      "        if not self.has_unfinished_requests():\n",
      "            # Drain async postprocessor (if exists)\n",
      "            if len(ctx.output_queue) > 0:\n",
      "                self._process_model_outputs(ctx=ctx)\n",
      "            assert len(ctx.output_queue) == 0\n",
      "\n",
      "            # Stop the execute model loop in parallel workers until there are\n",
      "            # more requests to process. This avoids waiting indefinitely in\n",
      "            # torch.distributed ops which may otherwise timeout, and unblocks\n",
      "            # the RPC thread in the workers so that they can process any other\n",
      "            # queued control plane messages, such as add/remove lora adapters.\n",
      "            logger.debug(\"Stopping remote worker execution loop.\")\n",
      "            self.model_executor.stop_remote_worker_execution_loop()\n",
      "\n",
      "        return ctx.request_outputs\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/vllm/custom_cache_manager.py ====\n",
      "\n",
      "import os\n",
      "\n",
      "try:\n",
      "    from triton.runtime.cache import (\n",
      "        FileCacheManager,\n",
      "        default_cache_dir,\n",
      "        default_dump_dir,\n",
      "        default_override_dir,\n",
      "    )\n",
      "\n",
      "    triton_available = True\n",
      "except ModuleNotFoundError:\n",
      "    triton_available = False\n",
      "\n",
      "    class FileCacheManager:\n",
      "        pass\n",
      "\n",
      "\n",
      "from realhf.base import constants, logging, topology\n",
      "\n",
      "logger = logging.getLogger(\"triton CustomCacheManager for vLLM\")\n",
      "\n",
      "\n",
      "def maybe_set_triton_cache_manager() -> None:\n",
      "    \"\"\"Set environment variable to tell Triton to use a custom cache\n",
      "    manager.\"\"\"\n",
      "    cache_manger = os.environ.get(\"TRITON_CACHE_MANAGER\", None)\n",
      "    if cache_manger is None and triton_available:\n",
      "        manager = \"realhf.impl.model.backend.thirdparty.vllm.custom_cache_manager:CustomCacheManager\"\n",
      "        logger.info(\"Setting Triton cache manager to: %s\", manager)\n",
      "        os.environ[\"TRITON_CACHE_MANAGER\"] = manager\n",
      "\n",
      "\n",
      "class CustomCacheManager(FileCacheManager):\n",
      "    \"\"\"Re-implements Triton's cache manager, ensuring that a unique cache\n",
      "    directory is created for each process. This is needed to avoid collisions\n",
      "    when running with tp>1 and using multi-processing as the distributed\n",
      "    backend.\n",
      "\n",
      "    Note this issue was fixed by triton-lang/triton/pull/4295, but the\n",
      "    fix is not yet included in triton==v3.0.0. However, it should be\n",
      "    included in the subsequent version.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, key, override=False, dump=False):\n",
      "        self.key = key\n",
      "        self.lock_path = None\n",
      "        if dump:\n",
      "            self.cache_dir = default_dump_dir()\n",
      "            self.cache_dir = os.path.join(self.cache_dir, self.key)\n",
      "            self.lock_path = os.path.join(self.cache_dir, \"lock\")\n",
      "            os.makedirs(self.cache_dir, exist_ok=True)\n",
      "        elif override:\n",
      "            self.cache_dir = default_override_dir()\n",
      "            self.cache_dir = os.path.join(self.cache_dir, self.key)\n",
      "        else:\n",
      "            # create cache directory if it doesn't exist\n",
      "            self.cache_dir = (\n",
      "                os.getenv(\"TRITON_CACHE_DIR\", \"\").strip() or default_cache_dir()\n",
      "            )\n",
      "            cache_id = f\"{constants.model_name()}_{constants.parallelism_rank()}\"\n",
      "            if self.cache_dir:\n",
      "                self.cache_dir = f\"{self.cache_dir}_{cache_id}\"\n",
      "                self.cache_dir = os.path.join(self.cache_dir, self.key)\n",
      "                self.lock_path = os.path.join(self.cache_dir, \"lock\")\n",
      "                os.makedirs(self.cache_dir, exist_ok=True)\n",
      "            else:\n",
      "                raise RuntimeError(\"Could not create or locate cache dir\")\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/vllm/executor.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import functools\n",
      "import gc\n",
      "import os\n",
      "import time\n",
      "from typing import Dict, List, Optional, Set, Tuple, Union\n",
      "\n",
      "import pynvml\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.nn as nn\n",
      "from vllm.executor.gpu_executor import GPUExecutor\n",
      "from vllm.model_executor.model_loader.loader import (\n",
      "    device_loading_context,\n",
      "    get_model_loader,\n",
      ")\n",
      "from vllm.model_executor.model_loader.utils import set_default_torch_dtype\n",
      "from vllm.worker.model_runner import GPUModelRunnerBase, ModelRunner\n",
      "from vllm.worker.multi_step_model_runner import MultiStepModelRunner\n",
      "from vllm.worker.multi_step_worker import MultiStepWorker\n",
      "from vllm.worker.worker import Worker, _check_if_gpu_supports_dtype\n",
      "\n",
      "from realhf.base import constants, logging\n",
      "\n",
      "from .custom_cache_manager import maybe_set_triton_cache_manager\n",
      "\n",
      "logger = logging.getLogger(\"vllm executor\")\n",
      "\n",
      "\n",
      "# Update weights patch\n",
      "def _update_weights_model_runner(self: ModelRunner, path: str) -> nn.Module:\n",
      "    target_device = torch.device(self.device_config.device)\n",
      "    loader = get_model_loader(self.load_config)\n",
      "    self.model_config.model = path\n",
      "\n",
      "    if getattr(self.model, \"_offloaded\", None):\n",
      "        self.model = loader.load_model(\n",
      "            model_config=self.model_config,\n",
      "            device_config=self.device_config,\n",
      "            lora_config=self.lora_config,\n",
      "            parallel_config=self.parallel_config,\n",
      "            scheduler_config=self.scheduler_config,\n",
      "            cache_config=self.cache_config,\n",
      "        )\n",
      "        return self.model.eval()\n",
      "\n",
      "    model_config = self.model_config\n",
      "    model = self.model\n",
      "    with set_default_torch_dtype(model_config.dtype):\n",
      "        # Skip model initialization here.\n",
      "        model.load_weights(loader._get_all_weights(self.model_config, model))\n",
      "        for _, module in model.named_modules():\n",
      "            quant_method = getattr(module, \"quant_method\", None)\n",
      "            if quant_method is not None:\n",
      "                # When quant methods need to process weights after loading\n",
      "                # (for repacking, quantizing, etc), they expect parameters\n",
      "                # to be on the global target device. This scope is for the\n",
      "                # case where cpu offloading is used, where we will move the\n",
      "                # parameters onto device for processing and back off after.\n",
      "                with device_loading_context(module, target_device):\n",
      "                    quant_method.process_weights_after_loading(module)\n",
      "    return model.eval()\n",
      "\n",
      "\n",
      "def _update_weights_multi_step_runner(\n",
      "    self: MultiStepModelRunner, path: str\n",
      ") -> nn.Module:\n",
      "    return _update_weights_model_runner(self._base_model_runner, path)\n",
      "\n",
      "\n",
      "def _update_weights_worker(self, path):\n",
      "    if isinstance(self, MultiStepWorker):\n",
      "        return _update_weights_multi_step_runner(self.model_runner, path)\n",
      "    elif isinstance(self, Worker):\n",
      "        return _update_weights_model_runner(self.model_runner, path)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Unsupported worker type: {type(self)}\")\n",
      "\n",
      "\n",
      "# Offload patch\n",
      "def _offload_weights_model_runner(self: ModelRunner):\n",
      "    for p in self.model.parameters():\n",
      "        p.data = p.data.new()\n",
      "    self.model._offloaded = True\n",
      "\n",
      "\n",
      "def _offload_weights_worker(self):\n",
      "    if isinstance(self, MultiStepWorker):\n",
      "        return _offload_weights_model_runner(self.model_runner._base_model_runner)\n",
      "    elif isinstance(self, Worker):\n",
      "        return _offload_weights_model_runner(self.model_runner)\n",
      "    else:\n",
      "        raise NotImplementedError(f\"Unsupported worker type: {type(self)}\")\n",
      "\n",
      "\n",
      "class GPUExecutor_(GPUExecutor):\n",
      "\n",
      "    def _init_executor(self):\n",
      "        # NOTE: Difference from vLLM:\n",
      "        # 1. Relax the assertion of TP == 1\n",
      "        # 2. Unroll worker.load_model() (or model_runner.load_model())\n",
      "\n",
      "        # Create worker.\n",
      "        # We use the name `driver_worker` to comfort vLLM's naming.\n",
      "        # The `driver_worker` may not be the actual driver.\n",
      "\n",
      "        # workaround for https://github.com/vllm-project/vllm/issues/6103\n",
      "        if constants.tp_and_pp_world_size() > 1:\n",
      "            maybe_set_triton_cache_manager()\n",
      "\n",
      "        self.driver_worker = self._create_worker(\n",
      "            local_rank=0,\n",
      "            rank=constants.tp_and_pp_rank(),\n",
      "        )\n",
      "        # Patch an `update_weights` method.\n",
      "        setattr(self.driver_worker.__class__, \"update_weights\", _update_weights_worker)\n",
      "        setattr(\n",
      "            self.driver_worker.__class__, \"offload_weights\", _offload_weights_worker\n",
      "        )\n",
      "\n",
      "        # Init device.\n",
      "        # Unroll the init_device method to skip the following two operations:\n",
      "        # 1) initializing the distributed environment\n",
      "        # 2) setting the random seed, because we have set it in model workers\n",
      "        if self.driver_worker.device_config.device.type == \"cuda\":\n",
      "            # torch.distributed.all_reduce does not free the input tensor until\n",
      "            # the synchronization point. This causes the memory usage to grow\n",
      "            # as the number of all_reduce calls increases. This env var disables\n",
      "            # this behavior.\n",
      "            # Related issue:\n",
      "            # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n",
      "            os.environ[\"TORCH_NCCL_AVOID_RECORD_STREAMS\"] = \"1\"\n",
      "\n",
      "            # This env var set by Ray causes exceptions with graph building.\n",
      "            os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n",
      "            self.driver_worker.device = torch.device(\n",
      "                f\"cuda:{self.driver_worker.local_rank}\"\n",
      "            )\n",
      "            torch.cuda.set_device(self.driver_worker.device)\n",
      "\n",
      "            _check_if_gpu_supports_dtype(self.driver_worker.model_config.dtype)\n",
      "            gc.collect()\n",
      "            torch.cuda.empty_cache()\n",
      "            self.driver_worker.init_gpu_memory = torch.cuda.mem_get_info()[0]\n",
      "        else:\n",
      "            raise RuntimeError(\n",
      "                f\"Not support device type: {self.driver_worker.device_config.device}\"\n",
      "            )\n",
      "\n",
      "        # Load model.\n",
      "        self.driver_worker.load_model()\n",
      "\n",
      "    def determine_num_available_blocks(self) -> Tuple[int, int]:\n",
      "        # NOTE: Difference from vLLM:\n",
      "        # 1. We use dist.all_reduce to get the minimum number of blocks\n",
      "        #    across all TP + PP parallel ranks.\n",
      "        # 2. We set the init GPU memory as the total GPU memory, such that\n",
      "        #    vLLM will allocate (total_mem * max_util - cur_mem) for KV caches.\n",
      "        #    Check the worker class in vLLM for details.\n",
      "        free_mem, total_mem = torch.cuda.mem_get_info()\n",
      "        used_mem = total_mem - free_mem\n",
      "        self.driver_worker.init_gpu_memory = total_mem\n",
      "        if (\n",
      "            total_mem * self.driver_worker.cache_config.gpu_memory_utilization\n",
      "            < used_mem\n",
      "        ):\n",
      "            raise torch.cuda.OutOfMemoryError(\n",
      "                f\"Not enough space of vLLM KV caches. \"\n",
      "                f\"Used memory: {used_mem / 1024**3:.2f} GB, total memory: {total_mem / 1024**3:.2f} GB, \"\n",
      "                f\"max_utilization: {self.driver_worker.cache_config.gpu_memory_utilization}, \"\n",
      "                f\"max possible memory usage: {self.driver_worker.cache_config.gpu_memory_utilization * total_mem / 1024**3:.2f}.\"\n",
      "            )\n",
      "        # Get the maximum number of blocks that can be allocated on GPU and CPU.\n",
      "        num_blocks = self.driver_worker.determine_num_available_blocks()\n",
      "\n",
      "        # Take the minimum across all ranks.\n",
      "        num_blocks = torch.tensor(\n",
      "            num_blocks, device=constants.current_device(), dtype=torch.long\n",
      "        )\n",
      "        # NOTE: this is the TP + PP group\n",
      "        dist.all_reduce(\n",
      "            num_blocks,\n",
      "            op=dist.ReduceOp.MIN,\n",
      "            group=constants.tp_and_pp_group(),\n",
      "        )\n",
      "\n",
      "        return int(num_blocks[0]), int(num_blocks[1])\n",
      "\n",
      "    def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:\n",
      "        # NOTE: Difference from vLLM: we log memory usage here.\n",
      "        logger.info(\n",
      "            \"Rank %d, # GPU blocks: %d, # CPU blocks: %d\",\n",
      "            dist.get_rank(),\n",
      "            num_gpu_blocks,\n",
      "            num_cpu_blocks,\n",
      "        )\n",
      "\n",
      "        self.cache_config.num_gpu_blocks = num_gpu_blocks\n",
      "        self.cache_config.num_cpu_blocks = num_cpu_blocks\n",
      "\n",
      "        torch.cuda.synchronize()\n",
      "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
      "        before_mem = float(pynvml.nvmlDeviceGetMemoryInfo(handle).used)\n",
      "        tik = time.perf_counter()\n",
      "\n",
      "        self.driver_worker.initialize_cache(\n",
      "            num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks\n",
      "        )\n",
      "\n",
      "        # Add memory logging here.\n",
      "        torch.cuda.synchronize()\n",
      "        tok = time.perf_counter()\n",
      "        after_mem = float(pynvml.nvmlDeviceGetMemoryInfo(handle).used)\n",
      "        is_dp_head = (\n",
      "            constants.is_last_pipe_stage() and constants.tensor_parallel_rank() == 0\n",
      "        )\n",
      "        if is_dp_head:\n",
      "            logger.info(\n",
      "                f\"vLLM DP rank {constants.data_parallel_rank()} \"\n",
      "                f\"KV cache memory: {before_mem / 1024**2:.2f} MB\"\n",
      "                f\" -> {after_mem / 1024**2:.2f} MB, \"\n",
      "                f\"Initializing KV cache time consumption: {tok - tik:.2f} seconds\"\n",
      "            )\n",
      "\n",
      "    def clear_kv_cache(self) -> None:\n",
      "        # Log the memory usage before clearing the cache\n",
      "        torch.cuda.synchronize()\n",
      "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
      "        before_mem = float(pynvml.nvmlDeviceGetMemoryInfo(handle).used)\n",
      "        tik = time.perf_counter()\n",
      "\n",
      "        # Clear the cache\n",
      "        self.driver_worker.gpu_cache = None  # Optional[List[List[torch.Tensor]]]\n",
      "        self.driver_worker.cache_engine = None\n",
      "        gc.collect()\n",
      "        torch.cuda.empty_cache()\n",
      "\n",
      "        # Log the memory usage after clearing the cache\n",
      "        torch.cuda.synchronize()\n",
      "        tok = time.perf_counter()\n",
      "        after_mem = float(pynvml.nvmlDeviceGetMemoryInfo(handle).used)\n",
      "        is_dp_head = (\n",
      "            constants.is_last_pipe_stage() and constants.tensor_parallel_rank() == 0\n",
      "        )\n",
      "        if is_dp_head:\n",
      "            logger.info(\n",
      "                f\"vLLM DP rank {constants.data_parallel_rank()} \"\n",
      "                f\"KV cache memory: {before_mem / 1024**2:.2f} MB\"\n",
      "                f\" -> {after_mem / 1024**2:.2f} MB, \"\n",
      "                f\"Clearing KV cache time consumption: {tok - tik:.2f} seconds\"\n",
      "            )\n",
      "\n",
      "    def update_weights(self, path):\n",
      "        return self.driver_worker.update_weights(path)\n",
      "\n",
      "    def offload_weights(self):\n",
      "        return self.driver_worker.offload_weights()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/vllm/context.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import enum\n",
      "import time\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.distributed\n",
      "from vllm.distributed.parallel_state import (\n",
      "    GroupCoordinator,\n",
      "    _get_unique_name,\n",
      "    _register_group,\n",
      ")\n",
      "from vllm.platforms import current_platform\n",
      "\n",
      "from realhf.api.core.config import ModelName\n",
      "from realhf.base import constants, logging, topology\n",
      "\n",
      "logger = logging.getLogger(\"vLLM third party init\")\n",
      "\n",
      "\n",
      "class _vLLMGroupType(enum.Enum):\n",
      "    \"\"\"VLLM group types.\n",
      "\n",
      "    In the world of vLLM, there's no data parallel semantics,\n",
      "    so we should map the `constants.tp_and_pp_group` to the\n",
      "    world group of vLLM.\n",
      "    \"\"\"\n",
      "\n",
      "    WORLD = 1\n",
      "    TP = 2\n",
      "    PP = 3\n",
      "\n",
      "\n",
      "def _vllm_group_rank(group_type: _vLLMGroupType):\n",
      "    if group_type == _vLLMGroupType.WORLD:\n",
      "        return constants.tp_and_pp_rank()\n",
      "    elif group_type == _vLLMGroupType.TP:\n",
      "        return constants.tensor_parallel_rank()\n",
      "    elif group_type == _vLLMGroupType.PP:\n",
      "        return constants.pipe_parallel_rank()\n",
      "\n",
      "\n",
      "def _vllm_group_size(group_type: _vLLMGroupType):\n",
      "    if group_type == _vLLMGroupType.WORLD:\n",
      "        return constants.tp_and_pp_world_size()\n",
      "    elif group_type == _vLLMGroupType.TP:\n",
      "        return constants.tensor_parallel_world_size()\n",
      "    elif group_type == _vLLMGroupType.PP:\n",
      "        return constants.pipe_parallel_world_size()\n",
      "\n",
      "\n",
      "def _vllm_parallel_group(group_type: _vLLMGroupType):\n",
      "    if group_type == _vLLMGroupType.WORLD:\n",
      "        return constants.tp_and_pp_group()\n",
      "    elif group_type == _vLLMGroupType.TP:\n",
      "        return constants.tensor_parallel_group()\n",
      "    elif group_type == _vLLMGroupType.PP:\n",
      "        return constants.pipe_parallel_group()\n",
      "\n",
      "\n",
      "def _vllm_cpu_parallel_group(group_type: _vLLMGroupType):\n",
      "    if group_type == _vLLMGroupType.WORLD:\n",
      "        return constants.grid().ds_model_proc_group_gloo\n",
      "    elif group_type == _vLLMGroupType.TP:\n",
      "        return constants.grid().slice_proc_group_gloo\n",
      "    elif group_type == _vLLMGroupType.PP:\n",
      "        return constants.grid().pp_proc_group_gloo\n",
      "\n",
      "\n",
      "def _vllm_parallel_ranks(group_type: _vLLMGroupType):\n",
      "    return torch.distributed.get_process_group_ranks(_vllm_parallel_group(group_type))\n",
      "\n",
      "\n",
      "class vLLMGroupCoordinator(GroupCoordinator):\n",
      "    def __init__(\n",
      "        self,\n",
      "        group_type: _vLLMGroupType,\n",
      "        group_name: Optional[str] = None,\n",
      "    ):\n",
      "        group_name = group_name or \"anonymous\"\n",
      "        self.unique_name = _get_unique_name(group_name)\n",
      "        _register_group(self)\n",
      "\n",
      "        # global rank\n",
      "        self.rank = torch.distributed.get_rank()\n",
      "        # global ranks in the group\n",
      "        self.ranks = _vllm_parallel_ranks(group_type)\n",
      "        # local rank used to assign devices\n",
      "        self.local_rank = 0  # because we have set independent CUDA_VISIBLE_DEVICES\n",
      "\n",
      "        # group for device communication\n",
      "        self.device_group = _vllm_parallel_group(group_type)\n",
      "        # group for cpu communication\n",
      "        self.cpu_group = _vllm_cpu_parallel_group(group_type)\n",
      "\n",
      "        # rank inside the group\n",
      "        self.rank_in_group = _vllm_group_rank(group_type)\n",
      "\n",
      "        # size of the group\n",
      "        self.world_size = _vllm_group_size(group_type)\n",
      "\n",
      "        if current_platform.is_cuda_alike():\n",
      "            self.device = torch.device(f\"cuda:{0}\")\n",
      "        else:\n",
      "            self.device = torch.device(\"cpu\")\n",
      "\n",
      "        self.use_pynccl = False\n",
      "        self.use_custom_allreduce = False\n",
      "        self.use_tpu_communicator = False\n",
      "\n",
      "        self.pynccl_comm = None\n",
      "        self.ca_comm = None\n",
      "        self.tpu_communicator = None\n",
      "        self.mq_broadcaster = None\n",
      "\n",
      "\n",
      "def init_vllm():\n",
      "    # TODO: support speculative decoding, where the draft model can\n",
      "    # have a different TP degree\n",
      "    from vllm.distributed import parallel_state\n",
      "\n",
      "    parallel_state._WORLD = vLLMGroupCoordinator(\n",
      "        group_type=_vLLMGroupType.WORLD,\n",
      "        group_name=\"world\",\n",
      "    )\n",
      "\n",
      "    # Build the tensor model-parallel groups.\n",
      "    assert (\n",
      "        parallel_state._TP is None\n",
      "    ), \"vLLM tensor model parallel group is already initialized\"\n",
      "    parallel_state._TP = vLLMGroupCoordinator(\n",
      "        group_type=_vLLMGroupType.TP,\n",
      "        group_name=\"tp\",\n",
      "    )\n",
      "\n",
      "    # Build the pipeline model-parallel groups.\n",
      "    assert (\n",
      "        parallel_state._PP is None\n",
      "    ), \"vLLM pipeline model parallel group is already initialized\"\n",
      "    parallel_state._PP = vLLMGroupCoordinator(\n",
      "        group_type=_vLLMGroupType.PP,\n",
      "        group_name=\"pp\",\n",
      "    )\n",
      "\n",
      "    assert parallel_state.model_parallel_is_initialized()\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/backend/thirdparty/vllm/__init__.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "from .context import init_vllm\n",
      "from .custom_cache_manager import maybe_set_triton_cache_manager\n",
      "from .engine import LLMEngine_\n",
      "from .executor import GPUExecutor_\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/tensor_parallel/mappings.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "import torch\n",
      "import torch.distributed\n",
      "\n",
      "from realhf.base import constants\n",
      "\n",
      "from .utils import split_tensor_along_last_dim\n",
      "\n",
      "\n",
      "def _reduce(input_):\n",
      "    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n",
      "\n",
      "    # Bypass the function if we are using only 1 GPU.\n",
      "    if constants.tensor_parallel_world_size() == 1:\n",
      "        return input_\n",
      "\n",
      "    # All-reduce.\n",
      "    torch.distributed.all_reduce(input_, group=constants.tensor_parallel_group())\n",
      "    return input_\n",
      "\n",
      "\n",
      "def _split_along_last_dim(input_):\n",
      "    \"\"\"Split the tensor along its last dimension and keep the corresponding\n",
      "    slice.\"\"\"\n",
      "\n",
      "    world_size = constants.tensor_parallel_world_size()\n",
      "    # Bypass the function if we are using only 1 GPU.\n",
      "    if world_size == 1:\n",
      "        return input_\n",
      "\n",
      "    # Split along last dimension.\n",
      "    input_list = split_tensor_along_last_dim(input_, world_size)\n",
      "\n",
      "    # Note: torch.split does not create contiguous tensors by default.\n",
      "    rank = constants.tensor_parallel_rank()\n",
      "    output = input_list[rank].contiguous()\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def _split_along_first_dim(input_):\n",
      "    \"\"\"Split the tensor along its first dimension and keep the corresponding\n",
      "    slice.\"\"\"\n",
      "\n",
      "    world_size = constants.tensor_parallel_world_size()\n",
      "    # Bypass the function if we are using only 1 GPU.\n",
      "    if world_size == 1:\n",
      "        return input_\n",
      "\n",
      "    # Split along first dimension.\n",
      "    dim_size = input_.size()[0]\n",
      "    assert (\n",
      "        dim_size % world_size == 0\n",
      "    ), \"First dimension of the tensor should be divisible by tensor parallel size\"\n",
      "    local_dim_size = dim_size // world_size\n",
      "    rank = constants.tensor_parallel_rank()\n",
      "    dim_offset = rank * local_dim_size\n",
      "\n",
      "    output = input_[dim_offset : dim_offset + local_dim_size].contiguous()\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def _gather_along_last_dim(input_):\n",
      "    \"\"\"Gather tensors and concatinate along the last dimension.\"\"\"\n",
      "\n",
      "    world_size = constants.tensor_parallel_world_size()\n",
      "    # Bypass the function if we are using only 1 GPU.\n",
      "    if world_size == 1:\n",
      "        return input_\n",
      "\n",
      "    # Size and dimension.\n",
      "    last_dim = input_.dim() - 1\n",
      "    rank = constants.tensor_parallel_rank()\n",
      "\n",
      "    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\n",
      "    tensor_list[rank] = input_\n",
      "    torch.distributed.all_gather(\n",
      "        tensor_list, input_, group=constants.tensor_parallel_group()\n",
      "    )\n",
      "\n",
      "    # Note: torch.cat already creates a contiguous tensor.\n",
      "    output = torch.cat(tensor_list, dim=last_dim).contiguous()\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def _gather_along_first_dim(input_):\n",
      "    \"\"\"Gather tensors and concatinate along the first dimension.\"\"\"\n",
      "\n",
      "    world_size = constants.tensor_parallel_world_size()\n",
      "    # Bypass the function if we are using only 1 GPU.\n",
      "    if world_size == 1:\n",
      "        return input_\n",
      "\n",
      "    dim_size = list(input_.size())\n",
      "    dim_size[0] = dim_size[0] * world_size\n",
      "\n",
      "    output = torch.empty(\n",
      "        dim_size, dtype=input_.dtype, device=constants.current_device()\n",
      "    )\n",
      "    torch.distributed._all_gather_base(\n",
      "        output, input_.contiguous(), group=constants.tensor_parallel_group()\n",
      "    )\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def _reduce_scatter_along_first_dim(input_):\n",
      "    \"\"\"Reduce-scatter the input tensor across model parallel group.\"\"\"\n",
      "    world_size = constants.tensor_parallel_world_size()\n",
      "    # Bypass the function if we are using only 1 GPU.\n",
      "    if world_size == 1:\n",
      "        return input_\n",
      "\n",
      "    dim_size = list(input_.size())\n",
      "    assert (\n",
      "        dim_size[0] % world_size == 0\n",
      "    ), \"First dimension of the tensor should be divisible by tensor parallel size\"\n",
      "\n",
      "    dim_size[0] = dim_size[0] // world_size\n",
      "\n",
      "    # NOTE: We don't use in-place reduce-scatter because we don't want to\n",
      "    # corrupt the activations which will be used during the backward pass.\n",
      "    output = torch.empty(\n",
      "        dim_size, dtype=input_.dtype, device=constants.current_device()\n",
      "    )\n",
      "    torch.distributed._reduce_scatter_base(\n",
      "        output, input_.contiguous(), group=constants.tensor_parallel_group()\n",
      "    )\n",
      "    return output\n",
      "\n",
      "\n",
      "class _CopyToModelParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"Pass the input to the model parallel region.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_):\n",
      "        return input_\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_):\n",
      "        return input_\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        return _reduce(grad_output)\n",
      "\n",
      "\n",
      "class _ReduceFromModelParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_):\n",
      "        return _reduce(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_):\n",
      "        return _reduce(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        return grad_output\n",
      "\n",
      "\n",
      "class _ScatterToModelParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_):\n",
      "        return _split_along_last_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_):\n",
      "        return _split_along_last_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        return _gather_along_last_dim(grad_output)\n",
      "\n",
      "\n",
      "class _GatherFromModelParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"Gather the input from model parallel region and concatinate.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_):\n",
      "        return _gather_along_last_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_):\n",
      "        return _gather_along_last_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        return _split_along_last_dim(grad_output)\n",
      "\n",
      "\n",
      "class _ScatterToSequenceParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_):\n",
      "        return _split_along_first_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_):\n",
      "        return _split_along_first_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        return _gather_along_first_dim(grad_output)\n",
      "\n",
      "\n",
      "class _GatherFromSequenceParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"Gather the input from sequence parallel region and concatinate.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_, model_parallel_output_grad=True):\n",
      "        return _gather_along_first_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_, model_parallel_output_grad=True):\n",
      "        ctx.model_parallel_output_grad = model_parallel_output_grad\n",
      "        return _gather_along_first_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        model_parallel_output_grad = ctx.model_parallel_output_grad\n",
      "\n",
      "        # If the computation graph after the gather operation is\n",
      "        # in the tensor parallel mode, output gradients need to reduce\n",
      "        # scattered and whereas if the computation is duplicated,\n",
      "        # output gradients need to be scattered.\n",
      "        if model_parallel_output_grad:\n",
      "            return _reduce_scatter_along_first_dim(grad_output), None\n",
      "        else:\n",
      "            return _split_along_first_dim(grad_output), None\n",
      "\n",
      "\n",
      "class _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):\n",
      "    \"\"\"Reduce scatter the input from the model parallel region.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def symbolic(graph, input_):\n",
      "        return _reduce_scatter_along_first_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, input_):\n",
      "        return _reduce_scatter_along_first_dim(input_)\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        return _gather_along_first_dim(grad_output)\n",
      "\n",
      "\n",
      "# -----------------\n",
      "# Helper functions.\n",
      "# -----------------\n",
      "\n",
      "\n",
      "def copy_to_tensor_model_parallel_region(input_):\n",
      "    return _CopyToModelParallelRegion.apply(input_)\n",
      "\n",
      "\n",
      "def reduce_from_tensor_model_parallel_region(input_):\n",
      "    return _ReduceFromModelParallelRegion.apply(input_)\n",
      "\n",
      "\n",
      "def scatter_to_tensor_model_parallel_region(input_):\n",
      "    return _ScatterToModelParallelRegion.apply(input_)\n",
      "\n",
      "\n",
      "def gather_from_tensor_model_parallel_region(input_):\n",
      "    return _GatherFromModelParallelRegion.apply(input_)\n",
      "\n",
      "\n",
      "def scatter_to_sequence_parallel_region(input_):\n",
      "    return _ScatterToSequenceParallelRegion.apply(input_)\n",
      "\n",
      "\n",
      "def gather_from_sequence_parallel_region(input_, model_parallel_output_grad=True):\n",
      "    return _GatherFromSequenceParallelRegion.apply(input_, model_parallel_output_grad)\n",
      "\n",
      "\n",
      "def reduce_scatter_to_sequence_parallel_region(input_):\n",
      "    return _ReduceScatterToSequenceParallelRegion.apply(input_)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/tensor_parallel/utils.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "from typing import List, Sequence\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "\n",
      "_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS = {\n",
      "    \"tensor_model_parallel\": False,\n",
      "    \"partition_dim\": -1,\n",
      "    \"partition_stride\": 1,\n",
      "}\n",
      "\n",
      "\n",
      "def param_is_not_model_parallel_duplicate(param):\n",
      "    return (\n",
      "        hasattr(param, \"tensor_model_parallel\") and param.tensor_model_parallel\n",
      "    ) or (constants.tensor_parallel_rank() == 0)\n",
      "\n",
      "\n",
      "def set_tensor_model_parallel_attributes(tensor, is_parallel, dim, stride):\n",
      "    # Make sure the attributes are not set.\n",
      "    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\n",
      "        assert not hasattr(tensor, attribute)\n",
      "    # Set the attributes.\n",
      "    setattr(tensor, \"tensor_model_parallel\", is_parallel)\n",
      "    setattr(tensor, \"partition_dim\", dim)\n",
      "    setattr(tensor, \"partition_stride\", stride)\n",
      "\n",
      "\n",
      "def set_defaults_if_not_set_tensor_model_parallel_attributes(tensor):\n",
      "\n",
      "    def maybe_set(attribute, value):\n",
      "        if not hasattr(tensor, attribute):\n",
      "            setattr(tensor, attribute, value)\n",
      "\n",
      "    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\n",
      "        maybe_set(attribute, _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS[attribute])\n",
      "\n",
      "\n",
      "def copy_tensor_model_parallel_attributes(destination_tensor, source_tensor):\n",
      "\n",
      "    def maybe_copy(attribute):\n",
      "        if hasattr(source_tensor, attribute):\n",
      "            setattr(destination_tensor, attribute, getattr(source_tensor, attribute))\n",
      "\n",
      "    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\n",
      "        maybe_copy(attribute)\n",
      "\n",
      "\n",
      "def ensure_divisibility(numerator, denominator):\n",
      "    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n",
      "    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n",
      "        numerator, denominator\n",
      "    )\n",
      "\n",
      "\n",
      "def divide(numerator, denominator):\n",
      "    \"\"\"Ensure that numerator is divisible by the denominator and return the\n",
      "    division value.\"\"\"\n",
      "    ensure_divisibility(numerator, denominator)\n",
      "    return numerator // denominator\n",
      "\n",
      "\n",
      "def split_tensor_along_last_dim(\n",
      "    tensor: torch.Tensor,\n",
      "    num_partitions: int,\n",
      "    contiguous_split_chunks: bool = False,\n",
      ") -> List[torch.Tensor]:\n",
      "    \"\"\"Split a tensor along its last dimension.\n",
      "\n",
      "    Arguments:\n",
      "        tensor: input tensor.\n",
      "        num_partitions: number of partitions to split the tensor\n",
      "        contiguous_split_chunks: If True, make each chunk contiguous\n",
      "                                 in memory.\n",
      "\n",
      "    Returns:\n",
      "        A list of Tensors\n",
      "    \"\"\"\n",
      "    # Get the size and dimension.\n",
      "    last_dim = tensor.dim() - 1\n",
      "    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n",
      "    # Split.\n",
      "    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n",
      "    # Note: torch.split does not create contiguous tensors by default.\n",
      "    if contiguous_split_chunks:\n",
      "        return tuple(chunk.contiguous() for chunk in tensor_list)\n",
      "\n",
      "    return tensor_list\n",
      "\n",
      "\n",
      "def split_tensor_into_1d_equal_chunks(tensor, new_buffer=False):\n",
      "    \"\"\"Break a tensor into equal 1D chunks across tensor parallel ranks.\n",
      "\n",
      "    Returns a Tensor or View with this rank's portion of the data.\n",
      "\n",
      "    Arguments:\n",
      "        tensor: The tensor to split\n",
      "\n",
      "    Keyword Arguments:\n",
      "        new_buffer (bool): If True, returns a new Tensor.\n",
      "                           If False, returns a view into the existing Tensor.\n",
      "                           Default is False\n",
      "    \"\"\"\n",
      "    partition_size = torch.numel(tensor) // constants.tensor_parallel_world_size()\n",
      "    start_index = partition_size * constants.tensor_parallel_rank()\n",
      "    end_index = start_index + partition_size\n",
      "    if new_buffer:\n",
      "        data = torch.empty(\n",
      "            partition_size,\n",
      "            dtype=tensor.dtype,\n",
      "            device=constants.current_device(),\n",
      "            requires_grad=False,\n",
      "        )\n",
      "        data.copy_(tensor.view(-1)[start_index:end_index])\n",
      "    else:\n",
      "        data = tensor.view(-1)[start_index:end_index]\n",
      "    return data\n",
      "\n",
      "\n",
      "def gather_split_1d_tensor(tensor):\n",
      "    \"\"\"Opposite of split_tensor_into_1d_equal_chunks. Gather values from tensor\n",
      "    model parallel ranks.\n",
      "\n",
      "    Returns a new Tensor with the gathered data.\n",
      "\n",
      "    Arguments:\n",
      "        tensor: A Tensor or view of this rank's portion of the data.\n",
      "    \"\"\"\n",
      "    numel_gathered = torch.numel(tensor) * constants.tensor_parallel_world_size()\n",
      "    gathered = torch.empty(\n",
      "        numel_gathered,\n",
      "        dtype=tensor.dtype,\n",
      "        device=constants.current_device(),\n",
      "        requires_grad=False,\n",
      "    )\n",
      "    # NOTE: This API is experimental in pytorch (as of Feb 2022) and\n",
      "    # this might break in future pytorch releases. We chose this API\n",
      "    # as opposed to torch.distributed.all_gather for efficiency reasons.\n",
      "    # This API calls directly NCCL all-gather versus the former does\n",
      "    # internal copies and can potentially cause slow down.\n",
      "    torch.distributed._all_gather_base(\n",
      "        gathered, tensor, group=constants.tensor_parallel_group()\n",
      "    )\n",
      "    return gathered\n",
      "\n",
      "\n",
      "class VocabUtility:\n",
      "    \"\"\"Split the vocabulary into `world_size` chunks and return the first\n",
      "    and last index of the vocabulary belonging to the `rank`\n",
      "    partition: Note that indices in [fist, last)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def vocab_range_from_per_partition_vocab_size(\n",
      "        per_partition_vocab_size: int, rank, world_size: int\n",
      "    ) -> Sequence[int]:\n",
      "        index_f = rank * per_partition_vocab_size\n",
      "        index_l = index_f + per_partition_vocab_size\n",
      "        return index_f, index_l\n",
      "\n",
      "    @staticmethod\n",
      "    def vocab_range_from_global_vocab_size(\n",
      "        global_vocab_size: int, rank: int, world_size: int\n",
      "    ) -> Sequence[int]:\n",
      "        per_partition_vocab_size = divide(global_vocab_size, world_size)\n",
      "        return VocabUtility.vocab_range_from_per_partition_vocab_size(\n",
      "            per_partition_vocab_size, rank, world_size\n",
      "        )\n",
      "\n",
      "\n",
      "def assert_viewless_tensor(tensor, extra_msg=None):\n",
      "    \"\"\"Assert that a tensor is not a view (i.e., its '._base' field is not\n",
      "    set).\"\"\"\n",
      "    if isinstance(tensor, list):\n",
      "        [assert_viewless_tensor(t) for t in tensor]\n",
      "        return tensor\n",
      "    if not isinstance(tensor, torch.Tensor):\n",
      "        return tensor\n",
      "    assert tensor._base is None, (\n",
      "        \"Ensure tensor._base is None before setting tensor.data or storing \"\n",
      "        \"tensor to memory buffer. Otherwise, a memory leak will occur (and \"\n",
      "        \"likely accumulate over iterations). %s\"\n",
      "    ) % extra_msg\n",
      "    return tensor\n",
      "\n",
      "\n",
      "def safely_set_viewless_tensor_data(tensor, new_data_tensor):\n",
      "    \"\"\"Safely set tensor's '.data' field.\n",
      "\n",
      "    Check first that the tensor is viewless (i.e., '._base' not set). If\n",
      "    not, raise an exception.\n",
      "    \"\"\"\n",
      "    assert_viewless_tensor(\n",
      "        tensor,\n",
      "        extra_msg=\"FYI, tensor._base has shape %s, and new_data_tensor has shape %s.\"\n",
      "        % (\n",
      "            \"--\" if tensor._base is None else tensor._base.shape,\n",
      "            new_data_tensor.shape,\n",
      "        ),\n",
      "    )\n",
      "    tensor.data = new_data_tensor\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/tensor_parallel/modules.py ====\n",
      "\n",
      "# Modified from Megatron-LM.\n",
      "# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n",
      "\n",
      "import functools\n",
      "import itertools\n",
      "import os\n",
      "import warnings\n",
      "from typing import Callable, List, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.nn.init as init\n",
      "from packaging.version import Version\n",
      "from torch.nn.parameter import Parameter\n",
      "\n",
      "from realhf.base import constants\n",
      "from realhf.impl.model.utils.random import _initialize_affine_weight_gpu\n",
      "\n",
      "from .mappings import (\n",
      "    copy_to_tensor_model_parallel_region,\n",
      "    gather_from_sequence_parallel_region,\n",
      "    gather_from_tensor_model_parallel_region,\n",
      "    reduce_from_tensor_model_parallel_region,\n",
      "    reduce_scatter_to_sequence_parallel_region,\n",
      "    scatter_to_tensor_model_parallel_region,\n",
      ")\n",
      "from .utils import VocabUtility, divide, set_tensor_model_parallel_attributes\n",
      "\n",
      "if Version(Version(torch.__version__).base_version) >= Version(\"2.4\"):\n",
      "    # To disable an annoying FutureWarning\n",
      "    from torch.amp import custom_bwd, custom_fwd\n",
      "\n",
      "    custom_bwd = functools.partial(custom_bwd, device_type=\"cuda\")\n",
      "    custom_fwd = functools.partial(custom_fwd, device_type=\"cuda\")\n",
      "else:\n",
      "    from torch.cuda.amp import custom_bwd, custom_fwd\n",
      "\n",
      "_grad_accum_fusion_available = True\n",
      "try:\n",
      "    import fused_weight_gradient_mlp_cuda\n",
      "except ImportError:\n",
      "    _grad_accum_fusion_available = False\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "\n",
      "logger = logging.getLogger(\"tensor_parallel.modules\")\n",
      "\n",
      "\n",
      "def get_activation_fn(activation_function: str) -> Callable:\n",
      "    if activation_function == \"gelu\":\n",
      "        return nn.functional.gelu\n",
      "    elif activation_function == \"gelu_new\":\n",
      "        from realhf.impl.model.modules.activations import new_gelu_activation\n",
      "\n",
      "        return new_gelu_activation\n",
      "    elif activation_function == \"silu\":\n",
      "        return nn.SiLU()\n",
      "    else:\n",
      "        raise NotImplementedError('Only \"gelu\" activation function is available.')\n",
      "\n",
      "\n",
      "class ParallelEmbedding(torch.nn.Module):\n",
      "    \"\"\"Embedding parallelized in the vocabulary dimension.\n",
      "\n",
      "    This is mainly adapted from torch.nn.Embedding and all the default\n",
      "    values are kept.\n",
      "    Arguments:\n",
      "        num_embeddings: vocabulary size.\n",
      "        embedding_dim: size of hidden state.\n",
      "\n",
      "    Keyword Arguments:\n",
      "        init_method: method to initialize weights.\n",
      "        perform_initialization\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        num_embeddings: int,\n",
      "        embedding_dim: int,\n",
      "        init_method=init.xavier_normal_,\n",
      "        # params_dtype: torch.dtype=torch.float32,\n",
      "        perform_initialization: bool = True,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super(ParallelEmbedding, self).__init__()\n",
      "        # Keep the input dimensions.\n",
      "        self.num_embeddings = num_embeddings\n",
      "        self.embedding_dim = embedding_dim\n",
      "        # Set the detauls for compatibility.\n",
      "        self.padding_idx = None\n",
      "        self.max_norm = None\n",
      "        self.norm_type = 2.0\n",
      "        self.scale_grad_by_freq = False\n",
      "        self.sparse = False\n",
      "        self._weight = None\n",
      "        self.tensor_model_parallel_size = constants.tensor_parallel_world_size()\n",
      "        # Divide the weight matrix along the vocaburaly dimension.\n",
      "        self.vocab_start_index, self.vocab_end_index = (\n",
      "            VocabUtility.vocab_range_from_global_vocab_size(\n",
      "                self.num_embeddings,\n",
      "                constants.tensor_parallel_rank(),\n",
      "                self.tensor_model_parallel_size,\n",
      "            )\n",
      "        )\n",
      "        self.num_embeddings_per_partition = (\n",
      "            self.vocab_end_index - self.vocab_start_index\n",
      "        )\n",
      "\n",
      "        logger.debug(\n",
      "            f\"ParallelEmbedding: num_embeddings={num_embeddings}, per_partition={self.num_embeddings_per_partition}, embedding_dim={embedding_dim},\"\n",
      "            f\"tp_rank={constants.tensor_parallel_rank()},tp_world_size={constants.tensor_parallel_world_size()}\"\n",
      "        )\n",
      "        # Allocate weights and initialize.\n",
      "        self.weight = Parameter(\n",
      "            torch.empty(\n",
      "                self.num_embeddings_per_partition,\n",
      "                self.embedding_dim,\n",
      "                device=device,\n",
      "                dtype=dtype,\n",
      "            )\n",
      "        )\n",
      "        if perform_initialization:\n",
      "            _initialize_affine_weight_gpu(\n",
      "                self.weight, init_method, partition_dim=0, stride=1\n",
      "            )\n",
      "\n",
      "    def forward(self, input_) -> torch.Tensor:\n",
      "        if self.tensor_model_parallel_size > 1:\n",
      "            # Build the mask.\n",
      "            input_mask = (input_ < self.vocab_start_index) | (\n",
      "                input_ >= self.vocab_end_index\n",
      "            )\n",
      "            # Mask the input.\n",
      "            masked_input = input_.clone() - self.vocab_start_index\n",
      "            masked_input[input_mask] = 0\n",
      "        else:\n",
      "            masked_input = input_\n",
      "            # Get the embeddings.\n",
      "\n",
      "        output_parallel = F.embedding(\n",
      "            masked_input,\n",
      "            self.weight,\n",
      "            self.padding_idx,\n",
      "            self.max_norm,\n",
      "            self.norm_type,\n",
      "            self.scale_grad_by_freq,\n",
      "            self.sparse,\n",
      "        )\n",
      "        # Mask the output embedding.\n",
      "        if self.tensor_model_parallel_size > 1:\n",
      "            output_parallel[input_mask, :] = 0.0\n",
      "        # Reduce across all the model parallel GPUs.\n",
      "        output = reduce_from_tensor_model_parallel_region(output_parallel)\n",
      "        return output\n",
      "\n",
      "\n",
      "class LinearWithFrozenWeight(torch.autograd.Function):\n",
      "    \"\"\"Linear operator that does not calculate gradient for weight. This op and\n",
      "    LinearWithGradAccumulationAndAsyncCommunication performs mathematically-\n",
      "    identical forward and DGRAD.\n",
      "\n",
      "    Conceptually this op is the same as torch.nn.functional.linear with\n",
      "    weight.requires_grad==False, but in realhf.experiments they are not\n",
      "    identical mathematically.\n",
      "    \"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    @custom_fwd\n",
      "    def forward(\n",
      "        ctx,\n",
      "        input,\n",
      "        weight,\n",
      "        bias,\n",
      "    ):\n",
      "        ctx.save_for_backward(weight)\n",
      "        output = torch.matmul(input, weight.t())\n",
      "        if bias is not None:\n",
      "            output = output + bias\n",
      "        return output\n",
      "\n",
      "    @staticmethod\n",
      "    @custom_bwd\n",
      "    def backward(ctx, grad_output):\n",
      "        (weight,) = ctx.saved_tensors\n",
      "        grad_input = grad_output.matmul(weight)\n",
      "        return grad_input, None, None\n",
      "\n",
      "\n",
      "def linear_with_frozen_weight(\n",
      "    input: torch.Tensor,\n",
      "    weight: torch.Tensor,\n",
      "    bias: Optional[torch.Tensor],\n",
      "    gradient_accumulation_fusion: bool,\n",
      "    async_grad_allreduce: bool,\n",
      "    sequence_parallel: bool,\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"Linear layer execution with weight.requires_grad == False.\n",
      "\n",
      "    This function handles linear layers with weight frozen (untrainable).\n",
      "    In the forward, it only saves weight and does not save input activations.\n",
      "    In the backward, it does not perform weight gradient calculation, or\n",
      "    weight gradient allreduce.\n",
      "\n",
      "    Arguments:\n",
      "\n",
      "    input (torch.Tensor required): input like torch.nn.functional.linear\n",
      "\n",
      "    weight (torch.Tensor required): weight like torch.nn.functional.linear\n",
      "\n",
      "    bias (torch.Tensor optional): bias like torch.nn.functional.linear\n",
      "\n",
      "    gradient_accumulation_fusion (bool required): dummy argument, used to\n",
      "    keep the API unified between all forward implementation functions.\n",
      "\n",
      "    async_grad_allreduce (bool required): dummy argument, used to\n",
      "    keep the API unified between all forward implementation functions.\n",
      "\n",
      "    sequence_parallel (bool required): Indicates that sequence\n",
      "        parallelism is used and thus in the forward pass the input is\n",
      "        all gathered, and the backward pass the input gradients are\n",
      "        reduce scattered.\n",
      "    \"\"\"\n",
      "\n",
      "    if sequence_parallel:\n",
      "        input = gather_from_sequence_parallel_region(\n",
      "            input, model_parallel_output_grad=True\n",
      "        )\n",
      "    else:\n",
      "        input = input\n",
      "\n",
      "    args = [\n",
      "        input,\n",
      "        weight,\n",
      "        bias,\n",
      "    ]\n",
      "\n",
      "    return LinearWithFrozenWeight.apply(*args)\n",
      "\n",
      "\n",
      "class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n",
      "    \"\"\"See linear_with_grad_accumulation_and_async_allreduce.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    @custom_fwd\n",
      "    def forward(\n",
      "        ctx,\n",
      "        input,\n",
      "        weight,\n",
      "        bias,\n",
      "        gradient_accumulation_fusion,\n",
      "        async_grad_allreduce,\n",
      "        sequence_parallel,\n",
      "    ):\n",
      "        # disable sequence parallel for now for it requires a global buffer\n",
      "        ctx.save_for_backward(input, weight)\n",
      "        ctx.use_bias = bias is not None\n",
      "        ctx.gradient_accumulation_fusion = gradient_accumulation_fusion\n",
      "        ctx.async_grad_allreduce = async_grad_allreduce\n",
      "        ctx.sequence_parallel = sequence_parallel\n",
      "\n",
      "        if sequence_parallel:\n",
      "            assert (\n",
      "                not ctx.async_grad_allreduce\n",
      "            ), \"async_grad_allreduce and sequence_parallel can not be both True\"\n",
      "            world_size = constants.tensor_parallel_world_size()\n",
      "            dim_size = list(input.size())\n",
      "            dim_size[0] = dim_size[0] * world_size\n",
      "\n",
      "            all_gather_buffer = constants.get_global_memory_buffer().get_tensor(\n",
      "                dim_size, input.dtype, \"mpu\"\n",
      "            )\n",
      "            torch.distributed._all_gather_base(\n",
      "                all_gather_buffer, input, group=constants.tensor_parallel_group()\n",
      "            )\n",
      "            total_input = all_gather_buffer\n",
      "        else:\n",
      "            total_input = input\n",
      "\n",
      "        output = torch.matmul(total_input, weight.t())\n",
      "        if bias is not None:\n",
      "            output = output + bias\n",
      "        return output\n",
      "\n",
      "    @staticmethod\n",
      "    @custom_bwd\n",
      "    def backward(ctx, grad_output):\n",
      "        input, weight = ctx.saved_tensors\n",
      "        use_bias = ctx.use_bias\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            world_size = constants.tensor_parallel_world_size()\n",
      "            dim_size = list(input.size())\n",
      "            dim_size[0] = dim_size[0] * world_size\n",
      "\n",
      "            all_gather_buffer = constants.get_global_memory_buffer().get_tensor(\n",
      "                dim_size, input.dtype, \"mpu\"\n",
      "            )\n",
      "            handle = torch.distributed._all_gather_base(\n",
      "                all_gather_buffer,\n",
      "                input,\n",
      "                group=constants.tensor_parallel_group(),\n",
      "                async_op=True,\n",
      "            )\n",
      "\n",
      "            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n",
      "            # gather is scheduled before the input gradient computation\n",
      "            total_input = all_gather_buffer\n",
      "        else:\n",
      "            total_input = input\n",
      "        grad_input = grad_output.matmul(weight)\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            handle.wait()\n",
      "\n",
      "        # Doing gather + slicing during the NeMo forward pass can make this tensor\n",
      "        # not be contiguous. PyTorch only checks if the tensor is contiguous, and only\n",
      "        # clones it if it's not contiguous:\n",
      "        # https://github.com/pytorch/pytorch/blob/c47cf9bc7f9e02f649ab4ed53fe4d35732c92ab6/torch/_refs/__init__.py#L2761\n",
      "        grad_output = grad_output.contiguous()\n",
      "        # Convert the tensor shapes to 2D for execution compatibility\n",
      "        grad_output = grad_output.view(-1, grad_output.shape[-1])\n",
      "        total_input = total_input.view(-1, total_input.shape[-1])\n",
      "\n",
      "        if ctx.async_grad_allreduce:\n",
      "            # Asynchronous all-reduce\n",
      "            handle = torch.distributed.all_reduce(\n",
      "                grad_input,\n",
      "                group=constants.tensor_parallel_group(),\n",
      "                async_op=True,\n",
      "            )\n",
      "            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n",
      "            # all-reduce is scheduled before the weight gradient computation\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            assert not ctx.async_grad_allreduce\n",
      "            dim_size = list(input.size())\n",
      "            sub_grad_input = torch.empty(\n",
      "                dim_size,\n",
      "                dtype=input.dtype,\n",
      "                device=constants.current_device(),\n",
      "                requires_grad=False,\n",
      "            )\n",
      "            # reduce_scatter\n",
      "            handle = torch.distributed._reduce_scatter_base(\n",
      "                sub_grad_input,\n",
      "                grad_input,\n",
      "                group=constants.tensor_parallel_group(),\n",
      "                async_op=True,\n",
      "            )\n",
      "            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n",
      "            # reduce scatter is scheduled before the weight gradient computation\n",
      "\n",
      "        if ctx.gradient_accumulation_fusion:\n",
      "            if weight.main_grad.dtype == torch.float32:\n",
      "                fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32(\n",
      "                    total_input, grad_output, weight.main_grad\n",
      "                )\n",
      "            elif weight.main_grad.dtype in (torch.float16, torch.bfloat16):\n",
      "                fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16(\n",
      "                    total_input, grad_output, weight.main_grad\n",
      "                )\n",
      "            else:\n",
      "                raise RuntimeError(\n",
      "                    \"Unsupported gradient type for gradient accumulation fusion\"\n",
      "                )\n",
      "\n",
      "            if hasattr(weight, \"grad_added_to_main_grad\"):\n",
      "                # When overlap_grad_reduce is True, need to ensure that backward hooks\n",
      "                # are all run on the main backprop thread to prevent deadlocks. Setup\n",
      "                # dummy grad_weight tensor to prevent backward hooks from being run\n",
      "                # in a background thread.\n",
      "                if getattr(weight, \"zero_out_wgrad\", False):\n",
      "                    grad_weight = torch.zeros(\n",
      "                        weight.main_grad.shape,\n",
      "                        dtype=input.dtype,\n",
      "                        device=constants.current_device(),\n",
      "                        requires_grad=False,\n",
      "                    )\n",
      "                else:\n",
      "                    grad_weight = torch.empty(\n",
      "                        weight.main_grad.shape,\n",
      "                        dtype=input.dtype,\n",
      "                        device=constants.current_device(),\n",
      "                        requires_grad=False,\n",
      "                    )\n",
      "                weight.grad_added_to_main_grad = True\n",
      "            else:\n",
      "                grad_weight = None\n",
      "        else:\n",
      "            grad_weight = grad_output.t().matmul(total_input)\n",
      "        grad_bias = grad_output.sum(dim=0) if use_bias else None\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            handle.wait()\n",
      "            return sub_grad_input, grad_weight, grad_bias, None, None, None\n",
      "\n",
      "        if ctx.async_grad_allreduce:\n",
      "            handle.wait()\n",
      "\n",
      "        return grad_input, grad_weight, grad_bias, None, None, None\n",
      "\n",
      "\n",
      "def linear_with_grad_accumulation_and_async_allreduce(\n",
      "    input: torch.Tensor,\n",
      "    weight: torch.Tensor,\n",
      "    bias: Optional[torch.Tensor],\n",
      "    gradient_accumulation_fusion: bool,\n",
      "    async_grad_allreduce: bool,\n",
      "    sequence_parallel: bool,\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"Linear layer execution with asynchronous communication and gradient\n",
      "    accumulation fusion in backprop.\n",
      "\n",
      "    This has the option to accumulate the result of backprop\n",
      "    calculation into an existing gradient buffer, preventing the need\n",
      "    to do an additional addition kernel after the gradient\n",
      "    calculation.\n",
      "\n",
      "    Additionally, the tensor parallel all reduce of the input\n",
      "    gradients can be done asynchronously with the calculation of\n",
      "    the weight gradients.\n",
      "\n",
      "    In the case of sequence parallelism, the reduce scatter of the\n",
      "    input gradients is done asynchronously with the calcluation of the\n",
      "    weight gradients.\n",
      "\n",
      "    Use of this module requires that the environment variable\n",
      "    CUDA_DEVICE_MAX_CONNECTIONS=1. There are a few collective\n",
      "    operations, noted in the code, that should be scheduled before\n",
      "    compute kernels to overlap the communication with the computation,\n",
      "    which is necessary for a speedup but not for correctness so that\n",
      "    ordering isn't imposed by the scheduler. Setting\n",
      "    CUDA_DEVICE_MAX_CONNECTIONS=1 forces the kernels to be scheduled\n",
      "    in the order they are called.\n",
      "\n",
      "    Arguments:\n",
      "\n",
      "    input (torch.Tensor required): input like torch.nn.functional.linear\n",
      "\n",
      "    weight (torch.Tensor required): weight like torch.nn.functional.linear\n",
      "\n",
      "    bias (torch.Tensor optional): bias like torch.nn.functional.linear\n",
      "\n",
      "    gradient_accumulation_fusion (bool required): Perform the gradient\n",
      "        accumulation fusion, requires the custom CUDA extension\n",
      "        fused_weight_gradient_mlp_cuda module. To use\n",
      "        gradient_accumulation_fusion you must install APEX with\n",
      "        --cpp_ext and --cuda_ext. For example: \"pip install\n",
      "        --global-option=\\\"--cpp_ext\\\" --global-option=\\\"--cuda_ext .\\\"\n",
      "        \" Note that the extension requires CUDA>=11. Otherwise, you\n",
      "        must turn off gradient accumulation fusion.\"\n",
      "\n",
      "    async_grad_allreduce (bool required): Do the allreduce of input\n",
      "        gradients asyncronously with the computation of weight\n",
      "        gradients. If sequence_parallel_enabled is True, this must be\n",
      "        False, as no all reduce is performed.\n",
      "\n",
      "    sequence_parallel (bool required): Indicates that sequence\n",
      "        parallelism is used and thus in the forward pass the input is\n",
      "        all gathered, and the backward pass the input gradients are\n",
      "        reduce scattered.\n",
      "    \"\"\"\n",
      "    args = [\n",
      "        input,\n",
      "        weight,\n",
      "        bias,\n",
      "        gradient_accumulation_fusion,\n",
      "        async_grad_allreduce,\n",
      "        sequence_parallel,\n",
      "    ]\n",
      "\n",
      "    if not linear_with_grad_accumulation_and_async_allreduce.warned:\n",
      "        if os.environ.get(\"CUDA_DEVICE_MAX_CONNECTIONS\") != \"1\":\n",
      "            if sequence_parallel:\n",
      "                warnings.warn(\n",
      "                    \"When using sequence parallelism it is recommended to set the \"\n",
      "                    \"environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for \"\n",
      "                    \"maximum speedup\"\n",
      "                )\n",
      "                linear_with_grad_accumulation_and_async_allreduce.warned = True\n",
      "\n",
      "            if async_grad_allreduce:\n",
      "                warnings.warn(\n",
      "                    \"When using async grad allreduce it is recommended to set the \"\n",
      "                    \"environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for \"\n",
      "                    \"maximum speedup\"\n",
      "                )\n",
      "                linear_with_grad_accumulation_and_async_allreduce.warned = True\n",
      "\n",
      "    return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)\n",
      "\n",
      "\n",
      "linear_with_grad_accumulation_and_async_allreduce.warned = False\n",
      "\n",
      "\n",
      "class MergedLinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    @custom_fwd\n",
      "    def forward(\n",
      "        ctx,\n",
      "        input,\n",
      "        gradient_accumulation_fusion,\n",
      "        async_grad_allreduce,\n",
      "        sequence_parallel,\n",
      "        is_w_parallel,\n",
      "        *wbs,\n",
      "    ):\n",
      "        # disable sequence parallel for now for it requires a global buffer\n",
      "        assert len(wbs) % 2 == 0\n",
      "        weights = wbs[::2]\n",
      "        biases = wbs[1::2]\n",
      "        assert len(is_w_parallel) == len(weights)\n",
      "        ctx.save_for_backward(input, *weights)\n",
      "        ctx.use_bias = tuple(b is not None for b in biases)\n",
      "        ctx.is_w_parallel = is_w_parallel\n",
      "\n",
      "        ctx.gradient_accumulation_fusion = gradient_accumulation_fusion\n",
      "        ctx.async_grad_allreduce = async_grad_allreduce\n",
      "        ctx.sequence_parallel = sequence_parallel\n",
      "\n",
      "        if sequence_parallel:\n",
      "            assert (\n",
      "                not ctx.async_grad_allreduce\n",
      "            ), \"async_grad_allreduce and sequence_parallel can not be both True\"\n",
      "            world_size = constants.tensor_parallel_world_size()\n",
      "            dim_size = list(input.size())\n",
      "            dim_size[0] = dim_size[0] * world_size\n",
      "\n",
      "            all_gather_buffer = constants.get_global_memory_buffer().get_tensor(\n",
      "                dim_size, input.dtype, \"mpu\"\n",
      "            )\n",
      "            torch.distributed._all_gather_base(\n",
      "                all_gather_buffer, input, group=constants.tensor_parallel_group()\n",
      "            )\n",
      "            total_input = all_gather_buffer\n",
      "        else:\n",
      "            total_input = input\n",
      "\n",
      "        xs = []\n",
      "        for w, b in zip(weights, biases):\n",
      "            x = torch.matmul(total_input, w.t())\n",
      "            if b is not None:\n",
      "                x = x + b\n",
      "            xs.append(x)\n",
      "        return tuple(xs)\n",
      "\n",
      "    @staticmethod\n",
      "    @custom_bwd\n",
      "    def backward(ctx, *grads):\n",
      "        grads = list(grads)\n",
      "        input, *weights = ctx.saved_tensors\n",
      "        assert len(weights) == len(grads)\n",
      "        use_bias = ctx.use_bias\n",
      "        is_w_parallel = ctx.is_w_parallel\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            world_size = constants.tensor_parallel_world_size()\n",
      "            dim_size = list(input.size())\n",
      "            dim_size[0] = dim_size[0] * world_size\n",
      "\n",
      "            all_gather_buffer = constants.get_global_memory_buffer().get_tensor(\n",
      "                dim_size, input.dtype, \"mpu\"\n",
      "            )\n",
      "            handle = torch.distributed._all_gather_base(\n",
      "                all_gather_buffer,\n",
      "                input,\n",
      "                group=constants.tensor_parallel_group(),\n",
      "                async_op=True,\n",
      "            )\n",
      "\n",
      "            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n",
      "            # gather is scheduled before the input gradient computation\n",
      "            total_input = all_gather_buffer\n",
      "        else:\n",
      "            total_input = input\n",
      "        grad_input = 0\n",
      "        for w, is_parallel, grad in zip(weights, is_w_parallel, grads):\n",
      "            if is_parallel or constants.tensor_parallel_rank() == 0:\n",
      "                grad_input = grad_input + grad.matmul(w)\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            handle.wait()\n",
      "\n",
      "        # Doing gather + slicing during the NeMo forward pass can make this tensor\n",
      "        # not be contiguous. PyTorch only checks if the tensor is contiguous, and only\n",
      "        # clones it if it's not contiguous:\n",
      "        # https://github.com/pytorch/pytorch/blob/c47cf9bc7f9e02f649ab4ed53fe4d35732c92ab6/torch/_refs/__init__.py#L2761\n",
      "        # Convert the tensor shapes to 2D for execution compatibility\n",
      "        for i in range(len(grads)):\n",
      "            grads[i] = grads[i].contiguous().view(-1, grads[i].shape[-1])\n",
      "        total_input = total_input.view(-1, total_input.shape[-1])\n",
      "\n",
      "        if ctx.async_grad_allreduce:\n",
      "            # Asynchronous all-reduce\n",
      "            handle = torch.distributed.all_reduce(\n",
      "                grad_input,\n",
      "                group=constants.tensor_parallel_group(),\n",
      "                async_op=True,\n",
      "            )\n",
      "            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n",
      "            # all-reduce is scheduled before the weight gradient computation\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            assert not ctx.async_grad_allreduce\n",
      "            dim_size = list(input.size())\n",
      "            sub_grad_input = torch.empty(\n",
      "                dim_size,\n",
      "                dtype=input.dtype,\n",
      "                device=constants.current_device(),\n",
      "                requires_grad=False,\n",
      "            )\n",
      "            # reduce_scatter\n",
      "            handle = torch.distributed._reduce_scatter_base(\n",
      "                sub_grad_input,\n",
      "                grad_input,\n",
      "                group=constants.tensor_parallel_group(),\n",
      "                async_op=True,\n",
      "            )\n",
      "            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n",
      "            # reduce scatter is scheduled before the weight gradient computation\n",
      "\n",
      "        if ctx.gradient_accumulation_fusion:\n",
      "            gws = []\n",
      "            for weight, grad_output in zip(weights, grads):\n",
      "                if weight.main_grad.dtype == torch.float32:\n",
      "                    fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32(\n",
      "                        total_input, grad_output, weight.main_grad\n",
      "                    )\n",
      "                elif weight.main_grad.dtype in (torch.float16, torch.bfloat16):\n",
      "                    fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16(\n",
      "                        total_input, grad_output, weight.main_grad\n",
      "                    )\n",
      "                else:\n",
      "                    raise RuntimeError(\n",
      "                        \"Unsupported gradient type for gradient accumulation fusion\"\n",
      "                    )\n",
      "                if hasattr(weight, \"grad_added_to_main_grad\"):\n",
      "                    # When overlap_grad_reduce is True, need to ensure that backward hooks\n",
      "                    # are all run on the main backprop thread to prevent deadlocks. Setup\n",
      "                    # dummy grad_weight tensor to prevent backward hooks from being run\n",
      "                    # in a background thread.\n",
      "                    if getattr(weight, \"zero_out_wgrad\", False):\n",
      "                        grad_weight = torch.zeros(\n",
      "                            weight.main_grad.shape,\n",
      "                            dtype=input.dtype,\n",
      "                            device=constants.current_device(),\n",
      "                            requires_grad=False,\n",
      "                        )\n",
      "                    else:\n",
      "                        grad_weight = torch.empty(\n",
      "                            weight.main_grad.shape,\n",
      "                            dtype=input.dtype,\n",
      "                            device=constants.current_device(),\n",
      "                            requires_grad=False,\n",
      "                        )\n",
      "                    weight.grad_added_to_main_grad = True\n",
      "                else:\n",
      "                    grad_weight = None\n",
      "                gws.append(grad_weight)\n",
      "        else:\n",
      "            gws = []\n",
      "            for w, g in zip(weights, grads):\n",
      "                gws.append(g.t().matmul(total_input))\n",
      "        gbs = [g.sum(dim=0) if use_bias[i] else None for i, g in enumerate(grads)]\n",
      "\n",
      "        if ctx.sequence_parallel:\n",
      "            handle.wait()\n",
      "            return (\n",
      "                sub_grad_input,\n",
      "                None,\n",
      "                None,\n",
      "                None,\n",
      "                None,\n",
      "                *list(itertools.chain.from_iterable(zip(gws, gbs))),\n",
      "            )\n",
      "\n",
      "        if ctx.async_grad_allreduce:\n",
      "            handle.wait()\n",
      "\n",
      "        return (\n",
      "            grad_input,\n",
      "            None,\n",
      "            None,\n",
      "            None,\n",
      "            None,\n",
      "            *list(itertools.chain.from_iterable(zip(gws, gbs))),\n",
      "        )\n",
      "\n",
      "\n",
      "def merged_linear_with_grad_accumulation_and_async_allreduce(\n",
      "    input: torch.Tensor,\n",
      "    gradient_accumulation_fusion: bool,\n",
      "    async_grad_allreduce: bool,\n",
      "    sequence_parallel: bool,\n",
      "    is_w_parallel: List[bool],\n",
      "    *wbs: List[torch.Tensor | None],\n",
      ") -> torch.Tensor:\n",
      "    \"\"\"Similar to linear_with_grad_accumulation_and_async_allreduce but does\n",
      "    multiple linear-layer forward/backward calls with a single all gather\n",
      "    operation.\"\"\"\n",
      "    args = [\n",
      "        input,\n",
      "        gradient_accumulation_fusion,\n",
      "        async_grad_allreduce,\n",
      "        sequence_parallel,\n",
      "        is_w_parallel,\n",
      "        *wbs,\n",
      "    ]\n",
      "\n",
      "    if not merged_linear_with_grad_accumulation_and_async_allreduce.warned:\n",
      "        if os.environ.get(\"CUDA_DEVICE_MAX_CONNECTIONS\") != \"1\":\n",
      "            if sequence_parallel:\n",
      "                warnings.warn(\n",
      "                    \"When using sequence parallelism it is recommended to set the \"\n",
      "                    \"environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for \"\n",
      "                    \"maximum speedup\"\n",
      "                )\n",
      "                merged_linear_with_grad_accumulation_and_async_allreduce.warned = True\n",
      "\n",
      "            if async_grad_allreduce:\n",
      "                warnings.warn(\n",
      "                    \"When using async grad allreduce it is recommended to set the \"\n",
      "                    \"environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for \"\n",
      "                    \"maximum speedup\"\n",
      "                )\n",
      "                merged_linear_with_grad_accumulation_and_async_allreduce.warned = True\n",
      "\n",
      "    return MergedLinearWithGradAccumulationAndAsyncCommunication.apply(*args)\n",
      "\n",
      "\n",
      "merged_linear_with_grad_accumulation_and_async_allreduce.warned = False\n",
      "\n",
      "\n",
      "class ColumnParallelLinear(torch.nn.Module):\n",
      "    \"\"\"Linear layer with column parallelism.\n",
      "\n",
      "    The linear layer is defined as Y = XA + b. A is parallelized along\n",
      "    its second dimension as A = [A_1, ..., A_p].\n",
      "\n",
      "    Arguments:\n",
      "        input_size: first dimension of matrix A.\n",
      "        output_size: second dimension of matrix A.\n",
      "\n",
      "    Keyword Arguments\n",
      "        bias: If true, add bias\n",
      "        gather_output: If true, call all-gather on output and make Y available\n",
      "                       to all GPUs, otherwise, every GPU will have its output\n",
      "                       which is Y_i = XA_i\n",
      "        init_method: method to initialize weights. Note that bias is always set\n",
      "                     to zero.\n",
      "        stride: For the strided linear layers.\n",
      "        keep_master_weight_for_test: This was added for testing and should be\n",
      "                                     set to False. It returns the master weights\n",
      "                                     used for initialization.\n",
      "        skip_bias_add: This was added to enable performance optimations where bias\n",
      "                       can be fused with other elementwise operations. we skip\n",
      "                       adding bias but instead return it.\n",
      "        sequence_parallel: Whether to all_gather input before doing linear\n",
      "        perform_initialization: Whether to perform initialization\n",
      "        gradient_accumulation_fusion: Whether to enable gradient accumulation fusion\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_size,\n",
      "        output_size,\n",
      "        bias=True,\n",
      "        gather_output=False,\n",
      "        init_method=init.xavier_normal_,\n",
      "        stride=1,\n",
      "        skip_bias_add=False,\n",
      "        is_expert=False,\n",
      "        perform_initialization=True,\n",
      "        gradient_accumulation_fusion=False,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super(ColumnParallelLinear, self).__init__()\n",
      "\n",
      "        # Keep input parameters\n",
      "        self.input_size = input_size\n",
      "        self.output_size = output_size\n",
      "        self.gather_output = gather_output\n",
      "        # Divide the weight matrix along the last dimension.\n",
      "        world_size = constants.tensor_parallel_world_size()\n",
      "        self.output_size_per_partition = divide(output_size, world_size)\n",
      "        self.skip_bias_add = skip_bias_add\n",
      "        self.is_expert = is_expert\n",
      "        assert skip_bias_add is False\n",
      "\n",
      "        # Parameters.\n",
      "        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n",
      "        # we allocate the transpose.\n",
      "        # Initialize weight.\n",
      "        # logger.info(\n",
      "        #     f\"ColumnLinear: input_size={input_size}, output_size={output_size}, output_size_per_partition={self.output_size_per_partition}\"\n",
      "        # )\n",
      "        self.weight = Parameter(\n",
      "            torch.empty(\n",
      "                self.output_size_per_partition,\n",
      "                self.input_size,\n",
      "                device=device,\n",
      "                dtype=dtype,\n",
      "            )\n",
      "        )\n",
      "        if perform_initialization:\n",
      "            _initialize_affine_weight_gpu(\n",
      "                self.weight, init_method, partition_dim=0, stride=stride\n",
      "            )\n",
      "\n",
      "        if bias:\n",
      "            self.bias = Parameter(\n",
      "                torch.empty(self.output_size_per_partition, device=device, dtype=dtype)\n",
      "            )\n",
      "            set_tensor_model_parallel_attributes(self.bias, True, 0, stride)\n",
      "            # Always initialize bias to zero.\n",
      "            with torch.no_grad():\n",
      "                self.bias.zero_()\n",
      "        else:\n",
      "            self.register_parameter(\"bias\", None)\n",
      "\n",
      "        if gradient_accumulation_fusion:\n",
      "            if not _grad_accum_fusion_available:\n",
      "                raise RuntimeError(\n",
      "                    \"ColumnParallelLinear was called with gradient_accumulation_fusion set \"\n",
      "                    \"to True but the custom CUDA extension fused_weight_gradient_mlp_cuda \"\n",
      "                    \"module is not found. To use gradient_accumulation_fusion you must \"\n",
      "                    \"install APEX with --cpp_ext and --cuda_ext. For example: \"\n",
      "                    'pip install --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext .\" '\n",
      "                    \"Note that the extension requires CUDA>=11. Otherwise, you must turn off \"\n",
      "                    \"gradient accumulation fusion.\"\n",
      "                )\n",
      "        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n",
      "\n",
      "    def forward(self, input_) -> torch.Tensor:\n",
      "        \"\"\"Forward of ColumnParallelLinear.\n",
      "\n",
      "        Args:\n",
      "            input_: 3D tensor whose order of dimension is [sequence, batch, hidden]\n",
      "\n",
      "        Returns:\n",
      "            - output\n",
      "            - bias\n",
      "        \"\"\"\n",
      "        bias = self.bias if not self.skip_bias_add else None\n",
      "        # NOTE: When sequence_parallel is enabled in MoE models, the gather and scatter of\n",
      "        # sequence parallel are done in MoE token dispatcher before and after permutation.\n",
      "        # Therefore, when used as experts, ColumnParallelLinear and RowParallelLinear\n",
      "        # in expert MLPs always behave as sequence parallel is not enabled.\n",
      "        sequence_parallel = constants.sequence_parallel() and not self.is_expert\n",
      "        async_tensor_model_parallel_allreduce = (\n",
      "            constants.tensor_parallel_world_size() > 1 and not sequence_parallel\n",
      "        )\n",
      "\n",
      "        if sequence_parallel:\n",
      "            input_parallel = input_\n",
      "        else:\n",
      "            input_parallel = copy_to_tensor_model_parallel_region(input_)\n",
      "        # Matrix multiply.\n",
      "        if not self.weight.requires_grad:\n",
      "            forward_impl = linear_with_frozen_weight\n",
      "        else:\n",
      "            forward_impl = linear_with_grad_accumulation_and_async_allreduce\n",
      "        output_parallel = forward_impl(\n",
      "            input=input_parallel,\n",
      "            weight=self.weight,\n",
      "            bias=bias,\n",
      "            gradient_accumulation_fusion=self.gradient_accumulation_fusion,\n",
      "            async_grad_allreduce=async_tensor_model_parallel_allreduce,\n",
      "            sequence_parallel=sequence_parallel,\n",
      "        )\n",
      "        if self.gather_output:\n",
      "            # All-gather across the partitions.\n",
      "            assert not sequence_parallel\n",
      "            output = gather_from_tensor_model_parallel_region(output_parallel)\n",
      "        else:\n",
      "            output = output_parallel\n",
      "        # output_bias = self.bias if self.skip_bias_add else None\n",
      "        return output\n",
      "\n",
      "\n",
      "class RowParallelLinear(torch.nn.Module):\n",
      "    \"\"\"Linear layer with row parallelism.\n",
      "\n",
      "    The linear layer is defined as Y = XA + b. A is parallelized along\n",
      "    its first dimension and X along its second dimension as:\n",
      "               -   -\n",
      "              | A_1 |\n",
      "              | .   |\n",
      "          A = | .   |        X = [X_1, ..., X_p]\n",
      "              | .   |\n",
      "              | A_p |\n",
      "               -   -\n",
      "    Arguments:\n",
      "        input_size: first dimension of matrix A.\n",
      "        output_size: second dimension of matrix A.\n",
      "\n",
      "    Keyword Arguments:\n",
      "        bias: If true, add bias. Note that bias is not parallelized.\n",
      "        input_is_parallel: If true, we assume that the input is already\n",
      "                           split across the GPUs and we do not split\n",
      "                           again.\n",
      "        sequence_parallel: Whether sequence parallel is enabled.\n",
      "        init_method: method to initialize weights. Note that bias is always set\n",
      "                     to zero.\n",
      "        stride: For the strided linear layers.\n",
      "        keep_master_weight_for_test: This was added for testing and should be\n",
      "                                     set to False. It returns the master weights\n",
      "                                     used for initialization.\n",
      "        skip_bias_add: This was added to enable performance optimization where bias\n",
      "                       can be fused with other elementwise operations. We skip\n",
      "                       adding bias but instead return it.\n",
      "        params_dtype:\n",
      "        use_cpu_initialization:\n",
      "        perform_initialization:\n",
      "        gradient_accumulation_fusion:\n",
      "        sequence_parallel_enabled:\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_size,\n",
      "        output_size,\n",
      "        bias=True,\n",
      "        input_is_parallel=True,\n",
      "        init_method=init.xavier_normal_,\n",
      "        stride=1,\n",
      "        skip_bias_add=False,\n",
      "        is_expert=False,\n",
      "        perform_initialization=True,\n",
      "        gradient_accumulation_fusion=False,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "        device: Optional[Union[str, torch.device]] = None,\n",
      "    ):\n",
      "        super(RowParallelLinear, self).__init__()\n",
      "\n",
      "        # Keep input parameters\n",
      "        self.input_size = input_size\n",
      "        self.output_size = output_size\n",
      "        self.input_is_parallel = input_is_parallel\n",
      "        # Divide the weight matrix along the last dimension.\n",
      "        world_size = constants.tensor_parallel_world_size()\n",
      "        self.input_size_per_partition = divide(input_size, world_size)\n",
      "        self.skip_bias_add = skip_bias_add\n",
      "        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n",
      "        self.is_expert = is_expert\n",
      "\n",
      "        # Parameters.\n",
      "        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n",
      "        # we allocate the transpose.\n",
      "        # Initialize weight.\n",
      "        self.weight = Parameter(\n",
      "            torch.empty(\n",
      "                self.output_size,\n",
      "                self.input_size_per_partition,\n",
      "                device=device,\n",
      "                dtype=dtype,\n",
      "            )\n",
      "        )\n",
      "        if perform_initialization:\n",
      "            _initialize_affine_weight_gpu(\n",
      "                self.weight, init_method, partition_dim=1, stride=stride\n",
      "            )\n",
      "        if bias:\n",
      "            self.bias = Parameter(\n",
      "                torch.empty(self.output_size, device=device, dtype=dtype)\n",
      "            )\n",
      "            setattr(self.bias, \"sequence_parallel\", False)\n",
      "\n",
      "            # Always initialize bias to zero.\n",
      "            with torch.no_grad():\n",
      "                self.bias.zero_()\n",
      "        else:\n",
      "            self.register_parameter(\"bias\", None)\n",
      "\n",
      "    def forward(self, input_) -> torch.Tensor:\n",
      "        \"\"\"Forward of RowParallelLinear.\n",
      "\n",
      "        Args:\n",
      "            input_: 3D tensor whose order of dimension is [sequence, batch, hidden]\n",
      "\n",
      "        Returns:\n",
      "            - output\n",
      "            - bias\n",
      "        \"\"\"\n",
      "        # NOTE: ColumnParallelLinear and RowParallelLinear in expert MLPs always behave\n",
      "        # as sequence parallel is not enabled. See ColumnParallelLinear for more details.\n",
      "        sequence_parallel = constants.sequence_parallel() and not self.is_expert\n",
      "        if sequence_parallel and not self.input_is_parallel:\n",
      "            raise RuntimeError(\n",
      "                \"To enable `sequence_parallel`, `input_is_parallel` must be `True`\"\n",
      "            )\n",
      "\n",
      "        # Set up backprop all-reduce.\n",
      "        if self.input_is_parallel:\n",
      "            input_parallel = input_\n",
      "        else:\n",
      "            input_parallel = scatter_to_tensor_model_parallel_region(input_)\n",
      "        # Matrix multiply.\n",
      "        if not self.weight.requires_grad:\n",
      "            _forward_impl = linear_with_frozen_weight\n",
      "        else:\n",
      "            _forward_impl = linear_with_grad_accumulation_and_async_allreduce\n",
      "        output_parallel = _forward_impl(\n",
      "            input=input_parallel,\n",
      "            weight=self.weight,\n",
      "            bias=None,\n",
      "            gradient_accumulation_fusion=self.gradient_accumulation_fusion,\n",
      "            async_grad_allreduce=False,\n",
      "            sequence_parallel=False,  # Here false because we do not need allreduce grad in backward here\n",
      "        )\n",
      "\n",
      "        # All-reduce across all the partitions.\n",
      "        if sequence_parallel:\n",
      "            output_ = reduce_scatter_to_sequence_parallel_region(output_parallel)\n",
      "        else:\n",
      "            output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n",
      "        output = output_ + self.bias if self.bias is not None else output_\n",
      "        return output\n",
      "\n",
      "\n",
      "def parallel_lm_logits(\n",
      "    input_: torch.HalfTensor,\n",
      "    word_embeddings_weight: torch.HalfTensor,\n",
      "    parallel_output: bool = False,\n",
      "    gradient_accumulation_fusion: bool = False,\n",
      "    bias=None,\n",
      "):\n",
      "    \"\"\"LM logits using word embedding weights.\"\"\"\n",
      "    tensor_parallel = constants.tensor_parallel_world_size() > 1\n",
      "    sequence_parallel = constants.sequence_parallel()\n",
      "    async_grad_allreduce = not sequence_parallel and tensor_parallel\n",
      "    # Parallel logits.\n",
      "    if sequence_parallel:\n",
      "        input_parallel = input_\n",
      "    else:\n",
      "        input_parallel = copy_to_tensor_model_parallel_region(input_)\n",
      "        async_grad_allreduce = False\n",
      "\n",
      "    # Matrix multiply.\n",
      "    logits_parallel = linear_with_grad_accumulation_and_async_allreduce(\n",
      "        input=input_parallel,\n",
      "        weight=word_embeddings_weight,\n",
      "        bias=bias,\n",
      "        gradient_accumulation_fusion=gradient_accumulation_fusion,\n",
      "        async_grad_allreduce=async_grad_allreduce,\n",
      "        sequence_parallel=sequence_parallel,\n",
      "    )\n",
      "    # Gather if needed.\n",
      "\n",
      "    if parallel_output:\n",
      "        return logits_parallel\n",
      "\n",
      "    return gather_from_tensor_model_parallel_region(logits_parallel)\n",
      "\n",
      "\n",
      "class _VocabParallelCrossEntropy(torch.autograd.Function):\n",
      "\n",
      "    @staticmethod\n",
      "    def forward(ctx, vocab_parallel_logits, target, label_smoothing=0.0):\n",
      "        # Maximum value along vocab dimension across all GPUs.\n",
      "        logits_max = torch.max(vocab_parallel_logits, dim=-1)[0]\n",
      "        torch.distributed.all_reduce(\n",
      "            logits_max,\n",
      "            op=torch.distributed.ReduceOp.MAX,\n",
      "            group=constants.tensor_parallel_group(),\n",
      "        )\n",
      "        # Subtract the maximum value.\n",
      "        vocab_parallel_logits = vocab_parallel_logits - logits_max.unsqueeze(dim=-1)\n",
      "\n",
      "        # Get the partition's vocab indecies\n",
      "        get_vocab_range = VocabUtility.vocab_range_from_per_partition_vocab_size\n",
      "        partition_vocab_size = vocab_parallel_logits.size()[-1]\n",
      "        rank = constants.tensor_parallel_rank()\n",
      "        world_size = constants.tensor_parallel_world_size()\n",
      "        vocab_start_index, vocab_end_index = get_vocab_range(\n",
      "            partition_vocab_size, rank, world_size\n",
      "        )\n",
      "\n",
      "        # Create a mask of valid vocab ids (1 means it needs to be masked).\n",
      "        target_mask = (target < vocab_start_index) | (target >= vocab_end_index)\n",
      "        masked_target = target.clone() - vocab_start_index\n",
      "        masked_target[target_mask] = 0\n",
      "\n",
      "        # Get predicted-logits = logits[target].\n",
      "        # For Simplicity, we convert logits to a 2-D tensor with size\n",
      "        # [*, partition-vocab-size] and target to a 1-D tensor of size [*].\n",
      "        logits_2d = vocab_parallel_logits.view(-1, partition_vocab_size)\n",
      "        masked_target_1d = masked_target.view(-1)\n",
      "        arange_1d = torch.arange(\n",
      "            start=0, end=logits_2d.size()[0], device=logits_2d.device\n",
      "        )\n",
      "        predicted_logits_1d = logits_2d[arange_1d, masked_target_1d]\n",
      "        predicted_logits_1d = predicted_logits_1d.clone().contiguous()\n",
      "        predicted_logits = predicted_logits_1d.view_as(target)\n",
      "        predicted_logits[target_mask] = 0.0\n",
      "        # All reduce is needed to get the chunks from other GPUs.\n",
      "        torch.distributed.all_reduce(\n",
      "            predicted_logits,\n",
      "            op=torch.distributed.ReduceOp.SUM,\n",
      "            group=constants.tensor_parallel_group(),\n",
      "        )\n",
      "\n",
      "        # Sum of exponential of logits along vocab dimension across all GPUs.\n",
      "        exp_logits = vocab_parallel_logits\n",
      "        torch.exp(vocab_parallel_logits, out=exp_logits)\n",
      "        sum_exp_logits = exp_logits.sum(dim=-1)\n",
      "        torch.distributed.all_reduce(\n",
      "            sum_exp_logits,\n",
      "            op=torch.distributed.ReduceOp.SUM,\n",
      "            group=constants.tensor_parallel_group(),\n",
      "        )\n",
      "\n",
      "        # Loss = log(sum(exp(logits))) - predicted-logit.\n",
      "        loss = torch.log(sum_exp_logits) - predicted_logits\n",
      "\n",
      "        # Normalize and optionally smooth logits\n",
      "        exp_logits.div_(sum_exp_logits.unsqueeze(dim=-1))\n",
      "\n",
      "        vocab_size = exp_logits.size(-1)\n",
      "        if label_smoothing > 0:\n",
      "            \"\"\"\n",
      "            We'd like to assign 1 / (K - 1) probability mass to every index that is not the ground truth.\n",
      "            = (1 - alpha) * y_gt + alpha * mean(y_{i for i != gt})\n",
      "            = (1 - alpha) * y_gt + (alpha / (K - 1)) * \\sum_{i != gt} y_i\n",
      "            = ((K - 1) * (1 - alpha) / (K - 1)) * y_gt + (alpha / (K - 1)) * \\sum_{i != gt} y_i\n",
      "            = (K * (1 - alpha) - 1) / (K - 1)) * y_gt  + (alpha / (K - 1)) * \\sum_{i} y_i\n",
      "            = (1 - (alpha * K) / (K - 1)) * y_gt + ( (alpha * K) / (K - 1) ) * \\sum_{i} y_i / K\n",
      "            From: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/common/losses/smoothed_cross_entropy.py\n",
      "            \"\"\"\n",
      "            assert 1.0 > label_smoothing > 0.0\n",
      "            smoothing = label_smoothing * vocab_size / (vocab_size - 1)\n",
      "\n",
      "            # Exp logits at this point are normalized probabilities. So we can just take the log to get log-probs.\n",
      "            log_probs = torch.log(exp_logits)\n",
      "            mean_log_probs = log_probs.mean(dim=-1)\n",
      "            loss = (1.0 - smoothing) * loss - smoothing * mean_log_probs\n",
      "\n",
      "        ctx.label_smoothing, ctx.vocab_size = label_smoothing, vocab_size\n",
      "\n",
      "        # Store softmax, target-mask and masked-target for backward pass.\n",
      "        ctx.save_for_backward(exp_logits, target_mask, masked_target_1d)\n",
      "\n",
      "        return loss\n",
      "\n",
      "    @staticmethod\n",
      "    def backward(ctx, grad_output):\n",
      "        # Retreive tensors from the forward path.\n",
      "        softmax, target_mask, masked_target_1d = ctx.saved_tensors\n",
      "        label_smoothing, vocab_size = ctx.label_smoothing, ctx.vocab_size\n",
      "\n",
      "        # All the inputs have softmax as thier gradient.\n",
      "        grad_input = softmax\n",
      "        # For simplicity, work with the 2D gradient.\n",
      "        partition_vocab_size = softmax.size()[-1]\n",
      "        grad_2d = grad_input.view(-1, partition_vocab_size)\n",
      "\n",
      "        # Add the gradient from matching classes.\n",
      "        arange_1d = torch.arange(start=0, end=grad_2d.size()[0], device=grad_2d.device)\n",
      "\n",
      "        softmax_update = 1.0 - target_mask.view(-1).float()\n",
      "\n",
      "        if label_smoothing > 0:\n",
      "            smoothing = label_smoothing * vocab_size / (vocab_size - 1)\n",
      "            grad_2d[arange_1d, masked_target_1d] -= (1.0 - smoothing) * softmax_update\n",
      "            average_grad = 1 / vocab_size\n",
      "            grad_2d[arange_1d, :] -= smoothing * average_grad\n",
      "        else:\n",
      "            grad_2d[arange_1d, masked_target_1d] -= softmax_update\n",
      "\n",
      "        # Finally elementwise multiplication with the output gradients.\n",
      "        grad_input.mul_(grad_output.unsqueeze(dim=-1))\n",
      "\n",
      "        return grad_input, None, None\n",
      "\n",
      "\n",
      "def vocab_parallel_cross_entropy(vocab_parallel_logits, target, label_smoothing=0.0):\n",
      "    \"\"\"Performs cross entropy loss when logits are split across tensor parallel\n",
      "    ranks.\n",
      "\n",
      "    Arguments:\n",
      "        vocab_parallel_logits: logits split across tensor parallel ranks\n",
      "                               dimension is [sequence_length, batch_size, hidden_size]\n",
      "\n",
      "        target: correct vocab ids of dimseion [sequence_length, micro_batch_size]\n",
      "\n",
      "        lobal_smoothing: smoothing factor, must be in range [0.0, 1.0)\n",
      "                         default is no smoothing (=0.0)\n",
      "    \"\"\"\n",
      "    return _VocabParallelCrossEntropy.apply(\n",
      "        vocab_parallel_logits, target, label_smoothing\n",
      "    )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/pipeline_parallel/tensor_storage.py ====\n",
      "\n",
      "# Modified from DeepSpeed.\n",
      "# Copyright [2025] Microsoft Corporation\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from collections import defaultdict\n",
      "from typing import Any, List, Optional, Tuple\n",
      "\n",
      "import torch\n",
      "\n",
      "import realhf.base.logging as logging\n",
      "import realhf.impl.model.parallelism.pipeline_parallel.p2p as p2p\n",
      "\n",
      "logger = logging.getLogger(\"tensor_utils\")\n",
      "\n",
      "\n",
      "def get_shape(tensor):\n",
      "    return tensor.shape if torch.is_tensor(tensor) else None\n",
      "\n",
      "\n",
      "def print_data_shapes(name, rank, mbid, x, ys):\n",
      "    if rank == 0:\n",
      "        logger.debug(f\"{name}: rank {rank} mbid {mbid}\")\n",
      "        logger.debug(\n",
      "            f\"shapes: x.pp_input {get_shape(x.pp_input)}, x.pp_output {get_shape(x.pp_output)},\"\n",
      "            f\" x.cu_seqlens {get_shape(x.cu_seqlens)}\"\n",
      "        )\n",
      "        for i, y in enumerate(ys):\n",
      "            logger.debug(\n",
      "                f\"shapes: ys[{i}].input_ids {get_shape(y.packed_input_ids)}, \"\n",
      "                f\"ys[{i}].k_cache {get_shape(y.k_cache)}, ys[{i}].v_cache {get_shape(y.v_cache)}, \"\n",
      "                f\"ys[{i}].cache_seqlens {get_shape(y.cache_seqlens)}\"\n",
      "            )\n",
      "\n",
      "\n",
      "class TensorBuffer:\n",
      "    # could store both tensors and other data\n",
      "\n",
      "    def __init__(self):\n",
      "        self.tensors = defaultdict(dict)\n",
      "\n",
      "    def put(self, name: str, mbid: int, x: torch.Tensor):\n",
      "        self.tensors[name][mbid] = x\n",
      "\n",
      "    def alloc(\n",
      "        self,\n",
      "        name: str,\n",
      "        mbid: int,\n",
      "        shape: Tuple[int],\n",
      "        dtype: torch.dtype,\n",
      "        device: torch.device,\n",
      "        require_grads: bool = False,\n",
      "    ):\n",
      "        self.tensors[name][mbid] = torch.zeros(\n",
      "            shape, dtype=dtype, device=device, requires_grad=require_grads\n",
      "        )\n",
      "        return self.tensors[name][mbid]\n",
      "\n",
      "    def get(\n",
      "        self,\n",
      "        name: str,\n",
      "        mbid: int,\n",
      "        remove: bool = False,\n",
      "        raise_error: bool = True,\n",
      "    ):\n",
      "        try:\n",
      "            if remove:\n",
      "                return self.tensors[name].pop(mbid)\n",
      "            else:\n",
      "                return self.tensors[name][mbid]\n",
      "        except KeyError as e:\n",
      "            if raise_error:\n",
      "                raise e\n",
      "            else:\n",
      "                return None\n",
      "\n",
      "    def remove(self, name: str, mbid: Optional[int] = None, check_exists: bool = False):\n",
      "        try:\n",
      "            if mbid is None:\n",
      "                del self.tensors[name]\n",
      "            else:\n",
      "                self.tensors[name].pop(mbid)\n",
      "        except KeyError:\n",
      "            if not check_exists:\n",
      "                return\n",
      "            raise KeyError(f\"TensorBuffer.remove: key {name} mbid {mbid} not found\")\n",
      "\n",
      "    def check_name(self, name: str):\n",
      "        return name in self.tensors\n",
      "\n",
      "    def check_mbid(self, name: str, mbid: int):\n",
      "        if name not in self.tensors:\n",
      "            return False\n",
      "        return mbid in self.tensors[name]\n",
      "\n",
      "    def clear(self):\n",
      "        self.tensors = defaultdict(dict)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/pipeline_parallel/p2p.py ====\n",
      "\n",
      "# Modified from DeepSpeed.\n",
      "# Copyright [2025] Microsoft Corporation\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "from packaging.version import Version\n",
      "\n",
      "import realhf.base.constants as constants\n",
      "\n",
      "\n",
      "def can_send_recv() -> bool:\n",
      "    # torch_version = Version(torch_info[\"version\"])\n",
      "    torch_version = Version(torch.__version__)\n",
      "    sendrecv_min = Version(\"1.8\")\n",
      "    return torch_version >= sendrecv_min\n",
      "\n",
      "\n",
      "assert can_send_recv()\n",
      "\n",
      "\n",
      "def _is_valid_send_recv(src_stage, dest_stage):\n",
      "    first_stage = 0\n",
      "    last_stage = constants.grid().pipe_parallel_size - 1\n",
      "    assert (\n",
      "        abs(src_stage - dest_stage) == 1\n",
      "        or (src_stage == first_stage and dest_stage == last_stage)\n",
      "        or (src_stage == last_stage and dest_stage == first_stage)\n",
      "    ), f\"Functionality currently limited to send and receive between adjacent ranks only (src={src_stage}, dst={dest_stage})\"\n",
      "\n",
      "\n",
      "def send(tensor, dest_stage, async_op=False):\n",
      "    # NOTE: The input is the stage id rather than the global rank\n",
      "    src_stage = constants.grid().get_stage_id()\n",
      "    _is_valid_send_recv(src_stage, dest_stage)\n",
      "\n",
      "    dest_rank = constants.grid().stage_to_global(stage_id=dest_stage)\n",
      "    send_method = dist.isend if async_op else dist.send\n",
      "    return send_method(tensor, constants.to_global_pg_rank(dest_rank))\n",
      "\n",
      "\n",
      "def recv(tensor, src_stage, async_op=False):\n",
      "    # NOTE: The input is the stage id rather than the global rank\n",
      "    dest_stage = constants.grid().get_stage_id()\n",
      "    _is_valid_send_recv(src_stage, dest_stage)\n",
      "\n",
      "    src_rank = constants.grid().stage_to_global(stage_id=src_stage)\n",
      "    recv_method = dist.irecv if async_op else dist.recv\n",
      "    return recv_method(tensor, constants.to_global_pg_rank(src_rank))\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/pipeline_parallel/instruction.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import sys\n",
      "from typing import *\n",
      "\n",
      "\n",
      "class PipeInstruction:\n",
      "    \"\"\"Base class for all instructions to be executed by the pipeline engine.\n",
      "\n",
      "    All keyword arguments are stored as members similar to a ``namedtuple``. These are\n",
      "    then accessible to the :class:`PipeEngine` during execution.\n",
      "\n",
      "    Each instruction has following attributes:\n",
      "    1. stage_id: pipeline stage that the instruction should be executed by.\n",
      "    2. micro_batch_id: the ID of data micro batch that this instruction should be executed on.\n",
      "    3. step_id: usually used by generation schedules, identical to generation token id.\n",
      "    4. priority: priority of the instruction, higher priority instructions should be scheduled first in\n",
      "                 the dynamic schedule.\n",
      "    5. deps: list of instructions that this instruction depends on.\n",
      "    6. bind: Instruction that this instruction is binded with. Binded instructions are send/recv instructions\n",
      "             that should appear in pair, belong to different (adjancent) stages and have the same micro_batch_id.\n",
      "             Two binded instructions should have the same dependency list. This feature is used to avoid NCCL\n",
      "             communication deadlock.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        stage_id: int,\n",
      "        micro_batch_id: int,\n",
      "        deps: List[\"PipeInstruction\"] = [],\n",
      "        bind: List[\"PipeInstruction\"] = [],\n",
      "        step_id: int = 0,\n",
      "        schedule_id: int = 0,\n",
      "    ):\n",
      "        self.stage_id = stage_id\n",
      "        self.micro_batch_id = micro_batch_id\n",
      "        self.deps = deps\n",
      "        self.bind = bind\n",
      "        self.name = self.__class__.__name__\n",
      "        self.step_id = step_id\n",
      "        self.args = (stage_id, micro_batch_id, step_id)\n",
      "        self.__str_encode = (\n",
      "            f\"{self.name};{self.stage_id};{self.micro_batch_id};{self.step_id}\"\n",
      "        )\n",
      "        self.schedule_id = schedule_id\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.name}{self.args}\"\n",
      "        # return call_to_str(self.name, self.args, self.kwargs)\n",
      "\n",
      "    def __eq__(self, other: \"PipeInstruction\"):\n",
      "        return (\n",
      "            self.stage_id == other.stage_id\n",
      "            and self.micro_batch_id == other.micro_batch_id\n",
      "            and self.step_id == other.step_id\n",
      "            and self.name == other.name\n",
      "        )\n",
      "\n",
      "    def __lt__(self, other: \"PipeInstruction\"):\n",
      "        # order by stage_id, micro_batch_id, step_id\n",
      "        # used to sort finded results in InstructionSet\n",
      "        return (\n",
      "            self.stage_id < other.stage_id\n",
      "            or (\n",
      "                self.stage_id == other.stage_id\n",
      "                and self.micro_batch_id < other.micro_batch_id\n",
      "            )\n",
      "            or (\n",
      "                self.stage_id == other.stage_id\n",
      "                and self.micro_batch_id == other.micro_batch_id\n",
      "                and self.step_id < other.step_id\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def encode_str(self) -> str:\n",
      "        return self.__str_encode\n",
      "\n",
      "    def encode(self):\n",
      "        return self.encode_str().encode(encoding=\"utf-8\")\n",
      "\n",
      "    @classmethod\n",
      "    def decode(cls, encoded: Union[bytes, str]) -> \"PipeInstruction\":\n",
      "        if isinstance(encoded, bytes):\n",
      "            s = encoded.decode(encoding=\"utf-8\")\n",
      "        else:\n",
      "            s = encoded\n",
      "        cls_name, stage_id, micro_batch_id, step_id = s.split(\";\")\n",
      "        cls_ = getattr(sys.modules[__name__], cls_name)\n",
      "        return cls_(\n",
      "            stage_id=int(stage_id),\n",
      "            micro_batch_id=int(micro_batch_id),\n",
      "            step_id=int(step_id),\n",
      "        )\n",
      "\n",
      "\n",
      "def decode_stage_by_encoded(s: str):\n",
      "    return int(s.split(\";\")[1])\n",
      "\n",
      "\n",
      "class OptimizerStep(PipeInstruction):\n",
      "    \"\"\"Performs one step with the optimizer and zeros gradients.\n",
      "\n",
      "    .. note:: Should be issued after :class:`ReduceGrads` and :class:`ReduceTiedGrads`.\n",
      "\n",
      "    .. note:: Can be a synchronization point among data-parallel ranks.\n",
      "    \"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class ReduceGrads(PipeInstruction):\n",
      "    \"\"\"Reduce the computed gradients among data-parallel processes within the\n",
      "    stage.\"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "# Compute\n",
      "class ForwardPass(PipeInstruction):\n",
      "    \"\"\"Compute a forward pass.\"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class BackwardPass(PipeInstruction):\n",
      "    \"\"\"Compute a backward pass and accumulate gradients.\"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "# Communication\n",
      "class SendActivation(PipeInstruction):\n",
      "    \"\"\"Send activations to the next stage in the pipeline.\"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class RecvActivation(PipeInstruction):\n",
      "    \"\"\"Receive activations from the previous stage in the pipeline.\"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class SendGrad(PipeInstruction):\n",
      "    \"\"\"Send computed gradients to the previous pipeline stage.\n",
      "\n",
      "    with respect to the received activations\n",
      "    \"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class RecvGrad(PipeInstruction):\n",
      "    \"\"\"Receive computed gradients the next pipeline stage.\"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "# generation\n",
      "class SendNextTokens(PipeInstruction):\n",
      "    \"\"\"In GenerateSchedule, send next tokens to the first stage.\n",
      "\n",
      "    Only available in the last stage.\n",
      "    \"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class RecvNextTokens(PipeInstruction):\n",
      "    \"\"\"In GenerateSchedule, recv next tokens from the last stage.\n",
      "\n",
      "    Only available in the first stage.\n",
      "    \"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/parallelism/pipeline_parallel/static_schedule.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "from realhf.impl.model.parallelism.pipeline_parallel.instruction import (\n",
      "    BackwardPass,\n",
      "    ForwardPass,\n",
      "    OptimizerStep,\n",
      "    RecvActivation,\n",
      "    RecvGrad,\n",
      "    RecvNextTokens,\n",
      "    ReduceGrads,\n",
      "    SendActivation,\n",
      "    SendGrad,\n",
      "    SendNextTokens,\n",
      ")\n",
      "\n",
      "\n",
      "class PipeSchedule(ABC):\n",
      "    \"\"\"Directs the execution of a pipeline engine by generating sequences of\n",
      "    :class:`PipeInstruction`.\n",
      "\n",
      "    Schedules are generators that yield sequences of\n",
      "    :class:`PipeInstruction` to process the micro-batches in one batch.\n",
      "    Each yielded step is atomic in the sense that a barrier\n",
      "    synchronization can be placed between successive steps without\n",
      "    deadlock.\n",
      "\n",
      "    Below is an example schedule that implements data parallelism with gradient accumulation:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        class DataParallelSchedule(PipeSchedule):\n",
      "            def steps(self):\n",
      "                for step_id in range(self.micro_batches):\n",
      "                    cmds = [\n",
      "                        LoadMicroBatch(buffer_id=0),\n",
      "                        ForwardPass(buffer_id=0),\n",
      "                        BackwardPass(buffer_id=0),\n",
      "                    ]\n",
      "                    if step_id == self.micro_batches - 1:\n",
      "                        cmds.extend([\n",
      "                            ReduceGrads(),\n",
      "                            OptimizerStep(),\n",
      "                        ])\n",
      "                    yield cmds\n",
      "\n",
      "            def num_pipe_buffers(self):\n",
      "                return 1\n",
      "\n",
      "    Args:\n",
      "        micro_batches (int): The number of micro-batches that comprise a batch.\n",
      "        stages (int): The number of pipeline stages.\n",
      "        stage_id (int): The pipe stage that will execute the generated schedule.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, micro_batches, stages, stage_id):\n",
      "        super().__init__()\n",
      "        self.micro_batches = micro_batches\n",
      "        self.stages = stages\n",
      "        self.stage_id = stage_id\n",
      "        self.prev_stage = self.stage_id - 1\n",
      "        self.next_stage = self.stage_id + 1\n",
      "\n",
      "        self.terminate_hooks = []\n",
      "        self.current_micro_batch_id = 0\n",
      "\n",
      "    def terminate(self):\n",
      "        return self.terminate_hooks\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"{self.__class__.__name__}(micro_batches={self.micro_batches}, stages={self.stages}, stage_id={self.stage_id})\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def steps(self):\n",
      "        \"\"\"Yield a list of :class:`PipeInstruction` for each step in the\n",
      "        schedule.\n",
      "\n",
      "        .. note::\n",
      "            Schedules must implement ``steps()`` to define the schedule.\n",
      "\n",
      "        Returns:\n",
      "            Instructions to be executed as one step of the pipeline\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def num_pipe_buffers(self):\n",
      "        \"\"\"The number of pipeline buffers that will be used by this stage.\n",
      "\n",
      "        .. note::\n",
      "            Schedules should specialize ``num_pipe_buffers()`` for memory savings at scale.\n",
      "\n",
      "        Returns:\n",
      "            The number of buffers for the engine to allocate.\n",
      "        \"\"\"\n",
      "        return self.micro_batches\n",
      "\n",
      "    def _valid_micro_batch(self, micro_batch_id):\n",
      "        return 0 <= micro_batch_id < self.micro_batches\n",
      "\n",
      "    def _valid_stage(self, stage_id):\n",
      "        return 0 <= stage_id < self.stages\n",
      "\n",
      "    @property\n",
      "    def stage(self):\n",
      "        \"\"\"Stage index used to configure this schedule.\"\"\"\n",
      "        return self.stage_id\n",
      "\n",
      "    @property\n",
      "    def num_stages(self):\n",
      "        \"\"\"The number of total pipeline stages used to configure this\n",
      "        schedule.\"\"\"\n",
      "        return self.stages\n",
      "\n",
      "    @property\n",
      "    def n_pp_mbs(self):\n",
      "        \"\"\"The number of total micro_batches used to configure this\n",
      "        schedule.\"\"\"\n",
      "        return self.micro_batches\n",
      "\n",
      "    @property\n",
      "    def is_first_stage(self):\n",
      "        \"\"\"True if the configured ``stage_id`` is the first stage in the\n",
      "        pipeline.\"\"\"\n",
      "        return self.stage_id == 0\n",
      "\n",
      "    @property\n",
      "    def is_last_stage(self):\n",
      "        \"\"\"True if the configured ``stage_id`` is the last stage in the\n",
      "        pipeline.\"\"\"\n",
      "        return self.stage_id == self.stages - 1\n",
      "\n",
      "    def _buffer_idx(self, micro_batch_id):\n",
      "        \"\"\"Map a micro-batch index to a pipeline buffer index.\n",
      "\n",
      "        This method uses a cyclic allocation strategy.\n",
      "\n",
      "        Args:\n",
      "            micro_batch_id (int): The micro-batch index relative to the beginning of the schedule.\n",
      "\n",
      "        Returns:\n",
      "            int: The index of the buffer that should store data.\n",
      "        \"\"\"\n",
      "        assert self._valid_micro_batch(micro_batch_id)\n",
      "        return micro_batch_id % self.num_pipe_buffers()\n",
      "\n",
      "    def __iter__(self):\n",
      "        self.it = None\n",
      "        return self\n",
      "\n",
      "    def __next__(self):\n",
      "        if self.it is None:\n",
      "            self.it = self.steps()\n",
      "        return next(self.it)\n",
      "\n",
      "\n",
      "class InferenceSchedule(PipeSchedule):\n",
      "    \"\"\"A schedule for inferencing batches using pipeline parallelism.\"\"\"\n",
      "\n",
      "    def steps(self):\n",
      "        \"\"\"\"\"\"\n",
      "        total_steps = self.micro_batches + self.stages - 1\n",
      "        for step_id in range(total_steps):\n",
      "            cmds = []\n",
      "            micro_batch_id = step_id - self.stage_id\n",
      "\n",
      "            if _is_even(self.stage_id):\n",
      "                if self._valid_stage(self.next_stage):\n",
      "                    if self._valid_micro_batch(micro_batch_id - 1):\n",
      "                        cmds.append(SendActivation(self.stage_id, micro_batch_id - 1))\n",
      "                if self._valid_stage(self.prev_stage):\n",
      "                    if self._valid_micro_batch(micro_batch_id):\n",
      "                        cmds.append(RecvActivation(self.stage_id, micro_batch_id))\n",
      "            else:\n",
      "                if self._valid_stage(self.prev_stage):\n",
      "                    if self._valid_micro_batch(micro_batch_id):\n",
      "                        cmds.append(RecvActivation(self.stage_id, micro_batch_id))\n",
      "\n",
      "                if self._valid_stage(self.next_stage):\n",
      "                    if self._valid_micro_batch(micro_batch_id - 1):\n",
      "                        cmds.append(SendActivation(self.stage_id, micro_batch_id - 1))\n",
      "\n",
      "            if self._valid_micro_batch(micro_batch_id):\n",
      "                cmds.append(ForwardPass(self.stage_id, micro_batch_id))\n",
      "\n",
      "            yield step_id, micro_batch_id, cmds\n",
      "\n",
      "    def num_pipe_buffers(self):\n",
      "        \"\"\"Only two pipeline buffers are required for inferencing.\n",
      "\n",
      "        Returns:\n",
      "            ``2``\n",
      "        \"\"\"\n",
      "        return 2\n",
      "\n",
      "\n",
      "class GenerateSchedule(PipeSchedule):\n",
      "    \"\"\"A schedule for generate.\n",
      "\n",
      "    Difference between this schedule and InferenceSchedule is that last\n",
      "    stage will not load data, and the last stage will send the result to\n",
      "    the first stage for the next generation round.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, micro_batches, stages, stage_id, max_new_tokens):\n",
      "        super().__init__(micro_batches, stages, stage_id)\n",
      "        self.prev_stage = self.prev_stage % self.stages\n",
      "        self.next_stage = self.next_stage % self.stages\n",
      "        self.max_new_tokens = max_new_tokens\n",
      "        self.max_steps = (\n",
      "            max_new_tokens * max(self.n_pp_mbs, self.stages) + self.n_pp_mbs - 1\n",
      "        )  # a configurable upper bound\n",
      "\n",
      "    def _valid_token_id(self, token_id):\n",
      "        return token_id < self.max_new_tokens\n",
      "\n",
      "    def steps(self):\n",
      "        last_micro_batch_id = -1\n",
      "        last_token_id = -1\n",
      "        for step_id in range(self.max_steps):\n",
      "            cmds = []\n",
      "            micro_batch_id = (\n",
      "                (step_id - self.stage_id) % max(self.n_pp_mbs, self.stages)\n",
      "                if step_id - self.stage_id >= 0\n",
      "                else -1\n",
      "            )  # micro batch id for current stage\n",
      "            first_round = (\n",
      "                step_id < self.n_pp_mbs\n",
      "            )  # whether it is the first round of generate\n",
      "            last_stage_last_mbid = (\n",
      "                (step_id - self.stages) % max(self.n_pp_mbs, self.stages)\n",
      "                if step_id >= self.stages\n",
      "                else -1\n",
      "            )\n",
      "            # the micro_batch_id of the last stage on last step\n",
      "            token_id = (step_id - self.stage_id) // max(self.n_pp_mbs, self.stages)\n",
      "            # token id in current round\n",
      "\n",
      "            # TODO: from last stage to first stage, need one buffer for each microbatch?\n",
      "            if _is_even(self.stage_id):\n",
      "                if (\n",
      "                    self._valid_micro_batch(last_micro_batch_id)\n",
      "                    and self._valid_token_id(last_token_id)\n",
      "                    and not self.is_last_stage\n",
      "                ):\n",
      "                    cmds.append(\n",
      "                        SendActivation(\n",
      "                            self.stage_id, last_micro_batch_id, step_id=token_id\n",
      "                        )\n",
      "                    )\n",
      "                # intermediate stage recv\n",
      "                if (\n",
      "                    self._valid_micro_batch(micro_batch_id)\n",
      "                    and self._valid_token_id(token_id)\n",
      "                    and not self.is_first_stage\n",
      "                ):\n",
      "                    cmds.append(\n",
      "                        RecvActivation(self.stage_id, micro_batch_id, step_id=token_id)\n",
      "                    )\n",
      "            else:\n",
      "                # odd stage could not be first stage\n",
      "                if self._valid_micro_batch(micro_batch_id) and self._valid_token_id(\n",
      "                    token_id\n",
      "                ):\n",
      "                    cmds.append(\n",
      "                        RecvActivation(self.stage_id, micro_batch_id, step_id=token_id)\n",
      "                    )\n",
      "                # last stage should not send activation except first stage requires\n",
      "                if (\n",
      "                    self._valid_micro_batch(last_micro_batch_id)\n",
      "                    and self._valid_token_id(last_token_id)\n",
      "                    and not self.is_last_stage\n",
      "                ):\n",
      "                    cmds.append(\n",
      "                        SendActivation(\n",
      "                            self.stage_id, last_micro_batch_id, step_id=token_id\n",
      "                        )\n",
      "                    )\n",
      "\n",
      "            # last stage send next tokens when first stage requires.\n",
      "            if (\n",
      "                self.is_last_stage\n",
      "                and self._valid_micro_batch(last_micro_batch_id)\n",
      "                and self._valid_token_id(last_token_id)\n",
      "            ):\n",
      "                cmds.append(\n",
      "                    SendNextTokens(\n",
      "                        self.stage_id,\n",
      "                        last_micro_batch_id,\n",
      "                        step_id=last_token_id,\n",
      "                    )\n",
      "                )\n",
      "            if self.is_first_stage and self._valid_micro_batch(last_stage_last_mbid):\n",
      "                cmds.append(\n",
      "                    RecvNextTokens(\n",
      "                        self.stage_id, last_stage_last_mbid, step_id=token_id\n",
      "                    )\n",
      "                )\n",
      "\n",
      "            if self._valid_micro_batch(micro_batch_id) and self._valid_token_id(\n",
      "                token_id\n",
      "            ):\n",
      "                cmds.append(\n",
      "                    ForwardPass(self.stage_id, micro_batch_id, step_id=token_id)\n",
      "                )\n",
      "\n",
      "            last_micro_batch_id = micro_batch_id\n",
      "            last_token_id = token_id\n",
      "            yield step_id, micro_batch_id, cmds\n",
      "\n",
      "    def num_pipe_buffers(self):\n",
      "        \"\"\"2 buffers for inter stage transfer (except last stage to first\n",
      "        stage) self.n_pp_mbs buffers for last stage to first stage transfer.\n",
      "\n",
      "        Returns:\n",
      "            ``2 + self.n_pp_mbs``\n",
      "        \"\"\"\n",
      "        return 2  # + self.n_pp_mbs\n",
      "\n",
      "\n",
      "class TrainSchedule(PipeSchedule):\n",
      "    \"\"\"A schedule for training a batch using hybrid parallelism.\n",
      "\n",
      "    Pipeline parallelism is extracted through gradient accumulation and\n",
      "    thus convergence follows that of a data parallel approach with the\n",
      "    same batch size.\n",
      "    \"\"\"\n",
      "\n",
      "    def steps(self):\n",
      "        \"\"\"\"\"\"\n",
      "        prev_micro_batch_id = -1\n",
      "        total_steps = 2 * (self.micro_batches + self.stages - 1)\n",
      "        for step_id in range(total_steps):\n",
      "            # Map the step of the pipeline to the micro-batch id and also whether it is a\n",
      "            # forward or backward pass step.\n",
      "            micro_batch_id, is_forward = self._step_to_micro_batch(step_id)\n",
      "            cmds = []\n",
      "\n",
      "            # Exchange activations\n",
      "            if is_forward:\n",
      "                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(\n",
      "                    self.prev_stage\n",
      "                ):\n",
      "                    cmds.append(SendGrad(self.stage_id, prev_micro_batch_id, step_id=0))\n",
      "                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(\n",
      "                    self.prev_stage\n",
      "                ):\n",
      "                    cmds.append(\n",
      "                        RecvActivation(self.stage_id, micro_batch_id, step_id=0)\n",
      "                    )\n",
      "            else:\n",
      "                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(\n",
      "                    self.next_stage\n",
      "                ):\n",
      "                    cmds.append(RecvGrad(self.stage_id, micro_batch_id, step_id=0))\n",
      "                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(\n",
      "                    self.next_stage\n",
      "                ):\n",
      "                    cmds.append(\n",
      "                        SendActivation(self.stage_id, prev_micro_batch_id, step_id=0)\n",
      "                    )\n",
      "\n",
      "            # Computation\n",
      "            if self._valid_micro_batch(micro_batch_id):\n",
      "                if is_forward:\n",
      "                    cmds.append(ForwardPass(self.stage_id, micro_batch_id, step_id=0))\n",
      "                else:\n",
      "                    cmds.append(BackwardPass(self.stage_id, micro_batch_id, step_id=0))\n",
      "\n",
      "            # Model step at the end of the batch\n",
      "            if step_id == total_steps - 1:\n",
      "                cmds.append(ReduceGrads(self.stage_id, micro_batch_id, step_id=0))\n",
      "                cmds.append(OptimizerStep(self.stage_id, micro_batch_id, step_id=0))\n",
      "\n",
      "            # Prepare state for next time\n",
      "            prev_micro_batch_id = micro_batch_id\n",
      "            yield step_id, micro_batch_id, cmds\n",
      "\n",
      "    def num_pipe_buffers(self):\n",
      "        \"\"\"Return the number of pipeline buffers required for this stage.\n",
      "\n",
      "        This is equivalent to the maximum number of in-flight forward\n",
      "        passes, since we need to remember the activations of forward\n",
      "        passes in order to run backpropagation. For synchronous 1F1B,\n",
      "        this is equivalent to the index difference between this stage\n",
      "        and the last stage.\n",
      "        \"\"\"\n",
      "        buffers = min(self.stages - self.stage_id, self.micro_batches)\n",
      "        return max(2, buffers)\n",
      "\n",
      "    def _step_to_micro_batch(self, step_id):\n",
      "        if _is_even(step_id) and _is_even(self.stage_id):\n",
      "            micro_batch_id = self._even_step_forward_id(step_id)\n",
      "            is_forward = True\n",
      "\n",
      "        elif _is_odd(step_id) and _is_odd(self.stage_id):\n",
      "            micro_batch_id = self._odd_step_forward_id(step_id)\n",
      "            is_forward = True\n",
      "\n",
      "        elif _is_even(step_id) and _is_odd(self.stage_id):\n",
      "            micro_batch_id = self._even_step_backward_id(step_id)\n",
      "            is_forward = False\n",
      "\n",
      "        elif _is_odd(step_id) and _is_even(self.stage_id):\n",
      "            micro_batch_id = self._odd_step_backward_id(step_id)\n",
      "            is_forward = False\n",
      "\n",
      "        else:\n",
      "            assert False\n",
      "\n",
      "        return micro_batch_id, is_forward\n",
      "\n",
      "    def _even_step_forward_id(self, step_id):\n",
      "        base = step_id // 2\n",
      "        micro_batch_id = int(base - self.stage_id // 2)\n",
      "        return micro_batch_id\n",
      "\n",
      "    def _odd_step_forward_id(self, step_id):\n",
      "        base = (step_id - 1) // 2\n",
      "        micro_batch_id = int(base - self.stage_id // 2)\n",
      "        return micro_batch_id\n",
      "\n",
      "    def _even_step_backward_id(self, step_id):\n",
      "        base = step_id // 2\n",
      "        micro_batch_id = int(base - self.stages + (self.stage_id + 1) // 2)\n",
      "        return micro_batch_id\n",
      "\n",
      "    def _odd_step_backward_id(self, step_id):\n",
      "        base = ((step_id - 1) // 2) - self.stages + 1\n",
      "        micro_batch_id = int(base + self.stage_id // 2)\n",
      "        return micro_batch_id\n",
      "\n",
      "\n",
      "class DataParallelSchedule(PipeSchedule):\n",
      "    \"\"\"An example schedule that trains using traditional data parallelism with\n",
      "    gradient accumulation.\"\"\"\n",
      "\n",
      "    def steps(self):\n",
      "        \"\"\"\"\"\"\n",
      "        for step_id in range(self.micro_batches):\n",
      "            cmds = [\n",
      "                ForwardPass(self.stage_id, step_id, step_id=0),\n",
      "                BackwardPass(self.stage_id, step_id, step_id=0),\n",
      "            ]\n",
      "            if step_id == self.micro_batches - 1:\n",
      "                cmds.extend(\n",
      "                    [\n",
      "                        ReduceGrads(self.stage_id, step_id, step_id=0),\n",
      "                        OptimizerStep(self.stage_id, step_id, step_id=0),\n",
      "                    ]\n",
      "                )\n",
      "            yield cmds\n",
      "\n",
      "    def num_pipe_buffers(self):\n",
      "        \"\"\"Only one pipeline buffer needed.\"\"\"\n",
      "        return 1\n",
      "\n",
      "\n",
      "def _is_even(x):\n",
      "    return x % 2 == 0\n",
      "\n",
      "\n",
      "def _is_odd(x):\n",
      "    return x % 2 != 0\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/model/conversion/hf_registry.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "# Copyright 2024 Wei Fu & Zhiyu Mei\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      "\n",
      "import dataclasses\n",
      "import json\n",
      "import os\n",
      "import time\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import *\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import transformers\n",
      "\n",
      "from realhf.api.core import model_api\n",
      "from realhf.base import constants, logging\n",
      "from realhf.base.saveload_utils import (\n",
      "    copy_hf_configs,\n",
      "    load_safetensor,\n",
      "    split_state_dict_into_shards,\n",
      ")\n",
      "from realhf.impl.model.nn.real_llm_api import ReaLModel\n",
      "from realhf.impl.model.nn.real_llm_parallel import (\n",
      "    tp_merge_key,\n",
      "    tp_partition_real_model_state_dict,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"HF Registry\")\n",
      "\n",
      "\n",
      "@dataclasses.dataclass\n",
      "class HFModelRegistry:\n",
      "    name: str\n",
      "    hf_cls_name: str\n",
      "    config_from_hf_converter: Callable[\n",
      "        [transformers.PretrainedConfig], model_api.ReaLModelConfig\n",
      "    ]\n",
      "    config_to_hf_converter: Callable[\n",
      "        [model_api.ReaLModelConfig], transformers.PretrainedConfig\n",
      "    ]\n",
      "    sd_from_hf_converter: Callable[[Dict, model_api.ReaLModelConfig], Dict]\n",
      "    sd_to_hf_converter: Callable[[Dict, model_api.ReaLModelConfig], Dict]\n",
      "    embedding_param_names: Callable[[model_api.ReaLModelConfig], List[str]]\n",
      "    tblock_param_names: Callable[[model_api.ReaLModelConfig, int], List[str]]\n",
      "    head_param_names: Callable[[model_api.ReaLModelConfig], List[str]]\n",
      "    real_config_maker: Optional[Callable] = None\n",
      "\n",
      "    def config_from_hf(\n",
      "        self,\n",
      "        hf_config: Optional[transformers.PretrainedConfig] = None,\n",
      "        model_path: Optional[str] = None,\n",
      "        is_critic: bool = False,\n",
      "    ) -> model_api.ReaLModelConfig:\n",
      "        if hf_config is None:\n",
      "            hf_config = transformers.AutoConfig.from_pretrained(\n",
      "                model_path,\n",
      "                trust_remote_code=True,\n",
      "                force_download=True,\n",
      "            )\n",
      "        config = self.config_from_hf_converter(hf_config)\n",
      "        config.base_model_path = model_path\n",
      "        config.is_critic = is_critic\n",
      "        if config.is_critic:\n",
      "            config.tied_embedding = False\n",
      "        return config\n",
      "\n",
      "    def config_to_hf(\n",
      "        self, real_config: model_api.ReaLModelConfig\n",
      "    ) -> transformers.PretrainedConfig:\n",
      "        return self.config_to_hf_converter(real_config)\n",
      "\n",
      "    def load(\n",
      "        self,\n",
      "        model: ReaLModel,\n",
      "        load_dir: str,\n",
      "        init_critic_from_actor: bool = False,\n",
      "    ):\n",
      "        tik = time.perf_counter()\n",
      "        with open(os.path.join(load_dir, \"config.json\"), \"r\") as f:\n",
      "            hf_config = json.load(f)\n",
      "        if \"architectures\" in hf_config:\n",
      "            assert (\n",
      "                self.hf_cls_name == hf_config[\"architectures\"][0]\n",
      "            ), f\"{self.hf_cls_name} != {hf_config['architectures'][0]}\"\n",
      "\n",
      "        layer_indices = range(model.layer_idx_start, model.layer_idx_end)\n",
      "\n",
      "        required_hf_sd_names = []\n",
      "        for lidx in layer_indices:\n",
      "            if lidx == 0:\n",
      "                required_hf_sd_names += self.embedding_param_names(model.config)\n",
      "            elif lidx == model.config.n_layers + 1:\n",
      "                required_hf_sd_names += self.head_param_names(model.config)\n",
      "            else:\n",
      "                required_hf_sd_names += self.tblock_param_names(model.config, lidx - 1)\n",
      "\n",
      "        # Load embedding weights as well if tied_embedding is True.\n",
      "        required_hf_sd_names = set(required_hf_sd_names)\n",
      "        if (\n",
      "            model.config.tied_embedding\n",
      "            and not model.config.is_critic\n",
      "            and constants.is_last_pipe_stage()\n",
      "        ):\n",
      "            required_hf_sd_names.union(self.embedding_param_names(model.config))\n",
      "\n",
      "        if os.path.exists(os.path.join(load_dir, \"pytorch_model.bin.index.json\")):\n",
      "            with open(os.path.join(load_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n",
      "                hf_sd_mapping = json.load(f)[\"weight_map\"]\n",
      "            files_to_load = set()\n",
      "            for name in required_hf_sd_names:\n",
      "                if name in hf_sd_mapping:\n",
      "                    files_to_load.add(hf_sd_mapping[name])\n",
      "        elif os.path.exists(os.path.join(load_dir, \"model.safetensors.index.json\")):\n",
      "            with open(os.path.join(load_dir, \"model.safetensors.index.json\"), \"r\") as f:\n",
      "                hf_sd_mapping = json.load(f)[\"weight_map\"]\n",
      "            files_to_load = set()\n",
      "            for name in required_hf_sd_names:\n",
      "                if name in hf_sd_mapping:\n",
      "                    files_to_load.add(hf_sd_mapping[name])\n",
      "        elif os.path.exists(os.path.join(load_dir, \"pytorch_model.bin\")):\n",
      "            files_to_load = [\"pytorch_model.bin\"]\n",
      "        elif os.path.exists(os.path.join(load_dir, \"model.safetensors\")):\n",
      "            files_to_load = [\"model.safetensors\"]\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                f\"Could not find model file in {load_dir}. \"\n",
      "                \"Make sure you have downloaded the model correctly.\"\n",
      "            )\n",
      "        setup_time = time.perf_counter() - tik\n",
      "\n",
      "        def _load_ckpt(fn):\n",
      "            load_tik = time.perf_counter()\n",
      "            if fn.endswith(\".safetensors\"):\n",
      "                sd = load_safetensor(os.path.join(load_dir, fn))\n",
      "            else:\n",
      "                # set map_location to be CPU is a little bit faster\n",
      "                sd = torch.load(\n",
      "                    os.path.join(load_dir, fn), map_location=\"cpu\", weights_only=True\n",
      "                )\n",
      "            partition_tik = time.perf_counter()\n",
      "            sd = {k: v for k, v in sd.items() if k in required_hf_sd_names}\n",
      "            sd = self.sd_from_hf_converter(sd, model.config)\n",
      "            psd = tp_partition_real_model_state_dict(\n",
      "                sd,\n",
      "                model.config,\n",
      "                constants.tensor_parallel_world_size(),\n",
      "                constants.tensor_parallel_rank(),\n",
      "            )\n",
      "            return psd, partition_tik - load_tik, time.perf_counter() - partition_tik\n",
      "\n",
      "        load_times, partition_times = [], []\n",
      "        state_dict = {}\n",
      "        with ThreadPoolExecutor(\n",
      "            max_workers=min(4, max(1, os.cpu_count() // 8))\n",
      "        ) as executor:\n",
      "            future_to_checkpoint = {\n",
      "                executor.submit(_load_ckpt, path): path for path in files_to_load\n",
      "            }\n",
      "\n",
      "            for future in as_completed(future_to_checkpoint):\n",
      "                path = future_to_checkpoint[future]\n",
      "                try:\n",
      "                    psd, loat_t, part_t = future.result()\n",
      "                    state_dict.update(psd)\n",
      "                    load_times.append(loat_t)\n",
      "                    partition_times.append(part_t)\n",
      "                except Exception as e:\n",
      "                    raise RuntimeError(f\"Error loading checkpoint from {path}: {e}\")\n",
      "\n",
      "        # Remap embedding weights to the last layer if tied_embedding is True.\n",
      "        if (\n",
      "            model.config.tied_embedding\n",
      "            and not model.config.is_critic\n",
      "            and constants.is_last_pipe_stage()\n",
      "        ):\n",
      "            state_dict[f\"{model.config.n_layers + 1}.weight\"] = state_dict[\n",
      "                \"0.wte.weight\"\n",
      "            ]\n",
      "        if not constants.is_first_pipe_stage() and \"0.wte.weight\" in state_dict:\n",
      "            state_dict.pop(\"0.wte.weight\")\n",
      "\n",
      "        copy_tik = time.perf_counter()\n",
      "        if init_critic_from_actor and constants.is_last_pipe_stage():\n",
      "            if f\"{model.config.n_layers + 1}.weight\" in state_dict:\n",
      "                state_dict.pop(f\"{model.config.n_layers + 1}.weight\")\n",
      "            assert len(state_dict) == len(model.state_dict()) - 1, (\n",
      "                len(state_dict),\n",
      "                len(model.state_dict()),\n",
      "            )\n",
      "            model.load_state_dict(state_dict, strict=False)\n",
      "        else:\n",
      "            try:\n",
      "                model.load_state_dict(state_dict, strict=True)\n",
      "            except Exception as e:\n",
      "                logger.error(\n",
      "                    f\"Loading state dict with strict=True failed. \"\n",
      "                    f\"Have you set init_critic_from_actor=True \"\n",
      "                    f\"in the model config if you are initializing \"\n",
      "                    f\"a critic model from a regular LLM? Err: {e}\"\n",
      "                )\n",
      "                raise e\n",
      "\n",
      "        # Some logging info\n",
      "        copy_time = time.perf_counter() - copy_tik\n",
      "        load_times = \"[\" + \", \".join(f\"{t:.2f}\" for t in load_times) + \"]\"\n",
      "        partition_times = \"[\" + \", \".join(f\"{t:.2f}\" for t in partition_times) + \"]\"\n",
      "        logger.debug(\n",
      "            f\"Loading from HuggingFace Model setup time cost={setup_time:.2f}s, load time cost={load_times}, \"\n",
      "            f\"partition time cost={partition_times}, copy time cost={copy_time:.2f}s\"\n",
      "        )\n",
      "        return model\n",
      "\n",
      "    def save(\n",
      "        self,\n",
      "        model: ReaLModel,\n",
      "        tokenizer: Optional[transformers.PreTrainedTokenizer],\n",
      "        save_dir: str,\n",
      "    ):\n",
      "        tik = time.perf_counter()\n",
      "        os.makedirs(save_dir, exist_ok=True)\n",
      "\n",
      "        dp_rank = constants.data_parallel_rank()\n",
      "        pp_rank = constants.pipe_parallel_rank()\n",
      "        tp_rank = constants.tensor_parallel_rank()\n",
      "        tp_size = constants.tensor_parallel_world_size()\n",
      "        pp_size = constants.pipe_parallel_world_size()\n",
      "        dp_size = constants.data_parallel_world_size()\n",
      "\n",
      "        # We will gather parameters across the model parallel group,\n",
      "        # and save parameters to separate shards across the pipeline parallel group.\n",
      "\n",
      "        # To decrease the size of each saved file, we split the file\n",
      "        # of each pipeline stage into smaller shards.\n",
      "        approx_param_size = (\n",
      "            sum(v.numel() * v.element_size() for v in model.state_dict().values())\n",
      "            * tp_size\n",
      "        )\n",
      "\n",
      "        # By default a shard is at most 1GB. A small size enables parallel saving during training.\n",
      "        max_shard_size_byte = int(os.getenv(\"REAL_SAVE_MAX_SHARD_SIZE_BYTE\", int(1e10)))\n",
      "        n_shards_this_stage = (\n",
      "            approx_param_size + max_shard_size_byte - 1\n",
      "        ) // max_shard_size_byte\n",
      "        if approx_param_size <= 0 or n_shards_this_stage <= 0:\n",
      "            raise ValueError(\n",
      "                f\"Invalid param_size={approx_param_size}, n_shards_this_stage={n_shards_this_stage}. \"\n",
      "                \"Have you instantiated the model?\"\n",
      "            )\n",
      "\n",
      "        n_shards_this_stage = torch.tensor(\n",
      "            n_shards_this_stage, dtype=torch.int32, device=model.device\n",
      "        )\n",
      "        pp_stage_n_shards = [\n",
      "            torch.zeros_like(n_shards_this_stage) for _ in range(pp_size)\n",
      "        ]\n",
      "        dist.all_gather(\n",
      "            pp_stage_n_shards,\n",
      "            n_shards_this_stage,\n",
      "            group=constants.pipe_parallel_group(),\n",
      "        )\n",
      "        pp_stage_n_shards = [int(n.item()) for n in pp_stage_n_shards]\n",
      "        assert all(x >= 1 for x in pp_stage_n_shards)\n",
      "\n",
      "        t1 = time.perf_counter()\n",
      "\n",
      "        # Gather parameters across the model parallel group.\n",
      "        sd = model.state_dict()\n",
      "        cpu_sd = {}\n",
      "        for k, v in sd.items():\n",
      "            if (\n",
      "                model.config.tied_embedding\n",
      "                and not model.config.is_critic\n",
      "                and k == f\"{model.config.n_layers + 1}.weight\"\n",
      "            ):\n",
      "                continue\n",
      "            gather_list = [torch.zeros_like(v) for _ in range(tp_size)]\n",
      "            dist.all_gather(gather_list, v, group=constants.tensor_parallel_group())\n",
      "            gathered = tp_merge_key(k, gather_list, model.config)\n",
      "            cpu_sd[k] = gathered.cpu()\n",
      "\n",
      "        t2 = time.perf_counter()\n",
      "\n",
      "        hf_sd = self.sd_to_hf_converter(cpu_sd, model.config)\n",
      "        hf_config = self.config_to_hf_converter(model.config)\n",
      "        hf_config.architectures = [self.hf_cls_name]\n",
      "        hf_config.name_or_path = str(save_dir)\n",
      "        hf_config.torch_dtype = str(model.dtype).strip(\"torch.\")\n",
      "\n",
      "        param_size = sum(\n",
      "            [value.numel() * value.element_size() for value in hf_sd.values()]\n",
      "        )\n",
      "        param_size = torch.tensor(param_size, dtype=torch.int64, device=model.device)\n",
      "        dist.all_reduce(\n",
      "            param_size,\n",
      "            op=dist.ReduceOp.SUM,\n",
      "            group=constants.pipe_parallel_group(),\n",
      "        )\n",
      "        param_size = param_size.item()\n",
      "\n",
      "        # Save tokenizer and huggingface model config.\n",
      "        if pp_rank == 0 and dp_rank == 0 and tp_rank == 0:\n",
      "            hf_config.save_pretrained(save_dir)\n",
      "            if tokenizer is not None:\n",
      "                tokenizer.save_pretrained(save_dir)\n",
      "\n",
      "        # Dump parameters to disk.\n",
      "        if len(pp_stage_n_shards) == 1 and pp_stage_n_shards[0] == 1:\n",
      "            fn = \"pytorch_model.bin\"\n",
      "            if pp_rank == 0 and dp_rank == 0 and tp_rank == 0:\n",
      "                torch.save(hf_sd, os.path.join(save_dir, fn))\n",
      "        else:\n",
      "            output_fn = (\n",
      "                \"pytorch_model\"\n",
      "                + \"-{shard:05d}\"\n",
      "                + f\"-of-{sum(pp_stage_n_shards):05d}.bin\"\n",
      "            )\n",
      "\n",
      "            n_shards = pp_stage_n_shards[pp_rank]\n",
      "            shard_offset = sum(pp_stage_n_shards[:pp_rank])\n",
      "\n",
      "            shards = split_state_dict_into_shards(hf_sd, n_shards)\n",
      "\n",
      "            bin_index = {}\n",
      "            bin_index[\"metadata\"] = dict(total_size=param_size)\n",
      "            bin_index[\"weight_map\"] = {}\n",
      "            weight_map = {}\n",
      "\n",
      "            mesh_size = dp_size * tp_size\n",
      "            mesh_idx = dp_rank * tp_size + tp_rank\n",
      "            n_shards_per_gpu = (n_shards + mesh_size - 1) // mesh_size\n",
      "            if mesh_idx < len(range(0, n_shards, n_shards_per_gpu)):\n",
      "                s = list(range(0, n_shards, n_shards_per_gpu))[mesh_idx]\n",
      "            else:\n",
      "                s = n_shards\n",
      "\n",
      "            # Since torch.save requires pickling, which is CPU-bound,\n",
      "            # parallelizing the saving process is not beneficial.\n",
      "            for i, shard in enumerate(shards[s : s + n_shards_per_gpu]):\n",
      "                shard_idx = shard_offset + i + s\n",
      "                torch.save(\n",
      "                    shard,\n",
      "                    os.path.join(save_dir, output_fn.format(shard=shard_idx + 1)),\n",
      "                )\n",
      "\n",
      "            for i, shard in enumerate(shards):\n",
      "                shard_idx = shard_offset + i\n",
      "                for k in shard:\n",
      "                    weight_map[k] = output_fn.format(shard=shard_idx + 1)\n",
      "\n",
      "            weight_map_list = [None for _ in range(pp_size)]\n",
      "            dist.all_gather_object(\n",
      "                weight_map_list,\n",
      "                weight_map,\n",
      "                group=constants.pipe_parallel_group(),\n",
      "            )\n",
      "            for wm in weight_map_list:\n",
      "                bin_index[\"weight_map\"].update(wm)\n",
      "\n",
      "            if pp_rank == 0 and dp_rank == 0 and tp_rank == 0:\n",
      "                with open(\n",
      "                    os.path.join(save_dir, \"pytorch_model.bin.index.json\"), \"w\"\n",
      "                ) as f:\n",
      "                    json.dump(bin_index, f, indent=4)\n",
      "\n",
      "        # Copy other configs and remote codes.\n",
      "        if (\n",
      "            constants.parallelism_rank() == 0\n",
      "            and model.config.base_model_path is not None\n",
      "        ):\n",
      "            copy_hf_configs(model.config.base_model_path, save_dir)\n",
      "\n",
      "        t3 = time.perf_counter()\n",
      "\n",
      "        metadata_t = t1 - tik\n",
      "        gather_cpu_t = t2 - t1\n",
      "        dump_t = t3 - t2\n",
      "        logger.debug(\n",
      "            f\"Saving to HuggingFace Model metadata cost={metadata_t:.2f}s, \"\n",
      "            f\"gather/cpu copy cost={gather_cpu_t:.2f}s, \"\n",
      "            f\"dump cost={dump_t:.2f}s\"\n",
      "        )\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/environment/medical_coding_final_answer_env.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import asyncio\n",
      "import os\n",
      "import re\n",
      "from typing import List, Tuple\n",
      "\n",
      "from functioncall.code.local_verify import code_verify as local_code_verify\n",
      "from functioncall.code.verify import code_verify\n",
      "from functioncall.math.verify import math_verify\n",
      "from realhf.api.core.env_api import EnvironmentService, register_environment\n",
      "from realhf.base import logging\n",
      "from realhf.impl.dataset.math_code_dataset import load_metadata\n",
      "from realhf.impl.dataset.math_parser import parse_lines_in_parallel\n",
      "\n",
      "ENABLE_FUNCTION_CALL = True if os.getenv(\"FUNCTIONCALL_SERVICE_DOMAIN\", \"\") else False\n",
      "math_verify_call = math_verify if ENABLE_FUNCTION_CALL else parse_lines_in_parallel\n",
      "code_verify_call = code_verify if ENABLE_FUNCTION_CALL else local_code_verify\n",
      "\n",
      "logger = logging.getLogger(\"Math Single Step Environment\")\n",
      "\n",
      "\n",
      "# verifier_icd.py\n",
      "import re\n",
      "from typing import List, Dict, Tuple, Set\n",
      "\n",
      "boxed_pattern = re.compile(r\"\\[(.*?)\\]\")\n",
      "code_pattern  = re.compile(r\"[A-Z][0-9]{2}(?:\\.[0-9A-Za-z]+)?\")\n",
      "\n",
      "def _extract_boxed_codes(text: str) -> Tuple[str, List[str]]:\n",
      "    \"\"\"\n",
      "    Original helper.  On any regex failure returns (\"\", []) so the\n",
      "    downstream logic still works and produces a score of 0.0\n",
      "    \"\"\"\n",
      "    m = boxed_pattern.search(text or \"\")\n",
      "    if not m:\n",
      "        return \"\", []\n",
      "    inside = m.group(1)\n",
      "    codes  = code_pattern.findall(inside)\n",
      "    uniq, seen = [], set()\n",
      "    for c in codes:\n",
      "        if c not in seen:\n",
      "            uniq.append(c)\n",
      "            seen.add(c)\n",
      "    return inside, uniq\n",
      "\n",
      "\n",
      "def _f1(pred: Set[str], gold: Set[str]) -> float:\n",
      "    if not pred and not gold:\n",
      "        return 1.0          # vacuously correct\n",
      "    if not pred or not gold:\n",
      "        return 0.0\n",
      "    tp = len(pred & gold)\n",
      "    precision = tp / len(pred)\n",
      "    recall    = tp / len(gold)\n",
      "    if precision + recall == 0:\n",
      "        return 0.0\n",
      "    return 2 * precision * recall / (precision + recall)\n",
      "\n",
      "\n",
      "def icd_verify(\n",
      "    id2info: Dict[str, Dict],\n",
      "    answers: List[str],\n",
      "    qids: List[str],\n",
      ") -> List[float]:\n",
      "    \"\"\"\n",
      "    Safe verifier.\n",
      "     Returns a float in [0, 1] (the F-score) for each answer.\n",
      "     On *any* exception, or if no codes are boxed, returns 0.0.\n",
      "     Accesses id2info the same way as parse_lines_in_parallel:\n",
      "        qid_base = qid.split(\"@idx:\")[0]\n",
      "    \"\"\"\n",
      "    scores: List[float] = []\n",
      "    for ans, qid in zip(answers, qids):\n",
      "        try:\n",
      "            qid_base   = qid.split(\"@idx:\")[0]\n",
      "            gold_codes = set(id2info[qid_base][\"solutions\"])\n",
      "            _, pred    = _extract_boxed_codes(ans or \"\")\n",
      "            f1         = _f1(set(pred), gold_codes)\n",
      "            scores.append(float(f1))\n",
      "        except Exception:\n",
      "            scores.append(0.0)          # fallback guarantees float() works\n",
      "    return scores\n",
      "\n",
      "\n",
      "\n",
      "def extract_code(text, min_length=20):\n",
      "    code_pattern = r\"(?i)```(?:python|py|cpp|CPP)?\\s*\\n?(.*?)\\n?```\"\n",
      "    code_blocks = re.findall(code_pattern, text, re.DOTALL)\n",
      "    valid_blocks = []\n",
      "    for block in code_blocks:\n",
      "        clean_block = block.strip()\n",
      "        if len(clean_block) < min_length:\n",
      "            continue\n",
      "\n",
      "        valid_blocks.append(clean_block)\n",
      "\n",
      "    if not valid_blocks:\n",
      "        # logger.warning(f\"failed to extract python code from {text}\")\n",
      "        return None\n",
      "    # return the last code block\n",
      "    return valid_blocks[-1]\n",
      "\n",
      "\n",
      "class MathCodeSingleStepEnv(EnvironmentService):\n",
      "    def __init__(self, dataset_path: str):\n",
      "        self.id2info, _ = load_metadata(dataset_path)\n",
      "\n",
      "    async def reset(self, seed=None, options=None):\n",
      "        return None, {}\n",
      "\n",
      "    async def step(self, action: Tuple[str, List[str]]):\n",
      "        qid, answers = action\n",
      "        group_size = len(answers)\n",
      "        qid = qid.split(\"@\")[0]\n",
      "        cur_task = self.id2info[qid][\"task\"]\n",
      "\n",
      "        if cur_task == \"math\":\n",
      "            format_rewards = await asyncio.to_thread(\n",
      "                icd_verify, self.id2info, answers, [qid for _ in range(group_size)]\n",
      "            )\n",
      "            logger.debug(f\"MATH task (qid: {qid}): Answers {answers} with these icd_verify results: {format_rewards}\")\n",
      "            # format_rewards = await asyncio.to_thread(\n",
      "            #     math_verify_call,\n",
      "            #     self.id2info,\n",
      "            #     answers,\n",
      "            #     [qid for _ in range(group_size)],\n",
      "            # )\n",
      "        elif cur_task == \"code\":\n",
      "            answers = [extract_code(x) for x in answers]\n",
      "            format_rewards = await asyncio.to_thread(\n",
      "                code_verify_call,\n",
      "                self.id2info,\n",
      "                answers,\n",
      "                [qid for _ in range(group_size)],\n",
      "            )\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "\n",
      "        return None, format_rewards, True, False, {}\n",
      "\n",
      "\n",
      "register_environment(\"medical-coding-final-answer-env\", MathCodeSingleStepEnv)\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/environment/math_code_single_step_env.py ====\n",
      "\n",
      "# Copyright 2025 Ant Group Inc.\n",
      "\n",
      "import asyncio\n",
      "import os\n",
      "import re\n",
      "from typing import List, Tuple\n",
      "\n",
      "from functioncall.code.local_verify import code_verify as local_code_verify\n",
      "from functioncall.code.verify import code_verify\n",
      "from functioncall.math.verify import math_verify\n",
      "from realhf.api.core.env_api import EnvironmentService, register_environment\n",
      "from realhf.base import logging\n",
      "from realhf.impl.dataset.math_code_dataset import load_metadata\n",
      "from realhf.impl.dataset.math_parser import parse_lines_in_parallel\n",
      "\n",
      "ENABLE_FUNCTION_CALL = True if os.getenv(\"FUNCTIONCALL_SERVICE_DOMAIN\", \"\") else False\n",
      "math_verify_call = math_verify if ENABLE_FUNCTION_CALL else parse_lines_in_parallel\n",
      "code_verify_call = code_verify if ENABLE_FUNCTION_CALL else local_code_verify\n",
      "\n",
      "logger = logging.getLogger(\"Math Single Step Environment\")\n",
      "\n",
      "\n",
      "def extract_code(text, min_length=20):\n",
      "    code_pattern = r\"(?i)```(?:python|py|cpp|CPP)?\\s*\\n?(.*?)\\n?```\"\n",
      "    code_blocks = re.findall(code_pattern, text, re.DOTALL)\n",
      "    valid_blocks = []\n",
      "    for block in code_blocks:\n",
      "        clean_block = block.strip()\n",
      "        if len(clean_block) < min_length:\n",
      "            continue\n",
      "\n",
      "        valid_blocks.append(clean_block)\n",
      "\n",
      "    if not valid_blocks:\n",
      "        # logger.warning(f\"failed to extract python code from {text}\")\n",
      "        return None\n",
      "    # return the last code block\n",
      "    return valid_blocks[-1]\n",
      "\n",
      "\n",
      "class MathCodeSingleStepEnv(EnvironmentService):\n",
      "    def __init__(self, dataset_path: str):\n",
      "        self.id2info, _ = load_metadata(dataset_path)\n",
      "\n",
      "    async def reset(self, seed=None, options=None):\n",
      "        return None, {}\n",
      "\n",
      "    async def step(self, action: Tuple[str, List[str]]):\n",
      "        qid, answers = action\n",
      "        group_size = len(answers)\n",
      "        qid = qid.split(\"@\")[0]\n",
      "        cur_task = self.id2info[qid][\"task\"]\n",
      "\n",
      "        if cur_task == \"math\":\n",
      "            format_rewards = await asyncio.to_thread(\n",
      "                math_verify_call,\n",
      "                self.id2info,\n",
      "                answers,\n",
      "                [qid for _ in range(group_size)],\n",
      "            )\n",
      "        elif cur_task == \"code\":\n",
      "            answers = [extract_code(x) for x in answers]\n",
      "            format_rewards = await asyncio.to_thread(\n",
      "                code_verify_call,\n",
      "                self.id2info,\n",
      "                answers,\n",
      "                [qid for _ in range(group_size)],\n",
      "            )\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "\n",
      "        return None, format_rewards, True, False, {}\n",
      "\n",
      "\n",
      "register_environment(\"math-code-single-step\", MathCodeSingleStepEnv)\n",
      "\n",
      "\n",
      "==== /home/roeseler/github/AReaL/realhf/impl/environment/__init__.py ====\n",
      "\n",
      "import realhf.impl.environment.math_code_single_step_env\n",
      "import realhf.impl.environment.medical_coding_final_answer_env\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "combined = gather_py_files_with_content(\"realhf\")\n",
    "print(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b1f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
